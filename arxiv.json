[
    {
        "title": "Exploring the Carbon Footprint of Hugging Face's ML Models: A Repository\n  Mining Study",
        "url": "http://arxiv.org/abs/2305.11164v1",
        "pub_date": "2023-05-18",
        "summary": "The rise of machine learning (ML) systems has exacerbated their carbon\nfootprint due to increased capabilities and model sizes. However, there is\nscarce knowledge on how the carbon footprint of ML models is actually measured,\nreported, and evaluated. In light of this, the paper aims to analyze the\nmeasurement of the carbon footprint of 1,417 ML models and associated datasets\non Hugging Face, which is the most popular repository for pretrained ML models.\nThe goal is to provide insights and recommendations on how to report and\noptimize the carbon efficiency of ML models. The study includes the first\nrepository mining study on the Hugging Face Hub API on carbon emissions. This\nstudy seeks to answer two research questions: (1) how do ML model creators\nmeasure and report carbon emissions on Hugging Face Hub?, and (2) what aspects\nimpact the carbon emissions of training ML models? The study yielded several\nkey findings. These include a decreasing proportion of carbon\nemissions-reporting models, a slight decrease in reported carbon footprint on\nHugging Face over the past 2 years, and a continued dominance of NLP as the\nmain application domain. Furthermore, the study uncovers correlations between\ncarbon emissions and various attributes such as model size, dataset size, and\nML application domains. These results highlight the need for software\nmeasurements to improve energy reporting practices and promote carbon-efficient\nmodel development within the Hugging Face community. In response to this issue,\ntwo classifications are proposed: one for categorizing models based on their\ncarbon emission reporting practices and another for their carbon efficiency.\nThe aim of these classification proposals is to foster transparency and\nsustainable model development within the ML community.",
        "translated": "机器学习(ML)系统的兴起加剧了它们的碳足印，原因是功能和模型尺寸的增加。然而，对于机器学习模型的碳足印实际上是如何测量、报告和评估的，我们知之甚少。有鉴于此，本文旨在分析“拥抱脸”上对1417个机器学习模型及相关数据集的碳足印测量结果。“拥抱脸”是最受欢迎的预训机器学习模型库。目标是就如何报告和优化机器学习模型的碳效率提供见解和建议。这项研究包括第一个关于碳排放的拥抱面中心 API 的知识库挖掘研究。这项研究试图回答两个研究问题: (1)机器学习模型的创建者如何测量和报告拥抱面部中心的碳排放量？以及(2)哪些方面影响训练机器学习模型的碳排放量？这项研究产生了几个关键的发现。其中包括碳排放报告模型的比例下降，过去两年“拥抱脸”上的报告碳足印略有下降，以及自然语言处理作为主要应用领域的持续主导地位。此外，该研究还揭示了碳排放与模型大小、数据集大小和机器学习应用领域等各种属性之间的相关性。这些结果突出了软件测量的必要性，以改善能源报告做法，并促进在拥抱面社区的碳效率模型开发。针对这一问题，提出了两种分类: 一种是根据其碳排放报告做法对模型进行分类，另一种是根据其碳效率进行分类。这些分类建议的目的是在 ML 社区内促进透明度和可持续的模型开发。"
    },
    {
        "title": "TOME: A Two-stage Approach for Model-based Retrieval",
        "url": "http://arxiv.org/abs/2305.11161v1",
        "pub_date": "2023-05-18",
        "summary": "Recently, model-based retrieval has emerged as a new paradigm in text\nretrieval that discards the index in the traditional retrieval model and\ninstead memorizes the candidate corpora using model parameters. This design\nemploys a sequence-to-sequence paradigm to generate document identifiers, which\nenables the complete capture of the relevance between queries and documents and\nsimplifies the classic indexretrieval-rerank pipeline. Despite its attractive\nqualities, there remain several major challenges in model-based retrieval,\nincluding the discrepancy between pre-training and fine-tuning, and the\ndiscrepancy between training and inference. To deal with the above challenges,\nwe propose a novel two-stage model-based retrieval approach called TOME, which\nmakes two major technical contributions, including the utilization of tokenized\nURLs as identifiers and the design of a two-stage generation architecture. We\nalso propose a number of training strategies to deal with the training\ndifficulty as the corpus size increases. Extensive experiments and analysis on\nMS MARCO and Natural Questions demonstrate the effectiveness of our proposed\napproach, and we investigate the scaling laws of TOME by examining various\ninfluencing factors.",
        "translated": "近年来，基于模型的检索已经成为文本检索的一种新范式，它抛弃了传统检索模型中的索引，而是利用模型参数记忆候选语料库。该设计采用序列到序列的方法生成文档标识符，能够完全捕获查询和文档之间的相关性，简化了经典的索引检索-重排序流水线。尽管基于模型的检索具有吸引人的优点，但仍然存在一些主要的挑战，包括预训练和微调之间的差异，以及训练和推理之间的差异。为了应对上述挑战，我们提出了一种新的基于两阶段模型的检索方法，称为 TOME，它做出了两个主要的技术贡献，包括使用标记化 URL 作为标识符和设计一个两阶段生成体系结构。随着语料库规模的增大，我们提出了一些训练策略来解决训练难度。大量的实验和分析 MS MARCO 和自然问题证明了我们提出的方法的有效性，我们研究了 TOME 的缩放规律通过检查各种影响因素。"
    },
    {
        "title": "Preference or Intent? Double Disentangled Collaborative Filtering",
        "url": "http://arxiv.org/abs/2305.11084v1",
        "pub_date": "2023-05-18",
        "summary": "People usually have different intents for choosing items, while their\npreferences under the same intent may also different. In traditional\ncollaborative filtering approaches, both intent and preference factors are\nusually entangled in the modeling process, which significantly limits the\nrobustness and interpretability of recommendation performances. For example,\nthe low-rating items are always treated as negative feedback while they\nactually could provide positive information about user intent. To this end, in\nthis paper, we propose a two-fold representation learning approach, namely\nDouble Disentangled Collaborative Filtering (DDCF), for personalized\nrecommendations. The first-level disentanglement is for separating the\ninfluence factors of intent and preference, while the second-level\ndisentanglement is performed to build independent sparse preference\nrepresentations under individual intent with limited computational complexity.\nSpecifically, we employ two variational autoencoder networks, intent\nrecognition network and preference decomposition network, to learn the intent\nand preference factors, respectively. In this way, the low-rating items will be\ntreated as positive samples for modeling intents while the negative samples for\nmodeling preferences. Finally, extensive experiments on three real-world\ndatasets and four evaluation metrics clearly validate the effectiveness and the\ninterpretability of DDCF.",
        "translated": "人们通常有不同的意图选择项目，而他们的偏好下，相同的意图也可能有所不同。在传统的协同过滤建模方法中，意图和偏好因素通常会在建模过程中纠缠在一起，这极大地限制了推荐性能的稳健性和可解释性。例如，低等级的项目总是被视为负面反馈，而实际上它们可以提供关于用户意图的正面信息。为此，在本文中，我们提出了一种双重表征学习方法，即双重分离协同过滤(DDCF) ，用于个性化推荐。第一级解缠是为了分离意图和偏好的影响因素，而第二级解缠是为了在计算复杂度有限的个体意图下构建独立的稀疏偏好表示。具体来说，我们使用两个变分自动编码器网络，意图识别网络和偏好分解网络，分别学习意图和偏好因素。这样，低等级的项目将被视为建模意图的正面样本，而负面样本将被视为建模偏好。最后，在三个实际数据集和四个评价指标上进行了广泛的实验，验证了 DDCF 的有效性和可解释性。"
    },
    {
        "title": "Contrastive State Augmentations for Reinforcement Learning-Based\n  Recommender Systems",
        "url": "http://arxiv.org/abs/2305.11081v1",
        "pub_date": "2023-05-18",
        "summary": "Learning reinforcement learning (RL)-based recommenders from historical\nuser-item interaction sequences is vital to generate high-reward\nrecommendations and improve long-term cumulative benefits. However, existing RL\nrecommendation methods encounter difficulties (i) to estimate the value\nfunctions for states which are not contained in the offline training data, and\n(ii) to learn effective state representations from user implicit feedback due\nto the lack of contrastive signals. In this work, we propose contrastive state\naugmentations (CSA) for the training of RL-based recommender systems. To tackle\nthe first issue, we propose four state augmentation strategies to enlarge the\nstate space of the offline data. The proposed method improves the\ngeneralization capability of the recommender by making the RL agent visit the\nlocal state regions and ensuring the learned value functions are similar\nbetween the original and augmented states. For the second issue, we propose\nintroducing contrastive signals between augmented states and the state randomly\nsampled from other sessions to improve the state representation learning\nfurther. To verify the effectiveness of the proposed CSA, we conduct extensive\nexperiments on two publicly accessible datasets and one dataset collected from\na real-life e-commerce platform. We also conduct experiments on a simulated\nenvironment as the online evaluation setting. Experimental results demonstrate\nthat CSA can effectively improve recommendation performance.",
        "translated": "从历史用户项目交互序列中学习基于强化学习的推荐对于产生高回报的推荐和提高长期累积效益至关重要。然而，现有的 RL 推荐方法遇到了困难(i)估计不包含在离线训练数据中的状态的值函数，以及(ii)由于缺乏对比信号而从用户隐式反馈中学习有效的状态表示。在这项工作中，我们提出了对比状态增强(CSA)的训练基于 RL 的推荐系统。针对第一个问题，我们提出了四种状态增强策略来扩大离线数据的状态空间。该方法通过使 RL 代理访问局部状态区域，保证学习值函数在原状态和增广状态之间相似，提高了推荐器的泛化能力。对于第二个问题，我们提出在增广状态和从其他会话中随机采样的状态之间引入对比信号，以进一步改善状态表示学习。为了验证所提出的 CSA 的有效性，我们对从现实生活中的电子商务平台收集的两个可公开访问的数据集和一个数据集进行了广泛的实验。我们还进行了模拟环境的实验，作为在线评价设置。实验结果表明，CSA 能有效提高推荐性能。"
    },
    {
        "title": "BERM: Training the Balanced and Extractable Representation for Matching\n  to Improve Generalization Ability of Dense Retrieval",
        "url": "http://arxiv.org/abs/2305.11052v1",
        "pub_date": "2023-05-18",
        "summary": "Dense retrieval has shown promise in the first-stage retrieval process when\ntrained on in-domain labeled datasets. However, previous studies have found\nthat dense retrieval is hard to generalize to unseen domains due to its weak\nmodeling of domain-invariant and interpretable feature (i.e., matching signal\nbetween two texts, which is the essence of information retrieval). In this\npaper, we propose a novel method to improve the generalization of dense\nretrieval via capturing matching signal called BERM. Fully fine-grained\nexpression and query-oriented saliency are two properties of the matching\nsignal. Thus, in BERM, a single passage is segmented into multiple units and\ntwo unit-level requirements are proposed for representation as the constraint\nin training to obtain the effective matching signal. One is semantic unit\nbalance and the other is essential matching unit extractability. Unit-level\nview and balanced semantics make representation express the text in a\nfine-grained manner. Essential matching unit extractability makes passage\nrepresentation sensitive to the given query to extract the pure matching\ninformation from the passage containing complex context. Experiments on BEIR\nshow that our method can be effectively combined with different dense retrieval\ntraining methods (vanilla, hard negatives mining and knowledge distillation) to\nimprove its generalization ability without any additional inference overhead\nand target domain data.",
        "translated": "密集检索在域内标记数据集训练的第一阶段检索过程中显示出希望。然而，先前的研究发现，由于密集检索对领域不变性和可解释特征(即两个文本之间的匹配信号，这是信息检索的本质)的建模较弱，因此很难将其推广到不可见的领域。在本文中，我们提出了一种新的方法来提高通过捕获匹配信号密集检索的泛化称为 BERM。完全细粒度表达式和面向查询的显著性是匹配信号的两个属性。因此，在误码率模型中，将一个通道分割成多个单元，并提出了两个单元级的要求作为训练中获得有效匹配信号的约束条件。一个是语义单元平衡，另一个是必要的匹配单元可提取性。单元级视图和平衡语义使表示以细粒度的方式表示文本。基本匹配单元可提取性使得文本表示对给定的查询敏感，从包含复杂上下文的文本中提取纯匹配信息。在 BEIR 上的实验表明，该方法可以有效地结合不同的密集检索训练方法(普通方法、硬负数挖掘和知识提取) ，在不增加任何推理开销和目标域数据的情况下提高其泛化能力。"
    },
    {
        "title": "Improving Recommendation System Serendipity Through Lexicase Selection",
        "url": "http://arxiv.org/abs/2305.11044v1",
        "pub_date": "2023-05-18",
        "summary": "Recommender systems influence almost every aspect of our digital lives.\nUnfortunately, in striving to give us what we want, they end up restricting our\nopen-mindedness. Current recommender systems promote echo chambers, where\npeople only see the information they want to see, and homophily, where users of\nsimilar background see similar content. We propose a new serendipity metric to\nmeasure the presence of echo chambers and homophily in recommendation systems\nusing cluster analysis. We then attempt to improve the diversity-preservation\nqualities of well known recommendation techniques by adopting a parent\nselection algorithm from the evolutionary computation literature known as\nlexicase selection. Our results show that lexicase selection, or a mixture of\nlexicase selection and ranking, outperforms its purely ranked counterparts in\nterms of personalization, coverage and our specifically designed serendipity\nbenchmark, while only slightly under-performing in terms of accuracy (hit\nrate). We verify these results across a variety of recommendation list sizes.\nIn this work we show that lexicase selection is able to maintain multiple\ndiverse clusters of item recommendations that are each relevant for the\nspecific user, while still maintaining a high hit-rate accuracy, a trade off\nthat is not achieved by other methods.",
        "translated": "推荐系统几乎影响了我们数字生活的方方面面。不幸的是，在努力给予我们想要的东西的过程中，他们最终限制了我们思想的开放性。目前的推荐系统推广回声室，人们只看到他们想看到的信息，同质性，相似背景的用户看到相似的内容。我们提出了一种新的意外发现度量方法，用来衡量使用数据聚类的推荐系统中是否存在回声室和同质性。然后，我们试图通过采用来自进化计算文献的父选择算法(称为 lexicase 选择)来改进众所周知的推荐技术的多样性保持质量。我们的研究结果表明，词汇表选择，或词汇表选择和排名的混合，在个性化，覆盖率和我们专门设计的意外发现基准方面表现优于纯粹的排名对应方，而在准确性(命中率)方面表现稍差。我们通过各种推荐列表大小来验证这些结果。在这项工作中，我们表明，词汇表选择能够维护多个不同的项目推荐集群，每个相关的特定用户，同时仍然保持高命中率的准确性，这是一个权衡，没有实现的其他方法。"
    },
    {
        "title": "Query Performance Prediction: From Ad-hoc to Conversational Search",
        "url": "http://arxiv.org/abs/2305.10923v1",
        "pub_date": "2023-05-18",
        "summary": "Query performance prediction (QPP) is a core task in information retrieval.\nThe QPP task is to predict the retrieval quality of a search system for a query\nwithout relevance judgments. Research has shown the effectiveness and\nusefulness of QPP for ad-hoc search. Recent years have witnessed considerable\nprogress in conversational search (CS). Effective QPP could help a CS system to\ndecide an appropriate action to be taken at the next turn. Despite its\npotential, QPP for CS has been little studied. We address this research gap by\nreproducing and studying the effectiveness of existing QPP methods in the\ncontext of CS. While the task of passage retrieval remains the same in the two\nsettings, a user query in CS depends on the conversational history, introducing\nnovel QPP challenges. In particular, we seek to explore to what extent findings\nfrom QPP methods for ad-hoc search generalize to three CS settings: (i)\nestimating the retrieval quality of different query rewriting-based retrieval\nmethods, (ii) estimating the retrieval quality of a conversational dense\nretrieval method, and (iii) estimating the retrieval quality for top ranks vs.\ndeeper-ranked lists. Our findings can be summarized as follows: (i) supervised\nQPP methods distinctly outperform unsupervised counterparts only when a\nlarge-scale training set is available; (ii) point-wise supervised QPP methods\noutperform their list-wise counterparts in most cases; and (iii) retrieval\nscore-based unsupervised QPP methods show high effectiveness in assessing the\nconversational dense retrieval method, ConvDR.",
        "translated": "查询性能预测是信息检索的核心任务。QPP 任务是在没有相关性判断的情况下预测查询检索系统的检索质量。研究表明 QPP 在自组织搜索中的有效性和实用性。近年来，会话搜索取得了长足的进步。有效的质量保证计划可以帮助 CS 系统决定下一轮要采取的适当行动。尽管 QPP 具有很大的潜力，但是对它的研究还很少。我们通过再现和研究现有的 QPP 方法在 CS 背景下的有效性来弥补这一研究差距。虽然在这两种情况下，文章检索的任务是相同的，但用户在 CS 中的查询依赖于会话历史，引入了新的 QPP 挑战。特别是，我们试图探索用于特别搜索的 QPP 方法的结果在多大程度上概括为三种 CS 设置: (i)估计不同基于查询重写的检索方法的检索质量，(ii)估计会话密集检索方法的检索质量，以及(iii)估计顶级与更深级列表的检索质量。我们的研究结果可以总结如下: (i)监督 QPP 方法只有在大规模训练集可用时才明显优于无监督的对应方法; (ii)点式监督 QPP 方法在大多数情况下优于其列表式对应方法; 和(iii)基于检索评分的无监督 QPP 方法在评估会话密集检索方法，ConvDR 方面显示出高效性。"
    },
    {
        "title": "Adaptive Graph Contrastive Learning for Recommendation",
        "url": "http://arxiv.org/abs/2305.10837v1",
        "pub_date": "2023-05-18",
        "summary": "Recently, graph neural networks (GNNs) have been successfully applied to\nrecommender systems as an effective collaborative filtering (CF) approach. The\nkey idea of GNN-based recommender system is to recursively perform the message\npassing along the user-item interaction edge for refining the encoded\nembeddings, relying on sufficient and high-quality training data. Since user\nbehavior data in practical recommendation scenarios is often noisy and exhibits\nskewed distribution, some recommendation approaches, e.g., SGL and SimGCL,\nleverage self-supervised learning to improve user representations against the\nabove issues. Despite their effectiveness, however, they conduct\nself-supervised learning through creating contrastvie views, depending on the\nexploration of data augmentations with the problem of tedious trial-and-error\nselection of augmentation methods. In this paper, we propose a novel Adaptive\nGraph Contrastive Learning (AdaptiveGCL) framework which conducts graph\ncontrastive learning with two adaptive contrastive view generators to better\nempower CF paradigm. Specifically, we use two trainable view generators, which\nare a graph generative model and a graph denoising model respectively, to\ncreate contrastive views. Two generators are able to create adaptive\ncontrastive views, addressing the problem of model collapse and achieving\nadaptive contrastive learning. With two adaptive contrasive views, more\nadditionally high-quality training signals will be introduced into the CF\nparadigm and help to alleviate the data sparsity and noise issues. Extensive\nexperiments on three benchmark datasets demonstrate the superiority of our\nmodel over various state-of-the-art recommendation methods. Further visual\nanalysis intuitively explains why our AdaptiveGCL outperforms existing\ncontrastive learning approaches based on selected data augmentation methods.",
        "translated": "最近，图形神经网络(GNN)已成功应用于推荐系统，作为一种有效的协同过滤(CF)方法。基于 GNN 的推荐系统的关键思想是依靠充分和高质量的训练数据，递归地执行沿用户项目交互边缘传递的消息，以完善编码的嵌入。由于实际推荐场景中的用户行为数据通常是有噪音的，并且呈现出倾斜的分布，因此一些推荐方法，如 SGL 和 SimGCL，利用自监督学习来改善用户对上述问题的表示。然而，尽管他们的有效性，他们进行自我监督学习通过创建对比观点，依赖于探索数据增强与繁琐的试错选择增强方法的问题。本文提出了一种新的自适应图形对比学习(AdaptiveGCL)框架，该框架使用两个自适应对比视图生成器进行图形对比学习，以更好地支持 CF 范式。具体来说，我们使用两个可训练的视图生成器，分别是一个图形生成模型和一个图形去噪模型，来创建对比视图。两个生成器能够创建自适应对比视图，解决模型崩溃问题，实现自适应对比学习。通过两个自适应对立视图，在 CF 范式中引入更多高质量的训练信号，有助于缓解数据稀疏和噪声问题。在三个基准数据集上的大量实验证明了我们的模型优于各种最先进的推荐方法。进一步的可视化分析直观地解释了为什么我们的 AdaptiveGCL 优于基于所选数据增强方法的现有对比学习方法。"
    },
    {
        "title": "Integrating Item Relevance in Training Loss for Sequential Recommender\n  Systems",
        "url": "http://arxiv.org/abs/2305.10824v1",
        "pub_date": "2023-05-18",
        "summary": "Sequential Recommender Systems (SRSs) are a popular type of recommender\nsystem that learns from a user's history to predict the next item they are\nlikely to interact with. However, user interactions can be affected by noise\nstemming from account sharing, inconsistent preferences, or accidental clicks.\nTo address this issue, we (i) propose a new evaluation protocol that takes\nmultiple future items into account and (ii) introduce a novel relevance-aware\nloss function to train a SRS with multiple future items to make it more robust\nto noise. Our relevance-aware models obtain an improvement of ~1.2% of NDCG@10\nand 0.88% in the traditional evaluation protocol, while in the new evaluation\nprotocol, the improvement is ~1.63% of NDCG@10 and ~1.5% of HR w.r.t the best\nperforming models.",
        "translated": "顺序推荐系统(SRSs)是一种流行的推荐系统，它可以从用户的历史中学习，预测他们可能接触到的下一个项目。然而，用户交互可能受到来自帐户共享、不一致的首选项或偶然点击的噪音的影响。为了解决这个问题，我们(i)提出了一个新的评估协议，考虑到多个未来项目，并且(ii)引入一个新的相关性感知损失函数来训练具有多个未来项目的 SRS，以使其对噪声更加鲁棒。我们的相关意识模型在传统的评估方案中获得了? 1.2% 的 NDCG@10和0.88% 的改善，而在新的评估方案中，改善是? 1.63% 的 NDCG@10和? 1.5% 的最佳表现模型。"
    },
    {
        "title": "When Search Meets Recommendation: Learning Disentangled Search\n  Representation for Recommendation",
        "url": "http://arxiv.org/abs/2305.10822v1",
        "pub_date": "2023-05-18",
        "summary": "Modern online service providers such as online shopping platforms often\nprovide both search and recommendation (S&amp;R) services to meet different user\nneeds. Rarely has there been any effective means of incorporating user behavior\ndata from both S&amp;R services. Most existing approaches either simply treat S&amp;R\nbehaviors separately, or jointly optimize them by aggregating data from both\nservices, ignoring the fact that user intents in S&amp;R can be distinctively\ndifferent. In our paper, we propose a Search-Enhanced framework for the\nSequential Recommendation (SESRec) that leverages users' search interests for\nrecommendation, by disentangling similar and dissimilar representations within\nS&amp;R behaviors. Specifically, SESRec first aligns query and item embeddings\nbased on users' query-item interactions for the computations of their\nsimilarities. Two transformer encoders are used to learn the contextual\nrepresentations of S&amp;R behaviors independently. Then a contrastive learning\ntask is designed to supervise the disentanglement of similar and dissimilar\nrepresentations from behavior sequences of S&amp;R. Finally, we extract user\ninterests by the attention mechanism from three perspectives, i.e., the\ncontextual representations, the two separated behaviors containing similar and\ndissimilar interests. Extensive experiments on both industrial and public\ndatasets demonstrate that SESRec consistently outperforms state-of-the-art\nmodels. Empirical studies further validate that SESRec successfully disentangle\nsimilar and dissimilar user interests from their S&amp;R behaviors.",
        "translated": "现代在线服务提供商，如在线购物平台，经常同时提供搜索和推荐(S & R)服务，以满足不同的用户需求。很少有任何有效的方法来整合来自 S & R 服务的用户行为数据。大多数现有的方法要么单独处理 S & R 行为，要么通过聚合来自两个服务的数据来共同优化它们，忽略了 S & R 中的用户意图可能截然不同的事实。在我们的论文中，我们提出了一个搜索增强的序列推荐框架(SESRec) ，它利用用户的搜索兴趣进行推荐，通过在 S & R 行为中分离相似和不相似的表示。具体来说，SESRec 首先根据用户的查询-项交互对查询和项嵌入进行对齐，以计算它们的相似性。使用两个变压器编码器分别学习 S & R 行为的上下文表示。然后设计了一个对比学习任务来监督 S & R 行为序列中相似和不相似表征的分离。最后，通过注意机制从三个方面提取用户的兴趣，即语境表征，两个分离的具有相似兴趣和不同兴趣的行为。在工业和公共数据集上的大量实验表明，SESRec 始终优于最先进的模型。实证研究进一步验证了 SESRec 成功地将相似和不同的用户兴趣从他们的 S & R 行为中分离出来。"
    },
    {
        "title": "How Does Generative Retrieval Scale to Millions of Passages?",
        "url": "http://arxiv.org/abs/2305.11841v1",
        "pub_date": "2023-05-19",
        "summary": "Popularized by the Differentiable Search Index, the emerging paradigm of\ngenerative retrieval re-frames the classic information retrieval problem into a\nsequence-to-sequence modeling task, forgoing external indices and encoding an\nentire document corpus within a single Transformer. Although many different\napproaches have been proposed to improve the effectiveness of generative\nretrieval, they have only been evaluated on document corpora on the order of\n100k in size. We conduct the first empirical study of generative retrieval\ntechniques across various corpus scales, ultimately scaling up to the entire MS\nMARCO passage ranking task with a corpus of 8.8M passages and evaluating model\nsizes up to 11B parameters. We uncover several findings about scaling\ngenerative retrieval to millions of passages; notably, the central importance\nof using synthetic queries as document representations during indexing, the\nineffectiveness of existing proposed architecture modifications when accounting\nfor compute cost, and the limits of naively scaling model parameters with\nrespect to retrieval performance. While we find that generative retrieval is\ncompetitive with state-of-the-art dual encoders on small corpora, scaling to\nmillions of passages remains an important and unsolved challenge. We believe\nthese findings will be valuable for the community to clarify the current state\nof generative retrieval, highlight the unique challenges, and inspire new\nresearch directions.",
        "translated": "由于可分辨搜索索引的普及，新兴的生成检索范式将经典的信息检索问题重新定义为一个序列到序列的建模任务，放弃外部索引，并在一个 former 中编码整个文档语料库。为了提高生成检索的有效性，人们提出了许多不同的方法，但这些方法在文献语料库中的检索效果只有10万次左右。我们进行了第一次实证研究的生成检索技术在不同的语料库尺度，最终扩大到整个 MS MARCO 段落排序任务与8.8 M 段落的语料库和评估模型大小高达11B 参数。我们发现了关于将生成性检索扩展到数百万段的一些发现; 值得注意的是，在索引过程中使用合成查询作为文档表示的核心重要性，在计算计算成本时现有提议的架构修改的无效性，以及天真地扩展模型参数对检索性能的限制。虽然我们发现生成检索与小型语料库上最先进的双编码器相比具有竞争力，但是扩展到数百万段仍然是一个重要的未解决的挑战。我们相信这些研究结果将有助于社区阐明生成性检索的现状，突出独特的挑战，并启发新的研究方向。"
    },
    {
        "title": "Visualization for Recommendation Explainability: A Survey and New\n  Perspectives",
        "url": "http://arxiv.org/abs/2305.11755v1",
        "pub_date": "2023-05-19",
        "summary": "Providing system-generated explanations for recommendations represents an\nimportant step towards transparent and trustworthy recommender systems.\nExplainable recommender systems provide a human-understandable rationale for\ntheir outputs. Over the last two decades, explainable recommendation has\nattracted much attention in the recommender systems research community. This\npaper aims to provide a comprehensive review of research efforts on visual\nexplanation in recommender systems. More concretely, we systematically review\nthe literature on explanations in recommender systems based on four dimensions,\nnamely explanation goal, explanation scope, explanation style, and explanation\nformat. Recognizing the importance of visualization, we approach the\nrecommender system literature from the angle of explanatory visualizations,\nthat is using visualizations as a display style of explanation. As a result, we\nderive a set of guidelines that might be constructive for designing explanatory\nvisualizations in recommender systems and identify perspectives for future work\nin this field. The aim of this review is to help recommendation researchers and\npractitioners better understand the potential of visually explainable\nrecommendation research and to support them in the systematic design of visual\nexplanations in current and future recommender systems.",
        "translated": "对建议提供系统生成的解释是朝向透明和可靠的推荐系统迈出的重要一步。可解释的推荐系统为其输出提供了一个人类可理解的理由。在过去的二十年中，可解释的推荐引起了推荐系统研究界的广泛关注。本文旨在对推荐系统中视觉解释的研究工作进行综述。更具体地，我们从解释目的、解释范围、解释风格和解释格式四个维度系统地回顾了关于推荐系统中解释的文献。认识到可视化的重要性，我们从解释性可视化的角度来看待推荐系统文献，即使用可视化作为一种解释的显示方式。因此，我们得出了一套指导方针，可能是建设性的设计解释性可视化在推荐系统，并确定未来的工作在这一领域的前景。本综述的目的是帮助推荐研究人员和从业人员更好地理解可视化解释推荐研究的潜力，并支持他们在目前和未来的推荐系统中系统地设计可视化解释。"
    },
    {
        "title": "Inference-time Re-ranker Relevance Feedback for Neural Information\n  Retrieval",
        "url": "http://arxiv.org/abs/2305.11744v1",
        "pub_date": "2023-05-19",
        "summary": "Neural information retrieval often adopts a retrieve-and-rerank framework: a\nbi-encoder network first retrieves K (e.g., 100) candidates that are then\nre-ranked using a more powerful cross-encoder model to rank the better\ncandidates higher. The re-ranker generally produces better candidate scores\nthan the retriever, but is limited to seeing only the top K retrieved\ncandidates, thus providing no improvements in retrieval performance as measured\nby Recall@K. In this work, we leverage the re-ranker to also improve retrieval\nby providing inference-time relevance feedback to the retriever. Concretely, we\nupdate the retriever's query representation for a test instance using a\nlightweight inference-time distillation of the re-ranker's prediction for that\ninstance. The distillation loss is designed to bring the retriever's candidate\nscores closer to those of the re-ranker. A second retrieval step is then\nperformed with the updated query vector. We empirically show that our approach,\nwhich can serve arbitrary retrieve-and-rerank pipelines, significantly improves\nretrieval recall in multiple domains, languages, and modalities.",
        "translated": "神经信息检索通常采用一个检索-重新排序框架: 一个双编码器网络首先检索 K (例如，100)候选人，然后使用一个更强大的交叉编码器模型重新排序，以排序更好的候选人更高。重新排名通常比检索器产生更好的候选人分数，但是仅限于看到被检索的最高 K 的候选人，因此没有提供根据 Recall@K 测量的检索性能的改善。在这项工作中，我们利用重新排名也提高检索提供推理时间关联反馈的检索。具体来说，我们使用重新排序器对测试实例的预测的轻量级推理时间精馏来更新检索器对该实例的查询表示。蒸馏损失的目的是使猎犬的候选分数更接近那些重新排名。然后使用更新的查询向量执行第二个检索步骤。我们的实验表明，我们的方法，可以服务任意的检索和重新排序管道，显着提高检索召回在多个领域，语言和模式。"
    },
    {
        "title": "Exploring the Upper Limits of Text-Based Collaborative Filtering Using\n  Large Language Models: Discoveries and Insights",
        "url": "http://arxiv.org/abs/2305.11700v1",
        "pub_date": "2023-05-19",
        "summary": "Text-based collaborative filtering (TCF) has become the mainstream approach\nfor text and news recommendation, utilizing text encoders, also known as\nlanguage models (LMs), to represent items. However, existing TCF models\nprimarily focus on using small or medium-sized LMs. It remains uncertain what\nimpact replacing the item encoder with one of the largest and most powerful\nLMs, such as the 175-billion parameter GPT-3 model, would have on\nrecommendation performance. Can we expect unprecedented results? To this end,\nwe conduct an extensive series of experiments aimed at exploring the\nperformance limits of the TCF paradigm. Specifically, we increase the size of\nitem encoders from one hundred million to one hundred billion to reveal the\nscaling limits of the TCF paradigm. We then examine whether these extremely\nlarge LMs could enable a universal item representation for the recommendation\ntask. Furthermore, we compare the performance of the TCF paradigm utilizing the\nmost powerful LMs to the currently dominant ID embedding-based paradigm and\ninvestigate the transferability of this TCF paradigm. Finally, we compare TCF\nwith the recently popularized prompt-based recommendation using ChatGPT. Our\nresearch findings have not only yielded positive results but also uncovered\nsome surprising and previously unknown negative outcomes, which can inspire\ndeeper reflection and innovative thinking regarding text-based recommender\nsystems. Codes and datasets will be released for further research.",
        "translated": "基于文本的协同过滤(TCF)已经成为文本和新闻推荐的主流方法，利用文本编码器(也称为语言模型(LMs))来表示项目。然而，现有的 TCF 模型主要侧重于使用中小型 LM。用最大最强的 LM (比如1750亿参数的 GPT-3模型)替换项目编码器会对推荐性能产生什么影响还不确定。我们能期待前所未有的结果吗？为此，我们进行了一系列广泛的实验，旨在探索 TCF 范式的性能极限。具体来说，我们将条目编码器的大小从1亿增加到1亿，以揭示 TCF 范例的伸缩限制。然后，我们检查这些极大的 LM 是否能够为推荐任务启用通用项表示。此外，我们比较了使用最强大的 LM 的 TCF 范式和目前占主导地位的基于 ID 嵌入的 TCF 范式的性能，并研究了这种 TCF 范式的可转移性。最后，我们使用 ChatGPT 比较 TCF 和最近推广的基于提示的推荐。我们的研究结果不仅产生了积极的结果，而且还揭示了一些令人惊讶的和以前未知的负面结果，这可以激发关于基于文本的推荐系统的更深层次的反思和创新思维。代码和数据集将被发布用于进一步的研究。"
    },
    {
        "title": "The Barriers to Online Clothing Websites for Visually Impaired People:\n  An Interview and Observation Approach to Understanding Needs",
        "url": "http://arxiv.org/abs/2305.11559v1",
        "pub_date": "2023-05-19",
        "summary": "Visually impaired (VI) people often face challenges when performing everyday\ntasks and identify shopping for clothes as one of the most challenging. Many\nengage in online shopping, which eliminates some challenges of physical\nshopping. However, clothes shopping online suffers from many other limitations\nand barriers. More research is needed to address these challenges, and extant\nworks often base their findings on interviews alone, providing only subjective,\nrecall-biased information. We conducted two complementary studies using both\nobservational and interview approaches to fill a gap in understanding about VI\npeople's behaviour when selecting and purchasing clothes online. Our findings\nshow that shopping websites suffer from inaccurate, misleading, and\ncontradictory clothing descriptions; that VI people mainly rely on (unreliable)\nsearch tools and check product descriptions by reviewing customer comments. Our\nfindings also indicate that VI people are hesitant to accept assistance from\nautomated, but that trust in such systems could be improved if researchers can\ndevelop systems that better accommodate users' needs and preferences.",
        "translated": "视力受损者在日常工作中经常面临挑战，购买衣服是最具挑战性的工作之一。许多人从事网上购物，这消除了实体购物的一些挑战。然而，网上购物的服装受到许多其他限制和障碍。需要更多的研究来解决这些挑战，现存的工作往往基于他们的调查结果仅仅访谈，只提供主观的，回忆偏见的信息。我们使用观察和访谈的方法进行了两个互补的研究，以填补在线选择和购买服装时对 VI 人群行为的理解差距。我们的研究结果表明，购物网站容易受到不准确、误导和自相矛盾的服装描述的困扰; VI 用户主要依靠(不可靠的)搜索工具，通过查看顾客评论来检查产品描述。我们的研究结果还表明，VI 人士不愿意接受自动化系统的帮助，但如果研究人员能够开发出更好地适应用户需求和偏好的系统，那么对这种系统的信任就可以得到改善。"
    },
    {
        "title": "InstructIE: A Chinese Instruction-based Information Extraction Dataset",
        "url": "http://arxiv.org/abs/2305.11527v1",
        "pub_date": "2023-05-19",
        "summary": "We introduce a new Information Extraction (IE) task dubbed Instruction-based\nIE, which aims to ask the system to follow specific instructions or guidelines\nto extract information. To facilitate research in this area, we construct a\ndataset called InstructIE, consisting of 270,000 weakly supervised data from\nChinese Wikipedia and 1,000 high-quality crowdsourced annotated instances. We\nfurther evaluate the performance of various baseline models on the InstructIE\ndataset. The results reveal that although current models exhibit promising\nperformance, there is still room for improvement. Furthermore, we conduct a\ncomprehensive case study analysis, underlining the challenges inherent in the\nInstruction-based IE task. Code and dataset are available at\nhttps://github.com/zjunlp/DeepKE/tree/main/example/llm.",
        "translated": "我们引入了一个新的基于指令的信息抽取(IE)任务，其目的是要求系统遵循特定的指令或指南来提取信息。为了促进这一领域的研究，我们构建了一个名为 DirectIE 的数据集，由来自中文维基百科的270,000个弱监督数据和1000个高质量的众包注释实例组成。我们进一步评估了各种基线模型在 DirectIE 数据集上的性能。结果表明，虽然目前的模型表现出良好的性能，仍然有改进的空间。此外，我们进行了一个全面的案例研究分析，强调了基于教学的 IE 任务固有的挑战。代码和数据集可在 https://github.com/zjunlp/deepke/tree/main/example/llm 下载。"
    },
    {
        "title": "Recouple Event Field via Probabilistic Bias for Event Extraction",
        "url": "http://arxiv.org/abs/2305.11498v1",
        "pub_date": "2023-05-19",
        "summary": "Event Extraction (EE), aiming to identify and classify event triggers and\narguments from event mentions, has benefited from pre-trained language models\n(PLMs). However, existing PLM-based methods ignore the information of\ntrigger/argument fields, which is crucial for understanding event schemas. To\nthis end, we propose a Probabilistic reCoupling model enhanced Event extraction\nframework (ProCE). Specifically, we first model the syntactic-related event\nfields as probabilistic biases, to clarify the event fields from ambiguous\nentanglement. Furthermore, considering multiple occurrences of the same\ntriggers/arguments in EE, we explore probabilistic interaction strategies among\nmultiple fields of the same triggers/arguments, to recouple the corresponding\nclarified distributions and capture more latent information fields. Experiments\non EE datasets demonstrate the effectiveness and generalization of our proposed\napproach.",
        "translated": "事件提取(EE) ，旨在从事件提及中识别和分类事件触发器和参数，已经受益于预训练语言模型(PLM)。然而，现有的基于 PLM 的方法忽略了触发器/参数字段的信息，这对于理解事件模式是至关重要的。为此，我们提出了一个概率重耦合模型增强的事件抽取框架(ProCE)。具体来说，我们首先将与句法相关的事件字段建模为概率偏差，以澄清来自模糊纠缠的事件字段。此外，考虑到同一触发器/参数在 EE 中的多次出现，我们探索了同一触发器/参数的多个域之间的概率交互策略，以重新耦合相应的澄清分布并捕获更多的潜在信息域。在 EE 数据集上的实验证明了该方法的有效性和推广性。"
    },
    {
        "title": "TELeR: A General Taxonomy of LLM Prompts for Benchmarking Complex Tasks",
        "url": "http://arxiv.org/abs/2305.11430v1",
        "pub_date": "2023-05-19",
        "summary": "While LLMs have shown great success in understanding and generating text in\ntraditional conversational settings, their potential for performing ill-defined\ncomplex tasks is largely under-studied. Indeed, we are yet to conduct\ncomprehensive benchmarking studies with multiple LLMs that are exclusively\nfocused on a complex task. However, conducting such benchmarking studies is\nchallenging because of the large variations in LLMs' performance when different\nprompt types/styles are used and different degrees of detail are provided in\nthe prompts. To address this issue, the paper proposes a general taxonomy that\ncan be used to design prompts with specific properties in order to perform a\nwide range of complex tasks. This taxonomy will allow future benchmarking\nstudies to report the specific categories of prompts used as part of the study,\nenabling meaningful comparisons across different studies. Also, by establishing\na common standard through this taxonomy, researchers will be able to draw more\naccurate conclusions about LLMs' performance on a specific complex task.",
        "translated": "虽然 LLM 在理解和生成传统会话环境中的文本方面取得了巨大的成功，但它们执行定义不清的复杂任务的潜力在很大程度上还没有得到充分的研究。事实上，我们还没有进行全面的基准研究与多个 LLM，专门集中在一个复杂的任务。然而，进行这样的基准测试研究是具有挑战性的，因为当使用不同的提示类型/风格和提示中提供不同程度的细节时，LLM 的性能有很大的差异。为了解决这个问题，本文提出了一个通用分类法，可用于设计具有特定属性的提示符，以便执行范围广泛的复杂任务。这种分类法将使未来的基准研究能够报告作为研究的一部分使用的特定类别的提示，使不同研究之间的有意义的比较成为可能。此外，通过建立一个共同的标准通过这个分类，研究人员将能够得出更准确的结论 LLM 的表现在一个特定的复杂的任务。"
    },
    {
        "title": "Online Learning in a Creator Economy",
        "url": "http://arxiv.org/abs/2305.11381v1",
        "pub_date": "2023-05-19",
        "summary": "The creator economy has revolutionized the way individuals can profit through\nonline platforms. In this paper, we initiate the study of online learning in\nthe creator economy by modeling the creator economy as a three-party game\nbetween the users, platform, and content creators, with the platform\ninteracting with the content creator under a principal-agent model through\ncontracts to encourage better content. Additionally, the platform interacts\nwith the users to recommend new content, receive an evaluation, and ultimately\nprofit from the content, which can be modeled as a recommender system.\n  Our study aims to explore how the platform can jointly optimize the contract\nand recommender system to maximize the utility in an online learning fashion.\nWe primarily analyze and compare two families of contracts: return-based\ncontracts and feature-based contracts. Return-based contracts pay the content\ncreator a fraction of the reward the platform gains. In contrast, feature-based\ncontracts pay the content creator based on the quality or features of the\ncontent, regardless of the reward the platform receives. We show that under\nsmoothness assumptions, the joint optimization of return-based contracts and\nrecommendation policy provides a regret $\\Theta(T^{2/3})$. For the\nfeature-based contract, we introduce a definition of intrinsic dimension $d$ to\ncharacterize the hardness of learning the contract and provide an upper bound\non the regret $\\mathcal{O}(T^{(d+1)/(d+2)})$. The upper bound is tight for the\nlinear family.",
        "translated": "创造者经济彻底改变了个人通过在线平台获利的方式。本文通过将创造者经济建模为用户、平台和内容创造者之间的三方博弈，平台与内容创造者在委托-代理模式下通过契约互动来鼓励更好的内容，开展了创造者经济中在线学习的研究。此外，该平台与用户互动，推荐新内容，接受评估，并最终从内容中获利，这些内容可以被建模为推荐系统。我们的研究旨在探讨这个平台如何能够共同优化合同和推荐系统，以便在网上学习的模式中最大限度地发挥效用。我们主要分析和比较了两类契约: 基于回报的契约和基于特征的契约。基于回报的合同支付给内容创作者的报酬只是平台收益的一小部分。相比之下，基于特性的合同根据内容的质量或特性支付给内容创作者，而不管平台获得什么报酬。结果表明，在平滑假设下，基于回报的合同和推荐策略的联合优化得到了遗憾的 $Θ (T ^ {2/3}) $。对于基于特征的契约，我们引入了本征维度 $d $的定义来描述学习契约的难度，并给出了遗憾 $数学{ O }(T ^ {(d + 1)/(d + 2)}) $的上界。线性族的上界是紧的。"
    },
    {
        "title": "Copy Recurrent Neural Network Structure Network",
        "url": "http://arxiv.org/abs/2305.13250v1",
        "pub_date": "2023-05-22",
        "summary": "Electronic Health Record (EHR) coding involves automatically classifying EHRs\ninto diagnostic codes. While most previous research treats this as a\nmulti-label classification task, generating probabilities for each code and\nselecting those above a certain threshold as labels, these approaches often\noverlook the challenge of identifying complex diseases. In this study, our\nfocus is on detecting complication diseases within EHRs.\n  We propose a novel coarse-to-fine ICD path generation framework called the\nCopy Recurrent Neural Network Structure Network (CRNNet), which employs a Path\nGenerator (PG) and a Path Discriminator (PD) for EHR coding. By using RNNs to\ngenerate sequential outputs and incorporating a copy module, we efficiently\nidentify complication diseases. Our method achieves a 57.30\\% ratio of complex\ndiseases in predictions, outperforming state-of-the-art and previous\napproaches.\n  Additionally, through an ablation study, we demonstrate that the copy\nmechanism plays a crucial role in detecting complex diseases.",
        "translated": "电子健康记录(EHR)编码涉及将 EHR 自动分类为诊断代码。虽然大多数以前的研究将其视为一个多标签分类任务，为每个代码产生概率，并选择那些高于某个阈值的代码作为标签，但这些方法往往忽视了识别复杂疾病的挑战。在这项研究中，我们的重点是检测并发症的 EHR 疾病。我们提出了一个新的由粗到精的 ICD 路径生成框架，称为拷贝递归神经网络结构网络(crnNet) ，它使用路径生成器(PG)和路径鉴别器(PD)进行电子健康记录(eHR)编码。通过使用 RNN 产生序列输出和合并拷贝模块，我们有效地识别并发症疾病。我们的方法在预测复杂疾病方面达到了57.30% 的比例，超过了最先进的方法和以前的方法。此外，通过消融研究，我们证明复制机制在检测复杂疾病中起着至关重要的作用。"
    },
    {
        "title": "Challenging Decoder helps in Masked Auto-Encoder Pre-training for Dense\n  Passage Retrieval",
        "url": "http://arxiv.org/abs/2305.13197v1",
        "pub_date": "2023-05-22",
        "summary": "Recently, various studies have been directed towards exploring dense passage\nretrieval techniques employing pre-trained language models, among which the\nmasked auto-encoder (MAE) pre-training architecture has emerged as the most\npromising. The conventional MAE framework relies on leveraging the passage\nreconstruction of decoder to bolster the text representation ability of\nencoder, thereby enhancing the performance of resulting dense retrieval\nsystems. Within the context of building the representation ability of the\nencoder through passage reconstruction of decoder, it is reasonable to\npostulate that a ``more demanding'' decoder will necessitate a corresponding\nincrease in the encoder's ability. To this end, we propose a novel token\nimportance aware masking strategy based on pointwise mutual information to\nintensify the challenge of the decoder. Importantly, our approach can be\nimplemented in an unsupervised manner, without adding additional expenses to\nthe pre-training phase. Our experiments verify that the proposed method is both\neffective and robust on large-scale supervised passage retrieval datasets and\nout-of-domain zero-shot retrieval benchmarks.",
        "translated": "近年来，各种研究都致力于探索使用预训练语言模型的密集通道检索技术，其中掩蔽自动编码器(MAE)预训练结构是最有前途的一种。传统的 MAE 框架依赖于利用解码器的通道重构来增强编码器的文本表示能力，从而提高密集检索系统的性能。在通过解码器的通道重构来建立编码器的表示能力的背景下，假设“更高要求”的解码器需要相应提高编码器的表示能力是合理的。为此，我们提出了一种新颖的基于点间互信息的标记重要性感知掩蔽策略，以增强解码器的挑战性。重要的是，我们的方法可以在一个无监督的方式实施，而不增加额外的费用，预培训阶段。实验结果表明，该方法对于大规模有监督通道检索数据集和域外零镜头检索基准具有良好的鲁棒性和有效性。"
    },
    {
        "title": "TEIMMA: The First Content Reuse Annotator for Text, Images, and Math",
        "url": "http://arxiv.org/abs/2305.13193v1",
        "pub_date": "2023-05-22",
        "summary": "This demo paper presents the first tool to annotate the reuse of text,\nimages, and mathematical formulae in a document pair -- TEIMMA. Annotating\ncontent reuse is particularly useful to develop plagiarism detection\nalgorithms. Real-world content reuse is often obfuscated, which makes it\nchallenging to identify such cases. TEIMMA allows entering the obfuscation type\nto enable novel classifications for confirmed cases of plagiarism. It enables\nrecording different reuse types for text, images, and mathematical formulae in\nHTML and supports users by visualizing the content reuse in a document pair\nusing similarity detection methods for text and math.",
        "translated": "本演示文件介绍了第一个注释文本、图像和数学公式在文档对中的重用的工具—— TEIMMA。注释内容重用对于开发剽窃检测算法特别有用。真实世界的内容重用通常是模糊的，这使得识别这种情况变得很困难。TEIMMA 允许输入模糊类型，以便对确认的剽窃案件进行新的分类。它支持在 HTML 中记录文本、图像和数学公式的不同重用类型，并通过使用文本和数学的相似性检测方法可视化文档对中的内容重用来支持用户。"
    },
    {
        "title": "Editing Large Language Models: Problems, Methods, and Opportunities",
        "url": "http://arxiv.org/abs/2305.13172v1",
        "pub_date": "2023-05-22",
        "summary": "Recent advancements in deep learning have precipitated the emergence of large\nlanguage models (LLMs) which exhibit an impressive aptitude for understanding\nand producing text akin to human language. Despite the ability to train highly\ncapable LLMs, the methodology for maintaining their relevancy and rectifying\nerrors remains elusive. To that end, the past few years have witnessed a surge\nin techniques for editing LLMs, the objective of which is to alter the behavior\nof LLMs within a specific domain without negatively impacting performance\nacross other inputs. This paper embarks on a deep exploration of the problems,\nmethods, and opportunities relating to model editing for LLMs. In particular,\nwe provide an exhaustive overview of the task definition and challenges\nassociated with model editing, along with an in-depth empirical analysis of the\nmost progressive methods currently at our disposal. We also build a new\nbenchmark dataset to facilitate a more robust evaluation and pinpoint enduring\nissues intrinsic to existing techniques. Our objective is to provide valuable\ninsights into the effectiveness and feasibility of each model editing\ntechnique, thereby assisting the research community in making informed\ndecisions when choosing the most appropriate method for a specific task or\ncontext. Code and datasets will be available at\nhttps://github.com/zjunlp/EasyEdit.",
        "translated": "深度学习的最新进展催生了大型语言模型(LLM)的出现，它们在理解和产生类似于人类语言的文本方面表现出令人印象深刻的天赋。尽管有能力培训高能力的 LLM，但保持其相关性和纠正错误的方法仍然是难以捉摸的。为此，过去几年见证了 LLM 编辑技术的激增，其目标是改变特定领域内 LLM 的行为，而不会对其他输入的性能产生负面影响。本文对 LLM 模型编辑的问题、方法和机会进行了深入的探讨。特别是，我们提供了任务定义和与模型编辑有关的挑战的详尽概述，以及对我们目前掌握的最进步的方法的深入实证分析。我们还建立了一个新的基准数据集，以促进更强大的评估，并确定持久的问题内在的现有技术。我们的目标是提供有价值的见解，每个模型编辑技术的有效性和可行性，从而协助研究界作出知情的决定时，选择最适当的方法，具体的任务或背景。代码和数据集将在 https://github.com/zjunlp/easyedit 提供。"
    },
    {
        "title": "LLMs for Knowledge Graph Construction and Reasoning: Recent Capabilities\n  and Future Opportunities",
        "url": "http://arxiv.org/abs/2305.13168v1",
        "pub_date": "2023-05-22",
        "summary": "This paper presents an exhaustive quantitative and qualitative evaluation of\nLarge Language Models (LLMs) for Knowledge Graph (KG) construction and\nreasoning. We employ eight distinct datasets that encompass aspects including\nentity, relation and event extraction, link prediction, and question answering.\nEmpirically, our findings suggest that GPT-4 outperforms ChatGPT in the\nmajority of tasks and even surpasses fine-tuned models in certain reasoning and\nquestion-answering datasets. Moreover, our investigation extends to the\npotential generalization ability of LLMs for information extraction, which\nculminates in the presentation of the Virtual Knowledge Extraction task and the\ndevelopment of the VINE dataset. Drawing on these empirical findings, we\nfurther propose AutoKG, a multi-agent-based approach employing LLMs for KG\nconstruction and reasoning, which aims to chart the future of this field and\noffer exciting opportunities for advancement. We anticipate that our research\ncan provide invaluable insights for future undertakings of KG\\footnote{Code and\ndatasets will be available in https://github.com/zjunlp/AutoKG.",
        "translated": "本文对知识图(KG)构造和推理的大语言模型(LLM)进行了详尽的定量和定性评价。我们使用八个不同的数据集，包括实体、关系和事件提取、链接预测和问题回答。经验上，我们的研究结果表明，GPT-4在大多数任务中的表现优于 ChatGPT，甚至在某些推理和问答数据集中优于微调模型。此外，我们的研究扩展到 LLM 对信息抽取的潜在推广能力，最终导致虚拟知识提取任务的介绍和 VINE 数据集的开发。基于这些实证研究结果，我们进一步提出了 AutoKG，这是一种基于多主体的方法，利用 LLM 进行 KG 的构建和推理，旨在描绘这一领域的未来，并提供令人兴奋的发展机会。我们期望我们的研究能为幼稚园日后的工作提供宝贵的意见。{ https://github.com/zjunlp/autokg 及数据集将可供参考。"
    },
    {
        "title": "Rethinking the Evaluation for Conversational Recommendation in the Era\n  of Large Language Models",
        "url": "http://arxiv.org/abs/2305.13112v1",
        "pub_date": "2023-05-22",
        "summary": "The recent success of large language models (LLMs) has shown great potential\nto develop more powerful conversational recommender systems (CRSs), which rely\non natural language conversations to satisfy user needs. In this paper, we\nembark on an investigation into the utilization of ChatGPT for conversational\nrecommendation, revealing the inadequacy of the existing evaluation protocol.\nIt might over-emphasize the matching with the ground-truth items or utterances\ngenerated by human annotators, while neglecting the interactive nature of being\na capable CRS. To overcome the limitation, we further propose an interactive\nEvaluation approach based on LLMs named iEvaLM that harnesses LLM-based user\nsimulators. Our evaluation approach can simulate various interaction scenarios\nbetween users and systems. Through the experiments on two publicly available\nCRS datasets, we demonstrate notable improvements compared to the prevailing\nevaluation protocol. Furthermore, we emphasize the evaluation of\nexplainability, and ChatGPT showcases persuasive explanation generation for its\nrecommendations. Our study contributes to a deeper comprehension of the\nuntapped potential of LLMs for CRSs and provides a more flexible and\neasy-to-use evaluation framework for future research endeavors. The codes and\ndata are publicly available at https://github.com/RUCAIBox/iEvaLM-CRS.",
        "translated": "大型语言模型(LLM)最近的成功显示了开发更强大的会话推荐系统(CRS)的巨大潜力，CRS 依赖于自然语言会话来满足用户的需求。本文对 ChatGPT 在会话推荐中的应用进行了研究，揭示了现有评价协议的不足之处。它可能过分强调与地面真相项目或人类注释者产生的话语的匹配，而忽视了作为一个有能力的 CRS 的互动性质。为了克服这一局限性，我们进一步提出了一种基于 LLM 的交互式评估方法 iEvaLM，该方法利用了基于 LLM 的用户模拟器。我们的评估方法可以模拟用户和系统之间的各种交互场景。通过对两个公开可用的 CRS 数据集的实验，我们发现与现行的评估协议相比有显著的改进。此外，我们强调可解释性的评价，ChatGPT 展示了其建议的说服性解释生成。我们的研究有助于更深入地了解 LLM 在 CRS 中尚未开发的潜力，并为未来的研究工作提供了一个更加灵活和易于使用的评估框架。这些代码和数据可以在 https://github.com/rucaibox/ievalm-crs 上公开获得。"
    },
    {
        "title": "Making Language Models Better Tool Learners with Execution Feedback",
        "url": "http://arxiv.org/abs/2305.13068v1",
        "pub_date": "2023-05-22",
        "summary": "Tools serve as pivotal interfaces that enable humans to understand and\nreshape the world. With the advent of foundational models, AI systems can\nutilize tools to expand their capabilities and interact with the world.\nExisting tool learning methodologies, encompassing supervised fine-tuning and\nprompt engineering approaches, often induce language models to utilize tools\nindiscriminately, as complex problems often exceed their own competencies.\nHowever, introducing tools for simple tasks, which the models themselves can\nreadily resolve, can inadvertently propagate errors rather than enhance\nperformance. This leads to the research question: can we teach language models\nwhen and how to use tools? To meet this need, we propose Tool leaRning wIth\nexeCution fEedback (TRICE), a two-stage end-to-end framework that enables the\nmodel to continually learn through feedback derived from tool execution,\nthereby learning when and how to use tools effectively. Experimental results,\nbacked by further analysis, show that TRICE can make the language model to\nselectively use tools by decreasing the model's dependency on tools while\nenhancing the performance. Code and datasets will be available in\nhttps://github.com/zjunlp/trice.",
        "translated": "工具作为关键的接口，使人类能够理解和重塑世界。随着基础模型的出现，人工智能系统可以利用工具来扩展它们的能力，并与世界互动。现有的工具学习方法，包括有监督的微调和及时的工程方法，经常诱导语言模型不加区分地使用工具，因为复杂的问题往往超出了它们自己的能力。然而，引入用于简单任务的工具(模型本身可以很容易地解决这些任务)可能会在无意中传播错误，而不是提高性能。这就引出了一个研究问题: 我们可以教语言模型何时以及如何使用工具吗？为了满足这一需求，我们提出工具学习与执行反馈(TRICE) ，一个两阶段的端到端框架，使模型能够通过反馈不断学习从工具执行，从而学习何时和如何有效地使用工具。实验结果表明，TRICE 能够使语言模型有选择地使用工具，降低模型对工具的依赖性，同时提高语言的性能。代码和数据集将以 https://github.com/zjunlp/trice 形式提供。"
    },
    {
        "title": "Evaluating and Enhancing Structural Understanding Capabilities of Large\n  Language Models on Tables via Input Designs",
        "url": "http://arxiv.org/abs/2305.13062v1",
        "pub_date": "2023-05-22",
        "summary": "Large language models (LLMs) are becoming attractive as few-shot reasoners to\nsolve NL-related tasks. However, there is still much to be learned about how\nwell LLMs understand structured data, such as tables. While it is true that\ntables can be used as inputs to LLMs with serialization, there lack\ncomprehensive studies examining whether LLMs can truly comprehend such data. In\nthis paper we try to understand this by designing a benchmark to evaluate\nstructural understanding capabilities (SUC) of LLMs. The benchmark we create\nincludes seven tasks, each with their own unique challenges, e.g,, cell lookup,\nrow retrieval and size detection. We run a series of evaluations on GPT-3\nfamily models (e.g., text-davinci-003). We discover that the performance varied\ndepending on a number of input choices, including table input format, content\norder, role prompting and partition marks. Drawing from the insights gained\nthrough the benchmark evaluations, we then propose self-augmentation for\neffective structural prompting, e.g., critical value / range identification\nusing LLMs' internal knowledge. When combined with carefully chosen input\nchoices, these structural prompting methods lead to promising improvements in\nLLM performance on a variety of tabular tasks, e.g., TabFact($\\uparrow2.31\\%$),\nHybridQA($\\uparrow2.13\\%$), SQA($\\uparrow2.72\\%$), Feverous($\\uparrow0.84\\%$),\nand ToTTo($\\uparrow5.68\\%$). We believe our benchmark and proposed prompting\nmethods can serve as a simple yet generic selection for future research. The\ncode and data are released in\nhttps://anonymous.4open.science/r/StructuredLLM-76F3.",
        "translated": "大型语言模型(LLM)作为解决大型语言相关任务的少量推理工具正变得越来越有吸引力。然而，关于 LLM 如何很好地理解结构化数据(如表) ，还有很多东西需要学习。虽然表确实可以用作具有序列化的 LLM 的输入，但是缺乏全面的研究来检查 LLM 是否能够真正理解这些数据。在本文中，我们试图通过设计一个基准来评估 LLM 的结构理解能力(SUC)来理解这一点。我们创建的基准测试包括七个任务，每个任务都有其独特的挑战，例如，单元格查找、行检索和大小检测。我们对 GPT-3家族模型(例如 text-davinci-003)进行了一系列的评估。我们发现，性能取决于许多输入选择，包括表输入格式、内容顺序、角色提示和分区标记。根据基准评估所获得的见解，我们提出自我增强的有效结构激励，例如，利用 LLM 的内部知识识识别临界值/范围。当结合精心选择的输入选择时，这些结构化的提示方法导致了各种表格任务的 LLM 性能的有希望的改善，例如 TabFact ($uparrow2.31% $) ，HybridQA ($uparrow2.13% $) ，SQA ($uparrow2.72% $) ，Feverous ($uparrow0.84% $)和 ToTTo ($uparrow5.68% $)。我们相信，我们的基准和提议的激励方法可以作为一个简单而通用的选择，为未来的研究。代码和数据以 https://anonymous.4open.science/r/structuredllm-76f3的形式发布。"
    },
    {
        "title": "Attentive Graph-based Text-aware Preference Modeling for Top-N\n  Recommendation",
        "url": "http://arxiv.org/abs/2305.12976v1",
        "pub_date": "2023-05-22",
        "summary": "Textual data are commonly used as auxiliary information for modeling user\npreference nowadays. While many prior works utilize user reviews for rating\nprediction, few focus on top-N recommendation, and even few try to incorporate\nitem textual contents such as title and description. Though delivering\npromising performance for rating prediction, we empirically find that many\nreview-based models cannot perform comparably well on top-N recommendation.\nAlso, user reviews are not available in some recommendation scenarios, while\nitem textual contents are more prevalent. On the other hand, recent graph\nconvolutional network (GCN) based models demonstrate state-of-the-art\nperformance for top-N recommendation. Thus, in this work, we aim to further\nimprove top-N recommendation by effectively modeling both item textual content\nand high-order connectivity in user-item graph. We propose a new model named\nAttentive Graph-based Text-aware Recommendation Model (AGTM). Extensive\nexperiments are provided to justify the rationality and effectiveness of our\nmodel design.",
        "translated": "文本数据是当今建立用户偏好模型的常用辅助信息。虽然许多以前的作品利用用户评论进行评分预测，但很少关注前 N 名的推荐，甚至很少尝试合并项目文本内容，如标题和描述。通过实证研究，我们发现许多基于评论的模型在排名前 N 的推荐中表现不佳。此外，在某些推荐场景中，用户评论是不可用的，而项目文本内容更为普遍。另一方面，最近基于图卷积网络(GCN)的模型展示了最高 N 推荐的最新性能。因此，在本研究中，我们的目标是通过有效地建立用户项目图中项目文本内容和高阶连通性的模型，进一步改善最高 N 推荐。提出了一种基于注意图的文本感知推荐模型(AGTM)。通过大量实验验证了模型设计的合理性和有效性。"
    },
    {
        "title": "It's Enough: Relaxing Diagonal Constraints in Linear Autoencoders for\n  Recommendation",
        "url": "http://arxiv.org/abs/2305.12922v1",
        "pub_date": "2023-05-22",
        "summary": "Linear autoencoder models learn an item-to-item weight matrix via convex\noptimization with L2 regularization and zero-diagonal constraints. Despite\ntheir simplicity, they have shown remarkable performance compared to\nsophisticated non-linear models. This paper aims to theoretically understand\nthe properties of two terms in linear autoencoders. Through the lens of\nsingular value decomposition (SVD) and principal component analysis (PCA), it\nis revealed that L2 regularization enhances the impact of high-ranked PCs.\nMeanwhile, zero-diagonal constraints reduce the impact of low-ranked PCs,\nleading to performance degradation for unpopular items. Inspired by this\nanalysis, we propose simple-yet-effective linear autoencoder models using\ndiagonal inequality constraints, called Relaxed Linear AutoEncoder (RLAE) and\nRelaxed Denoising Linear AutoEncoder (RDLAE). We prove that they generalize\nlinear autoencoders by adjusting the degree of diagonal constraints.\nExperimental results demonstrate that our models are comparable or superior to\nstate-of-the-art linear and non-linear models on six benchmark datasets; they\nsignificantly improve the accuracy of long-tail items. These results also\nsupport our theoretical insights on regularization and diagonal constraints in\nlinear autoencoders.",
        "translated": "线性自动编码器模型通过 L2正则化和零对角约束的凸优化学习一个项目对项目的权重矩阵。尽管它们很简单，但与复杂的非线性模型相比，它们表现出了显著的性能。本文旨在从理论上理解线性自动编码器中两项的性质。通过奇异值分解(SVD)和主成分分析(PCA)透镜，我们发现二语正规化增强了高等级个人电脑的影响。与此同时，零对角线约束减少了低排名个人电脑的影响，导致不受欢迎项目的性能下降。受此分析的启发，我们提出了使用对角不等式约束的简单而有效的线性自动编码器模型，称为松弛线性自动编码器(RLAE)和松弛去噪线性自动编码器(RDLAE)。我们证明了它们通过调整对角线约束的程度来推广线性自动编码器。实验结果表明，在六个基准数据集上，我们的模型与最先进的线性和非线性模型具有可比性或优越性，它们显著提高了长尾项目的准确性。这些结果也支持我们对线性自动编码器的正则化和对角线约束的理论认识。"
    },
    {
        "title": "Anchor Prediction: Automatic Refinement of Internet Links",
        "url": "http://arxiv.org/abs/2305.14337v1",
        "pub_date": "2023-05-23",
        "summary": "Internet links enable users to deepen their understanding of a topic by\nproviding convenient access to related information. However, the majority of\nlinks are unanchored -- they link to a target webpage as a whole, and readers\nmay expend considerable effort localizing the specific parts of the target\nwebpage that enrich their understanding of the link's source context. To help\nreaders effectively find information in linked webpages, we introduce the task\nof anchor prediction, where the goal is to identify the specific part of the\nlinked target webpage that is most related to the source linking context. We\nrelease the AuthorAnchors dataset, a collection of 34K naturally-occurring\nanchored links, which reflect relevance judgments by the authors of the source\narticle. To model reader relevance judgments, we annotate and release\nReaderAnchors, an evaluation set of anchors that readers find useful. Our\nanalysis shows that effective anchor prediction often requires jointly\nreasoning over lengthy source and target webpages to determine their implicit\nrelations and identify parts of the target webpage that are related but not\nredundant. We benchmark a performant T5-based ranking approach to establish\nbaseline performance on the task, finding ample room for improvement.",
        "translated": ""
    },
    {
        "title": "VIP5: Towards Multimodal Foundation Models for Recommendation",
        "url": "http://arxiv.org/abs/2305.14302v1",
        "pub_date": "2023-05-23",
        "summary": "Computer Vision (CV), Natural Language Processing (NLP), and Recommender\nSystems (RecSys) are three prominent AI applications that have traditionally\ndeveloped independently, resulting in disparate modeling and engineering\nmethodologies. This has impeded the ability for these fields to directly\nbenefit from each other's advancements. With the increasing availability of\nmultimodal data on the web, there is a growing need to consider various\nmodalities when making recommendations for users. With the recent emergence of\nfoundation models, large language models have emerged as a potential\ngeneral-purpose interface for unifying different modalities and problem\nformulations. In light of this, we propose the development of a multimodal\nfoundation model by considering both visual and textual modalities under the P5\nrecommendation paradigm (VIP5) to unify various modalities and recommendation\ntasks. This will enable the processing of vision, language, and personalization\ninformation in a shared architecture for improved recommendations. To achieve\nthis, we introduce multimodal personalized prompts to accommodate multiple\nmodalities under a shared format. Additionally, we propose a\nparameter-efficient training method for foundation models, which involves\nfreezing the backbone and fine-tuning lightweight adapters, resulting in\nimproved recommendation performance and increased efficiency in terms of\ntraining time and memory usage.",
        "translated": ""
    },
    {
        "title": "Simulating News Recommendation Ecosystem for Fun and Profit",
        "url": "http://arxiv.org/abs/2305.14103v1",
        "pub_date": "2023-05-23",
        "summary": "Understanding the evolution of online news communities is essential for\ndesigning more effective news recommender systems. However, due to the lack of\nappropriate datasets and platforms, the existing literature is limited in\nunderstanding the impact of recommender systems on this evolutionary process\nand the underlying mechanisms, resulting in sub-optimal system designs that may\naffect long-term utilities. In this work, we propose SimuLine, a simulation\nplatform to dissect the evolution of news recommendation ecosystems and present\na detailed analysis of the evolutionary process and underlying mechanisms.\nSimuLine first constructs a latent space well reflecting the human behaviors,\nand then simulates the news recommendation ecosystem via agent-based modeling.\nBased on extensive simulation experiments and the comprehensive analysis\nframework consisting of quantitative metrics, visualization, and textual\nexplanations, we analyze the characteristics of each evolutionary phase from\nthe perspective of life-cycle theory, and propose a relationship graph\nillustrating the key factors and affecting mechanisms. Furthermore, we explore\nthe impacts of recommender system designing strategies, including the\nutilization of cold-start news, breaking news, and promotion, on the\nevolutionary process, which shed new light on the design of recommender\nsystems.",
        "translated": ""
    },
    {
        "title": "BM25 Query Augmentation Learned End-to-End",
        "url": "http://arxiv.org/abs/2305.14087v1",
        "pub_date": "2023-05-23",
        "summary": "Given BM25's enduring competitiveness as an information retrieval baseline,\nwe investigate to what extent it can be even further improved by augmenting and\nre-weighting its sparse query-vector representation. We propose an approach to\nlearning an augmentation and a re-weighting end-to-end, and we find that our\napproach improves performance over BM25 while retaining its speed. We\nfurthermore find that the learned augmentations and re-weightings transfer well\nto unseen datasets.",
        "translated": ""
    },
    {
        "title": "Message Intercommunication for Inductive Relation Reasoning",
        "url": "http://arxiv.org/abs/2305.14074v1",
        "pub_date": "2023-05-23",
        "summary": "Inductive relation reasoning for knowledge graphs, aiming to infer missing\nlinks between brand-new entities, has drawn increasing attention. The models\ndeveloped based on Graph Inductive Learning, called GraIL-based models, have\nshown promising potential for this task. However, the uni-directional\nmessage-passing mechanism hinders such models from exploiting hidden mutual\nrelations between entities in directed graphs. Besides, the enclosing subgraph\nextraction in most GraIL-based models restricts the model from extracting\nenough discriminative information for reasoning. Consequently, the expressive\nability of these models is limited. To address the problems, we propose a novel\nGraIL-based inductive relation reasoning model, termed MINES, by introducing a\nMessage Intercommunication mechanism on the Neighbor-Enhanced Subgraph.\nConcretely, the message intercommunication mechanism is designed to capture the\nomitted hidden mutual information. It introduces bi-directed information\ninteractions between connected entities by inserting an undirected/bi-directed\nGCN layer between uni-directed RGCN layers. Moreover, inspired by the success\nof involving more neighbors in other graph-based tasks, we extend the\nneighborhood area beyond the enclosing subgraph to enhance the information\ncollection for inductive relation reasoning. Extensive experiments on twelve\ninductive benchmark datasets demonstrate that our MINES outperforms existing\nstate-of-the-art models, and show the effectiveness of our intercommunication\nmechanism and reasoning on the neighbor-enhanced subgraph.",
        "translated": ""
    },
    {
        "title": "When the Music Stops: Tip-of-the-Tongue Retrieval for Music",
        "url": "http://arxiv.org/abs/2305.14072v1",
        "pub_date": "2023-05-23",
        "summary": "We present a study of Tip-of-the-tongue (ToT) retrieval for music, where a\nsearcher is trying to find an existing music entity, but is unable to succeed\nas they cannot accurately recall important identifying information. ToT\ninformation needs are characterized by complexity, verbosity, uncertainty, and\npossible false memories. We make four contributions. (1) We collect a dataset -\n$ToT_{Music}$ - of 2,278 information needs and ground truth answers. (2) We\nintroduce a schema for these information needs and show that they often involve\nmultiple modalities encompassing several Music IR subtasks such as lyric\nsearch, audio-based search, audio fingerprinting, and text search. (3) We\nunderscore the difficulty of this task by benchmarking a standard text\nretrieval approach on this dataset. (4) We investigate the efficacy of query\nreformulations generated by a large language model (LLM), and show that they\nare not as effective as simply employing the entire information need as a query\n- leaving several open questions for future research.",
        "translated": ""
    },
    {
        "title": "DAPR: A Benchmark on Document-Aware Passage Retrieval",
        "url": "http://arxiv.org/abs/2305.13915v1",
        "pub_date": "2023-05-23",
        "summary": "Recent neural retrieval mainly focuses on ranking short texts and is\nchallenged with long documents. Existing work mainly evaluates either ranking\npassages or whole documents. However, there are many cases where the users want\nto find a relevant passage within a long document from a huge corpus, e.g.\nlegal cases, research papers, etc. In this scenario, the passage often provides\nlittle document context and thus challenges the current approaches to finding\nthe correct document and returning accurate results. To fill this gap, we\npropose and name this task Document-Aware Passage Retrieval (DAPR) and build a\nbenchmark including multiple datasets from various domains, covering both DAPR\nand whole-document retrieval. In experiments, we extend the state-of-the-art\nneural passage retrievers with document-level context via different approaches\nincluding prepending document summary, pooling over passage representations,\nand hybrid retrieval with BM25. The hybrid-retrieval systems, the overall best,\ncan only improve on the DAPR tasks marginally while significantly improving on\nthe document-retrieval tasks. This motivates further research in developing\nbetter retrieval systems for the new task. The code and the data are available\nat https://github.com/kwang2049/dapr",
        "translated": ""
    },
    {
        "title": "Term-Sets Can Be Strong Document Identifiers For Auto-Regressive Search\n  Engines",
        "url": "http://arxiv.org/abs/2305.13859v1",
        "pub_date": "2023-05-23",
        "summary": "Auto-regressive search engines emerge as a promising paradigm for next-gen\ninformation retrieval systems. These methods work with Seq2Seq models, where\neach query can be directly mapped to the identifier of its relevant document.\nAs such, they are praised for merits like being end-to-end differentiable.\nHowever, auto-regressive search engines also confront challenges in retrieval\nquality, given the requirement for the exact generation of the document\nidentifier. That's to say, the targeted document will be missed from the\nretrieval result if a false prediction about its identifier is made in any step\nof the generation process. In this work, we propose a novel framework, namely\nAutoTSG (Auto-regressive Search Engine with Term-Set Generation), which is\nfeatured by 1) the unordered term-based document identifier and 2) the\nset-oriented generation pipeline. With AutoTSG, any permutation of the term-set\nidentifier will lead to the retrieval of the corresponding document, thus\nlargely relaxing the requirement of exact generation. Besides, the Seq2Seq\nmodel is enabled to flexibly explore the optimal permutation of the document\nidentifier for the presented query, which may further contribute to the\nretrieval quality. AutoTSG is empirically evaluated with Natural Questions and\nMS MARCO, where notable improvements can be achieved against the existing\nauto-regressive search engines.",
        "translated": ""
    },
    {
        "title": "Advances and Challenges of Multi-task Learning Method in Recommender\n  System: A Survey",
        "url": "http://arxiv.org/abs/2305.13843v1",
        "pub_date": "2023-05-23",
        "summary": "Multi-task learning has been widely applied in computational vision, natural\nlanguage processing and other fields, which has achieved well performance. In\nrecent years, a lot of work about multi-task learning recommender system has\nbeen yielded, but there is no previous literature to summarize these works. To\nbridge this gap, we provide a systematic literature survey about multi-task\nrecommender systems, aiming to help researchers and practitioners quickly\nunderstand the current progress in this direction. In this survey, we first\nintroduce the background and the motivation of the multi-task learning-based\nrecommender systems. Then we provide a taxonomy of multi-task learning-based\nrecommendation methods according to the different stages of multi-task learning\ntechniques, which including task relationship discovery, model architecture and\noptimization strategy. Finally, we raise discussions on the application and\npromising future directions in this area.",
        "translated": ""
    },
    {
        "title": "Continual Learning on Dynamic Graphs via Parameter Isolation",
        "url": "http://arxiv.org/abs/2305.13825v1",
        "pub_date": "2023-05-23",
        "summary": "Many real-world graph learning tasks require handling dynamic graphs where\nnew nodes and edges emerge. Dynamic graph learning methods commonly suffer from\nthe catastrophic forgetting problem, where knowledge learned for previous\ngraphs is overwritten by updates for new graphs. To alleviate the problem,\ncontinual graph learning methods are proposed. However, existing continual\ngraph learning methods aim to learn new patterns and maintain old ones with the\nsame set of parameters of fixed size, and thus face a fundamental tradeoff\nbetween both goals. In this paper, we propose Parameter Isolation GNN (PI-GNN)\nfor continual learning on dynamic graphs that circumvents the tradeoff via\nparameter isolation and expansion. Our motivation lies in that different\nparameters contribute to learning different graph patterns. Based on the idea,\nwe expand model parameters to continually learn emerging graph patterns.\nMeanwhile, to effectively preserve knowledge for unaffected patterns, we find\nparameters that correspond to them via optimization and freeze them to prevent\nthem from being rewritten. Experiments on eight real-world datasets corroborate\nthe effectiveness of PI-GNN compared to state-of-the-art baselines.",
        "translated": ""
    },
    {
        "title": "NCHO: Unsupervised Learning for Neural 3D Composition of Humans and\n  Objects",
        "url": "http://arxiv.org/abs/2305.14345v1",
        "pub_date": "2023-05-23",
        "summary": "Deep generative models have been recently extended to synthesizing 3D digital\nhumans. However, previous approaches treat clothed humans as a single chunk of\ngeometry without considering the compositionality of clothing and accessories.\nAs a result, individual items cannot be naturally composed into novel\nidentities, leading to limited expressiveness and controllability of generative\n3D avatars. While several methods attempt to address this by leveraging\nsynthetic data, the interaction between humans and objects is not authentic due\nto the domain gap, and manual asset creation is difficult to scale for a wide\nvariety of objects. In this work, we present a novel framework for learning a\ncompositional generative model of humans and objects (backpacks, coats,\nscarves, and more) from real-world 3D scans. Our compositional model is\ninteraction-aware, meaning the spatial relationship between humans and objects,\nand the mutual shape change by physical contact is fully incorporated. The key\nchallenge is that, since humans and objects are in contact, their 3D scans are\nmerged into a single piece. To decompose them without manual annotations, we\npropose to leverage two sets of 3D scans of a single person with and without\nobjects. Our approach learns to decompose objects and naturally compose them\nback into a generative human model in an unsupervised manner. Despite our\nsimple setup requiring only the capture of a single subject with objects, our\nexperiments demonstrate the strong generalization of our model by enabling the\nnatural composition of objects to diverse identities in various poses and the\ncomposition of multiple objects, which is unseen in training data.",
        "translated": "深层生成模型最近已经扩展到合成3D 数字人类。然而，以前的方法没有考虑到衣服和配件的组合性，而是把穿着衣服的人当作一个单一的几何块来对待。因此，个别项目不能自然地组成新的身份，导致有限的表现力和可控性的生成3D 化身。虽然有几种方法试图通过利用合成数据来解决这个问题，但是由于领域间的差距，人与对象之间的交互并不真实，而且手动资产创建难以适用于各种各样的对象。在这项工作中，我们提出了一个新的框架来学习人类和物体(背包，外套，围巾等)的组合生成模型从现实世界的3 d 扫描。我们的构图模型是交互感知的，这意味着人与物体之间的空间关系，以及通过物理接触的相互形状变化被完全纳入。关键的挑战是，因为人类和物体是接触的，他们的3D 扫描合并成一个单一的部分。为了不用手动注释就可以分解它们，我们建议利用一个人的两组3D 扫描，有对象的和没有对象的。我们的方法学会了分解对象，并自然地以无监督的方式将它们重新组合成一个可生成的人类模型。尽管我们的简单设置只需要捕获具有对象的单个主体，但是我们的实验证明了我们的模型的强大泛化，通过使得对象的自然组合以不同姿势的不同身份和多个对象的组合，这在训练数据中是看不到的。"
    },
    {
        "title": "Siamese Masked Autoencoders",
        "url": "http://arxiv.org/abs/2305.14344v1",
        "pub_date": "2023-05-23",
        "summary": "Establishing correspondence between images or scenes is a significant\nchallenge in computer vision, especially given occlusions, viewpoint changes,\nand varying object appearances. In this paper, we present Siamese Masked\nAutoencoders (SiamMAE), a simple extension of Masked Autoencoders (MAE) for\nlearning visual correspondence from videos. SiamMAE operates on pairs of\nrandomly sampled video frames and asymmetrically masks them. These frames are\nprocessed independently by an encoder network, and a decoder composed of a\nsequence of cross-attention layers is tasked with predicting the missing\npatches in the future frame. By masking a large fraction ($95\\%$) of patches in\nthe future frame while leaving the past frame unchanged, SiamMAE encourages the\nnetwork to focus on object motion and learn object-centric representations.\nDespite its conceptual simplicity, features learned via SiamMAE outperform\nstate-of-the-art self-supervised methods on video object segmentation, pose\nkeypoint propagation, and semantic part propagation tasks. SiamMAE achieves\ncompetitive results without relying on data augmentation, handcrafted\ntracking-based pretext tasks, or other techniques to prevent representational\ncollapse.",
        "translated": "建立图像或场景之间的对应关系是计算机视觉中的一个重大挑战，特别是考虑到遮挡、视点变化和不同的物体外观。本文介绍了 Siamese 蒙版自动编码器(SiamMAE) ，它是蒙版自动编码器(MAE)的一个简单扩展，用于从视频中学习视觉对应。SiamMAE 对一对随机采样的视频帧进行操作，并非对称地屏蔽它们。这些帧由编码器网络独立处理，由一系列交叉注意层组成的解码器负责预测未来帧中丢失的补丁。SiamMAE 通过在未来帧中屏蔽大部分补丁(95%) ，同时保持过去帧不变，鼓励网络关注物体运动并学习以物体为中心的表示。尽管概念简单，通过 SiamMAE 学习的特征在视频对象分割、姿态关键点传播和语义部分传播任务方面优于最先进的自监督方法。SiamMAE 无需依赖数据增强、手工制作的基于跟踪的托辞任务或其他技术来防止表象崩溃，就能获得具有竞争力的结果。"
    },
    {
        "title": "Video Prediction Models as Rewards for Reinforcement Learning",
        "url": "http://arxiv.org/abs/2305.14343v1",
        "pub_date": "2023-05-23",
        "summary": "Specifying reward signals that allow agents to learn complex behaviors is a\nlong-standing challenge in reinforcement learning. A promising approach is to\nextract preferences for behaviors from unlabeled videos, which are widely\navailable on the internet. We present Video Prediction Rewards (VIPER), an\nalgorithm that leverages pretrained video prediction models as action-free\nreward signals for reinforcement learning. Specifically, we first train an\nautoregressive transformer on expert videos and then use the video prediction\nlikelihoods as reward signals for a reinforcement learning agent. VIPER enables\nexpert-level control without programmatic task rewards across a wide range of\nDMC, Atari, and RLBench tasks. Moreover, generalization of the video prediction\nmodel allows us to derive rewards for an out-of-distribution environment where\nno expert data is available, enabling cross-embodiment generalization for\ntabletop manipulation. We see our work as starting point for scalable reward\nspecification from unlabeled videos that will benefit from the rapid advances\nin generative modeling. Source code and datasets are available on the project\nwebsite: https://escontrela.me",
        "translated": "指定奖励信号，让代理人学习复杂的行为，是强化学习研究中一个长期存在的挑战。一个有前途的方法是从未标记的视频中提取行为偏好，这些视频在互联网上广泛可用。我们提出了视频预测奖励(VIPER)算法，该算法利用预先训练的视频预测模型作为强化学习的无动作奖励信号。具体来说，我们首先在专家视频上训练一个自回归变换器，然后使用视频预测可能性作为强化学习代理的奖励信号。VIPER 可以实现专家级别的控制，无需程序任务奖励，跨越广泛的 DMC、 Atari 和 RLBench 任务。此外，视频预测模型的推广使我们能够在没有专家数据可用的分布外环境中获得奖励，从而能够对桌面操作进行跨实施例的推广。我们将我们的工作视为从未标记的视频中获得可扩展奖励规范的起点，这些视频将受益于生成建模的快速发展。源代码和数据集可在项目网站下载:  https://escontrela.me"
    },
    {
        "title": "Diffusion Hyperfeatures: Searching Through Time and Space for Semantic\n  Correspondence",
        "url": "http://arxiv.org/abs/2305.14334v1",
        "pub_date": "2023-05-23",
        "summary": "Diffusion models have been shown to be capable of generating high-quality\nimages, suggesting that they could contain meaningful internal representations.\nUnfortunately, the feature maps that encode a diffusion model's internal\ninformation are spread not only over layers of the network, but also over\ndiffusion timesteps, making it challenging to extract useful descriptors. We\npropose Diffusion Hyperfeatures, a framework for consolidating multi-scale and\nmulti-timestep feature maps into per-pixel feature descriptors that can be used\nfor downstream tasks. These descriptors can be extracted for both synthetic and\nreal images using the generation and inversion processes. We evaluate the\nutility of our Diffusion Hyperfeatures on the task of semantic keypoint\ncorrespondence: our method achieves superior performance on the SPair-71k real\nimage benchmark. We also demonstrate that our method is flexible and\ntransferable: our feature aggregation network trained on the inversion features\nof real image pairs can be used on the generation features of synthetic image\npairs with unseen objects and compositions. Our code is available at\n\\url{https://diffusion-hyperfeatures.github.io}.",
        "translated": "扩散模型已被证明能够产生高质量的图像，表明它们可以包含有意义的内部表示。遗憾的是，编码扩散模型内部信息的特征映射不仅分布在网络的各个层上，而且还分布在扩散时间步长上，这使得提取有用的描述符变得非常困难。我们提出了扩散超特征，一个框架，巩固多尺度和多时间步特征映射到每像素特征描述符，可用于下游任务。这些描述符可以提取合成图像和真实图像使用生成和反演过程。我们评估扩散超特征在语义关键点对应任务中的效用: 我们的方法在 SPair-71k 真实图像基准上获得了优越的性能。实验结果表明，该方法具有灵活性和可移植性: 基于实际图像对反演特征的特征聚合网络可以用于具有不可见物体和成分的合成图像对的特征生成。我们的代码可以在 url { https://diffusion-hyperfeatures.github.io }找到。"
    },
    {
        "title": "Prototype Adaption and Projection for Few- and Zero-shot 3D Point Cloud\n  Semantic Segmentation",
        "url": "http://arxiv.org/abs/2305.14335v1",
        "pub_date": "2023-05-23",
        "summary": "In this work, we address the challenging task of few-shot and zero-shot 3D\npoint cloud semantic segmentation. The success of few-shot semantic\nsegmentation in 2D computer vision is mainly driven by the pre-training on\nlarge-scale datasets like imagenet. The feature extractor pre-trained on\nlarge-scale 2D datasets greatly helps the 2D few-shot learning. However, the\ndevelopment of 3D deep learning is hindered by the limited volume and instance\nmodality of datasets due to the significant cost of 3D data collection and\nannotation. This results in less representative features and large intra-class\nfeature variation for few-shot 3D point cloud segmentation. As a consequence,\ndirectly extending existing popular prototypical methods of 2D few-shot\nclassification/segmentation into 3D point cloud segmentation won't work as well\nas in 2D domain. To address this issue, we propose a Query-Guided Prototype\nAdaption (QGPA) module to adapt the prototype from support point clouds feature\nspace to query point clouds feature space. With such prototype adaption, we\ngreatly alleviate the issue of large feature intra-class variation in point\ncloud and significantly improve the performance of few-shot 3D segmentation.\nBesides, to enhance the representation of prototypes, we introduce a\nSelf-Reconstruction (SR) module that enables prototype to reconstruct the\nsupport mask as well as possible. Moreover, we further consider zero-shot 3D\npoint cloud semantic segmentation where there is no support sample. To this\nend, we introduce category words as semantic information and propose a\nsemantic-visual projection model to bridge the semantic and visual spaces. Our\nproposed method surpasses state-of-the-art algorithms by a considerable 7.90%\nand 14.82% under the 2-way 1-shot setting on S3DIS and ScanNet benchmarks,\nrespectively. Code is available at https://github.com/heshuting555/PAP-FZS3D.",
        "translated": "在这项工作中，我们解决了具有挑战性的任务少镜头和零镜头三维点云语义分割。二维计算机视觉中少镜头语义分割的成功主要是由对大规模数据集(如图像网)的预训练所驱动的。在大规模二维数据集上预训练的特征提取器对二维少镜头学习有很大的帮助。然而，由于三维数据的采集和注释成本较高，数据集的体积和实例形式有限，阻碍了三维深度学习的发展。这导致了少镜头三维点云分割的代表性较差的特征和较大的类内特征变化。因此，将现有流行的二维少镜头分类/分割原型方法直接扩展到三维点云分割将不能像在二维领域那样有效。针对这一问题，提出了一种基于查询引导的原型适配(QGPA)模块，将原型从支持点云特征空间调整到查询点云特征空间。采用这种原型自适应方法，大大减轻了点云中特征类内变化大的问题，显著提高了少镜头三维分割的性能。此外，为了提高原型的表示能力，我们引入了自重构(SR)模块，使原型能够尽可能地重构支撑掩模。此外，在没有支持样本的情况下，我们进一步考虑了零拍3D 点云语义分割。为此，我们引入类别词作为语义信息，并提出一个语义-视觉投影模型，以连接语义和视觉空间。在 S3DIS 和 ScanNet 基准的双向单镜头设置下，我们提出的方法分别以7.90% 和14.82% 的优势超越了最先进的算法。密码可于 https://github.com/heshuting555/pap-fzs3d 索取。"
    },
    {
        "title": "Large Language Models are Frame-level Directors for Zero-shot\n  Text-to-Video Generation",
        "url": "http://arxiv.org/abs/2305.14330v1",
        "pub_date": "2023-05-23",
        "summary": "In the paradigm of AI-generated content (AIGC), there has been increasing\nattention in extending pre-trained text-to-image (T2I) models to text-to-video\n(T2V) generation. Despite their effectiveness, these frameworks face challenges\nin maintaining consistent narratives and handling rapid shifts in scene\ncomposition or object placement from a single user prompt. This paper\nintroduces a new framework, dubbed DirecT2V, which leverages instruction-tuned\nlarge language models (LLMs) to generate frame-by-frame descriptions from a\nsingle abstract user prompt. DirecT2V utilizes LLM directors to divide user\ninputs into separate prompts for each frame, enabling the inclusion of\ntime-varying content and facilitating consistent video generation. To maintain\ntemporal consistency and prevent object collapse, we propose a novel value\nmapping method and dual-softmax filtering. Extensive experimental results\nvalidate the effectiveness of the DirecT2V framework in producing visually\ncoherent and consistent videos from abstract user prompts, addressing the\nchallenges of zero-shot video generation.",
        "translated": "在人工智能生成内容(AIGC)的范式中，将预先训练好的文本到图像(T2I)模型扩展到文本到视频(T2V)生成已经受到越来越多的关注。尽管这些框架很有效，但它们在保持一致的叙述和处理来自单一用户提示的场景构成或物体放置的快速变化方面面临挑战。本文介绍了一个名为 DirecT2V 的新框架，它利用指令调优的大型语言模型(LLM)从单个抽象用户提示符生成一帧一帧的描述。DirecT2V 利用 LLM 控制器将用户输入划分为每个帧的单独提示，从而能够包含时变内容并促进一致的视频生成。为了保持时间一致性和防止对象崩溃，提出了一种新的值映射方法和双软极大值滤波。广泛的实验结果验证了 DirecT2V 框架在从抽象用户提示生成视觉一致性和一致性视频方面的有效性，解决了零拍摄视频生成的挑战。"
    },
    {
        "title": "Improving Factuality and Reasoning in Language Models through Multiagent\n  Debate",
        "url": "http://arxiv.org/abs/2305.14325v1",
        "pub_date": "2023-05-23",
        "summary": "Large language models (LLMs) have demonstrated remarkable capabilities in\nlanguage generation, understanding, and few-shot learning in recent years. An\nextensive body of work has explored how their performance may be further\nimproved through the tools of prompting, ranging from verification,\nself-consistency, or intermediate scratchpads. In this paper, we present a\ncomplementary approach to improve language responses where multiple language\nmodel instances propose and debate their individual responses and reasoning\nprocesses over multiple rounds to arrive at a common final answer. Our findings\nindicate that this approach significantly enhances mathematical and strategic\nreasoning across a number of tasks. We also demonstrate that our approach\nimproves the factual validity of generated content, reducing fallacious answers\nand hallucinations that contemporary models are prone to. Our approach may be\ndirectly applied to existing black-box models and uses identical procedure and\nprompts for all tasks we investigate. Overall, our findings suggest that such\n\"society of minds\" approach has the potential to significantly advance the\ncapabilities of LLMs and pave the way for further breakthroughs in language\ngeneration and understanding.",
        "translated": "近年来，大语言模型(LLM)在语言生成、理解和短镜头学习等方面表现出了显著的能力。大量的工作已经探索了如何通过提示工具进一步提高他们的性能，从验证，自我一致性，或中间的暂存器。在本文中，我们提出了一个互补的方法来改善语言反应，其中多个语言模型实例提出和辩论他们的个别反应和推理过程，多轮得出一个共同的最终答案。我们的研究结果表明，这种方法显着提高了数学和战略推理的一些任务。我们还证明，我们的方法提高了生成内容的实际有效性，减少了当代模型容易产生的谬误答案和幻觉。我们的方法可以直接应用于现有的黑盒模型，并对我们调查的所有任务使用相同的过程和提示。总的来说，我们的研究结果表明，这种“心灵社会”的方法有潜力显着提高语言学习者的能力，并为语言生成和理解的进一步突破铺平道路。"
    },
    {
        "title": "Text-guided 3D Human Generation from 2D Collections",
        "url": "http://arxiv.org/abs/2305.14312v1",
        "pub_date": "2023-05-23",
        "summary": "3D human modeling has been widely used for engaging interaction in gaming,\nfilm, and animation. The customization of these characters is crucial for\ncreativity and scalability, which highlights the importance of controllability.\nIn this work, we introduce Text-guided 3D Human Generation (\\texttt{T3H}),\nwhere a model is to generate a 3D human, guided by the fashion description.\nThere are two goals: 1) the 3D human should render articulately, and 2) its\noutfit is controlled by the given text. To address this \\texttt{T3H} task, we\npropose Compositional Cross-modal Human (CCH). CCH adopts cross-modal attention\nto fuse compositional human rendering with the extracted fashion semantics.\nEach human body part perceives relevant textual guidance as its visual\npatterns. We incorporate the human prior and semantic discrimination to enhance\n3D geometry transformation and fine-grained consistency, enabling it to learn\nfrom 2D collections for data efficiency. We conduct evaluations on DeepFashion\nand SHHQ with diverse fashion attributes covering the shape, fabric, and color\nof upper and lower clothing. Extensive experiments demonstrate that CCH\nachieves superior results for \\texttt{T3H} with high efficiency.",
        "translated": "3D 人体建模已经广泛应用于游戏、电影和动画中的交互。这些字符的定制对于创造性和可扩展性至关重要，这突出了可控性的重要性。在这项工作中，我们介绍了文本引导的三维人类生成(texttt { T3H }) ，其中一个模型是生成一个三维人，在时尚描述的指导下。有两个目标: 1)3D 人应该清晰地渲染，2)它的装备是由给定的文本控制。为了解决这个文本{ T3H }任务，我们提出了组合跨模态人(CCH)。CCH 采用交叉模态注意将提取的时尚语义融合到组合人体绘制中。人体的每个部分都将相关的文本指导视为其视觉模式。我们结合了人类先验和语义识别，以增强三维几何变换和细粒度的一致性，使其能够学习从2D 集合的数据效率。我们对 DeepFashion 和 SHHQ 进行评估，它们具有多种时尚属性，包括上衣和下衣的形状、面料和颜色。大量的实验表明，CCH 对 texttt { T3H }具有较高的效率，取得了较好的效果。"
    },
    {
        "title": "Hierarchical Adaptive Voxel-guided Sampling for Real-time Applications\n  in Large-scale Point Clouds",
        "url": "http://arxiv.org/abs/2305.14306v1",
        "pub_date": "2023-05-23",
        "summary": "While point-based neural architectures have demonstrated their efficacy, the\ntime-consuming sampler currently prevents them from performing real-time\nreasoning on scene-level point clouds. Existing methods attempt to overcome\nthis issue by using random sampling strategy instead of the commonly-adopted\nfarthest point sampling~(FPS), but at the expense of lower performance. So the\neffectiveness/efficiency trade-off remains under-explored. In this paper, we\nreveal the key to high-quality sampling is ensuring an even spacing between\npoints in the subset, which can be naturally obtained through a grid. Based on\nthis insight, we propose a hierarchical adaptive voxel-guided point sampler\nwith linear complexity and high parallelization for real-time applications.\nExtensive experiments on large-scale point cloud detection and segmentation\ntasks demonstrate that our method achieves competitive performance with the\nmost powerful FPS, at an amazing speed that is more than 100 times faster. This\nbreakthrough in efficiency addresses the bottleneck of the sampling step when\nhandling scene-level point clouds. Furthermore, our sampler can be easily\nintegrated into existing models and achieves a 20$\\sim$80\\% reduction in\nruntime with minimal effort. The code will be available at\nhttps://github.com/OuyangJunyuan/pointcloud-3d-detector-tensorrt",
        "translated": "虽然基于点的神经结构已经证明了它们的功效，但耗时的采样器目前阻止它们对场景级别的点云进行实时推理。现有的方法试图用随机抽样策略代替常用的最远点抽样，但是以牺牲较低的性能为代价来克服这一问题。因此，有效性/效率之间的权衡仍未得到充分探讨。在本文中，我们揭示了高质量采样的关键是确保子集中点之间的均匀间距，这可以通过网格自然获得。在此基础上，我们提出了一种分层自适应体素引导的点采样器，它具有线性复杂度和高并行性，适用于实时应用。大规模点云检测和分割任务的大量实验表明，我们的方法实现了与最强大的 FPS 具有竞争力的性能，以惊人的速度，超过100倍的速度。这一效率上的突破解决了处理场景级点云时采样步骤的瓶颈。此外，我们的采样器可以很容易地集成到现有的模型，并实现20美元西姆 $80% 的运行时减少最小的努力。代码将在 https://github.com/ouyangjunyuan/pointcloud-3d-detector-tensorrt 公布"
    },
    {
        "title": "A Laplacian Pyramid Based Generative H&amp;E Stain Augmentation Network",
        "url": "http://arxiv.org/abs/2305.14301v1",
        "pub_date": "2023-05-23",
        "summary": "Hematoxylin and Eosin (H&amp;E) staining is a widely used sample preparation\nprocedure for enhancing the saturation of tissue sections and the contrast\nbetween nuclei and cytoplasm in histology images for medical diagnostics.\nHowever, various factors, such as the differences in the reagents used, result\nin high variability in the colors of the stains actually recorded. This\nvariability poses a challenge in achieving generalization for machine-learning\nbased computer-aided diagnostic tools. To desensitize the learned models to\nstain variations, we propose the Generative Stain Augmentation Network (G-SAN)\n-- a GAN-based framework that augments a collection of cell images with\nsimulated yet realistic stain variations. At its core, G-SAN uses a novel and\nhighly computationally efficient Laplacian Pyramid (LP) based generator\narchitecture, that is capable of disentangling stain from cell morphology.\nThrough the task of patch classification and nucleus segmentation, we show that\nusing G-SAN-augmented training data provides on average 15.7% improvement in F1\nscore and 7.3% improvement in panoptic quality, respectively. Our code is\navailable at https://github.com/lifangda01/GSAN-Demo.",
        "translated": "苏木精-伊红(H & E)染色是一种广泛应用的提高组织切片饱和度和增强组织学图像中细胞核和细胞质对比度的样品制备方法。然而，各种因素，例如使用的试剂的差异，导致实际记录的污渍颜色的高度可变性。这种可变性对基于机器学习的计算机辅助诊断工具的普及提出了挑战。为了使学习的模型对染色变化不敏感，我们提出生成染色增强网络(G-SAN)——一种基于 GAN 的框架，用模拟但真实的染色变化增强细胞图像集合。在其核心，G-SAN 使用了一种新的和高度计算效率的拉普拉斯金字塔(LP)为基础的生成器架构，这是能够从细胞形态学分离染色。通过斑块分类和细胞核分割实验，我们发现使用 G-SAN 增强训练数据，F1评分平均提高15.7% ，视觉质量平均提高7.3% 。我们的代码可以在 https://github.com/lifangda01/gsan-demo 找到。"
    },
    {
        "title": "Balancing the Picture: Debiasing Vision-Language Datasets with Synthetic\n  Contrast Sets",
        "url": "http://arxiv.org/abs/2305.15407v1",
        "pub_date": "2023-05-24",
        "summary": "Vision-language models are growing in popularity and public visibility to\ngenerate, edit, and caption images at scale; but their outputs can perpetuate\nand amplify societal biases learned during pre-training on uncurated image-text\npairs from the internet. Although debiasing methods have been proposed, we\nargue that these measurements of model bias lack validity due to dataset bias.\nWe demonstrate there are spurious correlations in COCO Captions, the most\ncommonly used dataset for evaluating bias, between background context and the\ngender of people in-situ. This is problematic because commonly-used bias\nmetrics (such as Bias@K) rely on per-gender base rates. To address this issue,\nwe propose a novel dataset debiasing pipeline to augment the COCO dataset with\nsynthetic, gender-balanced contrast sets, where only the gender of the subject\nis edited and the background is fixed. However, existing image editing methods\nhave limitations and sometimes produce low-quality images; so, we introduce a\nmethod to automatically filter the generated images based on their similarity\nto real images. Using our balanced synthetic contrast sets, we benchmark bias\nin multiple CLIP-based models, demonstrating how metrics are skewed by\nimbalance in the original COCO images. Our results indicate that the proposed\napproach improves the validity of the evaluation, ultimately contributing to\nmore realistic understanding of bias in vision-language models.",
        "translated": "视觉语言模型在大规模生成、编辑和标题图像方面越来越受欢迎和公众可见度越来越高; 但是它们的输出可以延续和放大在互联网上未经策划的图像-文本对的预培训中学到的社会偏见。虽然已经提出了消除偏差的方法，但是我们认为由于数据集的偏差，这些模型偏差的测量缺乏有效性。我们证明在 COCO 标题中存在虚假的相关性，COCO 标题是最常用于评估偏倚的数据集，背景环境和现场人员的性别之间存在虚假的相关性。这是有问题的，因为常用的偏见指标(如偏见@K)依赖于每个性别的基础比率。为了解决这一问题，我们提出了一种新的数据集去偏流水线，以增加合成的，性别平衡的对比集 COCO 数据集，其中只编辑主题的性别和背景是固定的。然而，现有的图像编辑方法存在一定的局限性，有时会产生低质量的图像，因此，我们提出了一种基于图像与真实图像相似度的自动过滤方法。使用我们的平衡合成对比度集，我们在多个基于 CLIP 的模型基准偏差，演示了如何在原始 COCO 图像的不平衡度量偏斜。我们的研究结果表明，提出的方法提高了评价的有效性，最终有助于更现实的理解偏见的视觉语言模型。"
    },
    {
        "title": "RoMa: Revisiting Robust Losses for Dense Feature Matching",
        "url": "http://arxiv.org/abs/2305.15404v1",
        "pub_date": "2023-05-24",
        "summary": "Dense feature matching is an important computer vision task that involves\nestimating all correspondences between two images of a 3D scene. In this paper,\nwe revisit robust losses for matching from a Markov chain perspective, yielding\ntheoretical insights and large gains in performance. We begin by constructing a\nunifying formulation of matching as a Markov chain, based on which we identify\ntwo key stages which we argue should be decoupled for matching. The first is\nthe coarse stage, where the estimated result needs to be globally consistent.\nThe second is the refinement stage, where the model needs precise localization\ncapabilities. Inspired by the insight that these stages concern distinct\nissues, we propose a coarse matcher following the regression-by-classification\nparadigm that provides excellent globally consistent, albeit not exactly\nlocalized, matches. This is followed by a local feature refinement stage using\nwell-motivated robust regression losses, yielding extremely precise matches.\nOur proposed approach, which we call RoMa, achieves significant improvements\ncompared to the state-of-the-art. Code is available at\nhttps://github.com/Parskatt/RoMa",
        "translated": "密集特征匹配是一项重要的计算机视觉任务，它涉及到估计三维场景中两幅图像之间的所有对应关系。在本文中，我们从马尔可夫链的角度重新审视匹配的鲁棒性损失，产生理论见解和性能的大幅提高。我们首先构造一个统一的匹配公式作为一个马尔可夫链，在此基础上，我们确定了两个关键的阶段，我们认为应该解耦匹配。第一个阶段是粗略阶段，其中估计的结果需要具有全局一致性。第二个阶段是精化阶段，在这个阶段模型需要精确的定位能力。受到这些阶段关注不同问题的洞察力的启发，我们提出了遵循分类回归范式的粗匹配器，它提供了优秀的全局一致性匹配，尽管不是完全本地化的匹配。然后是局部特征细化阶段，使用动机良好的鲁棒回归损失，产生非常精确的匹配。我们提出的方法，我们称之为 RoMa，与最先进的技术相比，取得了显著的改进。密码可于 https://github.com/parskatt/roma 索取"
    },
    {
        "title": "Sin3DM: Learning a Diffusion Model from a Single 3D Textured Shape",
        "url": "http://arxiv.org/abs/2305.15399v1",
        "pub_date": "2023-05-24",
        "summary": "Synthesizing novel 3D models that resemble the input example has long been\npursued by researchers and artists in computer graphics. In this paper, we\npresent Sin3DM, a diffusion model that learns the internal patch distribution\nfrom a single 3D textured shape and generates high-quality variations with fine\ngeometry and texture details. Training a diffusion model directly in 3D would\ninduce large memory and computational cost. Therefore, we first compress the\ninput into a lower-dimensional latent space and then train a diffusion model on\nit. Specifically, we encode the input 3D textured shape into triplane feature\nmaps that represent the signed distance and texture fields of the input. The\ndenoising network of our diffusion model has a limited receptive field to avoid\noverfitting, and uses triplane-aware 2D convolution blocks to improve the\nresult quality. Aside from randomly generating new samples, our model also\nfacilitates applications such as retargeting, outpainting and local editing.\nThrough extensive qualitative and quantitative evaluation, we show that our\nmodel can generate 3D shapes of various types with better quality than prior\nmethods.",
        "translated": "长期以来，计算机图形学的研究人员和艺术家一直致力于合成类似于输入样本的新颖3D 模型。本文提出了一种扩散模型 Sin3DM，该模型从单个三维纹理形状中学习内部斑块分布，并生成具有精细几何和纹理细节的高质量变化。直接在三维空间中训练扩散模型会产生大量的内存和计算开销。因此，我们首先将输入压缩到一个低维的潜在空间，然后在其上训练一个扩散模型。具体来说，我们将输入的三维纹理形状编码成三平面特征映射，表示输入的有符号距离和纹理字段。为了避免过拟合，扩散模型的去噪网络具有有限的接收域，并且使用了三平面感知的二维卷积块来提高结果的质量。除了随机产生新的样本，我们的模型还促进应用程序，如重定向，外绘和本地编辑。通过广泛的定性和定量评价，我们表明我们的模型可以生成各种类型的三维形状，比以往的方法更好的质量。"
    },
    {
        "title": "LayoutGPT: Compositional Visual Planning and Generation with Large\n  Language Models",
        "url": "http://arxiv.org/abs/2305.15393v1",
        "pub_date": "2023-05-24",
        "summary": "Attaining a high degree of user controllability in visual generation often\nrequires intricate, fine-grained inputs like layouts. However, such inputs\nimpose a substantial burden on users when compared to simple text inputs. To\naddress the issue, we study how Large Language Models (LLMs) can serve as\nvisual planners by generating layouts from text conditions, and thus\ncollaborate with visual generative models. We propose LayoutGPT, a method to\ncompose in-context visual demonstrations in style sheet language to enhance the\nvisual planning skills of LLMs. LayoutGPT can generate plausible layouts in\nmultiple domains, ranging from 2D images to 3D indoor scenes. LayoutGPT also\nshows superior performance in converting challenging language concepts like\nnumerical and spatial relations to layout arrangements for faithful\ntext-to-image generation. When combined with a downstream image generation\nmodel, LayoutGPT outperforms text-to-image models/systems by 20-40% and\nachieves comparable performance as human users in designing visual layouts for\nnumerical and spatial correctness. Lastly, LayoutGPT achieves comparable\nperformance to supervised methods in 3D indoor scene synthesis, demonstrating\nits effectiveness and potential in multiple visual domains.",
        "translated": "在可视化生成中获得高度的用户可控性通常需要复杂的、细粒度的输入，如布局。然而，与简单的文本输入相比，这种输入对用户造成了相当大的负担。为了解决这个问题，我们研究了大语言模型(LLM)如何通过根据文本条件生成布局来充当可视化规划器，从而与可视化生成模型进行协作。我们提出了 LayoutGPT，一种用样式表语言组合上下文视觉演示的方法，以提高 LLM 的视觉规划技巧。LayoutGPT 可以在多个领域生成合理的布局，从2D 图像到3D 室内场景。LayoutGPT 在将具有挑战性的语言概念(如数值和空间关系)转换为可靠的文本到图像生成的布局安排方面也表现出优越的性能。结合下游图像生成模型，LayoutGPT 的性能比文本到图像的模型/系统高出20-40% ，并且在数值和空间正确性的可视化布局设计方面达到与人类用户相当的性能。最后，LayoutGPT 在三维室内场景合成中实现了与监督方法相当的性能，证明了其在多视觉领域中的有效性和潜力。"
    },
    {
        "title": "A Neural Space-Time Representation for Text-to-Image Personalization",
        "url": "http://arxiv.org/abs/2305.15391v1",
        "pub_date": "2023-05-24",
        "summary": "A key aspect of text-to-image personalization methods is the manner in which\nthe target concept is represented within the generative process. This choice\ngreatly affects the visual fidelity, downstream editability, and disk space\nneeded to store the learned concept. In this paper, we explore a new\ntext-conditioning space that is dependent on both the denoising process\ntimestep (time) and the denoising U-Net layers (space) and showcase its\ncompelling properties. A single concept in the space-time representation is\ncomposed of hundreds of vectors, one for each combination of time and space,\nmaking this space challenging to optimize directly. Instead, we propose to\nimplicitly represent a concept in this space by optimizing a small neural\nmapper that receives the current time and space parameters and outputs the\nmatching token embedding. In doing so, the entire personalized concept is\nrepresented by the parameters of the learned mapper, resulting in a compact,\nyet expressive, representation. Similarly to other personalization methods, the\noutput of our neural mapper resides in the input space of the text encoder. We\nobserve that one can significantly improve the convergence and visual fidelity\nof the concept by introducing a textual bypass, where our neural mapper\nadditionally outputs a residual that is added to the output of the text\nencoder. Finally, we show how one can impose an importance-based ordering over\nour implicit representation, providing users control over the reconstruction\nand editability of the learned concept using a single trained model. We\ndemonstrate the effectiveness of our approach over a range of concepts and\nprompts, showing our method's ability to generate high-quality and controllable\ncompositions without fine-tuning any parameters of the generative model itself.",
        "translated": "文本到图像个性化方法的一个关键方面是目标概念在生成过程中的表示方式。这种选择极大地影响了视觉保真度、下游可编辑性和存储所学概念所需的磁盘空间。在本文中，我们探索了一个新的文本条件空间，它同时依赖于去噪过程的时间步长(时间)和去噪的 U-Net 层(空间) ，并展示了其引人注目的性质。时空表示中的一个概念由数百个向量组成，每个向量对应于时间和空间的组合，这使得直接优化这个空间具有挑战性。相反，我们建议通过优化一个接收当前时间和空间参数并输出匹配令牌嵌入的小型神经映射器来隐式表示这个空间中的一个概念。在这样做时，整个个性化的概念是由学习映射器的参数表示，导致一个紧凑，但表达，表示。与其他个性化方法类似，我们的神经映射器的输出驻留在文本编码器的输入空间中。我们观察到，通过引入文本旁路，可以显著提高概念的收敛性和视觉保真度，其中我们的神经映射器额外输出一个残差，添加到文本编码器的输出。最后，我们展示了如何在我们的隐式表示上强加一个基于重要性的排序，使用一个单一的训练模型为用户提供对所学概念的重构和可编辑性的控制。我们通过一系列的概念和提示展示了我们的方法的有效性，展示了我们的方法能够生成高质量和可控的合成物，而不需要对生成模型本身的任何参数进行微调。"
    },
    {
        "title": "What can generic neural networks learn from a child's visual experience?",
        "url": "http://arxiv.org/abs/2305.15372v1",
        "pub_date": "2023-05-24",
        "summary": "Young children develop sophisticated internal models of the world based on\ntheir egocentric visual experience. How much of this is driven by innate\nconstraints and how much is driven by their experience? To investigate these\nquestions, we train state-of-the-art neural networks on a realistic proxy of a\nchild's visual experience without any explicit supervision or domain-specific\ninductive biases. Specifically, we train both embedding models and generative\nmodels on 200 hours of headcam video from a single child collected over two\nyears. We train a total of 72 different models, exploring a range of model\narchitectures and self-supervised learning algorithms, and comprehensively\nevaluate their performance in downstream tasks. The best embedding models\nperform at 70% of a highly performant ImageNet-trained model on average. They\nalso learn broad semantic categories without any labeled examples and learn to\nlocalize semantic categories in an image without any location supervision.\nHowever, these models are less object-centric and more background-sensitive\nthan comparable ImageNet-trained models. Generative models trained with the\nsame data successfully extrapolate simple properties of partially masked\nobjects, such as their texture, color, orientation, and rough outline, but\nstruggle with finer object details. We replicate our experiments with two other\nchildren and find very similar results. Broadly useful high-level visual\nrepresentations are thus robustly learnable from a representative sample of a\nchild's visual experience without strong inductive biases.",
        "translated": "幼儿根据自我中心的视觉经验，发展出复杂的内在世界模型。这其中有多少是由先天约束驱动的，又有多少是由他们的经验驱动的？为了研究这些问题，我们训练最先进的神经网络，在没有任何明确的监督或领域特定的归纳偏见的情况下，对儿童的视觉经验进行现实的替代。具体来说，我们训练嵌入模型和生成模型的200个小时的头部摄像头视频从一个孩子收集了两年多。我们总共训练了72个不同的模型，探索了一系列模型结构和自我监督学习算法，并全面评估了它们在下游任务中的表现。最好的嵌入模型在高性能 ImageNet 训练模型中的平均执行率为70% 。他们还学习广泛的语义类别没有任何标记的例子和学习本地化的语义类别在一个图像没有任何位置监督。然而，与可比较的 ImageNet 训练模型相比，这些模型更少的以对象为中心，更多的是对背景敏感。使用相同数据训练的生成模型成功地推断出部分遮蔽对象的简单属性，例如它们的纹理、颜色、方向和粗略轮廓，但是难以获得更精细的对象细节。我们在另外两个孩子身上做了同样的实验，得到了非常相似的结果。因此，广泛有用的高层次视觉表征可以从一个具有代表性的儿童视觉经验样本中强有力地学习，而没有强烈的归纳偏见。"
    },
    {
        "title": "SAMScore: A Semantic Structural Similarity Metric for Image Translation\n  Evaluation",
        "url": "http://arxiv.org/abs/2305.15367v1",
        "pub_date": "2023-05-24",
        "summary": "Image translation has wide applications, such as style transfer and modality\nconversion, usually aiming to generate images having both high degrees of\nrealism and faithfulness. These problems remain difficult, especially when it\nis important to preserve semantic structures. Traditional image-level\nsimilarity metrics are of limited use, since the semantics of an image are\nhigh-level, and not strongly governed by pixel-wise faithfulness to an original\nimage. Towards filling this gap, we introduce SAMScore, a generic semantic\nstructural similarity metric for evaluating the faithfulness of image\ntranslation models. SAMScore is based on the recent high-performance Segment\nAnything Model (SAM), which can perform semantic similarity comparisons with\nstandout accuracy. We applied SAMScore on 19 image translation tasks, and found\nthat it is able to outperform all other competitive metrics on all of the\ntasks. We envision that SAMScore will prove to be a valuable tool that will\nhelp to drive the vibrant field of image translation, by allowing for more\nprecise evaluations of new and evolving translation models. The code is\navailable at https://github.com/Kent0n-Li/SAMScore.",
        "translated": "意象翻译在文体转换、情态转换等方面有着广泛的应用，通常意象翻译的目的是生成逼真度高、忠实度高的意象。这些问题仍然很难解决，特别是在保护语义结构很重要的情况下。传统的图像级相似度指标的用途有限，因为图像的语义是高级的，并且不受像素级对原始图像的忠实度的强烈支配。为了填补这个空白，我们引入了 SAMScore，这是一个通用的语义结构相似性指标，用于评估图像翻译模型的可靠性。SAMScore 是基于最新的高性能分段任意模型(Segment AnyModel，SAM) ，它可以执行语义相似度比较，具有突出的准确性。我们将 SAMScore 应用于19个图像翻译任务，发现它在所有任务中的表现都优于其他竞争指标。我们设想 SAMScore 将被证明是一个有价值的工具，通过允许对新的和不断发展的翻译模式进行更精确的评估，将有助于推动图像翻译这一充满活力的领域。密码可在 https://github.com/kent0n-li/samscore 查阅。"
    },
    {
        "title": "Boundary Attention Mapping (BAM): Fine-grained saliency maps for\n  segmentation of Burn Injuries",
        "url": "http://arxiv.org/abs/2305.15365v1",
        "pub_date": "2023-05-24",
        "summary": "Burn injuries can result from mechanisms such as thermal, chemical, and\nelectrical insults. A prompt and accurate assessment of burns is essential for\ndeciding definitive clinical treatments. Currently, the primary approach for\nburn assessments, via visual and tactile observations, is approximately 60%-80%\naccurate. The gold standard is biopsy and a close second would be non-invasive\nmethods like Laser Doppler Imaging (LDI) assessments, which have up to 97%\naccuracy in predicting burn severity and the required healing time. In this\npaper, we introduce a machine learning pipeline for assessing burn severities\nand segmenting the regions of skin that are affected by burn. Segmenting 2D\ncolour images of burns allows for the injured versus non-injured skin to be\ndelineated, clearly marking the extent and boundaries of the localized\nburn/region-of-interest, even during remote monitoring of a burn patient. We\ntrained a convolutional neural network (CNN) to classify four severities of\nburns. We built a saliency mapping method, Boundary Attention Mapping (BAM),\nthat utilises this trained CNN for the purpose of accurately localizing and\nsegmenting the burn regions from skin burn images. We demonstrated the\neffectiveness of our proposed pipeline through extensive experiments and\nevaluations using two datasets; 1) A larger skin burn image dataset consisting\nof 1684 skin burn images of four burn severities, 2) An LDI dataset that\nconsists of a total of 184 skin burn images with their associated LDI scans.\nThe CNN trained using the first dataset achieved an average F1-Score of 78% and\nmicro/macro- average ROC of 85% in classifying the four burn severities.\nMoreover, a comparison between the BAM results and LDI results for measuring\ninjury boundary showed that the segmentations generated by our method achieved\n91.60% accuracy, 78.17% sensitivity, and 93.37% specificity.",
        "translated": "烧伤可能是由热、化学和电气等机制造成的。及时和准确的评估烧伤是决定明确的临床治疗是必不可少的。目前，烧伤评估的主要方法，通过视觉和触觉观察，大约60% -80% 的准确率。黄金标准是活组织检查，紧随其后的是非侵入性方法，如激光多普勒成像(LDI)评估，它在预测烧伤严重程度和所需的愈合时间方面有高达97% 的准确性。在本文中，我们介绍了一个机器学习管道，用于评估烧伤的严重程度和分割皮肤受烧伤影响的区域。分割烧伤的二维彩色图像允许描绘受伤与未受伤的皮肤，即使在远程监测烧伤患者期间，也清楚地标记局部烧伤/感兴趣区域的范围和边界。我们训练了一个卷积神经网络(CNN)来分类烧伤的四种严重程度。我们建立了一个突出映射方法，边界注意映射(BAM) ，利用这个训练的 CNN 的目的是准确定位和分割烧伤区域从皮肤烧伤图像。我们通过使用两个数据集进行广泛的实验和评估，证明了我们提出的管道的有效性; 1)由1684个四种烧伤严重程度的皮肤烧伤图像组成的更大的皮肤烧伤图像数据集，2)总共由184个皮肤烧伤图像及其相关的 LDI 扫描组成的 LDI 数据集。使用第一个数据集训练的美国有线电视新闻网在对四种烧伤严重程度进行分类时，平均达到了78% 的 f1-得分和85% 的微观/宏观平均 ROC。此外，测量损伤边界的 BAM 结果与 LDI 结果之间的比较显示，由我们的方法产生的分割达到91.60% 的准确性，78.17% 的灵敏度和93.37% 的特异性。"
    },
    {
        "title": "Solving Diffusion ODEs with Optimal Boundary Conditions for Better Image\n  Super-Resolution",
        "url": "http://arxiv.org/abs/2305.15357v1",
        "pub_date": "2023-05-24",
        "summary": "Diffusion models, as a kind of powerful generative model, have given\nimpressive results on image super-resolution (SR) tasks. However, due to the\nrandomness introduced in the reverse process of diffusion models, the\nperformances of diffusion-based SR models are fluctuating at every time of\nsampling, especially for samplers with few resampled steps. This inherent\nrandomness of diffusion models results in ineffectiveness and instability,\nmaking it challenging for users to guarantee the quality of SR results.\nHowever, our work takes this randomness as an opportunity: fully analyzing and\nleveraging it leads to the construction of an effective plug-and-play sampling\nmethod that owns the potential to benefit a series of diffusion-based SR\nmethods. More in detail, we propose to steadily sample high-quality SR images\nfrom pretrained diffusion-based SR models by solving diffusion ordinary\ndifferential equations (diffusion ODEs) with optimal boundary conditions (BCs)\nand analyze the characteristics between the choices of BCs and their\ncorresponding SR results. Our analysis shows the route to obtain an\napproximately optimal BC via an efficient exploration in the whole space. The\nquality of SR results sampled by the proposed method with fewer steps\noutperforms the quality of results sampled by current methods with randomness\nfrom the same pretrained diffusion-based SR model, which means that our\nsampling method ``boosts'' current diffusion-based SR models without any\nadditional training.",
        "translated": "扩散模型作为一种强大的生成模型，在图像超分辨率(SR)任务中给出了令人印象深刻的结果。然而，由于扩散模型反向过程的随机性，基于扩散的 SR 模型在每次采样时的性能都会发生波动，特别是对于重采样步数较少的采样者。这种扩散模型固有的随机性导致无效性和不稳定性，使得用户难以保证 SR 结果的质量。然而，我们的工作把这种随机性作为一个机会: 充分分析和利用它导致建立一个有效的即插即用的抽样方法，拥有受益于一系列基于扩散的 SR 方法的潜力。具体来说，我们提出了通过求解具有最优边界条件的扩散常微分方程，从基于扩散的预训练 SR 模型中稳定地采集高质量的 SR 图像，并分析了扩散常微分方程的选择与其相应 SR 结果之间的特点。我们的分析表明，通过在整个空间进行有效的探索，可以获得一个近似最优 BC 的路径。本文提出的方法采样的 SR 结果的质量较少的步骤优于目前的方法采样的结果的质量与随机性从相同的预先训练的扩散为基础的 SR 模型，这意味着我们的采样方法“推动”目前的扩散为基础的 SR 模型没有任何额外的训练。"
    },
    {
        "title": "Mitigating Biased Activation in Weakly-supervised Object Localization\n  via Counterfactual Learning",
        "url": "http://arxiv.org/abs/2305.15354v1",
        "pub_date": "2023-05-24",
        "summary": "In this paper, we focus on an under-explored issue of biased activation in\nprior weakly-supervised object localization methods based on Class Activation\nMapping (CAM). We analyze the cause of this problem from a causal view and\nattribute it to the co-occurring background confounders. Following this\ninsight, we propose a novel Counterfactual Co-occurring Learning (CCL) paradigm\nto synthesize the counterfactual representations via coupling constant\nforeground and unrealized backgrounds in order to cut off their co-occurring\nrelationship. Specifically, we design a new network structure called\nCounterfactual-CAM, which embeds the counterfactual representation perturbation\nmechanism into the vanilla CAM-based model. This mechanism is responsible for\ndecoupling foreground as well as background and synthesizing the counterfactual\nrepresentations. By training the detection model with these synthesized\nrepresentations, we compel the model to focus on the constant foreground\ncontent while minimizing the influence of distracting co-occurring background.\nTo our best knowledge, it is the first attempt in this direction. Extensive\nexperiments on several benchmarks demonstrate that Counterfactual-CAM\nsuccessfully mitigates the biased activation problem, achieving improved object\nlocalization accuracy.",
        "translated": "本文研究了基于类激活映射(CAM)的弱监督目标定位方法中存在的偏向激活问题。我们从因果关系的角度分析了这一问题的原因，并将其归因于共同发生的背景混杂因素。根据这一观点，我们提出了一种新的反事实共现学习范式，通过耦合常数前景和未实现背景来综合反事实表征，以切断它们之间的共现关系。具体来说，我们设计了一种新的网络结构，称为反事实-CAM，它将反事实表示扰动机制嵌入到普通的基于 CAM 的模型中。该机制负责解耦前景和背景，并综合反事实表示。通过训练这些综合表征的检测模型，我们迫使模型集中在恒定的前景内容，同时尽量减少干扰共现背景的影响。据我们所知，这是朝这个方向的第一次尝试。在几个基准上的大量实验表明，反事实 CAM 成功地缓解了有偏激活问题，提高了目标定位精度。"
    },
    {
        "title": "Uni-ControlNet: All-in-One Control to Text-to-Image Diffusion Models",
        "url": "http://arxiv.org/abs/2305.16322v1",
        "pub_date": "2023-05-25",
        "summary": "Text-to-Image diffusion models have made tremendous progress over the past\ntwo years, enabling the generation of highly realistic images based on\nopen-domain text descriptions. However, despite their success, text\ndescriptions often struggle to adequately convey detailed controls, even when\ncomposed of long and complex texts. Moreover, recent studies have also shown\nthat these models face challenges in understanding such complex texts and\ngenerating the corresponding images. Therefore, there is a growing need to\nenable more control modes beyond text description. In this paper, we introduce\nUni-ControlNet, a novel approach that allows for the simultaneous utilization\nof different local controls (e.g., edge maps, depth map, segmentation masks)\nand global controls (e.g., CLIP image embeddings) in a flexible and composable\nmanner within one model. Unlike existing methods, Uni-ControlNet only requires\nthe fine-tuning of two additional adapters upon frozen pre-trained\ntext-to-image diffusion models, eliminating the huge cost of training from\nscratch. Moreover, thanks to some dedicated adapter designs, Uni-ControlNet\nonly necessitates a constant number (i.e., 2) of adapters, regardless of the\nnumber of local or global controls used. This not only reduces the fine-tuning\ncosts and model size, making it more suitable for real-world deployment, but\nalso facilitate composability of different conditions. Through both\nquantitative and qualitative comparisons, Uni-ControlNet demonstrates its\nsuperiority over existing methods in terms of controllability, generation\nquality and composability. Code is available at\n\\url{https://github.com/ShihaoZhaoZSH/Uni-ControlNet}.",
        "translated": "在过去的两年中，文本到图像的扩散模型取得了巨大的进展，使得基于开放域文本描述的高度真实感图像的生成成为可能。然而，尽管成功，文本描述往往难以充分传达详细的控制，即使是组成了长期和复杂的文本。此外，最近的研究也表明，这些模型在理解这些复杂的文本和生成相应的图像面临挑战。因此，越来越需要在文本描述之外启用更多的控制模式。在本文中，我们介绍了 Uni-ControlNet，一种新的方法，允许同时利用不同的局部控件(例如，边缘映射，深度映射，分割掩码)和全局控件(例如，CLIP 图像嵌入)在一个灵活和可组合的方式在一个模型。与现有的方法不同，Uni-ControlNet 只需要在冻结的预先训练的文本到图像扩散模型上对另外两个适配器进行微调，从而消除了从头开始训练的巨大成本。此外，由于一些专用的适配器设计，Uni-ControlNet 只需要一个常数(即2)的适配器，而不管所使用的本地或全局控件的数量。这不仅降低了微调成本和模型大小，使其更适合于实际部署，而且还促进了不同条件的可组合性。通过定量和定性比较，Uni-ControlNet 在可控性、生成质量和可组合性等方面均优于现有方法。代码可在网址{ https://github.com/shihaozhaozsh/uni-controlnet }下载。"
    },
    {
        "title": "Eclipse: Disambiguating Illumination and Materials using Unintended\n  Shadows",
        "url": "http://arxiv.org/abs/2305.16321v1",
        "pub_date": "2023-05-25",
        "summary": "Decomposing an object's appearance into representations of its materials and\nthe surrounding illumination is difficult, even when the object's 3D shape is\nknown beforehand. This problem is ill-conditioned because diffuse materials\nseverely blur incoming light, and is ill-posed because diffuse materials under\nhigh-frequency lighting can be indistinguishable from shiny materials under\nlow-frequency lighting. We show that it is possible to recover precise\nmaterials and illumination -- even from diffuse objects -- by exploiting\nunintended shadows, like the ones cast onto an object by the photographer who\nmoves around it. These shadows are a nuisance in most previous inverse\nrendering pipelines, but here we exploit them as signals that improve\nconditioning and help resolve material-lighting ambiguities. We present a\nmethod based on differentiable Monte Carlo ray tracing that uses images of an\nobject to jointly recover its spatially-varying materials, the surrounding\nillumination environment, and the shapes of the unseen light occluders who\ninadvertently cast shadows upon it.",
        "translated": "分解一个物体的外观到其材料和周围照明的表示是困难的，即使当物体的三维形状是已知的。这个问题是病态的，因为漫反射材料严重模糊入射光，而且是病态的，因为在高频照明下漫反射材料可以与低频照明下发光材料难以区分。我们展示了通过利用意想不到的阴影(如摄影师在物体周围移动时投射到物体上的阴影)来恢复精确的材料和照明——即使是从漫反射的物体上也是可能的。这些阴影在大多数以前的反向渲染管道中是一个麻烦，但在这里我们利用它们作为改善条件反射和帮助解决材质-照明模糊的信号。我们提出了一种基于可微蒙特卡罗射线追踪的方法，该方法利用一个物体的图像来共同恢复其空间变化的材料，周围的照明环境，以及无意中在其上投射阴影的看不见的光遮挡物的形状。"
    },
    {
        "title": "Image is First-order Norm+Linear Autoregressive",
        "url": "http://arxiv.org/abs/2305.16319v1",
        "pub_date": "2023-05-25",
        "summary": "This paper reveals that every image can be understood as a first-order\nnorm+linear autoregressive process, referred to as FINOLA, where norm+linear\ndenotes the use of normalization before the linear model. We demonstrate that\nimages of size 256$\\times$256 can be reconstructed from a compressed vector\nusing autoregression up to a 16$\\times$16 feature map, followed by upsampling\nand convolution. This discovery sheds light on the underlying partial\ndifferential equations (PDEs) governing the latent feature space. Additionally,\nwe investigate the application of FINOLA for self-supervised learning through a\nsimple masked prediction technique. By encoding a single unmasked quadrant\nblock, we can autoregressively predict the surrounding masked region.\nRemarkably, this pre-trained representation proves effective for image\nclassification and object detection tasks, even in lightweight networks,\nwithout requiring fine-tuning. The code will be made publicly available.",
        "translated": "本文揭示了每幅图像都可以理解为一阶范数 + 线性自回归过程，称为 FINOLA，其中范数 + 线性表示在线性模型之前使用归一化。我们证明了大小为256美元乘以256美元的图像可以从一个压缩向量重建使用自回归高达16美元乘以16美元的特征映射，然后上采样和卷积。这一发现揭示了控制潜在特征空间的基本偏微分方程(PDE)。此外，我们还通过一个简单的掩蔽预测技术研究了 FINOLA 在自监督学习中的应用。通过编码一个未遮蔽的象限块，我们可以自回归地预测周围的遮蔽区域。值得注意的是，这种预先训练的表示被证明对图像分类和目标检测任务非常有效，即使在轻量级网络中，也不需要进行微调。代码将公开发布。"
    },
    {
        "title": "Referred by Multi-Modality: A Unified Temporal Transformer for Video\n  Object Segmentation",
        "url": "http://arxiv.org/abs/2305.16318v1",
        "pub_date": "2023-05-25",
        "summary": "Recently, video object segmentation (VOS) referred by multi-modal signals,\ne.g., language and audio, has evoked increasing attention in both industry and\nacademia. It is challenging for exploring the semantic alignment within\nmodalities and the visual correspondence across frames. However, existing\nmethods adopt separate network architectures for different modalities, and\nneglect the inter-frame temporal interaction with references. In this paper, we\npropose MUTR, a Multi-modal Unified Temporal transformer for Referring video\nobject segmentation. With a unified framework for the first time, MUTR adopts a\nDETR-style transformer and is capable of segmenting video objects designated by\neither text or audio reference. Specifically, we introduce two strategies to\nfully explore the temporal relations between videos and multi-modal signals.\nFirstly, for low-level temporal aggregation before the transformer, we enable\nthe multi-modal references to capture multi-scale visual cues from consecutive\nvideo frames. This effectively endows the text or audio signals with temporal\nknowledge and boosts the semantic alignment between modalities. Secondly, for\nhigh-level temporal interaction after the transformer, we conduct inter-frame\nfeature communication for different object embeddings, contributing to better\nobject-wise correspondence for tracking along the video. On Ref-YouTube-VOS and\nAVSBench datasets with respective text and audio references, MUTR achieves\n+4.2% and +4.2% J&amp;F improvements to state-of-the-art methods, demonstrating our\nsignificance for unified multi-modal VOS. Code is released at\nhttps://github.com/OpenGVLab/MUTR.",
        "translated": "近年来，基于语言、音频等多模态信号的视频对象分割技术引起了业界和学术界的广泛关注。这对于探索模式内的语义对齐和跨框架的视觉对应是一个挑战。然而，现有的方法针对不同的模式采用不同的网络结构，而忽略了帧间与参考文献的时间交互。本文提出了一种多模态统一时态转换器 MUTR，用于参考视频对象分割。MUTR 首次采用了统一的框架，采用了 DETR 风格的变换器，能够对文本或音频参考指定的视频对象进行分割。具体来说，我们引入了两种策略来充分探索视频和多模态信号之间的时间关系。首先，对于转换前的低级时间聚合，我们使多模态参考能够从连续的视频帧中捕获多尺度的视觉线索。这有效地赋予了文本或音频信号时间知识，并提高了形态之间的语义对齐。其次，对于变压器后的高层次时间交互，针对不同的目标嵌入进行帧间特征通信，有助于提高视频跟踪的目标对应性。在 Ref-YouTube-VOS 和 AVSBench 数据集上，各自的文本和音频参考，MUTR 对最先进的方法实现了 + 4.2% 和 + 4.2% 的 J & F 改进，表明了我们对统一的多模态 VOS 的重要性。代码在 https://github.com/opengvlab/mutr 发布。"
    },
    {
        "title": "Making Vision Transformers Truly Shift-Equivariant",
        "url": "http://arxiv.org/abs/2305.16316v1",
        "pub_date": "2023-05-25",
        "summary": "For computer vision tasks, Vision Transformers (ViTs) have become one of the\ngo-to deep net architectures. Despite being inspired by Convolutional Neural\nNetworks (CNNs), ViTs remain sensitive to small shifts in the input image. To\naddress this, we introduce novel designs for each of the modules in ViTs, such\nas tokenization, self-attention, patch merging, and positional encoding. With\nour proposed modules, we achieve truly shift-equivariant ViTs on four\nwell-established models, namely, Swin, SwinV2, MViTv2, and CvT, both in theory\nand practice. Empirically, we tested these models on image classification and\nsemantic segmentation, achieving competitive performance across three different\ndatasets while maintaining 100% shift consistency.",
        "translated": "在计算机视觉任务中，视觉变换器(ViTs)已经成为一种常用的深层网络体系结构。尽管受到卷积神经网络(CNN)的启发，ViTs 仍然对输入图像的微小变化敏感。为了解决这个问题，我们为 ViT 中的每个模块引入了新的设计，例如标记化、自注意、补丁合并和位置编码。通过我们提出的模块，我们在理论和实践上实现了四个已经建立的模型，即 Swin，SwinV2，MViTv2和 CvT 上的真正的移位等变 VIT。经验上，我们在图像分类和语义分割上测试了这些模型，在保持100% 移位一致性的同时，在三个不同的数据集上实现了竞争性能。"
    },
    {
        "title": "NAP: Neural 3D Articulation Prior",
        "url": "http://arxiv.org/abs/2305.16315v1",
        "pub_date": "2023-05-25",
        "summary": "We propose Neural 3D Articulation Prior (NAP), the first 3D deep generative\nmodel to synthesize 3D articulated object models. Despite the extensive\nresearch on generating 3D objects, compositions, or scenes, there remains a\nlack of focus on capturing the distribution of articulated objects, a common\nobject category for human and robot interaction. To generate articulated\nobjects, we first design a novel articulation tree/graph parameterization and\nthen apply a diffusion-denoising probabilistic model over this representation\nwhere articulated objects can be generated via denoising from random complete\ngraphs. In order to capture both the geometry and the motion structure whose\ndistribution will affect each other, we design a graph-attention denoising\nnetwork for learning the reverse diffusion process. We propose a novel distance\nthat adapts widely used 3D generation metrics to our novel task to evaluate\ngeneration quality, and experiments demonstrate our high performance in\narticulated object generation. We also demonstrate several conditioned\ngeneration applications, including Part2Motion, PartNet-Imagination,\nMotion2Part, and GAPart2Object.",
        "translated": "我们提出了神经三维关节优先级(nAP) ，这是第一个合成三维关节物体模型的三维深度生成模型。尽管在生成三维物体、构图或场景方面有着广泛的研究，但是对于捕捉关节物体的分布这一人机交互的常见对象类别仍然缺乏关注。为了生成关节对象，我们首先设计了一个新的关节树/图形参量化，然后应用扩散去噪概率模型，通过从随机完整图中去噪来生成关节对象。为了捕捉反向扩散过程中几何和运动结构的相互影响，我们设计了一个图注意去噪网络来学习反向扩散过程。我们提出了一个新的距离，适应广泛使用的三维生成度量的新任务，以评估生成质量，实验表明我们的高性能的铰接对象生成。我们还演示了几个条件生成应用程序，包括 Part2Motion、 PartNet-Imagination、 Motion2Part 和 GAPart2Object。"
    },
    {
        "title": "Banana: Banach Fixed-Point Network for Pointcloud Segmentation with\n  Inter-Part Equivariance",
        "url": "http://arxiv.org/abs/2305.16314v1",
        "pub_date": "2023-05-25",
        "summary": "Equivariance has gained strong interest as a desirable network property that\ninherently ensures robust generalization. However, when dealing with complex\nsystems such as articulated objects or multi-object scenes, effectively\ncapturing inter-part transformations poses a challenge, as it becomes entangled\nwith the overall structure and local transformations. The interdependence of\npart assignment and per-part group action necessitates a novel equivariance\nformulation that allows for their co-evolution. In this paper, we present\nBanana, a Banach fixed-point network for equivariant segmentation with\ninter-part equivariance by construction. Our key insight is to iteratively\nsolve a fixed-point problem, where point-part assignment labels and per-part\nSE(3)-equivariance co-evolve simultaneously. We provide theoretical derivations\nof both per-step equivariance and global convergence, which induces an\nequivariant final convergent state. Our formulation naturally provides a strict\ndefinition of inter-part equivariance that generalizes to unseen inter-part\nconfigurations. Through experiments conducted on both articulated objects and\nmulti-object scans, we demonstrate the efficacy of our approach in achieving\nstrong generalization under inter-part transformations, even when confronted\nwith substantial changes in pointcloud geometry and topology.",
        "translated": "等方差作为一种理想的网络属性，在本质上确保了鲁棒的推广，已经引起了人们的极大兴趣。然而，当处理复杂的系统，如铰接对象或多对象场景，有效地捕获部分间的转换提出了一个挑战，因为它成为纠缠在整体结构和局部转换。部分分配和每部分群体行为的相互依赖性需要一个新的等方差公式，允许它们的协同演化。本文提出了一种基于构造的 Banana 不动点网络，用于部分间等方差的等变分割。我们的主要见解是迭代求解一个不动点问题，其中点部分分配标签和每部分 SE (3)-等方差同时共同演化。我们给出了每步等方差和全局收敛的理论推导，得到了一个等变的最终收敛状态。我们的公式自然提供了一个严格的部分间等方差的定义，推广到看不见的部分间配置。通过对关联对象和多目标扫描的实验，我们证明了该方法在部分间转换下实现强泛化的有效性，即使在面临点云几何和拓扑结构的实质性变化时也是如此。"
    },
    {
        "title": "Break-A-Scene: Extracting Multiple Concepts from a Single Image",
        "url": "http://arxiv.org/abs/2305.16311v1",
        "pub_date": "2023-05-25",
        "summary": "Text-to-image model personalization aims to introduce a user-provided concept\nto the model, allowing its synthesis in diverse contexts. However, current\nmethods primarily focus on the case of learning a single concept from multiple\nimages with variations in backgrounds and poses, and struggle when adapted to a\ndifferent scenario. In this work, we introduce the task of textual scene\ndecomposition: given a single image of a scene that may contain several\nconcepts, we aim to extract a distinct text token for each concept, enabling\nfine-grained control over the generated scenes. To this end, we propose\naugmenting the input image with masks that indicate the presence of target\nconcepts. These masks can be provided by the user or generated automatically by\na pre-trained segmentation model. We then present a novel two-phase\ncustomization process that optimizes a set of dedicated textual embeddings\n(handles), as well as the model weights, striking a delicate balance between\naccurately capturing the concepts and avoiding overfitting. We employ a masked\ndiffusion loss to enable handles to generate their assigned concepts,\ncomplemented by a novel loss on cross-attention maps to prevent entanglement.\nWe also introduce union-sampling, a training strategy aimed to improve the\nability of combining multiple concepts in generated images. We use several\nautomatic metrics to quantitatively compare our method against several\nbaselines, and further affirm the results using a user study. Finally, we\nshowcase several applications of our method. Project page is available at:\nhttps://omriavrahami.com/break-a-scene/",
        "translated": "文本到图像模型个性化旨在向模型引入用户提供的概念，允许在不同的上下文中进行合成。然而，目前的方法主要集中在从背景和姿势不同的多幅图像中学习单一概念的情况下，并在适应不同情景时进行斗争。在这项工作中，我们介绍了文本场景分解的任务: 给定一个场景的单个图像，可能包含几个概念，我们的目标是提取一个不同的文本标记为每个概念，使细粒度控制生成的场景。为此，我们提出用掩码来增强输入图像，以表明目标概念的存在。这些掩码可以由用户提供，也可以由预先训练好的分割模型自动生成。然后，我们提出了一个新的两阶段定制过程，优化了一组专用的文本嵌入(处理) ，以及模型权重，在准确捕捉概念和避免过度拟合之间找到人海万花筒(电影)。我们使用一个掩蔽的扩散损失，使处理能够生成其指定的概念，补充交叉注意地图上的一个新的损失，以防止纠缠。我们还引入了联合采样，这是一种旨在提高生成图像中多个概念组合能力的训练策略。我们使用几个自动指标来定量比较我们的方法与几个基线，并进一步确认结果使用用户研究。最后，我们展示了我们的方法的几个应用。项目网页可于以下 https://omriavrahami.com/break-a-scene/下载:"
    },
    {
        "title": "UMat: Uncertainty-Aware Single Image High Resolution Material Capture",
        "url": "http://arxiv.org/abs/2305.16312v1",
        "pub_date": "2023-05-25",
        "summary": "We propose a learning-based method to recover normals, specularity, and\nroughness from a single diffuse image of a material, using microgeometry\nappearance as our primary cue. Previous methods that work on single images tend\nto produce over-smooth outputs with artifacts, operate at limited resolution,\nor train one model per class with little room for generalization. Previous\nmethods that work on single images tend to produce over-smooth outputs with\nartifacts, operate at limited resolution, or train one model per class with\nlittle room for generalization. In contrast, in this work, we propose a novel\ncapture approach that leverages a generative network with attention and a U-Net\ndiscriminator, which shows outstanding performance integrating global\ninformation at reduced computational complexity. We showcase the performance of\nour method with a real dataset of digitized textile materials and show that a\ncommodity flatbed scanner can produce the type of diffuse illumination required\nas input to our method. Additionally, because the problem might be illposed\n-more than a single diffuse image might be needed to disambiguate the specular\nreflection- or because the training dataset is not representative enough of the\nreal distribution, we propose a novel framework to quantify the model's\nconfidence about its prediction at test time. Our method is the first one to\ndeal with the problem of modeling uncertainty in material digitization,\nincreasing the trustworthiness of the process and enabling more intelligent\nstrategies for dataset creation, as we demonstrate with an active learning\nexperiment.",
        "translated": "我们提出了一个基于学习的方法来恢复法线，反射率和粗糙度从一个单一的漫反射图像的材料，使用显微几何外观作为我们的主要线索。以前用于单幅图像的方法倾向于产生带有工件的过于平滑的输出，在有限的分辨率下操作，或者每类训练一个模型，几乎没有泛化的空间。以前用于单幅图像的方法倾向于产生带有工件的过于平滑的输出，在有限的分辨率下操作，或者每类训练一个模型，几乎没有泛化的空间。相比之下，在这项工作中，我们提出了一种新的捕获方法，利用生成网络的注意力和 U-Net 鉴别器，它显示了在降低计算复杂度的情况下集成全局信息的出色性能。我们展示了我们的方法的性能与数字化纺织材料的真实数据集，并表明，一个商品平板扫描仪可以产生所需的漫反射照明类型作为输入我们的方法。此外，由于问题可能是病态的——消除镜面反射(物理)可能需要不止一张漫反射图像——或者由于训练数据集不足以代表真实分布，我们提出了一个新的框架来量化模型在测试时对其预测的信心。我们的方法是第一个处理材料数字化建模不确定性的问题，提高过程的可信度，使数据集创建更智能的策略，正如我们用一个积极的学习实验所证明的那样。"
    },
    {
        "title": "Securing Deep Generative Models with Universal Adversarial Signature",
        "url": "http://arxiv.org/abs/2305.16310v1",
        "pub_date": "2023-05-25",
        "summary": "Recent advances in deep generative models have led to the development of\nmethods capable of synthesizing high-quality, realistic images. These models\npose threats to society due to their potential misuse. Prior research attempted\nto mitigate these threats by detecting generated images, but the varying traces\nleft by different generative models make it challenging to create a universal\ndetector capable of generalizing to new, unseen generative models. In this\npaper, we propose to inject a universal adversarial signature into an arbitrary\npre-trained generative model, in order to make its generated contents more\ndetectable and traceable. First, the imperceptible optimal signature for each\nimage can be found by a signature injector through adversarial training.\nSubsequently, the signature can be incorporated into an arbitrary generator by\nfine-tuning it with the images processed by the signature injector. In this\nway, the detector corresponding to the signature can be reused for any\nfine-tuned generator for tracking the generator identity. The proposed method\nis validated on the FFHQ and ImageNet datasets with various state-of-the-art\ngenerative models, consistently showing a promising detection rate. Code will\nbe made publicly available at \\url{https://github.com/zengxianyu/genwm}.",
        "translated": "深度生成模型的最新进展导致了能够合成高质量、真实图像的方法的发展。这些模式由于可能被滥用而对社会构成威胁。先前的研究试图通过检测生成的图像来减轻这些威胁，但不同的生成模型留下的不同痕迹使得创建一个能够推广到新的、看不见的生成模型的通用检测器具有挑战性。在这篇文章中，我们建议将一个通用的对抗性签名注入到一个任意的预先训练的生成模型中，以使其生成的内容更加可检测和可追踪。首先，通过对抗训练，利用签名注入器可以找到每幅图像的不可察觉的最优签名。随后，可以通过使用签名注入器处理的图像对签名进行微调，从而将签名合并到任意生成器中。这样，对应于签名的检测器可以重用于任何微调发生器，用于跟踪发生器标识。该方法在 FFHQ 和 ImageNet 数据集上通过各种最先进的生成模型进行了验证，一致地显示出有希望的检测率。代码将在 url { https://github.com/zengxianyu/genwm }公开发布。"
    },
    {
        "title": "NeuManifold: Neural Watertight Manifold Reconstruction with Efficient\n  and High-Quality Rendering Support",
        "url": "http://arxiv.org/abs/2305.17134v1",
        "pub_date": "2023-05-26",
        "summary": "We present a method for generating high-quality watertight manifold meshes\nfrom multi-view input images. Existing volumetric rendering methods are robust\nin optimization but tend to generate noisy meshes with poor topology.\nDifferentiable rasterization-based methods can generate high-quality meshes but\nare sensitive to initialization. Our method combines the benefits of both\nworlds; we take the geometry initialization obtained from neural volumetric\nfields, and further optimize the geometry as well as a compact neural texture\nrepresentation with differentiable rasterizers. Through extensive experiments,\nwe demonstrate that our method can generate accurate mesh reconstructions with\nfaithful appearance that are comparable to previous volume rendering methods\nwhile being an order of magnitude faster in rendering. We also show that our\ngenerated mesh and neural texture reconstruction is compatible with existing\ngraphics pipelines and enables downstream 3D applications such as simulation.\nProject page: https://sarahweiii.github.io/neumanifold/",
        "translated": "提出了一种从多视图输入图像中生成高质量防水流形网格的方法。现有的体绘制方法在优化时具有鲁棒性，但容易产生拓扑结构较差的噪声网格。基于可微栅格化的方法可以生成高质量的网格，但对初始化非常敏感。我们的方法结合了两个世界的好处，我们采取的几何初始化获得的神经体积领域，并进一步优化的几何以及紧凑的神经纹理表示与可微光栅。通过大量的实验，我们证明了我们的方法可以产生准确的网格重建与忠实的外观相比，以前的立体渲染方法，同时是一个数量级更快的渲染。我们还表明，我们生成的网格和神经纹理重建与现有的图形管道兼容，并支持下游3D 应用程序，如模拟。项目主页:  https://sarahweiii.github.io/neumanifold/"
    },
    {
        "title": "Manifold Regularization for Memory-Efficient Training of Deep Neural\n  Networks",
        "url": "http://arxiv.org/abs/2305.17119v1",
        "pub_date": "2023-05-26",
        "summary": "One of the prevailing trends in the machine- and deep-learning community is\nto gravitate towards the use of increasingly larger models in order to keep\npushing the state-of-the-art performance envelope. This tendency makes access\nto the associated technologies more difficult for the average practitioner and\nruns contrary to the desire to democratize knowledge production in the field.\nIn this paper, we propose a framework for achieving improved memory efficiency\nin the process of learning traditional neural networks by leveraging\ninductive-bias-driven network design principles and layer-wise\nmanifold-oriented regularization objectives. Use of the framework results in\nimproved absolute performance and empirical generalization error relative to\ntraditional learning techniques. We provide empirical validation of the\nframework, including qualitative and quantitative evidence of its effectiveness\non two standard image datasets, namely CIFAR-10 and CIFAR-100. The proposed\nframework can be seamlessly combined with existing network compression methods\nfor further memory savings.",
        "translated": "机器和深度学习领域的一个流行趋势是，为了不断推进最先进的性能外壳，越来越多地使用更大的模型。这一趋势使得普通从业者更难获得相关技术，并与实地知识生产民主化的愿望背道而驰。本文提出了一种在学习传统神经网络过程中，利用感应偏置驱动的网络设计原理和面向分层流形的正则化目标来提高存储效率的框架。相对于传统的学习方法，使用这个框架可以提高绝对表现和经验泛化误差。我们提供了框架的经验验证，包括定性和定量的证据，其有效性的两个标准图像数据集，即 CIFAR-10和 CIFAR-100。该框架可以与现有的网络压缩方法无缝结合，以进一步节省内存。"
    },
    {
        "title": "Random-Access Neural Compression of Material Textures",
        "url": "http://arxiv.org/abs/2305.17105v1",
        "pub_date": "2023-05-26",
        "summary": "The continuous advancement of photorealism in rendering is accompanied by a\ngrowth in texture data and, consequently, increasing storage and memory\ndemands. To address this issue, we propose a novel neural compression technique\nspecifically designed for material textures. We unlock two more levels of\ndetail, i.e., 16x more texels, using low bitrate compression, with image\nquality that is better than advanced image compression techniques, such as AVIF\nand JPEG XL. At the same time, our method allows on-demand, real-time\ndecompression with random access similar to block texture compression on GPUs,\nenabling compression on disk and memory. The key idea behind our approach is\ncompressing multiple material textures and their mipmap chains together, and\nusing a small neural network, that is optimized for each material, to\ndecompress them. Finally, we use a custom training implementation to achieve\npractical compression speeds, whose performance surpasses that of general\nframeworks, like PyTorch, by an order of magnitude.",
        "translated": "随着渲染技术的不断进步，纹理数据也不断增加，存储和内存需求也随之增加。为了解决这个问题，我们提出了一种新的神经压缩技术，专门为材质纹理设计。我们使用低比特率压缩技术解锁更多的细节，即16倍以上的文本，其图像质量优于高级图像压缩技术，如 AVIF 和 JPEG XL。与此同时，我们的方法允许按需实时解压缩，随机访问类似于 gpUs 上的块纹理压缩，允许在磁盘和内存上进行压缩。我们方法背后的关键思想是将多种材质的纹理和它们的 mipmap 链压缩在一起，并使用一个针对每种材质优化的小型神经网络来解压缩它们。最后，我们使用一个定制的训练实现来实现实际的压缩速度，其性能超过了一般框架(如 PyTorch)的一个数量级。"
    },
    {
        "title": "GeoVLN: Learning Geometry-Enhanced Visual Representation with Slot\n  Attention for Vision-and-Language Navigation",
        "url": "http://arxiv.org/abs/2305.17102v1",
        "pub_date": "2023-05-26",
        "summary": "Most existing works solving Room-to-Room VLN problem only utilize RGB images\nand do not consider local context around candidate views, which lack sufficient\nvisual cues about surrounding environment. Moreover, natural language contains\ncomplex semantic information thus its correlations with visual inputs are hard\nto model merely with cross attention. In this paper, we propose GeoVLN, which\nlearns Geometry-enhanced visual representation based on slot attention for\nrobust Visual-and-Language Navigation. The RGB images are compensated with the\ncorresponding depth maps and normal maps predicted by Omnidata as visual\ninputs. Technically, we introduce a two-stage module that combine local slot\nattention and CLIP model to produce geometry-enhanced representation from such\ninput. We employ V&amp;L BERT to learn a cross-modal representation that\nincorporate both language and vision informations. Additionally, a novel\nmultiway attention module is designed, encouraging different phrases of input\ninstruction to exploit the most related features from visual input. Extensive\nexperiments demonstrate the effectiveness of our newly designed modules and\nshow the compelling performance of the proposed method.",
        "translated": "现有的解决房间到房间 VLN 问题的作品大多只利用 RGB 图像，没有考虑候选视图周围的局部环境，缺乏对周围环境的足够的视觉线索。此外，自然语言包含复杂的语义信息，因此它与视觉输入的相关性很难仅仅通过交叉注意来建模。本文提出了一种基于时隙注意学习几何增强视觉表示的 GeoVLN，用于鲁棒的视觉语言导航。RGB 图像由相应的深度图和由 Omnidata 预测的法线图作为视觉输入进行补偿。在技术上，我们引入了一个两阶段的模块，结合本地时隙注意和 CLIP 模型，从这样的输入产生几何增强的表示。我们使用 V & L BERT 来学习包含语言和视觉信息的跨模态表示。此外，设计了一个新颖的多路注意模块，鼓励输入指令的不同阶段从视觉输入中发掘最相关的特征。大量的实验证明了我们新设计的模块的有效性，并显示了该方法的引人注目的性能。"
    },
    {
        "title": "ControlVideo: Adding Conditional Control for One Shot Text-to-Video\n  Editing",
        "url": "http://arxiv.org/abs/2305.17098v1",
        "pub_date": "2023-05-26",
        "summary": "In this paper, we present ControlVideo, a novel method for text-driven video\nediting. Leveraging the capabilities of text-to-image diffusion models and\nControlNet, ControlVideo aims to enhance the fidelity and temporal consistency\nof videos that align with a given text while preserving the structure of the\nsource video. This is achieved by incorporating additional conditions such as\nedge maps, fine-tuning the key-frame and temporal attention on the source\nvideo-text pair with carefully designed strategies. An in-depth exploration of\nControlVideo's design is conducted to inform future research on one-shot tuning\nvideo diffusion models. Quantitatively, ControlVideo outperforms a range of\ncompetitive baselines in terms of faithfulness and consistency while still\naligning with the textual prompt. Additionally, it delivers videos with high\nvisual realism and fidelity w.r.t. the source content, demonstrating\nflexibility in utilizing controls containing varying degrees of source video\ninformation, and the potential for multiple control combinations. The project\npage is available at\n\\href{https://ml.cs.tsinghua.edu.cn/controlvideo/}{https://ml.cs.tsinghua.edu.cn/controlvideo/}.",
        "translated": "本文提出了一种新的文本驱动视频编辑方法 ControlVideo。ControlVideo 利用文本到图像扩散模型和 ControlNet 的能力，旨在提高与给定文本对齐的视频的保真度和时间一致性，同时保留源视频的结构。这是通过加入额外的条件，如边缘地图，微调关键帧和时间注意力的源视频文本对与精心设计的策略。本文对 ControlVideo 的设计进行了深入的探索，为今后一次调谐视频扩散模型的研究提供了参考。在数量上，ControlVideo 在忠实性和一致性方面优于一系列竞争性基线，同时仍然与文本提示保持一致。此外，它提供的视频具有高度的视觉真实感和源内容的保真度，展示了在利用包含不同程度的源视频信息的控件方面的灵活性，以及多种控件组合的潜力。项目页面可在 href { https://ml.cs.tsinghua.edu.cn/controlvideo/}{ https://ml.cs.tsinghua.edu.cn/controlvideo/}获得。"
    },
    {
        "title": "GRAtt-VIS: Gated Residual Attention for Auto Rectifying Video Instance\n  Segmentation",
        "url": "http://arxiv.org/abs/2305.17096v1",
        "pub_date": "2023-05-26",
        "summary": "Recent trends in Video Instance Segmentation (VIS) have seen a growing\nreliance on online methods to model complex and lengthy video sequences.\nHowever, the degradation of representation and noise accumulation of the online\nmethods, especially during occlusion and abrupt changes, pose substantial\nchallenges. Transformer-based query propagation provides promising directions\nat the cost of quadratic memory attention. However, they are susceptible to the\ndegradation of instance features due to the above-mentioned challenges and\nsuffer from cascading effects. The detection and rectification of such errors\nremain largely underexplored. To this end, we introduce \\textbf{GRAtt-VIS},\n\\textbf{G}ated \\textbf{R}esidual \\textbf{Att}ention for \\textbf{V}ideo\n\\textbf{I}nstance \\textbf{S}egmentation. Firstly, we leverage a\nGumbel-Softmax-based gate to detect possible errors in the current frame. Next,\nbased on the gate activation, we rectify degraded features from its past\nrepresentation. Such a residual configuration alleviates the need for dedicated\nmemory and provides a continuous stream of relevant instance features.\nSecondly, we propose a novel inter-instance interaction using gate activation\nas a mask for self-attention. This masking strategy dynamically restricts the\nunrepresentative instance queries in the self-attention and preserves vital\ninformation for long-term tracking. We refer to this novel combination of Gated\nResidual Connection and Masked Self-Attention as \\textbf{GRAtt} block, which\ncan easily be integrated into the existing propagation-based framework.\nFurther, GRAtt blocks significantly reduce the attention overhead and simplify\ndynamic temporal modeling. GRAtt-VIS achieves state-of-the-art performance on\nYouTube-VIS and the highly challenging OVIS dataset, significantly improving\nover previous methods. Code is available at\n\\url{https://github.com/Tanveer81/GRAttVIS}.",
        "translated": "视频实例分割(VIS)最近的发展趋势是越来越依赖于在线方法来建模复杂和冗长的视频序列。然而，在线方法的表示退化和噪声积累，特别是在遮挡和突变过程中，带来了实质性的挑战。基于变压器的查询传播以二次内存注意力为代价提供了有希望的方向。然而，由于上述挑战，它们很容易受到实例特性退化的影响，并受到级联效应的影响。这些错误的发现和纠正仍然在很大程度上没有得到充分的探索。为此，我们引入 textbf { GRAtt-VIS } ，textbf { G }化 textbf { R }剩余 textbf { Att }为 textbf { V } ideo textbf { I } nstance textbf { S }分割。首先，我们利用 Gumbel-Softmax 门检测当前帧中可能存在的错误。接下来，基于门激活，我们纠正退化的特征从其过去的表示。这种剩余配置减轻了对专用内存的需求，并提供了相关实例特性的连续流。其次，我们提出了一种新的使用门激活作为自我注意掩蔽的实例间交互。这种掩蔽策略动态地限制了自注意中不具代表性的实例查询，并且为长期跟踪保留了重要信息。我们把这种门控残余连接和掩蔽自我注意的新颖组合称为 textbf { GRAtt }块，它可以很容易地集成到现有的基于传播的框架中。此外，GRAtt 块显著降低了注意开销，简化了动态时间建模。GRATt-VIS 在 YouTube-VIS 和极具挑战性的 OVIS 数据集上实现了最先进的性能，明显改进了以前的方法。代码可在网址{ https://github.com/tanveer81/grattvis }下载。"
    },
    {
        "title": "SSSegmenation: An Open Source Supervised Semantic Segmentation Toolbox\n  Based on PyTorch",
        "url": "http://arxiv.org/abs/2305.17091v1",
        "pub_date": "2023-05-26",
        "summary": "This paper presents SSSegmenation, which is an open source supervised\nsemantic image segmentation toolbox based on PyTorch. The design of this\ntoolbox is motivated by MMSegmentation while it is easier to use because of\nfewer dependencies and achieves superior segmentation performance under a\ncomparable training and testing setup. Moreover, the toolbox also provides\nplenty of trained weights for popular and contemporary semantic segmentation\nmethods, including Deeplab, PSPNet, OCRNet, MaskFormer, \\emph{etc}. We expect\nthat this toolbox can contribute to the future development of semantic\nsegmentation. Codes and model zoos are available at\n\\href{https://github.com/SegmentationBLWX/sssegmentation/}{SSSegmenation}.",
        "translated": "这篇文章介绍了一个基于 PyTorch 的开源监督语义图像分割工具箱。这个工具箱的设计是由 MMS 分割驱动的，它更容易使用，因为较少的依赖性和实现卓越的分割性能在一个可比较的培训和测试设置。此外，该工具箱还为流行的和当代的语义分割方法提供了大量的训练权重，包括 Deeplab、 PSPNet、 OCRNet、 MaskForm、 emph {等}。我们期望这个工具箱能够为语义分割的未来发展做出贡献。代码和模型动物园可在 href { https://github.com/segmentationblwx/sssegmentation/}{ SSSegmenation }获得。"
    },
    {
        "title": "Mindstorms in Natural Language-Based Societies of Mind",
        "url": "http://arxiv.org/abs/2305.17066v1",
        "pub_date": "2023-05-26",
        "summary": "Both Minsky's \"society of mind\" and Schmidhuber's \"learning to think\" inspire\ndiverse societies of large multimodal neural networks (NNs) that solve problems\nby interviewing each other in a \"mindstorm.\" Recent implementations of NN-based\nsocieties of minds consist of large language models (LLMs) and other NN-based\nexperts communicating through a natural language interface. In doing so, they\novercome the limitations of single LLMs, improving multimodal zero-shot\nreasoning. In these natural language-based societies of mind (NLSOMs), new\nagents -- all communicating through the same universal symbolic language -- are\neasily added in a modular fashion. To demonstrate the power of NLSOMs, we\nassemble and experiment with several of them (having up to 129 members),\nleveraging mindstorms in them to solve some practical AI tasks: visual question\nanswering, image captioning, text-to-image synthesis, 3D generation, egocentric\nretrieval, embodied AI, and general language-based task solving. We view this\nas a starting point towards much larger NLSOMs with billions of agents-some of\nwhich may be humans. And with this emergence of great societies of\nheterogeneous minds, many new research questions have suddenly become paramount\nto the future of artificial intelligence. What should be the social structure\nof an NLSOM? What would be the (dis)advantages of having a monarchical rather\nthan a democratic structure? How can principles of NN economies be used to\nmaximize the total reward of a reinforcement learning NLSOM? In this work, we\nidentify, discuss, and try to answer some of these questions.",
        "translated": "明斯基的“心智社会”和施密德胡贝尔的“学会思考”都激发了大型多模式神经网络(NN)的多样化社会，这些神经网络通过在“思维风暴”中相互访谈来解决问题最近基于神经网络的思维社会的实现包括大型语言模型(LLM)和其他基于神经网络的专家通过自然语言接口进行交流。这样，他们克服了单个 LLM 的局限性，改进了多模态零点推理。在这些以自然语言为基础的思维社会(NLSOM)中，新的主体——所有通过相同的通用符号语言进行交流的主体——很容易以模块化的方式添加进来。为了展示 NLSOM 的威力，我们组装并实验了其中的几个(拥有多达129个成员) ，利用其中的思维风暴来解决一些实际的人工智能任务: 视觉问题回答，图像字幕，文本到图像合成，3D 生成，自我中心检索，体现人工智能，以及一般的基于语言的任务解决。我们认为这是通往拥有数十亿代理人(其中一些可能是人类)的更大的非线性有机体的一个起点。随着这个由异质思维组成的伟大社会的出现，许多新的研究问题突然变得对人工智能的未来至关重要。什么是一个 NLSOM 的社会结构？拥有君主制而不是民主制的结构有什么好处？如何运用神经网络经济学的原理来最大限度地提高强化学习非劳动就业市场的总回报？在这项工作中，我们确定，讨论，并试图回答其中的一些问题。"
    },
    {
        "title": "Extremely weakly-supervised blood vessel segmentation with\n  physiologically based synthesis and domain adaptation",
        "url": "http://arxiv.org/abs/2305.17054v1",
        "pub_date": "2023-05-26",
        "summary": "Accurate analysis and modeling of renal functions require a precise\nsegmentation of the renal blood vessels. Micro-CT scans provide image data at\nhigher resolutions, making more small vessels near the renal cortex visible.\nAlthough deep-learning-based methods have shown state-of-the-art performance in\nautomatic blood vessel segmentations, they require a large amount of labeled\ntraining data. However, voxel-wise labeling in micro-CT scans is extremely\ntime-consuming given the huge volume sizes. To mitigate the problem, we\nsimulate synthetic renal vascular trees physiologically while generating\ncorresponding scans of the simulated trees by training a generative model on\nunlabeled scans. This enables the generative model to learn the mapping\nimplicitly without the need for explicit functions to emulate the image\nacquisition process. We further propose an additional segmentation branch over\nthe generative model trained on the generated scans. We demonstrate that the\nmodel can directly segment blood vessels on real scans and validate our method\non both 3D micro-CT scans of rat kidneys and a proof-of-concept experiment on\n2D retinal images. Code and 3D results are available at\nhttps://github.com/miccai2023anony/RenalVesselSeg",
        "translated": "精确的肾功能分析和建模需要精确的肾血管分割。微型 CT 扫描提供更高分辨率的图像数据，使肾皮质附近更多的小血管可见。尽管基于深度学习的方法在自动血管分割中表现出了最先进的性能，但是它们需要大量的标记训练数据。然而，在显微 CT 扫描的体素标记是非常耗时的，因为巨大的体积大小。为了缓解这个问题，我们在生理学上模拟合成肾血管树，同时通过训练一个生成模型对未标记的扫描产生相应的模拟树扫描。这使得生成模型能够隐式地学习映射，而不需要显式的函数来模拟图像采集过程。我们进一步提出了一个额外的分割分支超过生成模型训练生成的扫描。我们证明该模型可以直接分割血管的实际扫描和验证我们的方法都三维显微 CT 扫描大鼠肾脏和二维视网膜图像的概念验证实验。编码及立体效果可于 https://github.com/miccai2023anony/renalvesselseg 下载"
    },
    {
        "title": "SelfClean: A Self-Supervised Data Cleaning Strategy",
        "url": "http://arxiv.org/abs/2305.17048v1",
        "pub_date": "2023-05-26",
        "summary": "Most commonly used benchmark datasets for computer vision contain irrelevant\nimages, near duplicates, and label errors. Consequently, model performance on\nthese benchmarks may not be an accurate estimate of generalization ability.\nThis is a particularly acute concern in computer vision for medicine where\ndatasets are typically small, stakes are high, and annotation processes are\nexpensive and error-prone. In this paper, we propose SelfClean, a general\nprocedure to clean up image datasets exploiting a latent space learned with\nself-supervision. By relying on self-supervised learning, our approach focuses\non intrinsic properties of the data and avoids annotation biases. We formulate\ndataset cleaning as either a set of ranking problems, where human experts can\nmake decisions with significantly reduced effort, or a set of scoring problems,\nwhere decisions can be fully automated based on score distributions. We compare\nSelfClean against other algorithms on common computer vision benchmarks\nenhanced with synthetic noise and demonstrate state-of-the-art performance on\ndetecting irrelevant images, near duplicates, and label errors. In addition, we\napply our method to multiple image datasets and confirm an improvement in\nevaluation reliability.",
        "translated": "最常用的计算机视觉基准数据集包含不相关的图像、接近重复的图像和标签错误。因此，这些基准上的模型性能可能不是泛化能力的准确估计。在医学计算机视觉领域，这是一个特别严重的问题，因为数据集通常很小，风险很高，注释过程昂贵且容易出错。在本文中，我们提出了自我清理，一个通用的程序来清理图像数据集利用潜在的空间学习与自我监督。通过依赖于自监督学习，我们的方法侧重于数据的内在属性，避免了注释偏差。我们将数据集清理描述为一组排序问题，在这些问题中，人类专家可以大大减少工作量来做出决策; 或者将数据集清理描述为一组评分问题，在这些问题中，决策可以根据分数分布完全自动化。我们比较了 SelfClean 和其他基于合成噪声增强的常用计算机视觉基准的算法，并展示了在检测不相关图像、近重复图像和标签错误方面的最新性能。此外，将该方法应用于多个图像数据集，证实了该方法在评价信度方面的改进。"
    },
    {
        "title": "RAPHAEL: Text-to-Image Generation via Large Mixture of Diffusion Paths",
        "url": "http://arxiv.org/abs/2305.18295v1",
        "pub_date": "2023-05-29",
        "summary": "Text-to-image generation has recently witnessed remarkable achievements. We\nintroduce a text-conditional image diffusion model, termed RAPHAEL, to generate\nhighly artistic images, which accurately portray the text prompts, encompassing\nmultiple nouns, adjectives, and verbs. This is achieved by stacking tens of\nmixture-of-experts (MoEs) layers, i.e., space-MoE and time-MoE layers, enabling\nbillions of diffusion paths (routes) from the network input to the output. Each\npath intuitively functions as a \"painter\" for depicting a particular textual\nconcept onto a specified image region at a diffusion timestep. Comprehensive\nexperiments reveal that RAPHAEL outperforms recent cutting-edge models, such as\nStable Diffusion, ERNIE-ViLG 2.0, DeepFloyd, and DALL-E 2, in terms of both\nimage quality and aesthetic appeal. Firstly, RAPHAEL exhibits superior\nperformance in switching images across diverse styles, such as Japanese comics,\nrealism, cyberpunk, and ink illustration. Secondly, a single model with three\nbillion parameters, trained on 1,000 A100 GPUs for two months, achieves a\nstate-of-the-art zero-shot FID score of 6.61 on the COCO dataset. Furthermore,\nRAPHAEL significantly surpasses its counterparts in human evaluation on the\nViLG-300 benchmark. We believe that RAPHAEL holds the potential to propel the\nfrontiers of image generation research in both academia and industry, paving\nthe way for future breakthroughs in this rapidly evolving field. More details\ncan be found on a project webpage: https://raphael-painter.github.io/.",
        "translated": "文本到图像的生成近年来取得了显著的成就。我们引入了一个文本条件的图像扩散模型，称为 RAPHAEL，生成高度艺术化的图像，准确地描述文本提示，包括多个名词，形容词和动词。这是通过叠加数十个混合专家(MoEs)层来实现的，即空间-MoE 层和时间-MoE 层，使得从网络输入到输出的数十亿条扩散路径(路径)成为可能。每条路径直观地起到“画家”的作用，在扩散时间步骤中将特定的文本概念描绘到指定的图像区域。全面的实验表明，RAPHAEL 在图像质量和美学吸引力方面都优于最近的尖端模型，如稳定扩散，ERNIE-ViLG 2.0，DeepFloyd 和 DALL-E2。首先，RAPHAEL 在转换不同风格的图像方面表现出了卓越的表现，比如日本漫画、现实主义、赛博朋克和水墨插图。其次，一个拥有30亿个参数的单一模型，在1000个 A100图形处理器上训练了两个月，在 COCO 数据集上实现了最先进的零拍 FID 得分6.61。此外，RAPHAEL 在 ViLG-300基准的人体评估方面显著超越了同行。我们相信，RAPHAEL 具有推动学术界和工业界图像生成研究前沿的潜力，为这一快速发展领域的未来突破铺平了道路。更多详情可浏览计划网页:  https://raphael-painter.github.io/。"
    },
    {
        "title": "Mix-of-Show: Decentralized Low-Rank Adaptation for Multi-Concept\n  Customization of Diffusion Models",
        "url": "http://arxiv.org/abs/2305.18292v1",
        "pub_date": "2023-05-29",
        "summary": "Public large-scale text-to-image diffusion models, such as Stable Diffusion,\nhave gained significant attention from the community. These models can be\neasily customized for new concepts using low-rank adaptations (LoRAs). However,\nthe utilization of multiple concept LoRAs to jointly support multiple\ncustomized concepts presents a challenge. We refer to this scenario as\ndecentralized multi-concept customization, which involves single-client concept\ntuning and center-node concept fusion. In this paper, we propose a new\nframework called Mix-of-Show that addresses the challenges of decentralized\nmulti-concept customization, including concept conflicts resulting from\nexisting single-client LoRA tuning and identity loss during model fusion.\nMix-of-Show adopts an embedding-decomposed LoRA (ED-LoRA) for single-client\ntuning and gradient fusion for the center node to preserve the in-domain\nessence of single concepts and support theoretically limitless concept fusion.\nAdditionally, we introduce regionally controllable sampling, which extends\nspatially controllable sampling (e.g., ControlNet and T2I-Adaptor) to address\nattribute binding and missing object problems in multi-concept sampling.\nExtensive experiments demonstrate that Mix-of-Show is capable of composing\nmultiple customized concepts with high fidelity, including characters, objects,\nand scenes.",
        "translated": "公共的大规模文本到图像的扩散模型，如稳定扩散，已经引起了社会的重视。这些模型可以很容易地使用低等级适应性(LoRA)为新概念进行定制。然而，利用多个概念 LoRA 共同支持多个定制概念提出了一个挑战。我们将这种情况称为分散式多概念定制，其中包括单客户端概念调优和中心节点概念融合。在本文中，我们提出了一个名为 Mix-of-Show 的新框架，它解决了分散式多概念定制的挑战，包括现有的单客户 LoRA 调优导致的概念冲突和模型融合过程中的身份丢失。Mix-of-Show 采用嵌入分解 LoRA (ED-LoRA)进行单客户端调优和中心节点的梯度融合，以保留单个概念的域内实质，并支持理论上的无限概念融合。此外，本文还引入了区域可控抽样，扩展了空间可控抽样(如 ControlNet 和 T2I-Adaptor) ，解决了多概念抽样中的属性绑定和缺失对象问题。大量的实验表明，Mix-of-Show 能够以高保真度组合多个定制概念，包括人物、对象和场景。"
    },
    {
        "title": "LaFTer: Label-Free Tuning of Zero-shot Classifier using Language and\n  Unlabeled Image Collections",
        "url": "http://arxiv.org/abs/2305.18287v1",
        "pub_date": "2023-05-29",
        "summary": "Recently, large-scale pre-trained Vision and Language (VL) models have set a\nnew state-of-the-art (SOTA) in zero-shot visual classification enabling\nopen-vocabulary recognition of potentially unlimited set of categories defined\nas simple language prompts. However, despite these great advances, the\nperformance of these zeroshot classifiers still falls short of the results of\ndedicated (closed category set) classifiers trained with supervised fine\ntuning. In this paper we show, for the first time, how to reduce this gap\nwithout any labels and without any paired VL data, using an unlabeled image\ncollection and a set of texts auto-generated using a Large Language Model (LLM)\ndescribing the categories of interest and effectively substituting labeled\nvisual instances of those categories. Using our label-free approach, we are\nable to attain significant performance improvements over the zero-shot\nperformance of the base VL model and other contemporary methods and baselines\non a wide variety of datasets, demonstrating absolute improvement of up to\n11.7% (3.8% on average) in the label-free setting. Moreover, despite our\napproach being label-free, we observe 1.3% average gains over leading few-shot\nprompting baselines that do use 5-shot supervision.",
        "translated": "最近，大规模的预先训练的视觉和语言(VL)模型在零拍视觉分类中设置了一个新的最先进的(SOTA) ，使得开放词汇表能够识别定义为简单语言提示的潜在无限的类别集。然而，尽管有这些巨大的进步，这些零拍分类器的性能仍然不能满足专用(封闭类集)分类器的结果与监督微调训练。在本文中，我们首次展示了如何在不使用任何标签和任何配对 VL 数据的情况下，通过使用一个未标签的图像集和一组使用大语言模型(LLM)自动生成的文本来缩小这种差距，并有效地替换这些类别的标签视觉实例。使用我们的无标签方法，我们能够在广泛的数据集上获得比基础 VL 模型和其他当代方法和基线的零射击性能更显著的性能改进，显示在无标签设置下绝对改善高达11.7% (平均3.8%)。此外，尽管我们的方法是没有标签的，我们观察到1.3% 的平均收益超过领先的几杆提示基线，确实使用5杆监督。"
    },
    {
        "title": "Photoswap: Personalized Subject Swapping in Images",
        "url": "http://arxiv.org/abs/2305.18286v1",
        "pub_date": "2023-05-29",
        "summary": "In an era where images and visual content dominate our digital landscape, the\nability to manipulate and personalize these images has become a necessity.\nEnvision seamlessly substituting a tabby cat lounging on a sunlit window sill\nin a photograph with your own playful puppy, all while preserving the original\ncharm and composition of the image. We present Photoswap, a novel approach that\nenables this immersive image editing experience through personalized subject\nswapping in existing images. Photoswap first learns the visual concept of the\nsubject from reference images and then swaps it into the target image using\npre-trained diffusion models in a training-free manner. We establish that a\nwell-conceptualized visual subject can be seamlessly transferred to any image\nwith appropriate self-attention and cross-attention manipulation, maintaining\nthe pose of the swapped subject and the overall coherence of the image.\nComprehensive experiments underscore the efficacy and controllability of\nPhotoswap in personalized subject swapping. Furthermore, Photoswap\nsignificantly outperforms baseline methods in human ratings across subject\nswapping, background preservation, and overall quality, revealing its vast\napplication potential, from entertainment to professional editing.",
        "translated": "在一个图像和视觉内容主导我们的数字景观的时代，操纵和个性化这些图像的能力已成为一种必要。想象一下，在一张照片中，一只虎斑猫懒洋洋地躺在阳光照射下的窗台上，与你自己顽皮的小狗一起，同时保留了照片原有的魅力和构图。我们提出的 Photoswap，一种新颖的方法，使这种沉浸式图像编辑的经验，通过个性化的主题交换在现有的图像。照片交换首先从参考图像中学习目标的视觉概念，然后使用预先训练的扩散模型以一种无需训练的方式将其交换到目标图像中。我们建立了一个概念化的视觉主体可以通过适当的自我注意和交叉注意操作无缝地转移到任何图像上，保持交换主体的姿态和图像的整体一致性。全面的实验强调了 Photoswap 在个性化学科交换方面的有效性和可控性。此外，Photoswap 在主题交换、背景保存和整体质量方面显著优于基线评分方法，显示出其从娱乐到专业编辑的巨大应用潜力。"
    },
    {
        "title": "Contextual Object Detection with Multimodal Large Language Models",
        "url": "http://arxiv.org/abs/2305.18279v1",
        "pub_date": "2023-05-29",
        "summary": "Recent Multimodal Large Language Models (MLLMs) are remarkable in\nvision-language tasks, such as image captioning and question answering, but\nlack the essential perception ability, i.e., object detection. In this work, we\naddress this limitation by introducing a novel research problem of contextual\nobject detection -- understanding visible objects within different human-AI\ninteractive contexts. Three representative scenarios are investigated,\nincluding the language cloze test, visual captioning, and question answering.\nMoreover, we present ContextDET, a unified multimodal model that is capable of\nend-to-end differentiable modeling of visual-language contexts, so as to\nlocate, identify, and associate visual objects with language inputs for\nhuman-AI interaction. Our ContextDET involves three key submodels: (i) a visual\nencoder for extracting visual representations, (ii) a pre-trained LLM for\nmultimodal context decoding, and (iii) a visual decoder for predicting bounding\nboxes given contextual object words. The new generate-then-detect framework\nenables us to detect object words within human vocabulary. Extensive\nexperiments show the advantages of ContextDET on our proposed CODE benchmark,\nopen-vocabulary detection, and referring image segmentation. Github:\nhttps://github.com/yuhangzang/ContextDET.",
        "translated": "最近的多模态大语言模型(mLLMs)在视觉语言任务(如图像字幕和问答)中表现突出，但缺乏基本的感知能力，即目标检测。在这项工作中，我们通过引入一个关于语境目标检测的新的研究问题——在不同的人工智能交互环境中理解可见物体来解决这一局限性。研究了三种典型的情景，包括语言完形填空测试、视觉字幕和问答。此外，本文还提出了一个统一的多模态模型 ContextDET，该模型能够对视觉语言环境进行端到端的可微分建模，从而定位、识别和关联视觉对象和语言输入，实现人机交互。我们的 ContextDET 涉及三个关键子模型: (i)用于提取视觉表示的可视化编码器，(ii)用于多模态上下文解码的预先训练的 LLM，以及(iii)用于预测给定上下文对象词的边界框的可视化解码器。新的生成然后检测框架使我们能够检测人类词汇表中的对象词。大量的实验显示了 ContextDET 在我们提出的 CODE 基准、开放词汇表检测和引用图像分割上的优势。Https://Github.com/yuhangzang/contextdet."
    },
    {
        "title": "3DTeethSeg'22: 3D Teeth Scan Segmentation and Labeling Challenge",
        "url": "http://arxiv.org/abs/2305.18277v1",
        "pub_date": "2023-05-29",
        "summary": "Teeth localization, segmentation, and labeling from intra-oral 3D scans are\nessential tasks in modern dentistry to enhance dental diagnostics, treatment\nplanning, and population-based studies on oral health. However, developing\nautomated algorithms for teeth analysis presents significant challenges due to\nvariations in dental anatomy, imaging protocols, and limited availability of\npublicly accessible data. To address these challenges, the 3DTeethSeg'22\nchallenge was organized in conjunction with the International Conference on\nMedical Image Computing and Computer Assisted Intervention (MICCAI) in 2022,\nwith a call for algorithms tackling teeth localization, segmentation, and\nlabeling from intraoral 3D scans. A dataset comprising a total of 1800 scans\nfrom 900 patients was prepared, and each tooth was individually annotated by a\nhuman-machine hybrid algorithm. A total of 6 algorithms were evaluated on this\ndataset. In this study, we present the evaluation results of the 3DTeethSeg'22\nchallenge. The 3DTeethSeg'22 challenge code can be accessed at:\nhttps://github.com/abenhamadou/3DTeethSeg22_challenge",
        "translated": "口腔内3D 扫描的牙齿定位、分割和标记是现代牙科学加强口腔诊断、治疗计划和基于人群的口腔健康研究的基本任务。然而，开发用于牙齿分析的自动化算法由于牙齿解剖学的变化，成像协议和有限的可公开获取的数据提出了重大挑战。为了应对这些挑战，3DTeethSeg’22挑战与2022年国际医学图像计算和计算机辅助干预会议(MICCAI)一起组织，呼吁从口内3D 扫描中解决牙齿定位，分割和标记的算法。准备了一个包含来自900名患者的总共1800次扫描的数据集，并用人机混合算法对每颗牙齿进行单独注释。在这个数据集上总共评估了6种算法。在这项研究中，我们提出了3DTeethSeg’22挑战的评估结果。3dtethseg’22挑战码可在以下 https://github.com/abenhamadou/3dteethseg22_challenge 查阅:"
    },
    {
        "title": "Reconstructing the Mind's Eye: fMRI-to-Image with Contrastive Learning\n  and Diffusion Priors",
        "url": "http://arxiv.org/abs/2305.18274v1",
        "pub_date": "2023-05-29",
        "summary": "We present MindEye, a novel fMRI-to-image approach to retrieve and\nreconstruct viewed images from brain activity. Our model comprises two parallel\nsubmodules that are specialized for retrieval (using contrastive learning) and\nreconstruction (using a diffusion prior). MindEye can map fMRI brain activity\nto any high dimensional multimodal latent space, like CLIP image space,\nenabling image reconstruction using generative models that accept embeddings\nfrom this latent space. We comprehensively compare our approach with other\nexisting methods, using both qualitative side-by-side comparisons and\nquantitative evaluations, and show that MindEye achieves state-of-the-art\nperformance in both reconstruction and retrieval tasks. In particular, MindEye\ncan retrieve the exact original image even among highly similar candidates\nindicating that its brain embeddings retain fine-grained image-specific\ninformation. This allows us to accurately retrieve images even from large-scale\ndatabases like LAION-5B. We demonstrate through ablations that MindEye's\nperformance improvements over previous methods result from specialized\nsubmodules for retrieval and reconstruction, improved training techniques, and\ntraining models with orders of magnitude more parameters. Furthermore, we show\nthat MindEye can better preserve low-level image features in the\nreconstructions by using img2img, with outputs from a separate autoencoder. All\ncode is available on GitHub.",
        "translated": "我们提出了 MindEye，一种新的功能磁共振成像的方法来检索和重建从大脑活动中看到的图像。我们的模型包括两个并行的子模块，专门用于检索(使用对比学习)和重构(使用扩散先验)。MindEye 可以将 fMRI 大脑活动映射到任何高维多模式潜伏空间，比如 CLIP 图像空间，使得能够使用接受来自这个潜伏空间的嵌入的生成模型进行图像重建。我们全面比较了我们的方法与其他现有的方法，使用定性并列比较和定量评估，并表明 MindEye 实现了最先进的表现，在重建和检索任务。尤其值得一提的是，MindEye 甚至可以在高度相似的候选图像中检索到精确的原始图像，这表明它的大脑嵌入保留了细粒度的图像特定信息。这使我们能够准确地检索图像，甚至从大型数据库，如 LAION-5B。我们通过消融实验证明，MindEye 的性能优于以前的方法是由专门用于检索和重建的子模块、改进的训练技术以及具有更多参数的训练模型数量级的结果。此外，我们表明，MindEye 可以更好地保存低水平的图像特征，在重建使用 img2img，从一个单独的自动编码器的输出。所有代码都可以在 GitHub 上找到。"
    },
    {
        "title": "Pix2Repair: Implicit Shape Restoration from Images",
        "url": "http://arxiv.org/abs/2305.18273v1",
        "pub_date": "2023-05-29",
        "summary": "We present Pix2Repair, an automated shape repair approach that generates\nrestoration shapes from images to repair fractured objects. Prior repair\napproaches require a high-resolution watertight 3D mesh of the fractured object\nas input. Input 3D meshes must be obtained using expensive 3D scanners, and\nscanned meshes require manual cleanup, limiting accessibility and scalability.\nPix2Repair takes an image of the fractured object as input and automatically\ngenerates a 3D printable restoration shape. We contribute a novel shape\nfunction that deconstructs a latent code representing the fractured object into\na complete shape and a break surface. We show restorations for synthetic\nfractures from the Geometric Breaks and Breaking Bad datasets, and cultural\nheritage objects from the QP dataset, and for real fractures from the Fantastic\nBreaks dataset. We overcome challenges in restoring axially symmetric objects\nby predicting view-centered restorations. Our approach outperforms shape\ncompletion approaches adapted for shape repair in terms of chamfer distance,\nearth mover's distance, normal consistency, and percent restorations generated.",
        "translated": "我们介绍了 Pix2修复，一种自动形状修复方法，从图像生成修复形状，以修复断裂的对象。先前的修复方法需要将断裂物体的高分辨率水密3D 网格作为输入。输入3D 网格必须使用昂贵的3D 扫描仪获得，并扫描网格需要手动清理，限制可访问性和可伸缩性。Pix2Amendment 将断裂物体的图像作为输入，并自动生成一个3D 可打印的修复形状。我们贡献了一个新的形状函数，解构一个潜在的代码代表破碎的物体成为一个完整的形状和破裂表面。我们展示了来自几何断裂和绝命毒师数据集的合成骨折的修复，来自 QP 数据集的文化遗产对象，以及来自荒诞断裂数据集的真实骨折的修复。我们通过预测以视点为中心的复原来克服轴对称物体复原的挑战。我们的方法在倒角距离、推土机的距离、法向一致性和恢复百分比方面优于适用于形状修复的形状完成方法。"
    },
    {
        "title": "Gen-L-Video: Multi-Text to Long Video Generation via Temporal\n  Co-Denoising",
        "url": "http://arxiv.org/abs/2305.18264v1",
        "pub_date": "2023-05-29",
        "summary": "Leveraging large-scale image-text datasets and advancements in diffusion\nmodels, text-driven generative models have made remarkable strides in the field\nof image generation and editing. This study explores the potential of extending\nthe text-driven ability to the generation and editing of multi-text conditioned\nlong videos. Current methodologies for video generation and editing, while\ninnovative, are often confined to extremely short videos (typically less than\n24 frames) and are limited to a single text condition. These constraints\nsignificantly limit their applications given that real-world videos usually\nconsist of multiple segments, each bearing different semantic information. To\naddress this challenge, we introduce a novel paradigm dubbed as Gen-L-Video,\ncapable of extending off-the-shelf short video diffusion models for generating\nand editing videos comprising hundreds of frames with diverse semantic segments\nwithout introducing additional training, all while preserving content\nconsistency. We have implemented three mainstream text-driven video generation\nand editing methodologies and extended them to accommodate longer videos imbued\nwith a variety of semantic segments with our proposed paradigm. Our\nexperimental outcomes reveal that our approach significantly broadens the\ngenerative and editing capabilities of video diffusion models, offering new\npossibilities for future research and applications. The code is available at\nhttps://github.com/G-U-N/Gen-L-Video.",
        "translated": "利用大规模图文数据集和扩散模型的进步，文本驱动的生成模型在图像生成和编辑领域取得了显著的进步。本研究探讨文本驱动能力扩展到多文本条件长视频的生成和编辑的潜力。目前的视频生成和编辑方法，虽然具有创新性，但往往局限于极短的视频(通常少于24帧) ，并且仅限于一个文本条件。考虑到现实世界中的视频通常由多个片段组成，每个片段都有不同的语义信息，这些限制极大地限制了它们的应用。为了应对这一挑战，我们引入了一种称为 Gen-L-Video 的新型范例，该范例能够扩展现成的短视频扩散模型，用于生成和编辑包含数百帧具有不同语义片段的视频，而无需引入额外的训练，同时保持内容的一致性。我们已经实现了三种主流的文本驱动的视频生成和编辑方法，并扩展了它们，以适应与我们提出的范例中的各种语义片段灌输的较长的视频。我们的实验结果表明，我们的方法显着扩大了视频扩散模型的生成和编辑能力，为未来的研究和应用提供了新的可能性。密码可在 https://github.com/g-u-n/gen-l-video 查阅。"
    },
    {
        "title": "Synfeal: A Data-Driven Simulator for End-to-End Camera Localization",
        "url": "http://arxiv.org/abs/2305.18260v1",
        "pub_date": "2023-05-29",
        "summary": "Collecting real-world data is often considered the bottleneck of Artificial\nIntelligence, stalling the research progress in several fields, one of which is\ncamera localization. End-to-end camera localization methods are still\noutperformed by traditional methods, and we argue that the inconsistencies\nassociated with the data collection techniques are restraining the potential of\nend-to-end methods. Inspired by the recent data-centric paradigm, we propose a\nframework that synthesizes large localization datasets based on realistic 3D\nreconstructions of the real world. Our framework, termed Synfeal: Synthetic\nfrom Real, is an open-source, data-driven simulator that synthesizes RGB images\nby moving a virtual camera through a realistic 3D textured mesh, while\ncollecting the corresponding ground-truth camera poses. The results validate\nthat the training of camera localization algorithms on datasets generated by\nSynfeal leads to better results when compared to datasets generated by\nstate-of-the-art methods. Using Synfeal, we conducted the first analysis of the\nrelationship between the size of the dataset and the performance of camera\nlocalization algorithms. Results show that the performance significantly\nincreases with the dataset size. Our results also suggest that when a large\nlocalization dataset with high quality is available, training from scratch\nleads to better performances. Synfeal is publicly available at\nhttps://github.com/DanielCoelho112/synfeal.",
        "translated": "真实世界数据的采集往往被认为是人工智能的瓶颈，阻碍了人工智能在多个领域的研究进展，其中摄像机定位就是其中之一。端到端摄像机定位方法仍然比传统的定位方法要好，我们认为与数据采集技术相关的不一致性限制了端到端定位方法的潜力。受最近以数据为中心的范式的启发，我们提出了一个基于真实世界的三维重建的大型定位数据集合成框架。我们的框架，称为 Synfeal: 从真实合成，是一个开源的，数据驱动的模拟器，通过移动虚拟相机通过一个真实的3D 纹理网格合成 RGB 图像，同时收集相应的地面真相相机的姿势。实验结果表明，在 Synfeal 生成的数据集上进行摄像机定位算法的训练，比采用最先进的方法生成的数据集有更好的定位效果。使用 Synfeal，我们首先分析了数据集大小与摄像机定位算法性能之间的关系。结果表明，随着数据集大小的增加，性能显著提高。我们的结果还表明，当一个高质量的大型定位数据集是可用的，从头训练导致更好的性能。Synfeal 可以在 https://github.com/danielcoelho112/Synfeal 上公开使用。"
    },
    {
        "title": "Learning without Forgetting for Vision-Language Models",
        "url": "http://arxiv.org/abs/2305.19270v1",
        "pub_date": "2023-05-30",
        "summary": "Class-Incremental Learning (CIL) or continual learning is a desired\ncapability in the real world, which requires a learning system to adapt to new\ntasks without forgetting former ones. While traditional CIL methods focus on\nvisual information to grasp core features, recent advances in Vision-Language\nModels (VLM) have shown promising capabilities in learning generalizable\nrepresentations with the aid of textual information. However, when continually\ntrained with new classes, VLMs often suffer from catastrophic forgetting of\nformer knowledge. Applying VLMs to CIL poses two major challenges: 1) how to\nadapt the model without forgetting; and 2) how to make full use of the\nmulti-modal information. To this end, we propose PROjectiOn Fusion (PROOF) that\nenables VLMs to learn without forgetting. To handle the first challenge, we\npropose training task-specific projections based on the frozen image/text\nencoders. When facing new tasks, new projections are expanded and former\nprojections are fixed, alleviating the forgetting of old concepts. For the\nsecond challenge, we propose the fusion module to better utilize the\ncross-modality information. By jointly adjusting visual and textual features,\nthe model can capture semantic information with stronger representation\nability. Extensive experiments on nine benchmark datasets validate PROOF\nachieves state-of-the-art performance.",
        "translated": "课堂增量学习(CIL)或连续学习是现实世界需要的一种能力，它需要一个学习系统来适应新的任务而不忘记以前的任务。虽然传统的 CIL 方法侧重于视觉信息来掌握核心特征，但视觉语言模型(VLM)的最新进展已经显示出在借助文本信息学习可推广表示方面的有前途的能力。然而，当不断培训新的类，VLM 往往遭受灾难性的遗忘以前的知识。在 CIL 中应用 VLM 模型提出了两个主要的挑战: 1)如何在不遗忘的情况下调整模型; 2)如何充分利用多模态信息。为此，我们提出了投影融合(PROOF) ，使 VLM 学习而不会忘记。为了应对第一个挑战，我们提出了基于冻结图像/文本编码器的训练任务特定投影。当面对新的任务时，新的预测会被扩展，旧的预测会被固定，从而减轻对旧概念的遗忘。对于第二个挑战，我们提出了融合模块，以更好地利用交叉模态信息。通过对视觉特征和文本特征的联合调整，该模型可以捕捉具有更强表现能力的语义信息。在九个基准数据集上的大量实验验证了 PROOF 实现了最先进的性能。"
    },
    {
        "title": "Ambient Diffusion: Learning Clean Distributions from Corrupted Data",
        "url": "http://arxiv.org/abs/2305.19256v1",
        "pub_date": "2023-05-30",
        "summary": "We present the first diffusion-based framework that can learn an unknown\ndistribution using only highly-corrupted samples. This problem arises in\nscientific applications where access to uncorrupted samples is impossible or\nexpensive to acquire. Another benefit of our approach is the ability to train\ngenerative models that are less likely to memorize individual training samples\nsince they never observe clean training data. Our main idea is to introduce\nadditional measurement distortion during the diffusion process and require the\nmodel to predict the original corrupted image from the further corrupted image.\nWe prove that our method leads to models that learn the conditional expectation\nof the full uncorrupted image given this additional measurement corruption.\nThis holds for any corruption process that satisfies some technical conditions\n(and in particular includes inpainting and compressed sensing). We train models\non standard benchmarks (CelebA, CIFAR-10 and AFHQ) and show that we can learn\nthe distribution even when all the training samples have $90\\%$ of their pixels\nmissing. We also show that we can finetune foundation models on small corrupted\ndatasets (e.g. MRI scans with block corruptions) and learn the clean\ndistribution without memorizing the training set.",
        "translated": "我们提出了第一个扩散为基础的框架，可以学习一个未知的分布，只使用高度腐败的样本。这个问题出现在科学应用领域，因为无法获得未受污染的样品或获得这些样品的费用很高。我们的方法的另一个好处是能够训练生成模型，这些模型不太可能记住单独的训练样本，因为它们从来没有观察到干净的训练数据。我们的主要思想是在扩散过程中引入额外的测量失真，并要求该模型从进一步的损伤图像中预测出原始的损伤图像。我们证明了我们的方法导致模型学习完整未损坏的图像的条件期望，考虑到这种额外的测量损坏。这适用于任何满足某些技术条件的腐败过程(特别是包括油漆和压缩感知)。我们在标准基准(CelebA，CIFAR-10和 AFHQ)上训练模型，并表明即使所有的训练样本都丢失了90% 的像素，我们仍然可以学习分布。我们还展示了我们可以在小的损坏数据集上微调基础模型(例如带有块损坏的 MRI 扫描) ，并且不需要记忆训练集就可以学习干净的分布。"
    },
    {
        "title": "AlteredAvatar: Stylizing Dynamic 3D Avatars with Fast Style Adaptation",
        "url": "http://arxiv.org/abs/2305.19245v1",
        "pub_date": "2023-05-30",
        "summary": "This paper presents a method that can quickly adapt dynamic 3D avatars to\narbitrary text descriptions of novel styles. Among existing approaches for\navatar stylization, direct optimization methods can produce excellent results\nfor arbitrary styles but they are unpleasantly slow. Furthermore, they require\nredoing the optimization process from scratch for every new input. Fast\napproximation methods using feed-forward networks trained on a large dataset of\nstyle images can generate results for new inputs quickly, but tend not to\ngeneralize well to novel styles and fall short in quality. We therefore\ninvestigate a new approach, AlteredAvatar, that combines those two approaches\nusing the meta-learning framework. In the inner loop, the model learns to\noptimize to match a single target style well; while in the outer loop, the\nmodel learns to stylize efficiently across many styles. After training,\nAlteredAvatar learns an initialization that can quickly adapt within a small\nnumber of update steps to a novel style, which can be given using texts, a\nreference image, or a combination of both. We show that AlteredAvatar can\nachieve a good balance between speed, flexibility and quality, while\nmaintaining consistency across a wide range of novel views and facial\nexpressions.",
        "translated": "本文提出了一种快速自适应动态3D 化身的方法，以适应任意文本描述的新风格。在现有的化身风格化方法中，直接优化方法可以对任意风格产生优秀的结果，但是它们的速度慢得令人不快。此外，它们需要从头开始为每个新输入重做优化过程。使用前馈网络训练样式图像的快速逼近方法可以快速生成新输入的结果，但往往不能很好地推广到新的样式和质量不足。因此，我们研究了一种新的方法，AlteredAvatar，它使用元学习框架将这两种方法结合起来。在内部循环中，模型学习如何优化以很好地匹配单个目标样式; 而在外部循环中，模型学习如何有效地跨多种样式进行样式化。经过训练，AlteredAvatar 学会了一种初始化，它可以在少量更新步骤内快速适应一种新的风格，这种风格可以使用文本、参考图像或两者的组合。我们展示了 AlteredAvatar 可以在速度、灵活性和质量之间取得很好的平衡，同时保持广泛的新视图和面部表情的一致性。"
    },
    {
        "title": "Translation-Enhanced Multilingual Text-to-Image Generation",
        "url": "http://arxiv.org/abs/2305.19216v1",
        "pub_date": "2023-05-30",
        "summary": "Research on text-to-image generation (TTI) still predominantly focuses on the\nEnglish language due to the lack of annotated image-caption data in other\nlanguages; in the long run, this might widen inequitable access to TTI\ntechnology. In this work, we thus investigate multilingual TTI (termed mTTI)\nand the current potential of neural machine translation (NMT) to bootstrap mTTI\nsystems. We provide two key contributions. 1) Relying on a multilingual\nmulti-modal encoder, we provide a systematic empirical study of standard\nmethods used in cross-lingual NLP when applied to mTTI: Translate Train,\nTranslate Test, and Zero-Shot Transfer. 2) We propose Ensemble Adapter (EnsAd),\na novel parameter-efficient approach that learns to weigh and consolidate the\nmultilingual text knowledge within the mTTI framework, mitigating the language\ngap and thus improving mTTI performance. Our evaluations on standard mTTI\ndatasets COCO-CN, Multi30K Task2, and LAION-5B demonstrate the potential of\ntranslation-enhanced mTTI systems and also validate the benefits of the\nproposed EnsAd which derives consistent gains across all datasets. Further\ninvestigations on model variants, ablation studies, and qualitative analyses\nprovide additional insights on the inner workings of the proposed mTTI\napproaches.",
        "translated": "由于缺乏其他语言的注释图像标题数据，文本到图像生成(TTI)的研究仍然主要集中在英语上; 从长远来看，这可能会扩大 TTI 技术的不公平使用。在这项工作中，我们因此研究多语言 TTI (称为 mTTI)和神经机器翻译(NMT)目前的潜力引导 mTTI 系统。我们提供了两个关键的贡献。1)以多语言多模态编码器为基础，对跨语言自然语言处理中的标准方法进行了系统的实证研究。2)提出了一种新的参数有效方法 EnsAd，该方法可以在 mTTI 框架内学习权衡和整合多语言文本知识，减小语言差距，从而提高 mTTI 的性能。我们对标准 mTTI 数据集 COCO-CN，Multi30K Task2和 LAION-5B 的评估证明了翻译增强的 mTTI 系统的潜力，并且还验证了所提议的 EnsAd 的益处，其在所有数据集中获得一致的增益。对模型变异、消融研究和定性分析的进一步研究提供了对拟议的 mTTI 方法内部工作的额外见解。"
    },
    {
        "title": "Group Invariant Global Pooling",
        "url": "http://arxiv.org/abs/2305.19207v1",
        "pub_date": "2023-05-30",
        "summary": "Much work has been devoted to devising architectures that build\ngroup-equivariant representations, while invariance is often induced using\nsimple global pooling mechanisms. Little work has been done on creating\nexpressive layers that are invariant to given symmetries, despite the success\nof permutation invariant pooling in various molecular tasks. In this work, we\npresent Group Invariant Global Pooling (GIGP), an invariant pooling layer that\nis provably sufficiently expressive to represent a large class of invariant\nfunctions. We validate GIGP on rotated MNIST and QM9, showing improvements for\nthe latter while attaining identical results for the former. By making the\npooling process group orbit-aware, this invariant aggregation method leads to\nimproved performance, while performing well-principled group aggregation.",
        "translated": "许多工作致力于设计构建群等变表示的体系结构，而不变性通常是使用简单的全局池机制来诱导的。尽管在各种分子任务中排列不变量池的成功，但在创建对给定对称性不变的表达层方面几乎没有做什么工作。在这项工作中，我们提出了群不变全局池(GIGP) ，一个不变的池层，可证明充分表达，以表示一个大类的不变函数。我们在旋转的 MNIST 和 QM9上验证了 GIGP，显示了后者的改进，同时获得了前者相同的结果。这种不变聚集方法通过使池处理过程组轨道感知，提高了性能，同时执行了原则性良好的组聚集。"
    },
    {
        "title": "AMatFormer: Efficient Feature Matching via Anchor Matching Transformer",
        "url": "http://arxiv.org/abs/2305.19205v1",
        "pub_date": "2023-05-30",
        "summary": "Learning based feature matching methods have been commonly studied in recent\nyears. The core issue for learning feature matching is to how to learn (1)\ndiscriminative representations for feature points (or regions) within each\nintra-image and (2) consensus representations for feature points across\ninter-images. Recently, self- and cross-attention models have been exploited to\naddress this issue. However, in many scenes, features are coming with\nlarge-scale, redundant and outliers contaminated. Previous\nself-/cross-attention models generally conduct message passing on all primal\nfeatures which thus lead to redundant learning and high computational cost. To\nmitigate limitations, inspired by recent seed matching methods, in this paper,\nwe propose a novel efficient Anchor Matching Transformer (AMatFormer) for the\nfeature matching problem. AMatFormer has two main aspects: First, it mainly\nconducts self-/cross-attention on some anchor features and leverages these\nanchor features as message bottleneck to learn the representations for all\nprimal features. Thus, it can be implemented efficiently and compactly. Second,\nAMatFormer adopts a shared FFN module to further embed the features of two\nimages into the common domain and thus learn the consensus feature\nrepresentations for the matching problem. Experiments on several benchmarks\ndemonstrate the effectiveness and efficiency of the proposed AMatFormer\nmatching approach.",
        "translated": "基于学习的特征匹配方法是近年来研究的热点。学习特征匹配的核心问题是如何学习(1)图像内特征点(或区域)的区分表示和(2)图像间特征点的一致表示。最近，自我和交叉注意模型已经被用来解决这个问题。然而，在许多场景中，特性都伴随着大规模的、冗余的和受到污染的异常值。以往的自我/交叉注意模型通常将信息传递给所有的原始特征，从而导致冗余学习和高计算成本。为了解决这一问题，本文提出了一种新的基于特征匹配的锚匹配变换器。AMatForm 主要有两个方面: 首先，它主要对一些锚特征进行自我/交叉注意，并利用这些锚特征作为消息瓶颈来学习所有原始特征的表示。因此，它可以有效和紧凑地实现。其次，采用共享 FFN 模块将两幅图像的特征进一步嵌入到公共域中，从而学习匹配问题的一致特征表示。在多个基准上的实验结果表明了该方法的有效性和高效性。"
    },
    {
        "title": "DäRF: Boosting Radiance Fields from Sparse Inputs with Monocular Depth\n  Adaptation",
        "url": "http://arxiv.org/abs/2305.19201v1",
        "pub_date": "2023-05-30",
        "summary": "Neural radiance fields (NeRF) shows powerful performance in novel view\nsynthesis and 3D geometry reconstruction, but it suffers from critical\nperformance degradation when the number of known viewpoints is drastically\nreduced. Existing works attempt to overcome this problem by employing external\npriors, but their success is limited to certain types of scenes or datasets.\nEmploying monocular depth estimation (MDE) networks, pretrained on large-scale\nRGB-D datasets, with powerful generalization capability would be a key to\nsolving this problem: however, using MDE in conjunction with NeRF comes with a\nnew set of challenges due to various ambiguity problems exhibited by monocular\ndepths. In this light, we propose a novel framework, dubbed D\\\"aRF, that\nachieves robust NeRF reconstruction with a handful of real-world images by\ncombining the strengths of NeRF and monocular depth estimation through online\ncomplementary training. Our framework imposes the MDE network's powerful\ngeometry prior to NeRF representation at both seen and unseen viewpoints to\nenhance its robustness and coherence. In addition, we overcome the ambiguity\nproblems of monocular depths through patch-wise scale-shift fitting and\ngeometry distillation, which adapts the MDE network to produce depths aligned\naccurately with NeRF geometry. Experiments show our framework achieves\nstate-of-the-art results both quantitatively and qualitatively, demonstrating\nconsistent and reliable performance in both indoor and outdoor real-world\ndatasets. Project page is available at https://ku-cvlab.github.io/DaRF/.",
        "translated": "神经辐射场(NeRF)在新视点合成和三维几何重建方面表现出强大的性能，但当已知视点数大幅减少时，其性能会出现临界退化。现有的作品试图通过使用外部先验来克服这个问题，但是他们的成功仅限于某些类型的场景或数据集。使用单目深度估计(MDE)网络，在大规模 RGB-D 数据集上预先训练，具有强大的泛化能力将是解决这一问题的关键: 然而，使用 MDE 与 NeRF 结合带来了一系列新的挑战，由于单目深度表现出的各种模糊问题。在此基础上，我们提出了一种新的框架，称为 D“ aRF，通过在线互补训练结合了 NERF 和单目深度估计的优点，实现了对少量真实世界图像的强大的 NERF 重建。我们的框架将 MDE 网络的强大的几何形状强加于 NERF 表示之前，在可见和不可见的观点，以增强其健壮性和一致性。此外，通过分片尺度变换拟合和几何精馏克服了单目深度的模糊性问题，使 MDE 网络能够产生与 NeRF 几何精确对齐的深度。实验表明，我们的框架在定量和定性上都达到了最先进的结果，在室内和室外的真实世界数据集中表现出一致和可靠的性能。项目网页可于 https://ku-cvlab.github.io/darf/下载。"
    },
    {
        "title": "PanoGen: Text-Conditioned Panoramic Environment Generation for\n  Vision-and-Language Navigation",
        "url": "http://arxiv.org/abs/2305.19195v1",
        "pub_date": "2023-05-30",
        "summary": "Vision-and-Language Navigation (VLN) requires the agent to follow language\ninstructions to navigate through 3D environments. One main challenge in VLN is\nthe limited availability of photorealistic training environments, which makes\nit hard to generalize to new and unseen environments. To address this problem,\nwe propose PanoGen, a generation method that can potentially create an infinite\nnumber of diverse panoramic environments conditioned on text. Specifically, we\ncollect room descriptions by captioning the room images in existing\nMatterport3D environments, and leverage a state-of-the-art text-to-image\ndiffusion model to generate the new panoramic environments. We use recursive\noutpainting over the generated images to create consistent 360-degree panorama\nviews. Our new panoramic environments share similar semantic information with\nthe original environments by conditioning on text descriptions, which ensures\nthe co-occurrence of objects in the panorama follows human intuition, and\ncreates enough diversity in room appearance and layout with image outpainting.\nLastly, we explore two ways of utilizing PanoGen in VLN pre-training and\nfine-tuning. We generate instructions for paths in our PanoGen environments\nwith a speaker built on a pre-trained vision-and-language model for VLN\npre-training, and augment the visual observation with our panoramic\nenvironments during agents' fine-tuning to avoid overfitting to seen\nenvironments. Empirically, learning with our PanoGen environments achieves the\nnew state-of-the-art on the Room-to-Room, Room-for-Room, and CVDN datasets.\nPre-training with our PanoGen speaker data is especially effective for CVDN,\nwhich has under-specified instructions and needs commonsense knowledge. Lastly,\nwe show that the agent can benefit from training with more generated panoramic\nenvironments, suggesting promising results for scaling up the PanoGen\nenvironments.",
        "translated": "视觉和语言导航(VLN)要求代理遵循语言指令在3D 环境中导航。VLN 的一个主要挑战是有限的真实感训练环境，这使得它很难推广到新的和看不见的环境。为了解决这个问题，我们提出 PanoGen，一种生成方法，可以潜在地创建无限数量的不同的全景环境的文本条件。具体来说，我们通过在现有 Matterport3D 环境中标注房间图像来收集房间描述，并利用最先进的文本到图像扩散模型来生成新的全景环境。我们使用递归绘制生成的图像来创建一致的360度全景视图。我们新的全景环境与原始环境有着相似的语义信息，通过文字描述来确保全景中物体的同时出现遵循人类的直觉，并创造了足够多样化的房间外观和布局图像。最后，我们探讨了利用 PanoGen 在 VLN 预训练和微调中的两种方法。我们在 PanoGen 环境中使用一个基于预先训练的 VLN 预训视觉和语言模型的扬声器生成路径指令，并在代理的微调期间通过我们的全景环境增强视觉观察，以避免过度适应可见环境。根据经验，使用 PanoGen 环境学习可以在 Room-to-Room、 Room-for-Room 和 CVDN 数据集上实现最新的技术水平。使用 PanoGen 扬声器数据进行预训练对于 CVDN 尤其有效，因为 CVDN 的指令不够详细，而且需要常识性知识。最后，我们表明，该代理可以受益于更多生成的全景环境的培训，提出了扩展 PanoGen 环境的有希望的结果。"
    },
    {
        "title": "Video ControlNet: Towards Temporally Consistent Synthetic-to-Real Video\n  Translation Using Conditional Image Diffusion Models",
        "url": "http://arxiv.org/abs/2305.19193v1",
        "pub_date": "2023-05-30",
        "summary": "In this study, we present an efficient and effective approach for achieving\ntemporally consistent synthetic-to-real video translation in videos of varying\nlengths. Our method leverages off-the-shelf conditional image diffusion models,\nallowing us to perform multiple synthetic-to-real image generations in\nparallel. By utilizing the available optical flow information from the\nsynthetic videos, our approach seamlessly enforces temporal consistency among\ncorresponding pixels across frames. This is achieved through joint noise\noptimization, effectively minimizing spatial and temporal discrepancies. To the\nbest of our knowledge, our proposed method is the first to accomplish diverse\nand temporally consistent synthetic-to-real video translation using conditional\nimage diffusion models. Furthermore, our approach does not require any training\nor fine-tuning of the diffusion models. Extensive experiments conducted on\nvarious benchmarks for synthetic-to-real video translation demonstrate the\neffectiveness of our approach, both quantitatively and qualitatively. Finally,\nwe show that our method outperforms other baseline methods in terms of both\ntemporal consistency and visual quality.",
        "translated": "在这项研究中，我们提出了一个有效的方法来实现时间一致的合成到真实的视频在不同长度的视频翻译。我们的方法利用现成的条件图像扩散模型，允许我们并行执行多个合成到真实的图像生成。该方法利用合成视频中可用的光流信息，实现了帧间对应像素之间的时间一致性。这是通过联合噪声优化，有效地减少空间和时间差异。据我们所知，我们提出的方法是第一个使用条件图像扩散模型来实现多样化和时间一致的合成到真实的视频翻译。此外，我们的方法不需要任何培训或扩散模型的微调。针对合成到真实视频翻译的各种基准进行了大量的实验，从定量和定性两方面证明了该方法的有效性。最后，我们证明了我们的方法在时间一致性和视觉质量方面都优于其他基线方法。"
    },
    {
        "title": "Table Detection for Visually Rich Document Images",
        "url": "http://arxiv.org/abs/2305.19181v1",
        "pub_date": "2023-05-30",
        "summary": "Table Detection (TD) is a fundamental task towards visually rich document\nunderstanding. Current studies usually formulate the TD problem as an object\ndetection problem, then leverage Intersection over Union (IoU) based metrics to\nevaluate the model performance and IoU-based loss functions to optimize the\nmodel. TD applications usually require the prediction results to cover all the\ntable contents and avoid information loss. However, IoU and IoU-based loss\nfunctions cannot directly reflect the degree of information loss for the\nprediction results. Therefore, we propose to decouple IoU into a ground truth\ncoverage term and a prediction coverage term, in which the former can be used\nto measure the information loss of the prediction results.\n  Besides, tables in the documents are usually large, sparsely distributed, and\nhave no overlaps because they are designed to summarize essential information\nto make it easy to read and interpret for human readers. Therefore, in this\nstudy, we use SparseR-CNN as the base model, and further improve the model by\nusing Gaussian Noise Augmented Image Size region proposals and many-to-one\nlabel assignments.\n  To demonstrate the effectiveness of proposed method and compare with\nstate-of-the-art methods fairly, we conduct experiments and use IoU-based\nevaluation metrics to evaluate the model performance. The experimental results\nshow that the proposed method can consistently outperform state-of-the-art\nmethods under different IoU-based metric on a variety of datasets. We conduct\nfurther experiments to show the superiority of the proposed decoupled IoU for\nthe TD applications by replacing the IoU-based loss functions and evaluation\nmetrics with proposed decoupled IoU counterparts. The experimental results show\nthat our proposed decoupled IoU loss can encourage the model to alleviate\ninformation loss.",
        "translated": "表检测(TD)是实现视觉丰富的文档理解的基本任务。目前的研究通常将 TD 问题表述为一个目标检测问题，然后利用基于交叉比联盟(IoU)的度量来评估模型的性能，并利用基于 IoU 的损失函数来优化模型。TD 应用程序通常要求预测结果覆盖表中的所有内容，避免信息丢失。然而，基于 IoU 和 IoU 的损失函数不能直接反映预测结果的信息损失程度。因此，我们提出将 IU 解耦为一个地面真实覆盖项和一个预测覆盖项，其中前者可用来度量预测结果的信息损失。此外，文档中的表格通常很大，分布很稀疏，没有重叠，因为它们旨在总结必要的信息，以便于人类读者阅读和解释。因此，在本研究中，我们以稀疏 R-CNN 为基础模型，并进一步改进模型，使用高斯噪声增强图像大小区域方案和多对一标签分配。为了验证所提方法的有效性，并与现有方法进行比较，我们进行了实验，并使用基于 IoU 的评价指标来评价模型的性能。实验结果表明，在不同的物联网度量下，该方法在各种数据集上的性能均优于目前最先进的方法。通过进一步的实验，我们证明了所提出的解耦 IU 对 TD 应用的优越性，将基于 IU 的损失函数和评估指标替换为所提出的解耦 IU 对应物。实验结果表明，我们提出的解耦的 IU 损失可以鼓励模型，以减轻信息损失。"
    },
    {
        "title": "Humans in 4D: Reconstructing and Tracking Humans with Transformers",
        "url": "http://arxiv.org/abs/2305.20091v1",
        "pub_date": "2023-05-31",
        "summary": "We present an approach to reconstruct humans and track them over time. At the\ncore of our approach, we propose a fully \"transformerized\" version of a network\nfor human mesh recovery. This network, HMR 2.0, advances the state of the art\nand shows the capability to analyze unusual poses that have in the past been\ndifficult to reconstruct from single images. To analyze video, we use 3D\nreconstructions from HMR 2.0 as input to a tracking system that operates in 3D.\nThis enables us to deal with multiple people and maintain identities through\nocclusion events. Our complete approach, 4DHumans, achieves state-of-the-art\nresults for tracking people from monocular video. Furthermore, we demonstrate\nthe effectiveness of HMR 2.0 on the downstream task of action recognition,\nachieving significant improvements over previous pose-based action recognition\napproaches. Our code and models are available on the project website:\nhttps://shubham-goel.github.io/4dhumans/.",
        "translated": "我们提出了一种方法来重建人类，并随着时间的推移跟踪他们。在我们的方法的核心，我们提出了一个完全“转换”的网络版本的人类网格恢复。这个网络，HMR 2.0，提高了最先进的技术水平，并显示了分析不寻常姿势的能力，这些姿势在过去很难从单个图像重建。为了分析视频，我们使用来自 HMR 2.0的3D 重建作为输入到一个3D 跟踪系统中。这使我们能够处理多个人，并通过遮挡事件保持身份。我们的完整方法，4D 人类，实现了最先进的结果跟踪人从单目视频。此外，我们证明了 HMR 2.0在动作识别的下游任务上的有效性，实现了对以前基于姿势的动作识别方法的显著改进。我们的代码和模型可以在项目网站上找到:  https://shubham-goel.github.io/4dhumans/。"
    },
    {
        "title": "Learning Explicit Contact for Implicit Reconstruction of Hand-held\n  Objects from Monocular Images",
        "url": "http://arxiv.org/abs/2305.20089v1",
        "pub_date": "2023-05-31",
        "summary": "Reconstructing hand-held objects from monocular RGB images is an appealing\nyet challenging task. In this task, contacts between hands and objects provide\nimportant cues for recovering the 3D geometry of the hand-held objects. Though\nrecent works have employed implicit functions to achieve impressive progress,\nthey ignore formulating contacts in their frameworks, which results in\nproducing less realistic object meshes. In this work, we explore how to model\ncontacts in an explicit way to benefit the implicit reconstruction of hand-held\nobjects. Our method consists of two components: explicit contact prediction and\nimplicit shape reconstruction. In the first part, we propose a new subtask of\ndirectly estimating 3D hand-object contacts from a single image. The part-level\nand vertex-level graph-based transformers are cascaded and jointly learned in a\ncoarse-to-fine manner for more accurate contact probabilities. In the second\npart, we introduce a novel method to diffuse estimated contact states from the\nhand mesh surface to nearby 3D space and leverage diffused contact\nprobabilities to construct the implicit neural representation for the\nmanipulated object. Benefiting from estimating the interaction patterns between\nthe hand and the object, our method can reconstruct more realistic object\nmeshes, especially for object parts that are in contact with hands. Extensive\nexperiments on challenging benchmarks show that the proposed method outperforms\nthe current state of the arts by a great margin.",
        "translated": "从单目 RGB 图像重建手持物体是一个吸引人的但具有挑战性的任务。在这项任务中，手和物体之间的接触为恢复手持物体的三维几何形状提供了重要的线索。尽管最近的作品使用了隐式函数来取得令人印象深刻的进展，但是他们忽视了在框架中表述联系，这导致了产生不太真实的对象网格。在这项工作中，我们探讨了如何以显性的方式建模接触，以利于手持物体的隐式重建。该方法由显式接触预测和隐式形状重建两部分组成。在第一部分中，我们提出了一个新的子任务直接估计三维手-物体接触从一个单一的图像。基于部件级和顶点级图形的变压器以从粗到精的方式串联和联合学习，以获得更准确的接触概率。在第二部分中，我们提出了一种新的方法来扩散估计接触状态从手网格表面到附近的三维空间，并利用扩散接触概率来构造被操作物体的隐式神经表示。该方法通过估计手与物体之间的交互模式，可以重建出更加逼真的物体网格，特别是对于与手接触的物体部分。对具有挑战性的基准的大量实验表明，该方法的性能远远优于目前的技术水平。"
    },
    {
        "title": "Improving CLIP Training with Language Rewrites",
        "url": "http://arxiv.org/abs/2305.20088v1",
        "pub_date": "2023-05-31",
        "summary": "Contrastive Language-Image Pre-training (CLIP) stands as one of the most\neffective and scalable methods for training transferable vision models using\npaired image and text data. CLIP models are trained using contrastive loss,\nwhich typically relies on data augmentations to prevent overfitting and\nshortcuts. However, in the CLIP training paradigm, data augmentations are\nexclusively applied to image inputs, while language inputs remain unchanged\nthroughout the entire training process, limiting the exposure of diverse texts\nto the same image. In this paper, we introduce Language augmented CLIP\n(LaCLIP), a simple yet highly effective approach to enhance CLIP training\nthrough language rewrites. Leveraging the in-context learning capability of\nlarge language models, we rewrite the text descriptions associated with each\nimage. These rewritten texts exhibit diversity in sentence structure and\nvocabulary while preserving the original key concepts and meanings. During\ntraining, LaCLIP randomly selects either the original texts or the rewritten\nversions as text augmentations for each image. Extensive experiments on CC3M,\nCC12M, RedCaps and LAION-400M datasets show that CLIP pre-training with\nlanguage rewrites significantly improves the transfer performance without\ncomputation or memory overhead during training. Specifically for ImageNet\nzero-shot accuracy, LaCLIP outperforms CLIP by 8.2% on CC12M and 2.4% on\nLAION-400M. Code is available at https://github.com/LijieFan/LaCLIP.",
        "translated": "对比语言-图像预训练(CLIP)是利用成对图像和文本数据训练可转移视觉模型的最有效和可扩展的方法之一。CLIP 模型使用对比损失进行训练，对比损失通常依赖于数据增强以防止过度拟合和快捷方式。然而，在 CLIP 训练范式中，数据增强仅应用于图像输入，而语言输入在整个训练过程中保持不变，限制了不同文本暴露于同一图像。本文介绍了语言增强 CLIP 技术，这是一种通过语言重写来增强 CLIP 训练的简单而高效的方法。利用大型语言模型的上下文学习能力，我们重写与每个图像相关的文本描述。这些改写后的文本在保留原有关键概念和意义的同时，表现出句子结构和词汇的多样性。在训练过程中，LaCLIP 随机选择原始文本或改写版本作为每幅图像的文本增强。在 CC3M、 CC12M、 RedCaps 和 LAION-400M 数据集上进行的大量实验表明，使用语言重写的 CLIP 预训练可以在不增加训练过程中的计算和内存开销的情况下显著提高传输性能。具体到 ImageNet 的零射精度，LaCLIP 在 CC12M 上比 CLIP 高出8.2% ，在 LAION-400M 上高出2.4% 。密码可于 https://github.com/lijiefan/laclip 索取。"
    },
    {
        "title": "Too Large; Data Reduction for Vision-Language Pre-Training",
        "url": "http://arxiv.org/abs/2305.20087v2",
        "pub_date": "2023-05-31",
        "summary": "This paper examines the problems of severe image-text misalignment and high\nredundancy in the widely-used large-scale Vision-Language Pre-Training (VLP)\ndatasets. To address these issues, we propose an efficient and straightforward\nVision-Language learning algorithm called TL;DR, which aims to compress the\nexisting large VLP data into a small, high-quality set. Our approach consists\nof two major steps. First, a codebook-based encoder-decoder captioner is\ndeveloped to select representative samples. Second, a new caption is generated\nto complement the original captions for selected samples, mitigating the\ntext-image misalignment problem while maintaining uniqueness. As the result,\nTL;DR enables us to reduce the large dataset into a small set of high-quality\ndata, which can serve as an alternative pre-training dataset. This algorithm\nsignificantly speeds up the time-consuming pretraining process. Specifically,\nTL;DR can compress the mainstream VLP datasets at a high ratio, e.g., reduce\nwell-cleaned CC3M dataset from 2.82M to 0.67M ($\\sim$24\\%) and noisy YFCC15M\nfrom 15M to 2.5M ($\\sim$16.7\\%). Extensive experiments with three popular VLP\nmodels over seven downstream tasks show that VLP model trained on the\ncompressed dataset provided by TL;DR can perform similar or even better results\ncompared with training on the full-scale dataset. The code will be made\navailable at \\url{https://github.com/showlab/data-centric.vlp}.",
        "translated": "本文研究了目前广泛使用的大规模视觉语言预训练(VLP)数据集中存在的严重图文错位和高冗余问题。为了解决这些问题，我们提出了一种高效、直观的视觉语言学习算法 TL; DR，该算法旨在将现有的大型 VLP 数据压缩成一个小型、高质量的集合。我们的方法包括两个主要步骤。首先，开发了一种基于码本的编解码字幕器来选择有代表性的样本。其次，生成一个新的标题以补充所选样本的原始标题，在保持唯一性的同时缓解文本-图像不对齐问题。结果，TL; DR 使我们能够将大数据集减少为一个小的高质量数据集，这可以作为一个替代的预训练数据集。该算法显著加快了耗时的预训练过程。具体来说，TL; DR 可以高比例地压缩主流 VLP 数据集，例如，将清洁良好的 CC3M 数据集从2.82 M 减少到0.67 M ($sim $24%) ，将噪音较大的 YFCC15M 从15M 减少到250 M ($sim $16.7%)。通过对三种流行的 VLP 模型在7个下游任务上的大量实验表明，VLP 模型在 TL 提供的压缩数据集上进行训练，与在全尺寸数据集上进行训练相比，DR 可以获得相似甚至更好的结果。代码将在 url { https://github.com/showlab/data-centric.vlp }提供。"
    },
    {
        "title": "Understanding and Mitigating Copying in Diffusion Models",
        "url": "http://arxiv.org/abs/2305.20086v1",
        "pub_date": "2023-05-31",
        "summary": "Images generated by diffusion models like Stable Diffusion are increasingly\nwidespread. Recent works and even lawsuits have shown that these models are\nprone to replicating their training data, unbeknownst to the user. In this\npaper, we first analyze this memorization problem in text-to-image diffusion\nmodels. While it is widely believed that duplicated images in the training set\nare responsible for content replication at inference time, we observe that the\ntext conditioning of the model plays a similarly important role. In fact, we\nsee in our experiments that data replication often does not happen for\nunconditional models, while it is common in the text-conditional case.\nMotivated by our findings, we then propose several techniques for reducing data\nreplication at both training and inference time by randomizing and augmenting\nimage captions in the training set.",
        "translated": "由稳定扩散等扩散模型产生的图像越来越广泛。最近的工作，甚至诉讼已经表明，这些模型倾向于复制他们的训练数据，用户不知道。本文首先分析了文本-图像扩散模型中的记忆问题。虽然人们普遍认为训练集中的重复图像负责推理时的内容复制，但是我们观察到模型的文本条件作用也起着类似的重要作用。事实上，在我们的实验中，我们看到数据复制通常不会发生在无条件模型中，而在文本条件的情况下却很常见。在我们的研究结果的激励下，我们提出了几种技术，通过在训练集中随机化和增强图像标题来减少训练和推理时间的数据复制。"
    },
    {
        "title": "Control4D: Dynamic Portrait Editing by Learning 4D GAN from 2D\n  Diffusion-based Editor",
        "url": "http://arxiv.org/abs/2305.20082v1",
        "pub_date": "2023-05-31",
        "summary": "Recent years have witnessed considerable achievements in editing images with\ntext instructions. When applying these editors to dynamic scene editing, the\nnew-style scene tends to be temporally inconsistent due to the frame-by-frame\nnature of these 2D editors. To tackle this issue, we propose Control4D, a novel\napproach for high-fidelity and temporally consistent 4D portrait editing.\nControl4D is built upon an efficient 4D representation with a 2D\ndiffusion-based editor. Instead of using direct supervisions from the editor,\nour method learns a 4D GAN from it and avoids the inconsistent supervision\nsignals. Specifically, we employ a discriminator to learn the generation\ndistribution based on the edited images and then update the generator with the\ndiscrimination signals. For more stable training, multi-level information is\nextracted from the edited images and used to facilitate the learning of the\ngenerator. Experimental results show that Control4D surpasses previous\napproaches and achieves more photo-realistic and consistent 4D editing\nperformances. The link to our project website is\nhttps://control4darxiv.github.io.",
        "translated": "近年来，在利用文本指令编辑图像方面取得了相当大的成就。当这些编辑器应用于动态场景编辑时，由于这些二维编辑器的逐帧性质，新风格的场景往往会出现时间上的不一致。为了解决这个问题，我们提出 Control4D，一种新颖的高保真度和时间一致的4D 肖像编辑方法。Control4D 是建立在一个有效的4D 表示与2D 扩散为基础的编辑器。该方法不需要编辑器的直接监控，而是从编辑器中学习一个4D GAN，避免了监控信号的不一致。具体地说，我们使用一个鉴别器来学习基于编辑后的图像的生成分布，然后用鉴别信号更新生成器。为了获得更稳定的训练，从编辑后的图像中提取多层次信息，以便于生成器的学习。实验结果表明，Control4D 编辑方法优于以往的编辑方法，具有更好的逼真度和一致性。我们项目网站的链接是 https://control4darxiv.github.io 的。"
    },
    {
        "title": "Feature Learning in Image Hierarchies using Functional Maximal\n  Correlation",
        "url": "http://arxiv.org/abs/2305.20074v1",
        "pub_date": "2023-05-31",
        "summary": "This paper proposes the Hierarchical Functional Maximal Correlation Algorithm\n(HFMCA), a hierarchical methodology that characterizes dependencies across two\nhierarchical levels in multiview systems. By framing view similarities as\ndependencies and ensuring contrastivity by imposing orthonormality, HFMCA\nachieves faster convergence and increased stability in self-supervised\nlearning. HFMCA defines and measures dependencies within image hierarchies,\nfrom pixels and patches to full images. We find that the network topology for\napproximating orthonormal basis functions aligns with a vanilla CNN, enabling\nthe decomposition of density ratios between neighboring layers of feature maps.\nThis approach provides powerful interpretability, revealing the resemblance\nbetween supervision and self-supervision through the lens of internal\nrepresentations.",
        "translated": "本文提出了分层函数最大相关算法(HFMCA) ，这是一种描述多视图系统中两个层次之间依赖关系的分层方法。通过将视图相似性框架为依赖关系并通过正交性确保对比度，HFMCA 在自监督学习中实现了更快的收敛和更高的稳定性。HFMCA 定义和度量图像层次结构中的依赖关系，从像素和补丁到完整图像。我们发现，近似网络拓扑标准正交基函数的方法与传统的有线电视新闻网(CNN)方法相一致，能够分解相邻特征映射层之间的密度比。这种方法提供了强大的可解释性，通过内部表征的透镜揭示了监督和自我监督之间的相似性。"
    },
    {
        "title": "Chatting Makes Perfect -- Chat-based Image Retrieval",
        "url": "http://arxiv.org/abs/2305.20062v1",
        "pub_date": "2023-05-31",
        "summary": "Chats emerge as an effective user-friendly approach for information\nretrieval, and are successfully employed in many domains, such as customer\nservice, healthcare, and finance. However, existing image retrieval approaches\ntypically address the case of a single query-to-image round, and the use of\nchats for image retrieval has been mostly overlooked. In this work, we\nintroduce ChatIR: a chat-based image retrieval system that engages in a\nconversation with the user to elicit information, in addition to an initial\nquery, in order to clarify the user's search intent. Motivated by the\ncapabilities of today's foundation models, we leverage Large Language Models to\ngenerate follow-up questions to an initial image description. These questions\nform a dialog with the user in order to retrieve the desired image from a large\ncorpus. In this study, we explore the capabilities of such a system tested on a\nlarge dataset and reveal that engaging in a dialog yields significant gains in\nimage retrieval. We start by building an evaluation pipeline from an existing\nmanually generated dataset and explore different modules and training\nstrategies for ChatIR. Our comparison includes strong baselines derived from\nrelated applications trained with Reinforcement Learning. Our system is capable\nof retrieving the target image from a pool of 50K images with over 78% success\nrate after 5 dialogue rounds, compared to 75% when questions are asked by\nhumans, and 64% for a single shot text-to-image retrieval. Extensive\nevaluations reveal the strong capabilities and examine the limitations of\nCharIR under different settings.",
        "translated": "聊天作为一种有效的用户友好的方式出现在信息检索，并成功地应用于许多领域，如客户服务，医疗保健和金融。然而，现有的图像检索方法通常只处理一轮图像查询，而且大多忽视了聊天对图像检索的作用。在这项工作中，我们介绍了 ChatIR: 一个基于聊天的图像检索系统，除了初始查询之外，它还与用户进行对话以获取信息，从而阐明用户的搜索意图。受到当今基础模型功能的启发，我们利用大型语言模型来生成对初始图像描述的后续问题。这些问题与用户形成一个对话框，以便从大型语料库中检索所需的图像。在这项研究中，我们探讨了这样一个系统的能力，测试了一个大型数据集，并揭示了从事对话产生显着的图像检索收益。我们首先从现有的手动生成的数据集构建评估流水线，并探索 ChatIR 的不同模块和培训策略。我们的比较包括来自受过强化学习培训的相关应用程序的强大基线。我们的系统能够从50K 图像池中检索目标图像，经过5轮对话后，成功率超过78% ，相比之下，人类提问时的成功率为75% ，单镜头文本到图像检索时的成功率为64% 。广泛的评估揭示了强大的能力，并审查了不同设置下的 CharIR 的局限性。"
    },
    {
        "title": "Exploring Regions of Interest: Visualizing Histological Image\n  Classification for Breast Cancer using Deep Learning",
        "url": "http://arxiv.org/abs/2305.20058v1",
        "pub_date": "2023-05-31",
        "summary": "Computer aided detection and diagnosis systems based on deep learning have\nshown promising performance in breast cancer detection. However, there are\ncases where the obtained results lack justification. In this study, our\nobjective is to highlight the regions of interest used by a convolutional\nneural network (CNN) for classifying histological images as benign or\nmalignant. We compare these regions with the regions identified by\npathologists. To achieve this, we employed the VGG19 architecture and tested\nthree visualization methods: Gradient, LRP Z, and LRP Epsilon. Additionally, we\nexperimented with three pixel selection methods: Bins, K-means, and MeanShift.\nBased on the results obtained, the Gradient visualization method and the\nMeanShift selection method yielded satisfactory outcomes for visualizing the\nimages.",
        "translated": "基于深度学习的计算机辅助检测与诊断系统在乳腺癌检测中表现出良好的应用前景。然而，在某些情况下，所得结果缺乏合理性。在这项研究中，我们的目标是突出卷积神经网络(CNN)用于将组织学图像分类为良性或恶性的感兴趣区域。我们将这些区域与病理学家鉴定的区域进行比较。为了实现这一点，我们采用了 VGG19架构，并测试了三种可视化方法: 梯度、 LRP Z 和 LRP Epsilon。此外，我们还试验了三种像素选择方法: Bins、 K- 均值和 mean Shift。在此基础上，采用梯度可视化方法和 MeanShift 选择方法对图像进行可视化处理，取得了满意的效果。"
    },
    {
        "title": "Cross-Domain Car Detection Model with Integrated Convolutional Block\n  Attention Mechanism",
        "url": "http://arxiv.org/abs/2305.20055v1",
        "pub_date": "2023-05-31",
        "summary": "Car detection, particularly through camera vision, has become a major focus\nin the field of computer vision and has gained widespread adoption. While\ncurrent car detection systems are capable of good detection, reliable detection\ncan still be challenging due to factors such as proximity between the car,\nlight intensity, and environmental visibility. To address these issues, we\npropose a cross-domain car detection model that we apply to car recognition for\nautonomous driving and other areas. Our model includes several novelties:\n1)Building a complete cross-domain target detection framework. 2)Developing an\nunpaired target domain picture generation module with an integrated\nconvolutional attention mechanism. 3)Adopting Generalized Intersection over\nUnion (GIOU) as the loss function of the target detection framework.\n4)Designing an object detection model integrated with two-headed Convolutional\nBlock Attention Module(CBAM). 5)Utilizing an effective data enhancement method.\nTo evaluate the model's effectiveness, we performed a reduced will resolution\nprocess on the data in the SSLAD dataset and used it as the benchmark dataset\nfor our task. Experimental results show that the performance of the\ncross-domain car target detection model improves by 40% over the model without\nour framework, and our improvements have a significant impact on cross-domain\ncar recognition.",
        "translated": "汽车检测，特别是通过摄像机视觉，已成为计算机视觉领域的一个主要焦点，并得到了广泛的采用。虽然目前的汽车检测系统能够很好的检测，可靠的检测仍然是具有挑战性的因素，如汽车之间的接近，光强度和环境能见度。为了解决这些问题，我们提出了一个跨领域的汽车检测模型，我们应用于汽车识别的自主驾驶和其他领域。该模型具有以下特点: 1)建立了一个完整的跨域目标检测框架。2)开发具有集成卷积注意机制的非配对目标域图像生成模块。3)采用广义交叉口作为目标检测框架的损失函数。4)设计一个集成了双头卷积块注意模块(CBAM)的目标检测模型。5)采用有效的数据增强方法。为了评估模型的有效性，我们对 SSLAD 数据集中的数据进行了简化的意愿分辨率处理，并将其作为我们任务的基准数据集。实验结果表明，本文提出的跨域汽车目标检测模型的性能比没有本文框架的模型提高了40% ，并且本文的改进对跨域汽车识别具有重要的影响。"
    },
    {
        "title": "Hiera: A Hierarchical Vision Transformer without the Bells-and-Whistles",
        "url": "http://arxiv.org/abs/2306.00989v1",
        "pub_date": "2023-06-01",
        "summary": "Modern hierarchical vision transformers have added several vision-specific\ncomponents in the pursuit of supervised classification performance. While these\ncomponents lead to effective accuracies and attractive FLOP counts, the added\ncomplexity actually makes these transformers slower than their vanilla ViT\ncounterparts. In this paper, we argue that this additional bulk is unnecessary.\nBy pretraining with a strong visual pretext task (MAE), we can strip out all\nthe bells-and-whistles from a state-of-the-art multi-stage vision transformer\nwithout losing accuracy. In the process, we create Hiera, an extremely simple\nhierarchical vision transformer that is more accurate than previous models\nwhile being significantly faster both at inference and during training. We\nevaluate Hiera on a variety of tasks for image and video recognition. Our code\nand models are available at https://github.com/facebookresearch/hiera.",
        "translated": "现代分层视觉变换器为了追求有监督的分类性能，增加了几个特定于视觉的部件。虽然这些组件导致有效的准确性和吸引人的 FLOP 计数，但是增加的复杂性实际上使这些转换器比普通的 ViT 转换器慢。在本文中，我们认为这种额外的大量是不必要的。通过强视觉借口任务(MAE)的预训练，我们可以从最先进的多级视觉变换器中剔除所有花哨的东西而不会失去准确性。在这个过程中，我们创建了 Hiera，一个非常简单的分层视觉转换器，它比以前的模型更加精确，同时在推理和训练过程中都明显更快。我们对 Hiera 在图像和视频识别方面的各种任务进行了评估。我们的代码和模型可在 https://github.com/facebookresearch/hiera 获得。"
    },
    {
        "title": "StyleGAN knows Normal, Depth, Albedo, and More",
        "url": "http://arxiv.org/abs/2306.00987v1",
        "pub_date": "2023-06-01",
        "summary": "Intrinsic images, in the original sense, are image-like maps of scene\nproperties like depth, normal, albedo or shading. This paper demonstrates that\nStyleGAN can easily be induced to produce intrinsic images. The procedure is\nstraightforward. We show that, if StyleGAN produces $G({w})$ from latents\n${w}$, then for each type of intrinsic image, there is a fixed offset ${d}_c$\nso that $G({w}+{d}_c)$ is that type of intrinsic image for $G({w})$. Here\n${d}_c$ is {\\em independent of ${w}$}. The StyleGAN we used was pretrained by\nothers, so this property is not some accident of our training regime. We show\nthat there are image transformations StyleGAN will {\\em not} produce in this\nfashion, so StyleGAN is not a generic image regression engine.\n  It is conceptually exciting that an image generator should ``know'' and\nrepresent intrinsic images. There may also be practical advantages to using a\ngenerative model to produce intrinsic images. The intrinsic images obtained\nfrom StyleGAN compare well both qualitatively and quantitatively with those\nobtained by using SOTA image regression techniques; but StyleGAN's intrinsic\nimages are robust to relighting effects, unlike SOTA methods.",
        "translated": "内在图像，在原始意义上，是像图像一样的场景属性地图，如深度，法线，反照率或阴影。本文证明了 StyleGAN 可以很容易地诱导产生内部图像。程序很简单。我们展示了，如果 StyleGAN 从潜伏期 ${ w } $生成 $G ({ w }) $，那么对于每种类型的内部映像，都有一个固定的偏移量 ${ d } _ c $，因此 $G ({ w } + { d } _ c) $就是 $G ({ w }) $的内部映像类型。这里的 ${ d } _ c $是{ em，与 ${ w } $}无关。我们使用的 StyleGAN 是由其他人预先训练的，所以这个属性不是我们训练体制的某种意外。我们展示了 StyleGAN 将以这种方式产生图像变换，所以 StyleGAN 不是一个通用的图像回归引擎。图像生成器应该“知道”并表示内在图像，这在概念上是令人兴奋的。使用生成模型来产生内部图像可能也有实际的好处。从 StyleGAN 获得的内在图像与使用 SOTA 图像回归技术获得的图像在定性和定量上都比较好; 但是 StyleGAN 的内在图像对重新照明效应是稳健的，不像 SOTA 方法。"
    },
    {
        "title": "Continual Learning for Abdominal Multi-Organ and Tumor Segmentation",
        "url": "http://arxiv.org/abs/2306.00988v1",
        "pub_date": "2023-06-01",
        "summary": "The ability to dynamically extend a model to new data and classes is critical\nfor multiple organ and tumor segmentation. However, due to privacy regulations,\naccessing previous data and annotations can be problematic in the medical\ndomain. This poses a significant barrier to preserving the high segmentation\naccuracy of the old classes when learning from new classes because of the\ncatastrophic forgetting problem. In this paper, we first empirically\ndemonstrate that simply using high-quality pseudo labels can fairly mitigate\nthis problem in the setting of organ segmentation. Furthermore, we put forward\nan innovative architecture designed specifically for continuous organ and tumor\nsegmentation, which incurs minimal computational overhead. Our proposed design\ninvolves replacing the conventional output layer with a suite of lightweight,\nclass-specific heads, thereby offering the flexibility to accommodate newly\nemerging classes. These heads enable independent predictions for newly\nintroduced and previously learned classes, effectively minimizing the impact of\nnew classes on old ones during the course of continual learning. We further\npropose incorporating Contrastive Language-Image Pretraining (CLIP) embeddings\ninto the organ-specific heads. These embeddings encapsulate the semantic\ninformation of each class, informed by extensive image-text co-training. The\nproposed method is evaluated on both in-house and public abdominal CT datasets\nunder organ and tumor segmentation tasks. Empirical results suggest that the\nproposed design improves the segmentation performance of a baseline neural\nnetwork on newly-introduced and previously-learned classes along the learning\ntrajectory.",
        "translated": "动态扩展模型到新数据和类别的能力对于多器官和肿瘤分割是至关重要的。然而，由于隐私规定，访问以前的数据和注释可能在医学领域有问题。由于灾变遗忘问题的存在，使得在从新类中学习时很难保持原有类的高分割精度。在本文中，我们首先通过实验证明，在器官分割的设置中，简单地使用高质量的伪标签可以较好地缓解这个问题。此外，我们提出了一个创新的架构，专门设计的连续器官和肿瘤分割，这带来了最小的计算开销。我们提出的设计包括用一套轻量级的、特定类别的头取代传统的输出层，从而提供适应新兴类别的灵活性。这些头部能够独立预测新引入和以前学习的课程，有效地减少新课程对旧课程的影响，在不断学习的过程中。我们进一步建议将对比语言-图像预训练(CLIP)嵌入到特定器官的头部。这些嵌入式语义信息通过广泛的图像-文本联合训练，封装了每个类的内容。在内部和公共腹部 CT 数据集上，在器官和肿瘤分割任务下，对所提出的方法进行了评估。实验结果表明，提出的设计提高了基线神经网络的分割性能的新引入和以前学习的类沿学习轨迹。"
    },
    {
        "title": "Diffusion Self-Guidance for Controllable Image Generation",
        "url": "http://arxiv.org/abs/2306.00986v1",
        "pub_date": "2023-06-01",
        "summary": "Large-scale generative models are capable of producing high-quality images\nfrom detailed text descriptions. However, many aspects of an image are\ndifficult or impossible to convey through text. We introduce self-guidance, a\nmethod that provides greater control over generated images by guiding the\ninternal representations of diffusion models. We demonstrate that properties\nsuch as the shape, location, and appearance of objects can be extracted from\nthese representations and used to steer sampling. Self-guidance works similarly\nto classifier guidance, but uses signals present in the pretrained model\nitself, requiring no additional models or training. We show how a simple set of\nproperties can be composed to perform challenging image manipulations, such as\nmodifying the position or size of objects, merging the appearance of objects in\none image with the layout of another, composing objects from many images into\none, and more. We also show that self-guidance can be used to edit real images.\nFor results and an interactive demo, see our project page at\nhttps://dave.ml/selfguidance/",
        "translated": "大规模生成模型能够从详细的文本描述中生成高质量的图像。然而，图像的许多方面很难或不可能通过文本传达。我们介绍了自我引导，一种通过引导扩散模型的内部表示来提供对生成的图像更大控制的方法。我们演示了诸如物体的形状、位置和外观等属性可以从这些表示中提取出来并用于指导采样。自我引导的工作原理类似于分类器引导，但使用的信号存在于预先训练的模型本身，不需要额外的模型或训练。我们展示了如何组合一组简单的属性来执行具有挑战性的图像操作，例如修改对象的位置或大小，将一个图像中对象的外观与另一个图像的布局合并，将多个图像中的对象组合成一个等等。我们还表明，自我导航可以用来编辑真实的图像。有关结果和互动演示，请参阅我们的项目页面 https://dave.ml/selfguidance/"
    },
    {
        "title": "Using generative AI to investigate medical imagery models and datasets",
        "url": "http://arxiv.org/abs/2306.00985v1",
        "pub_date": "2023-06-01",
        "summary": "AI models have shown promise in many medical imaging tasks. However, our\nability to explain what signals these models have learned is severely lacking.\nExplanations are needed in order to increase the trust in AI-based models, and\ncould enable novel scientific discovery by uncovering signals in the data that\nare not yet known to experts. In this paper, we present a method for automatic\nvisual explanations leveraging team-based expertise by generating hypotheses of\nwhat visual signals in the images are correlated with the task. We propose the\nfollowing 4 steps: (i) Train a classifier to perform a given task (ii) Train a\nclassifier guided StyleGAN-based image generator (StylEx) (iii) Automatically\ndetect and visualize the top visual attributes that the classifier is sensitive\ntowards (iv) Formulate hypotheses for the underlying mechanisms, to stimulate\nfuture research. Specifically, we present the discovered attributes to an\ninterdisciplinary panel of experts so that hypotheses can account for social\nand structural determinants of health. We demonstrate results on eight\nprediction tasks across three medical imaging modalities: retinal fundus\nphotographs, external eye photographs, and chest radiographs. We showcase\nexamples of attributes that capture clinically known features, confounders that\narise from factors beyond physiological mechanisms, and reveal a number of\nphysiologically plausible novel attributes. Our approach has the potential to\nenable researchers to better understand, improve their assessment, and extract\nnew knowledge from AI-based models. Importantly, we highlight that attributes\ngenerated by our framework can capture phenomena beyond physiology or\npathophysiology, reflecting the real world nature of healthcare delivery and\nsocio-cultural factors. Finally, we intend to release code to enable\nresearchers to train their own StylEx models and analyze their predictive\ntasks.",
        "translated": "人工智能模型已经在许多医学成像任务中显示出希望。然而，我们严重缺乏解释这些模型所学到的信号的能力。为了增加人们对基于人工智能的模型的信任，需要做出解释，并且可以通过发现专家尚不知道的数据中的信号来实现新的科学发现。在本文中，我们提出了一种方法，自动视觉解释利用团队为基础的专业知识，通过生成的假设，什么视觉信号的图像是相关的任务。我们提出以下4个步骤: (i)训练分类器来执行给定的任务(ii)训练分类器引导的基于 StylEx 的图像生成器(StylEx)(iii)自动检测和可视化分类器对于(iv)制定潜在机制的假设，以刺激未来的研究。具体来说，我们将发现的属性提交给一个跨学科专家小组，以便假设能够解释健康的社会和结构决定因素。我们展示了三种医学成像方式的八个预测任务的结果: 视网膜眼底照片，外眼照片和胸片。我们展示了捕获临床已知特征的属性的例子，由生理机制以外的因素产生的混杂因素，并揭示了一些生理上合理的新属性。我们的方法有潜力使研究人员能够更好地理解，改进他们的评估，并从基于人工智能的模型中提取新的知识。重要的是，我们强调由我们的框架产生的属性可以捕获生理学或病理生理学之外的现象，反映医疗保健提供的真实世界性质和社会文化因素。最后，我们打算发布代码，使研究人员能够训练他们自己的 StylEx 模型和分析他们的预测任务。"
    },
    {
        "title": "StyleDrop: Text-to-Image Generation in Any Style",
        "url": "http://arxiv.org/abs/2306.00983v1",
        "pub_date": "2023-06-01",
        "summary": "Pre-trained large text-to-image models synthesize impressive images with an\nappropriate use of text prompts. However, ambiguities inherent in natural\nlanguage and out-of-distribution effects make it hard to synthesize image\nstyles, that leverage a specific design pattern, texture or material. In this\npaper, we introduce StyleDrop, a method that enables the synthesis of images\nthat faithfully follow a specific style using a text-to-image model. The\nproposed method is extremely versatile and captures nuances and details of a\nuser-provided style, such as color schemes, shading, design patterns, and local\nand global effects. It efficiently learns a new style by fine-tuning very few\ntrainable parameters (less than $1\\%$ of total model parameters) and improving\nthe quality via iterative training with either human or automated feedback.\nBetter yet, StyleDrop is able to deliver impressive results even when the user\nsupplies only a single image that specifies the desired style. An extensive\nstudy shows that, for the task of style tuning text-to-image models, StyleDrop\nimplemented on Muse convincingly outperforms other methods, including\nDreamBooth and textual inversion on Imagen or Stable Diffusion. More results\nare available at our project website: https://styledrop.github.io",
        "translated": "预先训练的大型文本到图像模型通过适当使用文本提示合成令人印象深刻的图像。然而，自然语言固有的模糊性和分布外效果使得很难综合利用特定设计模式、纹理或材质的图像风格。在本文中，我们介绍 StyleDrop，这是一种使用文本到图像模型实现忠实遵循特定风格的图像合成的方法。提出的方法是非常通用的，并捕获细微差别和用户提供的样式细节，如配色方案，阴影，设计模式，局部和全局效果。它通过微调很少的可训练参数(不到总模型参数的1%) ，并通过人工或自动反馈的迭代训练来提高质量，从而有效地学习了一种新的风格。更好的是，StyleDrop 能够提供令人印象深刻的结果，即使用户只提供一个指定所需样式的图像。一项广泛的研究表明，在风格调整文本到图像模型的任务中，在 Muse 上实现的 StyleDrop 比其他方法更有说服力，包括 DreamBooth 和在 Imagen 或稳定扩散上的文本反转。更多结果可浏览计划网页:  https://styledrop.github.io"
    },
    {
        "title": "StableRep: Synthetic Images from Text-to-Image Models Make Strong Visual\n  Representation Learners",
        "url": "http://arxiv.org/abs/2306.00984v1",
        "pub_date": "2023-06-01",
        "summary": "We investigate the potential of learning visual representations using\nsynthetic images generated by text-to-image models. This is a natural question\nin the light of the excellent performance of such models in generating\nhigh-quality images. We consider specifically the Stable Diffusion, one of the\nleading open source text-to-image models. We show that (1) when the generative\nmodel is configured with proper classifier-free guidance scale, training\nself-supervised methods on synthetic images can match or beat the real image\ncounterpart; (2) by treating the multiple images generated from the same text\nprompt as positives for each other, we develop a multi-positive contrastive\nlearning method, which we call StableRep. With solely synthetic images, the\nrepresentations learned by StableRep surpass the performance of representations\nlearned by SimCLR and CLIP using the same set of text prompts and corresponding\nreal images, on large scale datasets. When we further add language supervision,\nStableRep trained with 20M synthetic images achieves better accuracy than CLIP\ntrained with 50M real images.",
        "translated": "我们研究了使用由文本到图像模型生成的合成图像来学习视觉表征的潜力。鉴于此类模型在生成高质量图像方面的出色表现，这是一个自然而然的问题。我们特别考虑了稳定扩散，它是领先的开源文本到图像模型之一。我们发现(1)当生成模型配置了合适的无分类器指导标度时，合成图像上的自我监督训练方法可以匹配或者击败真实图像的对应物; (2)通过将同一文本提示生成的多个图像作为对应的正值处理，我们开发了一种多正值对比学习方法，我们称之为稳定代表。对于单独的合成图像，StableRep 学到的表示超过了 SimCLR 和 CLIP 在大规模数据集上使用相同的文本提示和相应的实际图像学到的表示。在进一步增加语言监控时，使用20M 合成图像训练的 StableRep 比使用50M 真实图像训练的 CLIP 具有更好的准确性。"
    },
    {
        "title": "SnapFusion: Text-to-Image Diffusion Model on Mobile Devices within Two\n  Seconds",
        "url": "http://arxiv.org/abs/2306.00980v1",
        "pub_date": "2023-06-01",
        "summary": "Text-to-image diffusion models can create stunning images from natural\nlanguage descriptions that rival the work of professional artists and\nphotographers. However, these models are large, with complex network\narchitectures and tens of denoising iterations, making them computationally\nexpensive and slow to run. As a result, high-end GPUs and cloud-based inference\nare required to run diffusion models at scale. This is costly and has privacy\nimplications, especially when user data is sent to a third party. To overcome\nthese challenges, we present a generic approach that, for the first time,\nunlocks running text-to-image diffusion models on mobile devices in less than\n$2$ seconds. We achieve so by introducing efficient network architecture and\nimproving step distillation. Specifically, we propose an efficient UNet by\nidentifying the redundancy of the original model and reducing the computation\nof the image decoder via data distillation. Further, we enhance the step\ndistillation by exploring training strategies and introducing regularization\nfrom classifier-free guidance. Our extensive experiments on MS-COCO show that\nour model with $8$ denoising steps achieves better FID and CLIP scores than\nStable Diffusion v$1.5$ with $50$ steps. Our work democratizes content creation\nby bringing powerful text-to-image diffusion models to the hands of users.",
        "translated": "文本到图像的扩散模型可以从自然语言描述中创造出令人惊叹的图像，与专业艺术家和摄影师的作品相媲美。然而，这些模型是庞大的，具有复杂的网络架构和数十个去噪迭代，使得它们计算昂贵，运行缓慢。因此，高端的 GPU 和基于云的推理需要在规模上运行扩散模型。这样做代价高昂，而且有隐私方面的影响，尤其是当用户数据被发送给第三方时。为了克服这些挑战，我们提出了一个通用的方法，第一次解锁运行文本到图像的扩散模型在移动设备上在不到2美元的秒。我们通过引入有效的网络结构和改进步骤精馏实现了这一目标。具体来说，我们提出了一种有效的 UNet，通过识别原始模型的冗余，并通过数据提取减少了图像解码器的计算量。进一步，我们通过探索训练策略和引入无分类器指导的正则化来增强步骤精馏。我们在 MS-COCO 上的大量实验表明，我们的模型使用 $8 $去噪步骤比使用 $50 $步骤的稳定扩散 v 使用 $1.5 $获得更好的 FID 和 CLIP 分数。我们的工作通过将强大的文本到图像的扩散模型带到用户手中，使内容创建民主化。"
    },
    {
        "title": "Building Rearticulable Models for Arbitrary 3D Objects from 4D Point\n  Clouds",
        "url": "http://arxiv.org/abs/2306.00979v1",
        "pub_date": "2023-06-01",
        "summary": "We build rearticulable models for arbitrary everyday man-made objects\ncontaining an arbitrary number of parts that are connected together in\narbitrary ways via 1 degree-of-freedom joints. Given point cloud videos of such\neveryday objects, our method identifies the distinct object parts, what parts\nare connected to what other parts, and the properties of the joints connecting\neach part pair. We do this by jointly optimizing the part segmentation,\ntransformation, and kinematics using a novel energy minimization framework. Our\ninferred animatable models, enables retargeting to novel poses with sparse\npoint correspondences guidance. We test our method on a new articulating robot\ndataset, and the Sapiens dataset with common daily objects, as well as\nreal-world scans. Experiments show that our method outperforms two leading\nprior works on various metrics.",
        "translated": "我们建立了任意日常人造物体的可表达模型，其中包含任意数量的部件，这些部件通过1个自由度的关节以任意方式连接在一起。给定这些日常物体的点云视频，我们的方法识别不同的物体部分，哪些部分连接到哪些其他部分，以及连接每个部分对的关节的属性。我们通过使用一种新的能量最小化框架来联合优化零件分割、变换和运动学。我们推断的动画模型，使稀疏点对应指导的新姿态重定向。我们测试我们的方法在一个新的铰接机器人数据集，智人数据集与常见的日常对象，以及真实世界的扫描。实验结果表明，该方法在各种度量指标上均优于前两种方法。"
    },
    {
        "title": "AGILE3D: Attention Guided Interactive Multi-object 3D Segmentation",
        "url": "http://arxiv.org/abs/2306.00977v1",
        "pub_date": "2023-06-01",
        "summary": "During interactive segmentation, a model and a user work together to\ndelineate objects of interest in a 3D point cloud. In an iterative process, the\nmodel assigns each data point to an object (or the background), while the user\ncorrects errors in the resulting segmentation and feeds them back into the\nmodel. From a machine learning perspective the goal is to design the model and\nthe feedback mechanism in a way that minimizes the required user input. The\ncurrent best practice segments objects one at a time, and asks the user to\nprovide positive clicks to indicate regions wrongly assigned to the background\nand negative clicks to indicate regions wrongly assigned to the object\n(foreground). Sequentially visiting objects is wasteful, since it disregards\nsynergies between objects: a positive click for a given object can, by\ndefinition, serve as a negative click for nearby objects, moreover a direct\ncompetition between adjacent objects can speed up the identification of their\ncommon boundary. We introduce AGILE3D, an efficient, attention-based model that\n(1) supports simultaneous segmentation of multiple 3D objects, (2) yields more\naccurate segmentation masks with fewer user clicks, and (3) offers faster\ninference. We encode the point cloud into a latent feature representation, and\nview user clicks as queries and employ cross-attention to represent contextual\nrelations between different click locations as well as between clicks and the\n3D point cloud features. Every time new clicks are added, we only need to run a\nlightweight decoder that produces updated segmentation masks. In experiments\nwith four different point cloud datasets, AGILE3D sets a new state of the art,\nmoreover, we also verify its practicality in real-world setups with a real user\nstudy.",
        "translated": "在交互式分割过程中，模型和用户一起工作来描绘三维点云中感兴趣的对象。在迭代过程中，模型将每个数据点分配给一个对象(或背景) ，同时用户纠正结果分割中的错误并将它们反馈给模型。从机器学习的角度来看，目标是设计模型和反馈机制，以最小化所需的用户输入。当前的最佳实践是一次分割一个对象，并要求用户提供正点击来指示错误地分配给背景的区域，以及负点击来指示错误地分配给对象(前景)的区域。顺序访问物体是浪费的，因为它忽视了物体之间的协同作用: 根据定义，对一个给定物体的正面点击可以作为对附近物体的负面点击，此外，相邻物体之间的直接竞争可以加速确定其共同边界。我们介绍了 AGILE3D，一个有效的，基于注意力的模型，它(1)支持多个3D 对象的同时分割，(2)产生更准确的分割掩码，用户点击更少，(3)提供更快的推断。我们将点云编码成一个潜在的特征表示，并将用户点击视为查询，并使用交叉注意来表示不同点击位置之间以及点击与3D 点云特征之间的上下文关系。每次新的点击被添加，我们只需要运行一个轻量级的解码器，产生更新的分割掩码。通过对四种不同的点云数据集进行实验，AGILE3D 提出了一种新的设置方法，并通过实际的用户研究验证了该方法的实用性。"
    },
    {
        "title": "OCBEV: Object-Centric BEV Transformer for Multi-View 3D Object Detection",
        "url": "http://arxiv.org/abs/2306.01738v1",
        "pub_date": "2023-06-02",
        "summary": "Multi-view 3D object detection is becoming popular in autonomous driving due\nto its high effectiveness and low cost. Most of the current state-of-the-art\ndetectors follow the query-based bird's-eye-view (BEV) paradigm, which benefits\nfrom both BEV's strong perception power and end-to-end pipeline. Despite\nachieving substantial progress, existing works model objects via globally\nleveraging temporal and spatial information of BEV features, resulting in\nproblems when handling the challenging complex and dynamic autonomous driving\nscenarios. In this paper, we proposed an Object-Centric query-BEV detector\nOCBEV, which can carve the temporal and spatial cues of moving targets more\neffectively. OCBEV comprises three designs: Object Aligned Temporal Fusion\naligns the BEV feature based on ego-motion and estimated current locations of\nmoving objects, leading to a precise instance-level feature fusion. Object\nFocused Multi-View Sampling samples more 3D features from an adaptive local\nheight ranges of objects for each scene to enrich foreground information.\nObject Informed Query Enhancement replaces part of pre-defined decoder queries\nin common DETR-style decoders with positional features of objects on\nhigh-confidence locations, introducing more direct object positional priors.\nExtensive experimental evaluations are conducted on the challenging nuScenes\ndataset. Our approach achieves a state-of-the-art result, surpassing the\ntraditional BEVFormer by 1.5 NDS points. Moreover, we have a faster convergence\nspeed and only need half of the training iterations to get comparable\nperformance, which further demonstrates its effectiveness.",
        "translated": "多视角三维目标检测由于其高效、低成本的特点，在自动驾驶领域越来越受到人们的青睐。目前大多数最先进的检测器遵循基于查询的鸟瞰(BEV)范式，这得益于 BEV 强大的感知能力和端到端流水线。尽管取得了重大进展，现有的工程模型通过全球利用 BEV 功能的时间和空间信息对对象进行建模，导致在处理具有挑战性的复杂和动态自主驾驶场景时出现问题。本文提出了一种以对象为中心的查询-BEV 检测器 OCBEV，它可以更有效地刻画运动目标的时间和空间线索。OCBEV 包括三种设计: 基于自我运动和运动物体当前位置估计的对象对齐时间融合(Object AlliedTimalFusion)对准 BEV 特征，实现精确的实例级特征融合。对象聚焦多视点采样从一个自适应的局部高度范围的对象更多的三维特征为每个场景，以丰富前景信息。对象知情查询增强在常见的 DETR 样式的解码器中替换了部分预定义的解码器查询，在高置信度位置上引入了对象的位置特征，引入了更直接的对象位置先验。广泛的实验评估是在具有挑战性的 nuScenes 数据集上进行的。我们的方法获得了最先进的结果，比传统的 BEV 方法提高了1.5个 NDS 点。此外，算法收敛速度较快，只需要一半的训练迭代就可以获得相当的性能，进一步证明了算法的有效性。"
    },
    {
        "title": "DaTaSeg: Taming a Universal Multi-Dataset Multi-Task Segmentation Model",
        "url": "http://arxiv.org/abs/2306.01736v1",
        "pub_date": "2023-06-02",
        "summary": "Observing the close relationship among panoptic, semantic and instance\nsegmentation tasks, we propose to train a universal multi-dataset multi-task\nsegmentation model: DaTaSeg.We use a shared representation (mask proposals with\nclass predictions) for all tasks. To tackle task discrepancy, we adopt\ndifferent merge operations and post-processing for different tasks. We also\nleverage weak-supervision, allowing our segmentation model to benefit from\ncheaper bounding box annotations. To share knowledge across datasets, we use\ntext embeddings from the same semantic embedding space as classifiers and share\nall network parameters among datasets. We train DaTaSeg on ADE semantic, COCO\npanoptic, and Objects365 detection datasets. DaTaSeg improves performance on\nall datasets, especially small-scale datasets, achieving 54.0 mIoU on ADE\nsemantic and 53.5 PQ on COCO panoptic. DaTaSeg also enables weakly-supervised\nknowledge transfer on ADE panoptic and Objects365 instance segmentation.\nExperiments show DaTaSeg scales with the number of training datasets and\nenables open-vocabulary segmentation through direct transfer. In addition, we\nannotate an Objects365 instance segmentation set of 1,000 images and will\nrelease it as a public benchmark.",
        "translated": "考虑到视觉、语义和实例分割任务之间的密切关系，我们提出了一个通用的多数据集多任务分割模型: DataSeg. 我们对所有任务使用共享表示(带有类预测的掩码提议)。为了解决任务间的差异，我们针对不同的任务采用不同的合并操作和后处理。我们还利用弱监督，允许我们的细分模型受益于更便宜的边界框注释。为了在数据集之间共享知识，我们使用来自同一语义嵌入空间的文本嵌入作为分类器，并在数据集之间共享所有的网络参数。我们在 ADE 语义、 COCO 全景和 Objects365检测数据集上训练 DataSeg。DaTaSeg 提高了所有数据集的性能，特别是小规模数据集，在 ADE 语义上达到54.0 mIoU，在 COCO 泛光网络上达到53.5 PQ。DaTaSeg 还支持弱监督的 ADE 全景和 Objects365实例分割知识转移。实验结果表明，DataSeg 可以根据训练数据集的数量进行分割，并且可以通过直接传输实现开放词汇表的分割。此外，我们还注释了一个由1,000个图像组成的 Objects365实例分割集，并将其作为公共基准发布。"
    },
    {
        "title": "Multilingual Conceptual Coverage in Text-to-Image Models",
        "url": "http://arxiv.org/abs/2306.01735v1",
        "pub_date": "2023-06-02",
        "summary": "We propose \"Conceptual Coverage Across Languages\" (CoCo-CroLa), a technique\nfor benchmarking the degree to which any generative text-to-image system\nprovides multilingual parity to its training language in terms of tangible\nnouns. For each model we can assess \"conceptual coverage\" of a given target\nlanguage relative to a source language by comparing the population of images\ngenerated for a series of tangible nouns in the source language to the\npopulation of images generated for each noun under translation in the target\nlanguage. This technique allows us to estimate how well-suited a model is to a\ntarget language as well as identify model-specific weaknesses, spurious\ncorrelations, and biases without a-priori assumptions. We demonstrate how it\ncan be used to benchmark T2I models in terms of multilinguality, and how\ndespite its simplicity it is a good proxy for impressive generalization.",
        "translated": "我们提出了“跨语言的概念覆盖”(CoCo-CroLa) ，一种基准测试的程度，任何生成性文本到图像系统提供多语言平等的训练语言在有形名词方面。对于每个模型，我们可以通过比较源语言中一系列有形名词生成的图像的总体与目标语言中翻译下的每个名词生成的图像的总体来评估给定目标语言相对于源语言的“概念覆盖”。这种技术使我们能够估计模型与目标语言的匹配程度，并且在没有先验假设的情况下识别特定于模型的弱点、虚假的相关性和偏差。我们展示了如何使用它来基准 T2I 模型的多语言性，以及如何尽管它的简单性，它是一个令人印象深刻的推广良好的代理。"
    },
    {
        "title": "DocFormerv2: Local Features for Document Understanding",
        "url": "http://arxiv.org/abs/2306.01733v1",
        "pub_date": "2023-06-02",
        "summary": "We propose DocFormerv2, a multi-modal transformer for Visual Document\nUnderstanding (VDU). The VDU domain entails understanding documents (beyond\nmere OCR predictions) e.g., extracting information from a form, VQA for\ndocuments and other tasks. VDU is challenging as it needs a model to make sense\nof multiple modalities (visual, language and spatial) to make a prediction. Our\napproach, termed DocFormerv2 is an encoder-decoder transformer which takes as\ninput - vision, language and spatial features. DocFormerv2 is pre-trained with\nunsupervised tasks employed asymmetrically i.e., two novel document tasks on\nencoder and one on the auto-regressive decoder. The unsupervised tasks have\nbeen carefully designed to ensure that the pre-training encourages\nlocal-feature alignment between multiple modalities. DocFormerv2 when evaluated\non nine datasets shows state-of-the-art performance over strong baselines e.g.\nTabFact (4.3%), InfoVQA (1.4%), FUNSD (1%). Furthermore, to show generalization\ncapabilities, on three VQA tasks involving scene-text, Doc- Formerv2\noutperforms previous comparably-sized models and even does better than much\nlarger models (such as GIT2, PaLi and Flamingo) on some tasks. Extensive\nablations show that due to its pre-training, DocFormerv2 understands multiple\nmodalities better than prior-art in VDU.",
        "translated": "我们提出 DocFormerv2，一个用于可视化文档理解(VDU)的多模式转换器。VDU 领域需要理解文档(超越单纯的 OCR 预测) ，例如，从表单中提取信息，文档的 VQA 和其他任务。VDU 是具有挑战性的，因为它需要一个模型来理解多种形式(视觉、语言和空间)来做出预测。我们的方法，称为 DocFormerv2是一个编码器-解码器转换器，它采取作为输入-视觉，语言和空间特征。DocFormerv2预先训练了非对称使用的非监督任务，即编码器上的两个新的文档任务和自动回归解码器上的一个任务。这些无监督的任务经过精心设计，以确保预先培训鼓励多种模式之间的局部特征对齐。对9个数据集进行评估后，DocFormerv2显示出超过强基线的最先进性能，例如 TabFact (4.3%) ，InfoVQA (1.4%) ，FUNSD (1%)。此外，为了显示泛化能力，在涉及场景文本的三个 VQA 任务中，Doc-Formerv2在一些任务中表现优于以前的同等大小的模型，甚至优于更大的模型(如 GIT2、 PaLi 和 Flamingo)。广泛的消融表明，由于其预先培训，DocFormerv2了解多种形式更好地比先前的技术在 VDU。"
    },
    {
        "title": "Video Colorization with Pre-trained Text-to-Image Diffusion Models",
        "url": "http://arxiv.org/abs/2306.01732v1",
        "pub_date": "2023-06-02",
        "summary": "Video colorization is a challenging task that involves inferring plausible\nand temporally consistent colors for grayscale frames. In this paper, we\npresent ColorDiffuser, an adaptation of a pre-trained text-to-image latent\ndiffusion model for video colorization. With the proposed adapter-based\napproach, we repropose the pre-trained text-to-image model to accept input\ngrayscale video frames, with the optional text description, for video\ncolorization. To enhance the temporal coherence and maintain the vividness of\ncolorization across frames, we propose two novel techniques: the Color\nPropagation Attention and Alternated Sampling Strategy. Color Propagation\nAttention enables the model to refine its colorization decision based on a\nreference latent frame, while Alternated Sampling Strategy captures\nspatiotemporal dependencies by using the next and previous adjacent latent\nframes alternatively as reference during the generative diffusion sampling\nsteps. This encourages bidirectional color information propagation between\nadjacent video frames, leading to improved color consistency across frames. We\nconduct extensive experiments on benchmark datasets, and the results\ndemonstrate the effectiveness of our proposed framework. The evaluations show\nthat ColorDiffuser achieves state-of-the-art performance in video colorization,\nsurpassing existing methods in terms of color fidelity, temporal consistency,\nand visual quality.",
        "translated": "视频着色是一个具有挑战性的任务，涉及推断合理和时间一致的灰度帧颜色。在本文中，我们提出了一个比较适合于视频彩色化的文本到图像潜在扩散模型。采用基于适配器的方法，我们重新提出了预训练的文本到图像模型来接受输入的灰度视频帧，以及可选的文本描述，用于视频彩色化。为了提高时间一致性和保持跨帧彩色化的生动性，我们提出了两种新的技术: 色彩传播注意和交替采样策略。色彩传播注意力使模型能够基于参考潜框架精化其着色决策，而交替采样策略通过在生成扩散采样步骤中交替使用下一个和前一个相邻潜框架作为参考来捕获时空依赖性。这将鼓励相邻视频帧之间的双向颜色信息传播，从而提高帧间的颜色一致性。我们对基准数据集进行了广泛的实验，实验结果表明了我们提出的框架的有效性。评估结果表明，ColorDiffuser 在视频彩色化方面达到了最先进的水平，在色彩保真度、时间一致性和视觉质量方面都超过了现有的彩色化方法。"
    },
    {
        "title": "Denoising Diffusion Semantic Segmentation with Mask Prior Modeling",
        "url": "http://arxiv.org/abs/2306.01721v1",
        "pub_date": "2023-06-02",
        "summary": "The evolution of semantic segmentation has long been dominated by learning\nmore discriminative image representations for classifying each pixel. Despite\nthe prominent advancements, the priors of segmentation masks themselves, e.g.,\ngeometric and semantic constraints, are still under-explored. In this paper, we\npropose to ameliorate the semantic segmentation quality of existing\ndiscriminative approaches with a mask prior modeled by a recently-developed\ndenoising diffusion generative model. Beginning with a unified architecture\nthat adapts diffusion models for mask prior modeling, we focus this work on a\nspecific instantiation with discrete diffusion and identify a variety of key\ndesign choices for its successful application. Our exploratory analysis\nrevealed several important findings, including: (1) a simple integration of\ndiffusion models into semantic segmentation is not sufficient, and a\npoorly-designed diffusion process might lead to degradation in segmentation\nperformance; (2) during the training, the object to which noise is added is\nmore important than the type of noise; (3) during the inference, the strict\ndiffusion denoising scheme may not be essential and can be relaxed to a simpler\nscheme that even works better. We evaluate the proposed prior modeling with\nseveral off-the-shelf segmentors, and our experimental results on ADE20K and\nCityscapes demonstrate that our approach could achieve competitively\nquantitative performance and more appealing visual quality.",
        "translated": "长期以来，语义分割的发展主要是通过学习更具鉴别力的图像表示来对每个像素进行分类。尽管取得了显著的进步，但是先前的分割掩盖本身，例如，几何和语义约束，仍然没有得到充分的探索。在这篇文章中，我们建议改善现有的区分方法的语义分割质量，使用最近开发的去噪扩散生成模型建模的掩模。从一个统一的体系结构开始，该体系结构将扩散模型应用于掩模先验建模，我们将工作集中在一个具体的离散扩散实例上，并确定了各种关键的设计选择，以使其成功应用。我们的探索性分析揭示了几个重要的发现，包括: (1)扩散模型在语义分割中的简单整合是不够的，设计不当的扩散过程可能导致分割性能的下降; (2)在训练过程中，加入噪声的对象比噪声类型更重要; (3)在推断过程中，严格的扩散去噪方案可能不是必要的，可以放宽到一个更简单的方案，甚至更好的工作。我们用几个现成的分割器来评估提出的先验模型，我们在 ADE20K 和 Cityscape 上的实验结果表明，我们的方法可以实现有竞争力的定量性能和更吸引人的视觉质量。"
    },
    {
        "title": "Resolving Interference When Merging Models",
        "url": "http://arxiv.org/abs/2306.01708v1",
        "pub_date": "2023-06-02",
        "summary": "Transfer learning - i.e., further fine-tuning a pre-trained model on a\ndownstream task - can confer significant advantages, including improved\ndownstream performance, faster convergence, and better sample efficiency. These\nadvantages have led to a proliferation of task-specific fine-tuned models,\nwhich typically can only perform a single task and do not benefit from one\nanother. Recently, model merging techniques have emerged as a solution to\ncombine multiple task-specific models into a single multitask model without\nperforming additional training. However, existing merging methods often ignore\nthe interference between parameters of different models, resulting in large\nperformance drops when merging multiple models. In this paper, we demonstrate\nthat prior merging techniques inadvertently lose valuable information due to\ntwo major sources of interference: (a) interference due to redundant parameter\nvalues and (b) disagreement on the sign of a given parameter's values across\nmodels. To address this, we propose our method, TrIm, Elect Sign &amp; Merge\n(TIES-Merging), which introduces three novel steps when merging models: (1)\nresetting parameters that only changed a small amount during fine-tuning, (2)\nresolving sign conflicts, and (3) merging only the parameters that are in\nalignment with the final agreed-upon sign. We find that TIES-Merging\noutperforms several existing methods in diverse settings covering a range of\nmodalities, domains, number of tasks, model sizes, architectures, and\nfine-tuning settings. We further analyze the impact of different types of\ninterference on model parameters, highlight the importance of resolving sign\ninterference. Our code is available at\nhttps://github.com/prateeky2806/ties-merging",
        "translated": "转移学习——即进一步微调下游任务的预先训练的模型——可以带来显著的优势，包括改善下游性能、加快收敛速度和提高采样效率。这些优势导致了特定于任务的微调模型的激增，这些模型通常只能执行单个任务，并且不能从彼此中受益。最近，模型合并技术已经成为一种解决方案，可以将多个任务特定的模型合并成一个单一的多任务模型，而不需要进行额外的训练。然而，现有的合并方法往往忽略了不同模型参数之间的干扰，导致合并多个模型时性能大幅度下降。在本文中，我们证明了先前的合并技术无意中失去了有价值的信息，由于两个主要的干扰来源: (a)由于冗余参数值的干扰和(b)在给定的参数值的符号不一致跨模型。为了解决这个问题，我们提出了我们的方法，TrIm，Elect Sign & Merge (TIES-Merging) ，它在合并模型时引入了三个新的步骤: (1)重置在微调过程中只改变了很少量的参数，(2)解决符号冲突，(3)只合并与最终达成一致的符号一致的参数。我们发现 TIES-Merging 在不同的设置中优于几种现有的方法，包括一系列模式、领域、任务数量、模型大小、架构和微调设置。进一步分析了不同类型的干扰对模型参数的影响，强调了解决符号干扰的重要性。我们的代码可以在 https://github.com/prateeky2806/ties-merging 找到"
    },
    {
        "title": "Is Generative Modeling-based Stylization Necessary for Domain Adaptation\n  in Regression Tasks?",
        "url": "http://arxiv.org/abs/2306.01706v1",
        "pub_date": "2023-06-02",
        "summary": "Unsupervised domain adaptation (UDA) aims to bridge the gap between source\nand target domains in the absence of target domain labels using two main\ntechniques: input-level alignment (such as generative modeling and stylization)\nand feature-level alignment (which matches the distribution of the feature\nmaps, e.g. gradient reversal layers). Motivated from the success of generative\nmodeling for image classification, stylization-based methods were recently\nproposed for regression tasks, such as pose estimation. However, use of\ninput-level alignment via generative modeling and stylization incur additional\noverhead and computational complexity which limit their use in real-world DA\ntasks. To investigate the role of input-level alignment for DA, we ask the\nfollowing question: Is generative modeling-based stylization necessary for\nvisual domain adaptation in regression? Surprisingly, we find that\ninput-alignment has little effect on regression tasks as compared to\nclassification. Based on these insights, we develop a non-parametric\nfeature-level domain alignment method -- Implicit Stylization (ImSty) -- which\nresults in consistent improvements over SOTA regression task, without the need\nfor computationally intensive stylization and generative modeling. Our work\nconducts a critical evaluation of the role of generative modeling and\nstylization, at a time when these are also gaining popularity for domain\ngeneralization.",
        "translated": "无监督域自适应(UDA)的目标是在没有目标域标签的情况下，使用两种主要技术来弥合源域和目标域之间的差距: 输入级别对齐(如生成建模和样式化)和特征级别对齐(匹配特征映射的分布，如梯度反转层)。由于生成建模在图像分类中的成功，基于风格化的方法最近被提出用于回归任务，如姿态估计。然而，通过生成建模和样式化使用输入级对齐会带来额外的开销和计算复杂性，从而限制了它们在实际 DA 任务中的使用。为了研究输入水平对齐在 DA 中的作用，我们提出以下问题: 基于生成建模的程式化对于回归中的视觉领域适应是必要的吗？令人惊讶的是，我们发现与分类相比，输入对齐对回归任务的影响很小。基于这些见解，我们开发了一种非参数特征级别的领域对齐方法——隐式样式化(ImSty)——它能够在 SOTA 回归任务上取得一致的改进，而不需要计算密集型样式化和生成建模。我们的工作对生成建模和程式化的作用进行了批判性的评估，同时它们也因领域泛化而越来越受欢迎。"
    },
    {
        "title": "Unique Brain Network Identification Number for Parkinson's Individuals\n  Using Structural MRI",
        "url": "http://arxiv.org/abs/2306.01689v1",
        "pub_date": "2023-06-02",
        "summary": "We propose a novel algorithm called Unique Brain Network Identification\nNumber (UBNIN) for encoding brain networks of individual subject. To realize\nthis objective, we employed T1-weighted structural MRI of 180 Parkinson's\ndisease (PD) patients from National Institute of Mental Health and\nNeurosciences, India. We parcellated each subject's brain volume and\nconstructed individual adjacency matrix using correlation between grey matter\n(GM) volume of every pair of regions. The unique code is derived from values\nrepresenting connections of every node (i), weighted by a factor of 2^-(i-1).\nThe numerical representation UBNIN was observed to be distinct for each\nindividual brain network, which may also be applied to other neuroimaging\nmodalities. This model may be implemented as neural signature of a person's\nunique brain connectivity, thereby useful for brainprinting applications.\nAdditionally, we segregated the above dataset into five age-cohorts:\nA:22-32years, B:33-42years, C:43-52years, D:53-62years and E:63-72years to\nstudy the variation in network topology over age. Sparsity was adopted as the\nthreshold estimate to binarize each age-based correlation matrix. Connectivity\nmetrics were obtained using Brain Connectivity toolbox-based MATLAB functions.\nFor each age-cohort, a decreasing trend was observed in mean clustering\ncoefficient with increasing sparsity. Significantly different clustering\ncoefficient was noted between age-cohort B and C (sparsity: 0.63,0.66), C and E\n(sparsity: 0.66,0.69). Our findings suggest network connectivity patterns\nchange with age, indicating network disruption due to the underlying\nneuropathology. Varying clustering coefficient for different cohorts indicate\nthat information transfer between neighboring nodes change with age. This\nprovides evidence on age-related brain shrinkage and network degeneration.",
        "translated": "我们提出了一种新的算法称为唯一的脑网络识别号(UBNIN)编码的个人主题的大脑网络。为了实现这一目标，我们对来自印度国家精神卫生和神经科学研究所的180名帕金森病(PD)患者进行了 T1加权结构 MRI 检查。我们将每个受试者的大脑体积打包，并利用每对区域的灰质(GM)体积之间的相关性构建个体邻接矩阵。唯一的代码是从代表每个节点(i)的连接的值派生出来的，由2 ^-(i-1)的因子加权。数字代表 UBNIN 被观察到是不同的每个个人的大脑网络，这也可以应用于其他神经影像模式。这个模型可以作为一个人独特的大脑连通性的神经特征来实现，因此对于大脑印记应用是有用的。此外，我们将上述数据集分为五个年龄组: A: 22-32岁，B: 33-42岁，C: 43-52岁，D: 53-62岁和 E: 63-72岁，以研究网络拓扑随年龄的变化。采用稀疏度作为阈值估计，对每个基于年龄的相关矩阵进行二值化。使用基于 MATLAB 函数的脑连通性工具箱获得连通性度量。对于每个年龄组，随着稀疏程度的增加，平均集聚系数呈下降趋势。年龄组别乙与丙(稀疏度: 0.63,0.66)、丙与戊(稀疏度: 0.66,0.69)的集聚系数有显著差异。我们的研究结果表明，网络连接模式随着年龄的增长而改变，表明网络中断是由于潜在的神经病理学。不同群组的不同集聚系数表明，相邻节点之间的信息传递随着年龄的增长而变化。这为年龄相关的大脑萎缩和网络退化提供了证据。"
    },
    {
        "title": "MKOR: Momentum-Enabled Kronecker-Factor-Based Optimizer Using Rank-1\n  Updates",
        "url": "http://arxiv.org/abs/2306.01685v1",
        "pub_date": "2023-06-02",
        "summary": "This work proposes a Momentum-Enabled Kronecker-Factor-Based Optimizer Using\nRank-1 updates, called MKOR, that improves the training time and convergence\nproperties of deep neural networks (DNNs). Second-order techniques, while\nenjoying higher convergence rates vs first-order counterparts, have cubic\ncomplexity with respect to either the model size and/or the training batch\nsize. Hence they exhibit poor scalability and performance in transformer\nmodels, e.g. large language models (LLMs), because the batch sizes in these\nmodels scale by the attention mechanism sequence length, leading to large model\nsize and batch sizes. MKOR's complexity is quadratic with respect to the model\nsize, alleviating the computation bottlenecks in second-order methods. Because\nof their high computation complexity, state-of-the-art implementations of\nsecond-order methods can only afford to update the second order information\ninfrequently, and thus do not fully exploit the promise of better convergence\nfrom these updates. By reducing the communication complexity of the\nsecond-order updates as well as achieving a linear communication complexity,\nMKOR increases the frequency of second order updates. We also propose a hybrid\nversion of MKOR (called MKOR-H) that mid-training falls backs to a first order\noptimizer if the second order updates no longer accelerate convergence. Our\nexperiments show that MKOR outperforms state -of-the-art first order methods,\ne.g. the LAMB optimizer, and best implementations of second-order methods, i.e.\nKAISA/KFAC, up to 2.57x and 1.85x respectively on BERT-Large-Uncased on 64\nGPUs.",
        "translated": "这项工作提出了一个动量启用 Kronecker 因子为基础的优化使用秩-1更新，称为 MKOR，改善训练时间和收敛性能的深层神经网络(DNN)。二阶技术虽然比一阶技术具有更高的收敛速度，但在模型大小和/或训练批量大小方面具有立方复杂度。因此，它们在变压器模型(例如大语言模型(LLM))中表现出较差的可扩展性和性能，因为这些模型中的批量大小是由注意机制序列长度决定的，从而导致模型大小和批量大小。MKOR 算法的复杂度与模型大小呈二次函数关系，减少了二阶方法的计算瓶颈。由于二阶方法的高计算复杂度，现有的二阶方法只能很少地更新二阶信息，因此无法充分利用这些更新所带来的更好收敛的前景。通过降低二阶更新的通信复杂度以及实现线性通信复杂度，MKOR 增加了二阶更新的频率。我们还提出了 MKOR 的一个混合版本(称为 MKOR-H) ，如果二阶更新不再加速收敛，则中间训练退回到一阶优化器。我们的实验表明，MKOR 优于最先进的一阶方法(例如 LAMB 优化器)和二阶方法(例如 KAISA/KFAC)的最佳实现，在 BERT-Large-Uncase 上分别达到2.57 x 和1.85 x，在64个 GPU 上。"
    },
    {
        "title": "Neuralangelo: High-Fidelity Neural Surface Reconstruction",
        "url": "http://arxiv.org/abs/2306.03092v1",
        "pub_date": "2023-06-05",
        "summary": "Neural surface reconstruction has been shown to be powerful for recovering\ndense 3D surfaces via image-based neural rendering. However, current methods\nstruggle to recover detailed structures of real-world scenes. To address the\nissue, we present Neuralangelo, which combines the representation power of\nmulti-resolution 3D hash grids with neural surface rendering. Two key\ningredients enable our approach: (1) numerical gradients for computing\nhigher-order derivatives as a smoothing operation and (2) coarse-to-fine\noptimization on the hash grids controlling different levels of details. Even\nwithout auxiliary inputs such as depth, Neuralangelo can effectively recover\ndense 3D surface structures from multi-view images with fidelity significantly\nsurpassing previous methods, enabling detailed large-scale scene reconstruction\nfrom RGB video captures.",
        "translated": "神经表面重建已被证明是有效的恢复密集的三维表面的图像基于神经绘制。然而，目前的方法很难恢复真实世界场景的详细结构。为了解决这个问题，我们提出了 Neuralangelo，它结合了多分辨率3D 哈希网格的表示能力和神经表面绘制。两个关键因素使我们的方法: (1)数值梯度计算高阶导数作为一个平滑运算和(2)粗细优化哈希网格控制不同层次的细节。即使没有像深度这样的辅助输入，Neuralangelo 也可以有效地从多视图图像中恢复密集的3D 表面结构，其逼真度远远超过以往的方法，从而能够利用 RGB 视频捕捉进行详细的大规模场景重建。"
    },
    {
        "title": "Brain Diffusion for Visual Exploration: Cortical Discovery using Large\n  Scale Generative Models",
        "url": "http://arxiv.org/abs/2306.03089v1",
        "pub_date": "2023-06-05",
        "summary": "A long standing goal in neuroscience has been to elucidate the functional\norganization of the brain. Within higher visual cortex, functional accounts\nhave remained relatively coarse, focusing on regions of interest (ROIs) and\ntaking the form of selectivity for broad categories such as faces, places,\nbodies, food, or words. Because the identification of such ROIs has typically\nrelied on manually assembled stimulus sets consisting of isolated objects in\nnon-ecological contexts, exploring functional organization without robust a\npriori hypotheses has been challenging. To overcome these limitations, we\nintroduce a data-driven approach in which we synthesize images predicted to\nactivate a given brain region using paired natural images and fMRI recordings,\nbypassing the need for category-specific stimuli. Our approach -- Brain\nDiffusion for Visual Exploration (\"BrainDiVE\") -- builds on recent generative\nmethods by combining large-scale diffusion models with brain-guided image\nsynthesis. Validating our method, we demonstrate the ability to synthesize\npreferred images with appropriate semantic specificity for well-characterized\ncategory-selective ROIs. We then show that BrainDiVE can characterize\ndifferences between ROIs selective for the same high-level category. Finally we\nidentify novel functional subdivisions within these ROIs, validated with\nbehavioral data. These results advance our understanding of the fine-grained\nfunctional organization of human visual cortex, and provide well-specified\nconstraints for further examination of cortical organization using\nhypothesis-driven methods.",
        "translated": "神经科学的一个长期目标是阐明大脑的功能组织。在高级视觉皮层中，功能性描述仍然相对粗糙，集中在感兴趣的区域(ROI) ，并采取广泛类别的选择性形式，如面孔、地点、身体、食物或词汇。由于这种 ROI 的识别通常依赖于在非生态环境中由孤立对象组成的人工组装的刺激集，因此在没有强有力的先验假设的情况下探索功能性组织一直是具有挑战性的。为了克服这些局限性，我们引入了一种数据驱动的方法，其中我们使用配对的自然图像和 fMRI 记录合成预测激活给定大脑区域的图像，绕过对类别特定刺激的需求。我们的方法——用于视觉探索的大脑扩散(BrainDiVE)——建立在最近的生成方法的基础上，将大规模扩散模型与大脑引导的图像合成相结合。通过验证我们的方法，我们证明了合成具有适当语义特异性的首选图像的能力，以及特征明确的类别选择性 ROI。然后，我们表明 BrainDiVE 可以表征不同的 ROI 选择相同的高级类别。最后，我们在这些 ROI 中识别新的功能细分，并用行为数据进行验证。这些结果提高了我们对人类视觉皮层细粒度功能组织的理解，并为使用假设驱动的方法进一步检查皮层组织提供了明确的约束条件。"
    },
    {
        "title": "Of Mice and Mates: Automated Classification and Modelling of Mouse\n  Behaviour in Groups using a Single Model across Cages",
        "url": "http://arxiv.org/abs/2306.03066v1",
        "pub_date": "2023-06-05",
        "summary": "Behavioural experiments often happen in specialised arenas, but this may\nconfound the analysis. To address this issue, we provide tools to study mice in\nthe homecage environment, equipping biologists with the possibility to capture\nthe temporal aspect of the individual's behaviour and model the interaction and\ninterdependence between cage-mates with minimal human intervention. We develop\nthe Activity Labelling Module (ALM) to automatically classify mouse behaviour\nfrom video, and a novel Group Behaviour Model (GBM) for summarising their joint\nbehaviour across cages, using a permutation matrix to match the mouse\nidentities in each cage to the model. We also release two datasets, ABODe for\ntraining behaviour classifiers and IMADGE for modelling behaviour.",
        "translated": "行为实验往往发生在专业领域，但这可能会混淆分析。为了解决这个问题，我们提供了在家庭环境中研究小鼠的工具，使生物学家有可能捕获个体行为的时间方面，并以最小的人类干预模拟笼友之间的相互作用和相互依赖。我们开发了活动标签模块(ALM)来自动分类视频中的老鼠行为，以及一个新的群体行为模型(GBM)来总结它们在笼子中的联合行为，使用一个置换矩阵来匹配每个笼子中的老鼠身份与模型。我们还发布了两个数据集，ABODe 用于训练行为分类器，IMADGE 用于建模行为。"
    },
    {
        "title": "ELEV-VISION: Automated Lowest Floor Elevation Estimation from Segmenting\n  Street View Images",
        "url": "http://arxiv.org/abs/2306.03050v1",
        "pub_date": "2023-06-05",
        "summary": "We propose an automated lowest floor elevation (LFE) estimation algorithm\nbased on computer vision techniques to leverage the latent information in\nstreet view images. Flood depth-damage models use a combination of LFE and\nflood depth for determining flood risk and extent of damage to properties. We\nused image segmentation for detecting door bottoms and roadside edges from\nGoogle Street View images. The characteristic of equirectangular projection\nwith constant spacing representation of horizontal and vertical angles allows\nextraction of the pitch angle from the camera to the door bottom. The depth\nfrom the camera to the door bottom was obtained from the depthmap paired with\nthe Google Street View image. LFEs were calculated from the pitch angle and the\ndepth. The testbed for application of the proposed method is Meyerland (Harris\nCounty, Texas). The results show that the proposed method achieved mean\nabsolute error of 0.190 m (1.18 %) in estimating LFE. The height difference\nbetween the street and the lowest floor (HDSL) was estimated to provide\ninformation for flood damage estimation. The proposed automatic LFE estimation\nalgorithm using Street View images and image segmentation provides a rapid and\ncost-effective method for LFE estimation compared with the surveys using total\nstation theodolite and unmanned aerial systems. By obtaining more accurate and\nup-to-date LFE data using the proposed method, city planners, emergency\nplanners and insurance companies could make a more precise estimation of flood\ndamage.",
        "translated": "提出了一种基于计算机视觉技术的自动最低楼层高度(LFE)估计算法，以利用街景图像中的潜在信息。洪水深度-损害模型使用 LFE 和洪水深度的组合来确定洪水风险和对财产的损害程度。我们使用图像分割来检测谷歌街景图像中的门底和路边边缘。等矩形投影的特点是水平和垂直角度的间距表示为恒定的，可以提取从摄像机到门底部的俯仰角。从相机到门底部的深度是通过谷歌街景图获得的。根据节距角和深度计算 LFEs。提出的方法的应用试验台是 Meyerland (德克萨斯州哈里斯县)。结果表明，该方法估计 LFE 的平均绝对误差为0.190 m (1.18%)。估算了街道与最低层之间的高度差，为洪水损失估算提供了依据。与使用图像分割和无人驾驶航空系统进行的调查相比，建议的街景图像自动估算算法提供了一个快速和具成本效益的估算全站仪生命周期的方法。通过使用这种方法获得更准确和最新的 LFE 数据，城市规划者、应急规划者和保险公司可以对洪水损失进行更精确的估计。"
    },
    {
        "title": "HeadSculpt: Crafting 3D Head Avatars with Text",
        "url": "http://arxiv.org/abs/2306.03038v1",
        "pub_date": "2023-06-05",
        "summary": "Recently, text-guided 3D generative methods have made remarkable advancements\nin producing high-quality textures and geometry, capitalizing on the\nproliferation of large vision-language and image diffusion models. However,\nexisting methods still struggle to create high-fidelity 3D head avatars in two\naspects: (1) They rely mostly on a pre-trained text-to-image diffusion model\nwhilst missing the necessary 3D awareness and head priors. This makes them\nprone to inconsistency and geometric distortions in the generated avatars. (2)\nThey fall short in fine-grained editing. This is primarily due to the inherited\nlimitations from the pre-trained 2D image diffusion models, which become more\npronounced when it comes to 3D head avatars. In this work, we address these\nchallenges by introducing a versatile coarse-to-fine pipeline dubbed HeadSculpt\nfor crafting (i.e., generating and editing) 3D head avatars from textual\nprompts. Specifically, we first equip the diffusion model with 3D awareness by\nleveraging landmark-based control and a learned textual embedding representing\nthe back view appearance of heads, enabling 3D-consistent head avatar\ngenerations. We further propose a novel identity-aware editing score\ndistillation strategy to optimize a textured mesh with a high-resolution\ndifferentiable rendering technique. This enables identity preservation while\nfollowing the editing instruction. We showcase HeadSculpt's superior fidelity\nand editing capabilities through comprehensive experiments and comparisons with\nexisting methods.",
        "translated": "最近，文本引导的3D 生成方法在生成高质量的纹理和几何图形方面取得了显著的进展，利用了大型视觉语言和图像扩散模型的增殖。然而，现有的方法仍然难以在两个方面创建高保真度的3D 头像: (1)他们主要依赖于预先训练的文本到图像的扩散模型，同时缺乏必要的3D 意识和头部先验。这使得他们容易产生不一致性和几何失真的化身。(2)在细粒度编辑方面存在不足。这主要是由于预先训练的2D 图像扩散模型遗传的局限性，当涉及到3D 头像时，这种局限性变得更加明显。在这项工作中，我们通过引入一个多功能的从粗到精的流水线，称为 HeadSculpt，用于从文本提示中生成(即，生成和编辑)3D 头部化身，来解决这些挑战。具体来说，我们首先通过利用基于里程碑的控制和代表头部背景外观的学习文本嵌入，使扩散模型具有3D 感知能力，从而实现3D 一致的头部头像生成。我们进一步提出了一种新的身份感知编辑分数提取策略，以优化纹理网格与高分辨率可微渲染技术。这样可以在按照编辑指令进行编辑的同时进行食品生产履历。我们通过全面的实验和与现有方法的比较，展示了 HeadScult 卓越的保真度和编辑能力。"
    },
    {
        "title": "Interpretable Alzheimer's Disease Classification Via a Contrastive\n  Diffusion Autoencoder",
        "url": "http://arxiv.org/abs/2306.03022v1",
        "pub_date": "2023-06-05",
        "summary": "In visual object classification, humans often justify their choices by\ncomparing objects to prototypical examples within that class. We may therefore\nincrease the interpretability of deep learning models by imbuing them with a\nsimilar style of reasoning. In this work, we apply this principle by\nclassifying Alzheimer's Disease based on the similarity of images to training\nexamples within the latent space. We use a contrastive loss combined with a\ndiffusion autoencoder backbone, to produce a semantically meaningful latent\nspace, such that neighbouring latents have similar image-level features. We\nachieve a classification accuracy comparable to black box approaches on a\ndataset of 2D MRI images, whilst producing human interpretable model\nexplanations. Therefore, this work stands as a contribution to the pertinent\ndevelopment of accurate and interpretable deep learning within medical imaging.",
        "translated": "在可视对象分类中，人们经常通过比较对象和类中的原型例子来证明他们的选择是正确的。因此，我们可以通过灌输类似的推理风格来增加深度学习模型的可解释性。在这项工作中，我们应用这个原则，通过分类阿尔茨海默氏病的基础上相似的图像训练例子在潜在的空间。我们使用对比度损失结合扩散自动编码骨干，产生一个语义上有意义的潜在空间，使邻近的潜在具有相似的图像级特征。我们在二维 MRI 图像的数据集上获得了与黑匣子方法相当的分类精度，同时产生了人类可解释的模型解释。因此，这项工作是对准确和可解释的医学影像深度学习的相关发展的一个贡献。"
    },
    {
        "title": "Automating Style Analysis and Visualization With Explainable AI -- Case\n  Studies on Brand Recognition",
        "url": "http://arxiv.org/abs/2306.03021v1",
        "pub_date": "2023-06-05",
        "summary": "Incorporating style-related objectives into shape design has been centrally\nimportant to maximize product appeal. However, stylistic features such as\naesthetics and semantic attributes are hard to codify even for experts. As\nsuch, algorithmic style capture and reuse have not fully benefited from\nautomated data-driven methodologies due to the challenging nature of design\ndescribability. This paper proposes an AI-driven method to fully automate the\ndiscovery of brand-related features. Our approach introduces BIGNet, a two-tier\nBrand Identification Graph Neural Network (GNN) to classify and analyze scalar\nvector graphics (SVG). First, to tackle the scarcity of vectorized product\nimages, this research proposes two data acquisition workflows: parametric\nmodeling from small curve-based datasets, and vectorization from large\npixel-based datasets. Secondly, this study constructs a novel hierarchical GNN\narchitecture to learn from both SVG's curve-level and chunk-level parameters.\nIn the first case study, BIGNet not only classifies phone brands but also\ncaptures brand-related features across multiple scales, such as the location of\nthe lens, the height-width ratio, and the screen-frame gap, as confirmed by AI\nevaluation. In the second study, this paper showcases the generalizability of\nBIGNet learning from a vectorized car image dataset and validates the\nconsistency and robustness of its predictions given four scenarios. The results\nmatch the difference commonly observed in luxury vs. economy brands in the\nautomobile market. Finally, this paper also visualizes the activation maps\ngenerated from a convolutional neural network and shows BIGNet's advantage of\nbeing a more human-friendly, explainable, and explicit style-capturing agent.\nCode and dataset can be found on Github:\n  1. Phone case study: github.com/parksandrecfan/bignet-phone 2. Car case\nstudy: github.com/parksandrecfan/bignet-car",
        "translated": "将与风格相关的目标整合到形状设计中，对于最大限度地提高产品的吸引力至关重要。然而，诸如美学和语义属性之类的文体特征即使对于专家来说也很难编纂成文。因此，由于设计可描述性的挑战性，算法样式捕获和重用并没有完全受益于自动化的数据驱动方法。本文提出了一种人工智能驱动的方法来完全自动发现品牌相关的特征。该方法引入双层品牌识别图神经网络 BIGNet 对标量向量图进行分类和分析。首先，针对产品图像矢量化的不足，本研究提出了两种数据采集工作流程: 基于小型曲线数据集的参数化建模和基于大型像素数据集的矢量化。其次，本研究建构了一个新颖的层次式 GNN 架构，以学习 SVG 的曲线层和组块层参数。在第一个案例研究中，BIGNet 不仅对手机品牌进行分类，而且还在多个尺度上捕获与品牌相关的特征，如镜头的位置、高宽比和屏幕框架间隙，这一点得到了 AI 评估的证实。在第二个研究中，本文展示了 BIGNet 从一个向量化的汽车图像数据集中学习的可推广性，并在四个场景中验证了其预测的一致性和鲁棒性。研究结果与汽车市场上普遍观察到的奢侈品牌与经济型品牌之间的差异相吻合。最后，本文还展示了由卷积神经网络生成的激活地图，并展示了 BIGNet 作为一个更人性化、更易于解释和更明确的风格捕获代理的优势。代码和数据集可以在 Github 上找到: 1。电话案例研究:  github.com/parksandrecfan/bignet-Phone 2。汽车案例研究:  github.com/parksandrecfan/bignet-Car"
    },
    {
        "title": "Nonparametric Iterative Machine Teaching",
        "url": "http://arxiv.org/abs/2306.03007v2",
        "pub_date": "2023-06-05",
        "summary": "In this paper, we consider the problem of Iterative Machine Teaching (IMT),\nwhere the teacher provides examples to the learner iteratively such that the\nlearner can achieve fast convergence to a target model. However, existing IMT\nalgorithms are solely based on parameterized families of target models. They\nmainly focus on convergence in the parameter space, resulting in difficulty\nwhen the target models are defined to be functions without dependency on\nparameters. To address such a limitation, we study a more general task --\nNonparametric Iterative Machine Teaching (NIMT), which aims to teach\nnonparametric target models to learners in an iterative fashion. Unlike\nparametric IMT that merely operates in the parameter space, we cast NIMT as a\nfunctional optimization problem in the function space. To solve it, we propose\nboth random and greedy functional teaching algorithms. We obtain the iterative\nteaching dimension (ITD) of the random teaching algorithm under proper\nassumptions, which serves as a uniform upper bound of ITD in NIMT. Further, the\ngreedy teaching algorithm has a significantly lower ITD, which reaches a\ntighter upper bound of ITD in NIMT. Finally, we verify the correctness of our\ntheoretical findings with extensive experiments in nonparametric scenarios.",
        "translated": "本文研究迭代机器教学(IMT)问题，教师迭代地向学习者提供例子，使学习者能够快速收敛到目标模型。然而，现有的 IMT 算法仅仅基于目标模型的参数化族。这些问题主要集中在参数空间的收敛性上，给目标模型定义为不依赖于参数的函数带来困难。为了解决这一局限性，我们研究了一个更为普遍的课题——非参数迭代机器教学(NIMT) ，其目的是以迭代的方式向学习者传授非参数目标模型。与仅在参数空间中运行的参数 IMT 不同，我们将 NIMT 作为函数空间中的函数最佳化问题。为了解决这个问题，我们提出了随机函数教学算法和贪婪函数教学算法。在适当的假设条件下，得到了随机教学算法的迭代教学维数(ITD) ，作为 NIMT 中迭代教学维数的一个统一上界。此外，贪婪教学算法的 ITD 显著降低，在 NIMT 中达到更紧的 ITD 上限。最后，我们通过在非参数情景下的大量实验验证了理论结果的正确性。"
    },
    {
        "title": "Unveiling the Two-Faced Truth: Disentangling Morphed Identities for Face\n  Morphing Detection",
        "url": "http://arxiv.org/abs/2306.03002v1",
        "pub_date": "2023-06-05",
        "summary": "Morphing attacks keep threatening biometric systems, especially face\nrecognition systems. Over time they have become simpler to perform and more\nrealistic, as such, the usage of deep learning systems to detect these attacks\nhas grown. At the same time, there is a constant concern regarding the lack of\ninterpretability of deep learning models. Balancing performance and\ninterpretability has been a difficult task for scientists. However, by\nleveraging domain information and proving some constraints, we have been able\nto develop IDistill, an interpretable method with state-of-the-art performance\nthat provides information on both the identity separation on morph samples and\ntheir contribution to the final prediction. The domain information is learnt by\nan autoencoder and distilled to a classifier system in order to teach it to\nseparate identity information. When compared to other methods in the literature\nit outperforms them in three out of five databases and is competitive in the\nremaining.",
        "translated": "变形攻击不断威胁着生物识别系统，尤其是人脸识别系统。随着时间的推移，它们的执行变得更加简单，也更加现实，因此，使用深度学习系统来检测这些攻击已经增长。与此同时，深度学习模型缺乏可解释性一直是人们关注的问题。平衡性能和可解释性一直是科学家的一项艰巨任务。然而，通过利用领域信息和证明一些限制，我们已经能够开发 IDistill，一种可解释的方法，具有最先进的性能，提供变形样本的身份分离和它们对最终预测的贡献的信息。领域信息由自动编码器学习，并提取到分类器系统中，以便教它分离身份信息。与文献中的其他方法相比，它在五个数据库中的三个数据库中表现优于其他方法，并在其余数据库中具有竞争力。"
    },
    {
        "title": "BeyondPixels: A Comprehensive Review of the Evolution of Neural Radiance\n  Fields",
        "url": "http://arxiv.org/abs/2306.03000v1",
        "pub_date": "2023-06-05",
        "summary": "Neural rendering combines ideas from classical computer graphics and machine\nlearning to synthesize images from real-world observations. NeRF, short for\nNeural Radiance Fields, is a recent innovation that uses AI algorithms to\ncreate 3D objects from 2D images. By leveraging an interpolation approach, NeRF\ncan produce new 3D reconstructed views of complicated scenes. Rather than\ndirectly restoring the whole 3D scene geometry, NeRF generates a volumetric\nrepresentation called a ``radiance field,'' which is capable of creating color\nand density for every point within the relevant 3D space. The broad appeal and\nnotoriety of NeRF make it imperative to examine the existing research on the\ntopic comprehensively. While previous surveys on 3D rendering have primarily\nfocused on traditional computer vision-based or deep learning-based approaches,\nonly a handful of them discuss the potential of NeRF. However, such surveys\nhave predominantly focused on NeRF's early contributions and have not explored\nits full potential. NeRF is a relatively new technique continuously being\ninvestigated for its capabilities and limitations. This survey reviews recent\nadvances in NeRF and categorizes them according to their architectural designs,\nespecially in the field of novel view synthesis.",
        "translated": "神经渲染结合了经典计算机图形学和机器学习的思想，从真实世界的观察中合成图像。NeRF 是神经辐射场的简称，是最近的一项创新，它使用人工智能算法从2D 图像中创建3D 对象。通过利用插值方法，NERF 可以生成复杂场景的新的三维重建视图。NERF 不直接恢复整个3D 场景的几何形状，而是生成一个称为“辐射场”的体积表示，它能够为相关3D 空间中的每个点创建颜色和密度。由于 NERF 的广泛吸引力和恶名，因此必须全面审查关于该专题的现有研究。虽然之前的3D 渲染调查主要集中在传统的基于计算机视觉或基于深度学习的方法，但只有少数人讨论了 NeRF 的潜力。然而，这些调查主要集中在 NERF 的早期贡献，并没有充分挖掘其潜力。NERF 是一种相对较新的技术，由于其能力和局限性，正在不断地被研究。这项调查回顾了最近在 NERF 的进展，并根据他们的建筑设计进行分类，特别是在新视图合成领域。"
    },
    {
        "title": "SAM3D: Segment Anything in 3D Scenes",
        "url": "http://arxiv.org/abs/2306.03908v1",
        "pub_date": "2023-06-06",
        "summary": "In this work, we propose SAM3D, a novel framework that is able to predict\nmasks in 3D point clouds by leveraging the Segment-Anything Model (SAM) in RGB\nimages without further training or finetuning. For a point cloud of a 3D scene\nwith posed RGB images, we first predict segmentation masks of RGB images with\nSAM, and then project the 2D masks into the 3D points. Later, we merge the 3D\nmasks iteratively with a bottom-up merging approach. At each step, we merge the\npoint cloud masks of two adjacent frames with the bidirectional merging\napproach. In this way, the 3D masks predicted from different frames are\ngradually merged into the 3D masks of the whole 3D scene. Finally, we can\noptionally ensemble the result from our SAM3D with the over-segmentation\nresults based on the geometric information of the 3D scenes. Our approach is\nexperimented with ScanNet dataset and qualitative results demonstrate that our\nSAM3D achieves reasonable and fine-grained 3D segmentation results without any\ntraining or finetuning of SAM.",
        "translated": "在这项工作中，我们提出了 SAM3D，一个新颖的框架，能够通过利用分段任何模型(SAM)在 RGB 图像预测掩码，而不需要进一步的训练或微调。对于三维场景中的点云，我们首先用 SAM 预测 RGB 图像的分割模板，然后将二维模板投影到三维点上。随后，我们使用自底向上的合并方法迭代地合并3D 掩码。在每一步，我们合并两个相邻帧点云掩码的双向合并方法。这样，由不同帧预测的3D 掩模逐渐合并到整个3D 场景的3D 掩模中。最后，我们可以根据三维场景的几何信息，选择性地将来自 SAM3D 的结果与过度分割的结果进行集成。该方法在 ScanNet 数据集上进行了实验，定性结果表明，我们的 SAM3D 不需要对 SAM 进行任何训练或微调，就可以获得合理的、细粒度的三维分割结果。"
    },
    {
        "title": "Towards Label-free Scene Understanding by Vision Foundation Models",
        "url": "http://arxiv.org/abs/2306.03899v1",
        "pub_date": "2023-06-06",
        "summary": "Vision foundation models such as Contrastive Vision-Language Pre-training\n(CLIP) and Segment Anything (SAM) have demonstrated impressive zero-shot\nperformance on image classification and segmentation tasks. However, the\nincorporation of CLIP and SAM for label-free scene understanding has yet to be\nexplored. In this paper, we investigate the potential of vision foundation\nmodels in enabling networks to comprehend 2D and 3D worlds without labelled\ndata. The primary challenge lies in effectively supervising networks under\nextremely noisy pseudo labels, which are generated by CLIP and further\nexacerbated during the propagation from the 2D to the 3D domain. To tackle\nthese challenges, we propose a novel Cross-modality Noisy Supervision (CNS)\nmethod that leverages the strengths of CLIP and SAM to supervise 2D and 3D\nnetworks simultaneously. In particular, we introduce a prediction consistency\nregularization to co-train 2D and 3D networks, then further impose the\nnetworks' latent space consistency using the SAM's robust feature\nrepresentation. Experiments conducted on diverse indoor and outdoor datasets\ndemonstrate the superior performance of our method in understanding 2D and 3D\nopen environments. Our 2D and 3D network achieves label-free semantic\nsegmentation with 28.4% and 33.5% mIoU on ScanNet, improving 4.7% and 7.9%,\nrespectively. And for nuScenes dataset, our performance is 26.8% with an\nimprovement of 6%. Code will be released\n(https://github.com/runnanchen/Label-Free-Scene-Understanding).",
        "translated": "对比视觉语言预训练(CLIP)和分段任意(SAM)等视觉基础模型在图像分类和分割任务中表现出了令人印象深刻的零镜头性能。然而，将 CLIP 和 SAM 结合起来用于无标签场景理解还有待探索。在本文中，我们研究的潜力，视觉基础模型，使网络理解二维和三维世界没有标记的数据。主要的挑战在于在极其嘈杂的伪标签下有效地监督网络，伪标签是由 CLIP 生成的，并在从2D 到3D 域的传播过程中进一步加剧。为了应对这些挑战，我们提出了一种新的跨模态噪声监控(CNS)方法，利用 CLIP 和 SAM 的优势，同时监控2D 和3D 网络。特别地，我们引入预测一致性正则化来协同训练二维和三维网络，然后利用 SAM 的鲁棒特征表示进一步强化网络的潜在空间一致性。在不同的室内和室外数据集上进行的实验表明，该方法在理解二维和三维开放环境方面具有优越的性能。我们的二维和三维网络在 ScanNet 上分别实现了28.4% 和33.5% 的无标签语义分割，分别提高了4.7% 和7.9% 。对于 nuScenes 数据集，我们的性能是26.8% ，提高了6% 。代码将被发布( https://github.com/runnanchen/label-free-scene-understanding )。"
    },
    {
        "title": "Emergent Correspondence from Image Diffusion",
        "url": "http://arxiv.org/abs/2306.03881v1",
        "pub_date": "2023-06-06",
        "summary": "Finding correspondences between images is a fundamental problem in computer\nvision. In this paper, we show that correspondence emerges in image diffusion\nmodels without any explicit supervision. We propose a simple strategy to\nextract this implicit knowledge out of diffusion networks as image features,\nnamely DIffusion FeaTures (DIFT), and use them to establish correspondences\nbetween real images. Without any additional fine-tuning or supervision on the\ntask-specific data or annotations, DIFT is able to outperform both\nweakly-supervised methods and competitive off-the-shelf features in identifying\nsemantic, geometric, and temporal correspondences. Particularly for semantic\ncorrespondence, DIFT from Stable Diffusion is able to outperform DINO and\nOpenCLIP by 19 and 14 accuracy points respectively on the challenging SPair-71k\nbenchmark. It even outperforms the state-of-the-art supervised methods on 9 out\nof 18 categories while remaining on par for the overall performance. Project\npage: https://diffusionfeatures.github.io",
        "translated": "寻找图像之间的对应关系是计算机视觉中的一个基本问题。在本文中，我们证明了在没有任何显式监督的情况下，在图像扩散模型中出现了对应。提出了一种简单的从扩散网络中提取隐含知识作为图像特征的方法，即扩散特征(DIFT) ，并利用它们建立真实图像之间的对应关系。在没有对任务特定数据或注释进行任何额外的微调或监督的情况下，DIFT 能够在识别语义、几何和时间对应方面胜过弱监督方法和竞争性现成特征。特别是在语义对应方面，在具有挑战性的 SPair-71k 基准测试中，来自稳定扩散的 DIFT 能够比 DINO 和 OpenCLIP 分别高出19和14个精度点。它甚至在18个类别中的9个方面表现优于最先进的监督方法，同时在整体表现上保持同等水平。项目主页:  https://diffusionfeatures.github.io"
    },
    {
        "title": "Conditional Diffusion Models for Weakly Supervised Medical Image\n  Segmentation",
        "url": "http://arxiv.org/abs/2306.03878v1",
        "pub_date": "2023-06-06",
        "summary": "Recent advances in denoising diffusion probabilistic models have shown great\nsuccess in image synthesis tasks. While there are already works exploring the\npotential of this powerful tool in image semantic segmentation, its application\nin weakly supervised semantic segmentation (WSSS) remains relatively\nunder-explored. Observing that conditional diffusion models (CDM) is capable of\ngenerating images subject to specific distributions, in this work, we utilize\ncategory-aware semantic information underlied in CDM to get the prediction mask\nof the target object with only image-level annotations. More specifically, we\nlocate the desired class by approximating the derivative of the output of CDM\nw.r.t the input condition. Our method is different from previous diffusion\nmodel methods with guidance from an external classifier, which accumulates\nnoises in the background during the reconstruction process. Our method\noutperforms state-of-the-art CAM and diffusion model methods on two public\nmedical image segmentation datasets, which demonstrates that CDM is a promising\ntool in WSSS. Also, experiment shows our method is more time-efficient than\nexisting diffusion model methods, making it practical for wider applications.",
        "translated": "近年来，扩散概率模型在图像合成任务中取得了很大的成功。虽然已经有工作探索这一强大的工具在图像语义分割的潜力，它在弱监督语义分割(WSSS)的应用仍然相对缺乏探索。考虑到条件扩散模型能够生成受特定分布影响的图像，在这项工作中，我们利用条件扩散模型下的类别感知语义信息来获得目标对象的预测掩码，只使用图像级别的注释。更具体地说，我们通过近似于输入条件 CDM w.r.t 输出的导数来定位所需的类。该方法不同于以往采用外部分类器引导的扩散模型方法，在重建过程中会在背景中积累噪声。我们的方法在两个公共医疗图像分割数据集上优于最先进的 CAM 和扩散模型方法，这表明 CDM 是 WSSS 中一个很有前途的工具。实验表明，该方法比现有的扩散模型方法具有更高的时间效率，具有更广泛的应用前景。"
    },
    {
        "title": "Learning with a Mole: Transferable latent spatial representations for\n  navigation without reconstruction",
        "url": "http://arxiv.org/abs/2306.03857v1",
        "pub_date": "2023-06-06",
        "summary": "Agents navigating in 3D environments require some form of memory, which\nshould hold a compact and actionable representation of the history of\nobservations useful for decision taking and planning. In most end-to-end\nlearning approaches the representation is latent and usually does not have a\nclearly defined interpretation, whereas classical robotics addresses this with\nscene reconstruction resulting in some form of map, usually estimated with\ngeometry and sensor models and/or learning. In this work we propose to learn an\nactionable representation of the scene independently of the targeted downstream\ntask and without explicitly optimizing reconstruction. The learned\nrepresentation is optimized by a blind auxiliary agent trained to navigate with\nit on multiple short sub episodes branching out from a waypoint and, most\nimportantly, without any direct visual observation. We argue and show that the\nblindness property is important and forces the (trained) latent representation\nto be the only means for planning. With probing experiments we show that the\nlearned representation optimizes navigability and not reconstruction. On\ndownstream tasks we show that it is robust to changes in distribution, in\nparticular the sim2real gap, which we evaluate with a real physical robot in a\nreal office building, significantly improving performance.",
        "translated": "在3D 环境中导航的代理需要某种形式的存储器，这种存储器应该能够对观测历史进行紧凑的、可操作的表示，这对于决策制定和规划非常有用。在大多数端到端的学习方法中，这种表示是潜在的，通常没有明确的解释，而经典的机器人通过场景重建来解决这个问题，从而产生某种形式的地图，通常用几何和传感器模型估计和/或学习。在这项工作中，我们建议学习一个独立于目标下游任务的可操作的场景表示，而不需要显式地优化重建。经过训练的盲辅助代理优化了学习表示，使其能够在多个从路径点分支出来的短子集中导航，最重要的是，没有任何直接的视觉观察。我们认为，并表明，盲性性质是重要的，迫使(训练)潜在表征是唯一的手段规划。通过探索性实验，我们发现学习表示优化了导航性，而不是重构性。在下游的任务，我们表明它是鲁棒的分布变化，特别是模拟真实的差距，我们评估了一个真实的物理机器人在一个真实的办公楼，显着提高性能。"
    },
    {
        "title": "Learning Human Mesh Recovery in 3D Scenes",
        "url": "http://arxiv.org/abs/2306.03847v1",
        "pub_date": "2023-06-06",
        "summary": "We present a novel method for recovering the absolute pose and shape of a\nhuman in a pre-scanned scene given a single image. Unlike previous methods that\nperform sceneaware mesh optimization, we propose to first estimate absolute\nposition and dense scene contacts with a sparse 3D CNN, and later enhance a\npretrained human mesh recovery network by cross-attention with the derived 3D\nscene cues. Joint learning on images and scene geometry enables our method to\nreduce the ambiguity caused by depth and occlusion, resulting in more\nreasonable global postures and contacts. Encoding scene-aware cues in the\nnetwork also allows the proposed method to be optimization-free, and opens up\nthe opportunity for real-time applications. The experiments show that the\nproposed network is capable of recovering accurate and physically-plausible\nmeshes by a single forward pass and outperforms state-of-the-art methods in\nterms of both accuracy and speed.",
        "translated": "我们提出了一种新的方法来恢复绝对位置和形状的人在一个预先扫描场景给定的一个单一的图像。与以前执行场景网格优化的方法不同，我们建议首先用稀疏的3D CNN 估计绝对位置和密集场景接触，然后通过与派生的3D 场景线索交叉注意来增强预先训练的人类网格恢复网络。通过对图像和场景几何的联合学习，该方法可以减少深度和遮挡引起的模糊，从而得到更合理的全局姿态和接触。在网络中对场景感知线索进行编码也允许提出的方法无需优化，并为实时应用提供了机会。实验结果表明，该网络能够通过单次前传恢复精确的和物理上合理的网格，并且在精度和速度方面都优于最先进的方法。"
    },
    {
        "title": "Atrial Septal Defect Detection in Children Based on Ultrasound Video\n  Using Multiple Instances Learning",
        "url": "http://arxiv.org/abs/2306.03835v1",
        "pub_date": "2023-06-06",
        "summary": "Purpose: Congenital heart defect (CHD) is the most common birth defect.\nThoracic echocardiography (TTE) can provide sufficient cardiac structure\ninformation, evaluate hemodynamics and cardiac function, and is an effective\nmethod for atrial septal defect (ASD) examination. This paper aims to study a\ndeep learning method based on cardiac ultrasound video to assist in ASD\ndiagnosis. Materials and methods: We select two standard views of the atrial\nseptum (subAS) and low parasternal four-compartment view (LPS4C) as the two\nviews to identify ASD. We enlist data from 300 children patients as part of a\ndouble-blind experiment for five-fold cross-validation to verify the\nperformance of our model. In addition, data from 30 children patients (15\npositives and 15 negatives) are collected for clinician testing and compared to\nour model test results (these 30 samples do not participate in model training).\nWe propose an echocardiography video-based atrial septal defect diagnosis\nsystem. In our model, we present a block random selection, maximal agreement\ndecision and frame sampling strategy for training and testing respectively,\nresNet18 and r3D networks are used to extract the frame features and aggregate\nthem to build a rich video-level representation. Results: We validate our model\nusing our private dataset by five-cross validation. For ASD detection, we\nachieve 89.33 AUC, 84.95 accuracy, 85.70 sensitivity, 81.51 specificity and\n81.99 F1 score. Conclusion: The proposed model is multiple instances\nlearning-based deep learning model for video atrial septal defect detection\nwhich effectively improves ASD detection accuracy when compared to the\nperformances of previous networks and clinical doctors.",
        "translated": "目的: 先天性心脏病是最常见的先天缺陷。胸超声心动图能提供足够的心脏结构资料、评估血流动力学及心脏功能，是检查心房中隔缺损的有效方法。本文旨在研究一种基于心脏超声视频的深度学习方法，以辅助 ASD 的诊断。材料和方法: 我们选择房间隔(subAS)和胸骨旁低位四室视(LPS4C)两种标准视图作为房间隔缺损的鉴别视图。作为双盲实验的一部分，我们征集了300名儿童患者的数据，用5倍交叉验证来验证我们模型的性能。此外，从30名儿童患者(15个阳性和15个阴性)收集数据用于临床医生测试，并与我们的模型测试结果进行比较(这30个样本不参加模型培训)。我们提出一个基于超声心动图视频的心房中隔缺损诊断系统。该模型分别采用块随机选择、最大一致性决策和帧采样策略进行训练和测试，利用 resNet18网络和 r3D 网络提取帧特征，并将其聚合成丰富的视频级表示。结果: 我们使用我们的私有数据集通过五交叉验证来验证我们的模型。对于 ASD 检测，我们获得了89.33 AUC，84.95准确性，85.70敏感性，81.51特异性和81.99 F1评分。结论: 该模型是基于多实例学习的深度学习视频心房中隔缺损检测模型，与以前的网络和临床医生相比，有效地提高了 ASD 检测的准确性。"
    },
    {
        "title": "GEO-Bench: Toward Foundation Models for Earth Monitoring",
        "url": "http://arxiv.org/abs/2306.03831v1",
        "pub_date": "2023-06-06",
        "summary": "Recent progress in self-supervision has shown that pre-training large neural\nnetworks on vast amounts of unsupervised data can lead to substantial increases\nin generalization to downstream tasks. Such models, recently coined foundation\nmodels, have been transformational to the field of natural language processing.\nVariants have also been proposed for image data, but their applicability to\nremote sensing tasks is limited. To stimulate the development of foundation\nmodels for Earth monitoring, we propose a benchmark comprised of six\nclassification and six segmentation tasks, which were carefully curated and\nadapted to be both relevant to the field and well-suited for model evaluation.\nWe accompany this benchmark with a robust methodology for evaluating models and\nreporting aggregated results to enable a reliable assessment of progress.\nFinally, we report results for 20 baselines to gain information about the\nperformance of existing models. We believe that this benchmark will be a driver\nof progress across a variety of Earth monitoring tasks.",
        "translated": "最近在自我监督方面的进展表明，在大量无监督数据上预先训练大型神经网络可以导致对下游任务的一般化程度的大幅提高。这些模型，最近创造的基础模型，已经转化为自然语言处理领域。对于图像数据也提出了不同的方案，但是它们对于遥感任务的适用性是有限的。为了促进地球监测基础模型的发展，我们提出了一个由六个分类和六个分段任务组成的基准，这些任务经过精心策划和调整，既与实地相关又适合于模型评估。我们在这一基准的同时，还采用了一种强有力的方法来评估模型和报告总体结果，以便能够对进展情况进行可靠的评估。最后，我们报告20个基线的结果，以获得关于现有模型性能的信息。我们相信，这一基准将推动各种地球监测任务取得进展。"
    },
    {
        "title": "X-Align++: cross-modal cross-view alignment for Bird's-eye-view\n  segmentation",
        "url": "http://arxiv.org/abs/2306.03810v1",
        "pub_date": "2023-06-06",
        "summary": "Bird's-eye-view (BEV) grid is a typical representation of the perception of\nroad components, e.g., drivable area, in autonomous driving. Most existing\napproaches rely on cameras only to perform segmentation in BEV space, which is\nfundamentally constrained by the absence of reliable depth information. The\nlatest works leverage both camera and LiDAR modalities but suboptimally fuse\ntheir features using simple, concatenation-based mechanisms. In this paper, we\naddress these problems by enhancing the alignment of the unimodal features in\norder to aid feature fusion, as well as enhancing the alignment between the\ncameras' perspective view (PV) and BEV representations. We propose X-Align, a\nnovel end-to-end cross-modal and cross-view learning framework for BEV\nsegmentation consisting of the following components: (i) a novel Cross-Modal\nFeature Alignment (X-FA) loss, (ii) an attention-based Cross-Modal Feature\nFusion (X-FF) module to align multi-modal BEV features implicitly, and (iii) an\nauxiliary PV segmentation branch with Cross-View Segmentation Alignment (X-SA)\nlosses to improve the PV-to-BEV transformation. We evaluate our proposed method\nacross two commonly used benchmark datasets, i.e., nuScenes and KITTI-360.\nNotably, X-Align significantly outperforms the state-of-the-art by 3 absolute\nmIoU points on nuScenes. We also provide extensive ablation studies to\ndemonstrate the effectiveness of the individual components.",
        "translated": "鸟瞰图(BEV)网格是自主驾驶中道路组成部分(如可行驶区域)感知的典型表示。大多数现有的方法仅仅依靠摄像机在 BEV 空间中进行分割，这是由于缺乏可靠的深度信息而造成的。最新的作品利用相机和激光雷达模式，但次优融合他们的功能使用简单，串联为基础的机制。本文通过提高单峰特征的对齐度来辅助特征融合，以及提高摄像机透视图(PV)和 BEV 表示之间的对齐度来解决这些问题。我们提出了 X-Align，一个新颖的端到端跨模态和跨视图的 BEV 分割学习框架，它包括以下组成部分: (i)一个新的跨模态特征对齐(X-FA)损失，(ii)一个基于注意力的跨模态特征融合(X-FF)模块隐式地对齐多模态 BEV 特征，和(iii)一个辅助的 PV 分割分支与跨视图分割对齐(X-SA)损失，以改善 PV-to-BEV 转换。我们通过两个常用的基准数据集，即 nuScenes 和 KITTI-360来评估我们提出的方法。值得注意的是，X-Align 在 nuScenes 上的性能明显比最先进的技术高出3个绝对 mIoU 点。我们还提供了广泛的消融研究，以证明个别组件的有效性。"
    },
    {
        "title": "Learning to Ground Instructional Articles in Videos through Narrations",
        "url": "http://arxiv.org/abs/2306.03802v1",
        "pub_date": "2023-06-06",
        "summary": "In this paper we present an approach for localizing steps of procedural\nactivities in narrated how-to videos. To deal with the scarcity of labeled data\nat scale, we source the step descriptions from a language knowledge base\n(wikiHow) containing instructional articles for a large variety of procedural\ntasks. Without any form of manual supervision, our model learns to temporally\nground the steps of procedural articles in how-to videos by matching three\nmodalities: frames, narrations, and step descriptions. Specifically, our method\naligns steps to video by fusing information from two distinct pathways: i) {\\em\ndirect} alignment of step descriptions to frames, ii) {\\em indirect} alignment\nobtained by composing steps-to-narrations with narrations-to-video\ncorrespondences. Notably, our approach performs global temporal grounding of\nall steps in an article at once by exploiting order information, and is trained\nwith step pseudo-labels which are iteratively refined and aggressively\nfiltered. In order to validate our model we introduce a new evaluation\nbenchmark -- HT-Step -- obtained by manually annotating a 124-hour subset of\nHowTo100M\\footnote{A test server is accessible at\n\\url{https://eval.ai/web/challenges/challenge-page/2082}.} with steps sourced\nfrom wikiHow articles. Experiments on this benchmark as well as zero-shot\nevaluations on CrossTask demonstrate that our multi-modality alignment yields\ndramatic gains over several baselines and prior works. Finally, we show that\nour inner module for matching narration-to-video outperforms by a large margin\nthe state of the art on the HTM-Align narration-video alignment benchmark.",
        "translated": "在本文中，我们提出了一种方法来本地化的程序性活动的步骤叙述如何视频。为了处理大规模标记数据的稀缺性，我们从一个语言知识库(wikiHow)中获取步骤描述，该知识库包含用于大量程序性任务的教学文章。没有任何形式的人工监督，我们的模型学会了通过匹配三种模式(框架、叙述和步骤描述)在时间上将程序性文章的步骤置于 how-to 视频中。具体来说，我们的方法通过融合来自两个不同途径的信息来将步骤与视频对齐: i){ em 直接}步骤描述与帧的对齐，ii){ em 间接}通过将步骤到叙述与叙述到视频的对应组合而获得的步骤到叙述的对齐。值得注意的是，我们的方法通过利用订单信息一次性执行文章中所有步骤的全局时间基础，并使用迭代改进和积极过滤的步骤伪标签进行训练。为了验证我们的模型，我们引入了一个新的评估基准—— HT-step ——通过手动注释 HowTo100M 脚注的一个124小时子集获得的(一个测试服务器可以在 url { https://eval.ai/web/challenges/challenge-page/2082}访问)步骤来源于 wikiHow 文章。在这个基准上的实验以及在 CrossTask 上的零射击评估表明，我们的多模态对齐比几个基线和以前的工作产生了巨大的收益。最后，我们表明，我们的内部模块的匹配叙述视频优于一个很大的差距的艺术状态的 HTM-Align 叙述视频对齐基准。"
    },
    {
        "title": "GP-UNIT: Generative Prior for Versatile Unsupervised Image-to-Image\n  Translation",
        "url": "http://arxiv.org/abs/2306.04636v1",
        "pub_date": "2023-06-07",
        "summary": "Recent advances in deep learning have witnessed many successful unsupervised\nimage-to-image translation models that learn correspondences between two visual\ndomains without paired data. However, it is still a great challenge to build\nrobust mappings between various domains especially for those with drastic\nvisual discrepancies. In this paper, we introduce a novel versatile framework,\nGenerative Prior-guided UNsupervised Image-to-image Translation (GP-UNIT), that\nimproves the quality, applicability and controllability of the existing\ntranslation models. The key idea of GP-UNIT is to distill the generative prior\nfrom pre-trained class-conditional GANs to build coarse-level cross-domain\ncorrespondences, and to apply the learned prior to adversarial translations to\nexcavate fine-level correspondences. With the learned multi-level content\ncorrespondences, GP-UNIT is able to perform valid translations between both\nclose domains and distant domains. For close domains, GP-UNIT can be\nconditioned on a parameter to determine the intensity of the content\ncorrespondences during translation, allowing users to balance between content\nand style consistency. For distant domains, semi-supervised learning is\nexplored to guide GP-UNIT to discover accurate semantic correspondences that\nare hard to learn solely from the appearance. We validate the superiority of\nGP-UNIT over state-of-the-art translation models in robust, high-quality and\ndiversified translations between various domains through extensive experiments.",
        "translated": "深度学习的最新进展见证了许多成功的无监督的图像到图像的转换模型，它们在没有配对数据的情况下学习两个视觉域之间的对应关系。然而，在不同领域之间建立健壮的映射仍然是一个巨大的挑战，特别是对于那些具有严重视觉差异的领域。本文介绍了一种新的通用翻译框架——生成先验引导的无监督图像到图像翻译(GP-UNIT) ，它提高了现有翻译模型的质量、适用性和可控性。GP-UNIT 的核心思想是从预先训练的类条件 GAN 中提取生成先验，构建粗级跨域通信，并应用在对抗性翻译之前学到的知识挖掘细级通信。通过学习多级内容对应，GP-UNIT 能够在近域和远域之间进行有效的翻译。对于封闭领域，GP-UNIT 可以根据一个参数来确定翻译过程中内容对应的强度，使用户能够在内容和风格一致性之间取得平衡。对于遥远的领域，探索半监督学习来指导 GP-UNIT 发现准确的语义对应，这些对应很难仅仅从外观上学习。我们通过大量的实验验证了 GP-UNIT 相对于最先进的翻译模型在不同领域之间稳健、高质量和多样化的翻译方面的优越性。"
    },
    {
        "title": "Contrastive Lift: 3D Object Instance Segmentation by Slow-Fast\n  Contrastive Fusion",
        "url": "http://arxiv.org/abs/2306.04633v1",
        "pub_date": "2023-06-07",
        "summary": "Instance segmentation in 3D is a challenging task due to the lack of\nlarge-scale annotated datasets. In this paper, we show that this task can be\naddressed effectively by leveraging instead 2D pre-trained models for instance\nsegmentation. We propose a novel approach to lift 2D segments to 3D and fuse\nthem by means of a neural field representation, which encourages multi-view\nconsistency across frames. The core of our approach is a slow-fast clustering\nobjective function, which is scalable and well-suited for scenes with a large\nnumber of objects. Unlike previous approaches, our method does not require an\nupper bound on the number of objects or object tracking across frames. To\ndemonstrate the scalability of the slow-fast clustering, we create a new\nsemi-realistic dataset called the Messy Rooms dataset, which features scenes\nwith up to 500 objects per scene. Our approach outperforms the state-of-the-art\non challenging scenes from the ScanNet, Hypersim, and Replica datasets, as well\nas on our newly created Messy Rooms dataset, demonstrating the effectiveness\nand scalability of our slow-fast clustering method.",
        "translated": "由于缺乏大规模的注释数据集，三维实例分割是一项具有挑战性的任务。在本文中，我们表明，这项任务可以有效地解决，利用而不是2D 预训练模型的实例分割。我们提出了一种新的方法，提升二维段到三维和融合它们的手段的神经场表示，鼓励多视图一致性跨帧。我们的方法的核心是一个慢-快聚类目标函数，它是可扩展的，并且非常适合有大量对象的场景。与以前的方法不同，我们的方法不需要对对象数量或对象跨帧跟踪的上限。为了演示慢-快聚类的可伸缩性，我们创建了一个新的半现实数据集，称为 Messy Rooms 数据集，它的特点是每个场景最多有500个对象。我们的方法在具有挑战性的场景扫描网络，Hypersim 和副本数据集，以及我们新创建的混乱的房间数据集优于最先进的表现，展示了我们的缓慢快速聚类方法的有效性和可扩展性。"
    },
    {
        "title": "Designing a Better Asymmetric VQGAN for StableDiffusion",
        "url": "http://arxiv.org/abs/2306.04632v1",
        "pub_date": "2023-06-07",
        "summary": "StableDiffusion is a revolutionary text-to-image generator that is causing a\nstir in the world of image generation and editing. Unlike traditional methods\nthat learn a diffusion model in pixel space, StableDiffusion learns a diffusion\nmodel in the latent space via a VQGAN, ensuring both efficiency and quality. It\nnot only supports image generation tasks, but also enables image editing for\nreal images, such as image inpainting and local editing. However, we have\nobserved that the vanilla VQGAN used in StableDiffusion leads to significant\ninformation loss, causing distortion artifacts even in non-edited image\nregions. To this end, we propose a new asymmetric VQGAN with two simple\ndesigns. Firstly, in addition to the input from the encoder, the decoder\ncontains a conditional branch that incorporates information from task-specific\npriors, such as the unmasked image region in inpainting. Secondly, the decoder\nis much heavier than the encoder, allowing for more detailed recovery while\nonly slightly increasing the total inference cost. The training cost of our\nasymmetric VQGAN is cheap, and we only need to retrain a new asymmetric decoder\nwhile keeping the vanilla VQGAN encoder and StableDiffusion unchanged. Our\nasymmetric VQGAN can be widely used in StableDiffusion-based inpainting and\nlocal editing methods. Extensive experiments demonstrate that it can\nsignificantly improve the inpainting and editing performance, while maintaining\nthe original text-to-image capability. The code is available at\n\\url{https://github.com/buxiangzhiren/Asymmetric_VQGAN}.",
        "translated": "稳定扩散是一个革命性的文本到图像生成器，它在图像生成和编辑领域引起了轰动。不像传统的方法在像素空间学习扩散模型，稳定扩散通过 VQGAN 在潜在空间学习扩散模型，确保效率和质量。它不仅支持图像生成任务，还支持对真实图像进行图像编辑，如图像修补和局部编辑。然而，我们已经观察到，在 StableDefusion 中使用的普通 VQGAN 会导致显著的信息丢失，甚至在未经编辑的图像区域中也会造成失真。为此，我们提出了一种新的非对称 VQGAN 的两个简单的设计。首先，除了来自编码器的输入外，解码器还包含一个条件分支，该分支包含来自任务特定先验的信息，例如内绘中未掩盖的图像区域。其次，解码器比编码器重得多，允许更详细的恢复，同时只略微增加总的推理成本。我们的非对称 VQGAN 的训练成本是低廉的，我们只需要重新训练一个新的非对称解码器，同时保持普通的 VQGAN 编码器和稳定扩散不变。我们的非对称 VQGAN 可以广泛应用于基于稳定扩散的修补和局部编辑方法。大量实验表明，该算法能够显著提高修补和编辑性能，同时保持原始的文本到图像的能力。该代码可在 url { https://github.com/buxiangzhiren/asymmetric_vqgan }获得。"
    },
    {
        "title": "Yet Another Algorithm for Supervised Principal Component Analysis:\n  Supervised Linear Centroid-Encoder",
        "url": "http://arxiv.org/abs/2306.04622v1",
        "pub_date": "2023-06-07",
        "summary": "We propose a new supervised dimensionality reduction technique called\nSupervised Linear Centroid-Encoder (SLCE), a linear counterpart of the\nnonlinear Centroid-Encoder (CE) \\citep{ghosh2022supervised}. SLCE works by\nmapping the samples of a class to its class centroid using a linear\ntransformation. The transformation is a projection that reconstructs a point\nsuch that its distance from the corresponding class centroid, i.e.,\ncentroid-reconstruction loss, is minimized in the ambient space. We derive a\nclosed-form solution using an eigendecomposition of a symmetric matrix. We did\na detailed analysis and presented some crucial mathematical properties of the\nproposed approach. %We also provide an iterative solution approach based\nsolving the optimization problem using a descent method. We establish a\nconnection between the eigenvalues and the centroid-reconstruction loss. In\ncontrast to Principal Component Analysis (PCA) which reconstructs a sample in\nthe ambient space, the transformation of SLCE uses the instances of a class to\nrebuild the corresponding class centroid. Therefore the proposed method can be\nconsidered a form of supervised PCA. Experimental results show the performance\nadvantage of SLCE over other supervised methods.",
        "translated": "我们提出了一种新的监督降维技术，称为监督线性质心编码器(SLCE) ，它是非线性质心编码器(CE) citep { ghosh2022监督}的线性副本。SLCE 的工作方式是使用一个线性映射将一个类的样本映射到它的类 centroid。该变换是一种投影，它重建一个点，使其与相应类质心的距离(即质心重建损失)在环境空间中最小化。我们利用对称矩阵的特征分解得到一个封闭形式的解。我们做了一个详细的分析，并提出了一些关键的数学性质的建议的方法。% 我们还提供了一种基于迭代解法的方法，用下降法求解最佳化问题。我们建立了特征值与质心重构损失之间的联系。与在环境空间中重建样本的主成分分析(PCA)不同，SLCE 的转换使用一个类的实例来重建相应的类 centroid。因此，该方法可以看作是一种有监督的主成分分析方法。实验结果表明，SLCE 算法的性能优于其他监督方法。"
    },
    {
        "title": "Align, Distill, and Augment Everything All at Once for Imbalanced\n  Semi-Supervised Learning",
        "url": "http://arxiv.org/abs/2306.04621v1",
        "pub_date": "2023-06-07",
        "summary": "Addressing the class imbalance in long-tailed semi-supervised learning (SSL)\nposes a few significant challenges stemming from differences between the\nmarginal distributions of unlabeled data and the labeled data, as the former is\noften unknown and potentially distinct from the latter. The first challenge is\nto avoid biasing the pseudo-labels towards an incorrect distribution, such as\nthat of the labeled data or a balanced distribution, during training. However,\nwe still wish to ensure a balanced unlabeled distribution during inference,\nwhich is the second challenge. To address both of these challenges, we propose\na three-faceted solution: a flexible distribution alignment that progressively\naligns the classifier from a dynamically estimated unlabeled prior towards a\nbalanced distribution, a soft consistency regularization that exploits\nunderconfident pseudo-labels discarded by threshold-based methods, and a schema\nfor expanding the unlabeled set with input data from the labeled partition.\nThis last facet comes in as a response to the commonly-overlooked fact that\ndisjoint partitions of labeled and unlabeled data prevent the benefits of\nstrong data augmentation on the labeled set. Our overall framework requires no\nadditional training cycles, so it will align, distill, and augment everything\nall at once (ADALLO). Our extensive evaluations of ADALLO on imbalanced SSL\nbenchmark datasets, including CIFAR10-LT, CIFAR100-LT, and STL10-LT with\nvarying degrees of class imbalance, amount of labeled data, and distribution\nmismatch, demonstrate significant improvements in the performance of imbalanced\nSSL under large distribution mismatch, as well as competitiveness with\nstate-of-the-art methods when the labeled and unlabeled data follow the same\nmarginal distribution. Our code will be released upon paper acceptance.",
        "translated": "解决长尾半监督学习中的类别失衡问题，是由于未标记数据的边际分布与标记数据之间的差异而带来的一些重大挑战，因为前者往往是未知的，而且可能与后者不同。第一个挑战是在训练期间避免使伪标签偏向于不正确的分布，例如标记数据或平衡分布。然而，我们仍然希望在推理过程中确保一个平衡的无标记分布，这是第二个挑战。为了解决这两个挑战，我们提出了一个三方面的解决方案: 一个灵活的分布对齐，逐步将分类器从一个动态估计的未标记先于一个平衡的分布，一个软一致性正则化，利用基于阈值的方法丢弃的不自信的伪标签，和一个模式来扩展未标记的集从标记的分区输入数据。最后一个方面是对一个常被忽视的事实的回应，即标记数据和未标记数据的不相交分区阻碍了标记集上强大数据增强的好处。我们的总体框架不需要额外的培训周期，因此它将一次性对齐、提取和增强所有内容(ADALLO)。我们对 ADallo 在不平衡 SSL 基准数据集上的广泛评估，包括 CIFAR10-LT、 CIFAR100-LT 和 STL10-LT，这些数据集具有不同程度的类不平衡、标记数据的数量和分布不匹配，表明在大的分布不匹配情况下，不平衡 SSL 的性能有显著改善，当标记和未标记数据遵循相同的边缘分布时，与最先进的方法竞争。我们的代码将在书面验收后发布。"
    },
    {
        "title": "ARTIC3D: Learning Robust Articulated 3D Shapes from Noisy Web Image\n  Collections",
        "url": "http://arxiv.org/abs/2306.04619v1",
        "pub_date": "2023-06-07",
        "summary": "Estimating 3D articulated shapes like animal bodies from monocular images is\ninherently challenging due to the ambiguities of camera viewpoint, pose,\ntexture, lighting, etc. We propose ARTIC3D, a self-supervised framework to\nreconstruct per-instance 3D shapes from a sparse image collection in-the-wild.\nSpecifically, ARTIC3D is built upon a skeleton-based surface representation and\nis further guided by 2D diffusion priors from Stable Diffusion. First, we\nenhance the input images with occlusions/truncation via 2D diffusion to obtain\ncleaner mask estimates and semantic features. Second, we perform\ndiffusion-guided 3D optimization to estimate shape and texture that are of\nhigh-fidelity and faithful to input images. We also propose a novel technique\nto calculate more stable image-level gradients via diffusion models compared to\nexisting alternatives. Finally, we produce realistic animations by fine-tuning\nthe rendered shape and texture under rigid part transformations. Extensive\nevaluations on multiple existing datasets as well as newly introduced noisy web\nimage collections with occlusions and truncation demonstrate that ARTIC3D\noutputs are more robust to noisy images, higher quality in terms of shape and\ntexture details, and more realistic when animated. Project page:\nhttps://chhankyao.github.io/artic3d/",
        "translated": "由于摄像机视角、姿势、纹理、光照等的模糊性，从单目图像中估计像动物身体这样的三维铰接形状本身就具有挑战性。我们提出 ARTIC3D，一个自我监督的框架来重建每个实例的三维形状从稀疏的图像收集在野外。具体来说，ARTIC3D 是建立在一个基于骨架的表面表示，并进一步指导二维扩散从稳定扩散的先决条件。首先，通过二维扩散增强遮挡/截断输入图像，获得更清晰的掩模估计和语义特征。其次，我们进行扩散引导的三维优化，以估计形状和纹理的高保真度和忠实的输入图像。我们还提出了一种新的技术，计算更稳定的图像水平梯度通过扩散模型相比，现有的替代方案。最后，我们通过在刚性部分变换下微调渲染的形状和纹理来生成逼真的动画。对多个现有数据集的广泛评估以及新引入的具有遮挡和截断的噪声网络图像集表明，ARTIC3D 输出对噪声图像更加稳健，在形状和纹理细节方面更高质量，并且在动画时更加真实。项目主页:  https://chhankyao.github.io/artic3d/"
    },
    {
        "title": "ICON$^2$: Reliably Benchmarking Predictive Inequity in Object Detection",
        "url": "http://arxiv.org/abs/2306.04482v1",
        "pub_date": "2023-06-07",
        "summary": "As computer vision systems are being increasingly deployed at scale in\nhigh-stakes applications like autonomous driving, concerns about social bias in\nthese systems are rising. Analysis of fairness in real-world vision systems,\nsuch as object detection in driving scenes, has been limited to observing\npredictive inequity across attributes such as pedestrian skin tone, and lacks a\nconsistent methodology to disentangle the role of confounding variables e.g.\ndoes my model perform worse for a certain skin tone, or are such scenes in my\ndataset more challenging due to occlusion and crowds? In this work, we\nintroduce ICON$^2$, a framework for robustly answering this question. ICON$^2$\nleverages prior knowledge on the deficiencies of object detection systems to\nidentify performance discrepancies across sub-populations, compute correlations\nbetween these potential confounders and a given sensitive attribute, and\ncontrol for the most likely confounders to obtain a more reliable estimate of\nmodel bias. Using our approach, we conduct an in-depth study on the performance\nof object detection with respect to income from the BDD100K driving dataset,\nrevealing useful insights.",
        "translated": "随着计算机视觉系统在自动驾驶等高风险应用领域的大规模应用，人们越来越担心这些系统中存在社会偏见。对现实世界视觉系统中公平性的分析，例如驾驶场景中的目标检测，仅限于观察行人肤色等属性的预测不公平性，并且缺乏一个一致的方法来解决混杂变量的作用，例如，我的模型在某种肤色下表现更差，还是由于遮挡和拥挤，这些场景在我的数据集中更具挑战性？在这项工作中，我们介绍了图标 $^ 2 $，一个框架，以稳健地回答这个问题。ICON $^ 2 $利用对目标检测系统缺陷的先前知识来识别不同亚群之间的性能差异，计算这些潜在混杂因素与给定敏感属性之间的相关性，并控制最可能的混杂因素以获得更可靠的模型偏差估计。利用我们的方法，我们进行了一个深入的研究表现的目标检测与收入方面的 BDD100k 驾驶数据集，揭示了有用的见解。"
    },
    {
        "title": "Integrating Geometric Control into Text-to-Image Diffusion Models for\n  High-Quality Detection Data Generation via Text Prompt",
        "url": "http://arxiv.org/abs/2306.04607v2",
        "pub_date": "2023-06-07",
        "summary": "Diffusion models have attracted significant attention due to their remarkable\nability to create content and generate data for tasks such as image\nclassification. However, the usage of diffusion models to generate high-quality\nobject detection data remains an underexplored area, where not only the\nimage-level perceptual quality but also geometric conditions such as bounding\nboxes and camera views are essential. Previous studies have utilized either\ncopy-paste synthesis or layout-to-image (L2I) generation with specifically\ndesigned modules to encode semantic layouts. In this paper, we propose\nGeoDiffusion, a simple framework that can flexibly translate various geometric\nconditions into text prompts and empower the pre-trained text-to-image (T2I)\ndiffusion models for high-quality detection data generation. Unlike previous\nL2I methods, our GeoDiffusion is able to encode not only bounding boxes but\nalso extra geometric conditions such as camera views in self-driving scenes.\nExtensive experiments demonstrate GeoDiffusion outperforms previous L2I methods\nwhile maintaining 4x training time faster. To the best of our knowledge, this\nis the first work to adopt diffusion models for layout-to-image generation with\ngeometric conditions and demonstrate that L2I-generated images can be\nbeneficial for improving the performance of object detectors.",
        "translated": "扩散模型由于具有为图像分类等任务创建内容和生成数据的显著能力而引起了人们的高度重视。然而，使用扩散模型来生成高质量的目标检测数据仍然是一个探索不足的领域，不仅图像水平的感知质量，而且几何条件，如边界框和相机视图都是必不可少的。先前的研究已经利用复制粘贴合成或布局到图像(L2I)与专门设计的模块生成来编码语义布局。本文提出了一个简单的 GeoDefusion 框架，该框架可以灵活地将各种几何条件转换为文本提示，并赋予预先训练的文本到图像(T2I)扩散模型以高质量的检测数据生成能力。与以前的 L2I 方法不同，我们的 GeoDefusion 不仅能够编码边界框，而且还能够编码额外的几何条件，例如自动驾驶场景中的摄像机视图。大量的实验表明，GeoDiffusion 比以前的 L2I 方法更好，同时保持4倍的训练时间更快。据我们所知，这是第一个采用扩散模型的布局图像生成的几何条件，并证明了 L2I 生成的图像可以有利于提高性能的目标检测器。"
    },
    {
        "title": "MarineVRS: Marine Video Retrieval System with Explainability via\n  Semantic Understanding",
        "url": "http://arxiv.org/abs/2306.04593v1",
        "pub_date": "2023-06-07",
        "summary": "Building a video retrieval system that is robust and reliable, especially for\nthe marine environment, is a challenging task due to several factors such as\ndealing with massive amounts of dense and repetitive data, occlusion,\nblurriness, low lighting conditions, and abstract queries. To address these\nchallenges, we present MarineVRS, a novel and flexible video retrieval system\ndesigned explicitly for the marine domain. MarineVRS integrates\nstate-of-the-art methods for visual and linguistic object representation to\nenable efficient and accurate search and analysis of vast volumes of underwater\nvideo data. In addition, unlike the conventional video retrieval system, which\nonly permits users to index a collection of images or videos and search using a\nfree-form natural language sentence, our retrieval system includes an\nadditional Explainability module that outputs the segmentation masks of the\nobjects that the input query referred to. This feature allows users to identify\nand isolate specific objects in the video footage, leading to more detailed\nanalysis and understanding of their behavior and movements. Finally, with its\nadaptability, explainability, accuracy, and scalability, MarineVRS is a\npowerful tool for marine researchers and scientists to efficiently and\naccurately process vast amounts of data and gain deeper insights into the\nbehavior and movements of marine species.",
        "translated": "建立一个健壮可靠的视频检索系统，特别是对于海洋环境来说，是一个具有挑战性的任务，因为有几个因素，如处理大量密集和重复的数据，遮挡，模糊，低照明条件和抽象查询。为了应对这些挑战，我们提出了 MarineVRS，一个新颖的和灵活的视频检索系统，明确地为海洋领域设计。MarineVRS 集成了最先进的视觉和语言对象表示方法，能够高效、准确地搜索和分析海量水下视频数据。此外，与传统的视频检索系统不同，传统的视频检索系统只允许用户索引一组图像或视频并使用自由格式的自然语言句子进行搜索，我们的检索系统包括一个额外的可解释性模块，该模块输出输入查询引用的对象的分割掩码。这个功能允许用户识别和隔离视频画面中的特定物体，从而对它们的行为和动作进行更详细的分析和理解。最后，凭借其适应性、可解释性、准确性和可扩展性，MarineVRS 是海洋研究人员和科学家有效和准确地处理大量数据并获得对海洋物种行为和运动的更深刻见解的强大工具。"
    },
    {
        "title": "A Dataset for Deep Learning-based Bone Structure Analyses in Total Hip\n  Arthroplasty",
        "url": "http://arxiv.org/abs/2306.04579v1",
        "pub_date": "2023-06-07",
        "summary": "Total hip arthroplasty (THA) is a widely used surgical procedure in\northopedics. For THA, it is of clinical significance to analyze the bone\nstructure from the CT images, especially to observe the structure of the\nacetabulum and femoral head, before the surgical procedure. For such bone\nstructure analyses, deep learning technologies are promising but require\nhigh-quality labeled data for the learning, while the data labeling is costly.\nWe address this issue and propose an efficient data annotation pipeline for\nproducing a deep learning-oriented dataset. Our pipeline consists of\nnon-learning-based bone extraction (BE) and acetabulum and femoral head\nsegmentation (AFS) and active-learning-based annotation refinement (AAR). For\nBE we use the classic graph-cut algorithm. For AFS we propose an improved\nalgorithm, including femoral head boundary localization using first-order and\nsecond-order gradient regularization, line-based non-maximum suppression, and\nanatomy prior-based femoral head extraction. For AAR, we refine the\nalgorithm-produced pseudo labels with the help of trained deep models: we\nmeasure the uncertainty based on the disagreement between the original pseudo\nlabels and the deep model predictions, and then find out the samples with the\nlargest uncertainty to ask for manual labeling. Using the proposed pipeline, we\nconstruct a large-scale bone structure analyses dataset from more than 300\nclinical and diverse CT scans. We perform careful manual labeling for the test\nset of our data. We then benchmark multiple state-of-the art deep\nlearning-based methods of medical image segmentation using the training and\ntest sets of our data. The extensive experimental results validate the efficacy\nof the proposed data annotation pipeline. The dataset, related codes and models\nwill be publicly available at https://github.com/hitachinsk/THA.",
        "translated": "全髋关节置换术(THA)是一种广泛应用于骨科的外科手术。对于全髋关节置换术来说，术前从 CT 图像上分析骨结构，特别是观察髋臼和股骨头的结构具有重要的临床意义。对于这样的骨结构分析，深度学习技术是有前途的，但需要高质量的标记数据进行学习，而数据标记是昂贵的。我们解决了这个问题，并提出了一个有效的数据注释流水线，以产生一个面向深度学习的数据集。我们的流水线包括基于非学习的骨提取(BE)和髋臼和股骨头分割(AFS)以及基于主动学习的注释细化(AAR)。对于 BE，我们使用经典的图割算法。对于 AFS，我们提出了一种改进的算法，包括使用一阶和二阶梯度正则化的股骨头边界定位、基于线的非最大抑制和基于解剖学先验的股骨头提取。对于 AAR，我们利用训练好的深度模型对算法生成的伪标签进行改进: 我们根据原始伪标签与深度模型预测的不一致性来测量不确定性，然后找出不确定性最大的样本进行人工标签。使用该流水线，我们构建了一个来自300多个临床和不同 CT 扫描的大规模骨结构分析数据集。我们对数据的测试集执行仔细的手动标记。然后，我们利用数据的训练和测试集，对基于深度学习的多种医学图像分割方法进行评估。广泛的实验结果验证了所提出的数据注释流水线的有效性。数据集、相关代码和模型将在 https://github.com/hitachinsk/tha 公开发布。"
    },
    {
        "title": "Grounded Text-to-Image Synthesis with Attention Refocusing",
        "url": "http://arxiv.org/abs/2306.05427v1",
        "pub_date": "2023-06-08",
        "summary": "Driven by scalable diffusion models trained on large-scale paired text-image\ndatasets, text-to-image synthesis methods have shown compelling results.\nHowever, these models still fail to precisely follow the text prompt when\nmultiple objects, attributes, and spatial compositions are involved in the\nprompt. In this paper, we identify the potential reasons in both the\ncross-attention and self-attention layers of the diffusion model. We propose\ntwo novel losses to refocus the attention maps according to a given layout\nduring the sampling process. We perform comprehensive experiments on the\nDrawBench and HRS benchmarks using layouts synthesized by Large Language\nModels, showing that our proposed losses can be integrated easily and\neffectively into existing text-to-image methods and consistently improve their\nalignment between the generated images and the text prompts.",
        "translated": "在大规模成对文本-图像数据集上训练的可扩展扩散模型的驱动下，文本-图像合成方法已经显示出引人注目的结果。但是，当提示包含多个对象、属性和空间组合时，这些模型仍然不能精确地跟随文本提示。本文从扩散模型的交叉注意层和自我注意层两个方面分析了影响扩散模型的潜在原因。我们提出了两个新的损失重新聚焦的注意图根据给定的布局在采样过程中。我们在 DrawBench 和 HRS 基准上进行了全面的实验，使用了大型语言模型合成的布局，表明我们提出的损失可以很容易和有效地整合到现有的文本到图像的方法中，并持续改善生成的图像和文本提示符之间的对齐。"
    },
    {
        "title": "Background Prompting for Improved Object Depth",
        "url": "http://arxiv.org/abs/2306.05428v1",
        "pub_date": "2023-06-08",
        "summary": "Estimating the depth of objects from a single image is a valuable task for\nmany vision, robotics, and graphics applications. However, current methods\noften fail to produce accurate depth for objects in diverse scenes. In this\nwork, we propose a simple yet effective Background Prompting strategy that\nadapts the input object image with a learned background. We learn the\nbackground prompts only using small-scale synthetic object datasets. To infer\nobject depth on a real image, we place the segmented object into the learned\nbackground prompt and run off-the-shelf depth networks. Background Prompting\nhelps the depth networks focus on the foreground object, as they are made\ninvariant to background variations. Moreover, Background Prompting minimizes\nthe domain gap between synthetic and real object images, leading to better\nsim2real generalization than simple finetuning. Results on multiple synthetic\nand real datasets demonstrate consistent improvements in real object depths for\na variety of existing depth networks. Code and optimized background prompts can\nbe found at: https://mbaradad.github.io/depth_prompt.",
        "translated": "对于许多视觉、机器人和图形应用程序来说，从单幅图像估计物体的深度是一项非常有价值的任务。然而，目前的方法往往不能产生准确的深度对物体在不同的场景。在这项工作中，我们提出了一个简单而有效的背景提示策略，适应输入的物体图像与学习背景。我们只使用小规模的合成对象数据集来学习背景提示。为了在真实图像上推断目标深度，我们将分割后的目标放入学习后的背景提示中，运行现成的深度网络。背景提示有助于深度网络聚焦于前景物体，因为它们对背景变化具有不变性。此外，背景提示最小化了合成和真实物体图像之间的域间隔，从而比简单的微调更好地实现了模拟和真实图像的泛化。对多个合成和真实数据集的结果表明，对于现有的各种深度网络，在实际目标深度方面有一致的改进。代码和优化后的背景提示可以在以下 https://mbaradad.github.io/depth_prompt 找到:。"
    },
    {
        "title": "Video-ChatGPT: Towards Detailed Video Understanding via Large Vision and\n  Language Models",
        "url": "http://arxiv.org/abs/2306.05424v1",
        "pub_date": "2023-06-08",
        "summary": "Conversation agents fueled by Large Language Models (LLMs) are providing a\nnew way to interact with visual data. While there have been initial attempts\nfor image-based conversation models, this work addresses the underexplored\nfield of video-based conversation by introducing Video-ChatGPT. It is a\nmultimodal model that merges a video-adapted visual encoder with a LLM. The\nmodel is capable of understanding and generating human-like conversations about\nvideos. We introduce a new dataset of 100,000 video-instruction pairs used to\ntrain Video-ChatGPT acquired via manual and semi-automated pipeline that is\neasily scalable and robust to label noise. We also develop a quantiative\nevaluation framework for video-based dialogue models to objectively analyse the\nstrengths and weaknesses of proposed models. Our code, models, instruction-sets\nand demo are released at https://github.com/mbzuai-oryx/Video-ChatGPT.",
        "translated": "由大型语言模型(LLM)驱动的会话代理提供了一种与可视化数据交互的新方法。虽然基于图像的会话模型已经有了初步的尝试，但是本文通过引入 Video-ChatGPT 解决了基于视频的会话中未被充分发掘的领域。它是一个多模态模型，融合了视频适应的视觉编码器和 LLM。该模型能够理解和生成关于视频的类人对话。我们引入了一个新的数据集100,000视频指令对用于训练视频聊天 GPT 获得通过手动和半自动管道，这是很容易扩展和鲁棒的标签噪声。我们还开发了一个基于视频对话模型的定量评估框架，以客观地分析所提出模型的优缺点。我们的代码、模型、指令集和演示 https://github.com/mbzuai-oryx/video-chatgpt 全部发布。"
    },
    {
        "title": "MIMIC-IT: Multi-Modal In-Context Instruction Tuning",
        "url": "http://arxiv.org/abs/2306.05425v1",
        "pub_date": "2023-06-08",
        "summary": "High-quality instructions and responses are essential for the zero-shot\nperformance of large language models on interactive natural language tasks. For\ninteractive vision-language tasks involving intricate visual scenes, a large\nquantity of diverse and creative instruction-response pairs should be\nimperative to tune vision-language models (VLMs). Nevertheless, the current\navailability of vision-language instruction-response pairs in terms of\nquantity, diversity, and creativity remains limited, posing challenges to the\ngeneralization of interactive VLMs. Here we present MultI-Modal In-Context\nInstruction Tuning (MIMIC-IT), a dataset comprising 2.8 million multimodal\ninstruction-response pairs, with 2.2 million unique instructions derived from\nimages and videos. Each pair is accompanied by multi-modal in-context\ninformation, forming conversational contexts aimed at empowering VLMs in\nperception, reasoning, and planning. The instruction-response collection\nprocess, dubbed as Syphus, is scaled using an automatic annotation pipeline\nthat combines human expertise with GPT's capabilities. Using the MIMIC-IT\ndataset, we train a large VLM named Otter. Based on extensive evaluations\nconducted on vision-language benchmarks, it has been observed that Otter\ndemonstrates remarkable proficiency in multi-modal perception, reasoning, and\nin-context learning. Human evaluation reveals it effectively aligns with the\nuser's intentions. We release the MIMIC-IT dataset, instruction-response\ncollection pipeline, benchmarks, and the Otter model.",
        "translated": "高质量的指令和响应对于大型语言模型在交互式自然语言任务中的零点性能至关重要。对于涉及复杂视觉场景的交互式视觉语言任务，必须调优视觉语言模型(VLM)。然而，目前视觉-语言教学-反应对在数量、多样性和创造性方面的可用性仍然有限，对交互式 VLM 的普及提出了挑战。在这里，我们介绍了多模态上下文指令调优(MIMIC-IT) ，一个包含280万个多模态指令-响应对的数据集，其中有220万个来自图像和视频的独特指令。每一对都伴随着多模态的语境信息，形成旨在赋予 VLM 感知、推理和计划能力的会话语境。这个被称为 Syphus 的指令-响应收集过程使用一个自动注释管道进行扩展，该管道将人类的专业知识与 GPT 的功能结合在一起。使用 MIMIC-IT 数据集，我们训练了一个名为 Otter 的大型 VLM。基于对视觉语言基准的广泛评估，我们发现 Otter 在多模态知觉、推理和语境学习方面表现出显著的能力。人工评估显示它有效地与用户的意图保持一致。我们发布 MIMIC-IT 数据集、指令-响应收集管道、基准测试和 Otter 模型。"
    },
    {
        "title": "ADDP: Learning General Representations for Image Recognition and\n  Generation with Alternating Denoising Diffusion Process",
        "url": "http://arxiv.org/abs/2306.05423v1",
        "pub_date": "2023-06-08",
        "summary": "Image recognition and generation have long been developed independently of\neach other. With the recent trend towards general-purpose representation\nlearning, the development of general representations for both recognition and\ngeneration tasks is also promoted. However, preliminary attempts mainly focus\non generation performance, but are still inferior on recognition tasks. These\nmethods are modeled in the vector-quantized (VQ) space, whereas leading\nrecognition methods use pixels as inputs. Our key insights are twofold: (1)\npixels as inputs are crucial for recognition tasks; (2) VQ tokens as\nreconstruction targets are beneficial for generation tasks. These observations\nmotivate us to propose an Alternating Denoising Diffusion Process (ADDP) that\nintegrates these two spaces within a single representation learning framework.\nIn each denoising step, our method first decodes pixels from previous VQ\ntokens, then generates new VQ tokens from the decoded pixels. The diffusion\nprocess gradually masks out a portion of VQ tokens to construct the training\nsamples. The learned representations can be used to generate diverse\nhigh-fidelity images and also demonstrate excellent transfer performance on\nrecognition tasks. Extensive experiments show that our method achieves\ncompetitive performance on unconditional generation, ImageNet classification,\nCOCO detection, and ADE20k segmentation. Importantly, our method represents the\nfirst successful development of general representations applicable to both\ngeneration and dense recognition tasks. Code shall be released.",
        "translated": "图像识别和生成技术一直是相互独立发展的。随着通用表征学习的发展，识别任务和生成任务的通用表征也得到了进一步的发展。然而，初步的尝试主要集中在生成性能上，但在识别任务上仍然较差。这些方法建模在矢量量化(VQ)空间，而领先的识别方法使用像素作为输入。我们的主要见解有两个方面: (1)像素作为输入对识别任务是至关重要的; (2) VQ 标记作为重构目标对生成任务是有益的。这些观察促使我们提出一种交替去噪扩散过程(ADDP) ，它将这两个空间集成在一个单一的表示学习框架中。在每一个去噪步骤中，我们的方法首先从先前的 VQ 标记中解码像素，然后从解码的像素中生成新的 VQ 标记。扩散过程逐渐掩盖一部分 VQ 标记来构造训练样本。学习表征可以用来生成不同的高保真图像，并且在识别任务中表现出优异的传递性能。大量实验表明，该方法在无条件生成、 ImageNet 分类、 COCO 检测和 ADE20k 分割等方面具有较好的性能。重要的是，我们的方法首次成功地开发了适用于生成和密集识别任务的通用表示。密码将被公布。"
    },
    {
        "title": "Tracking Everything Everywhere All at Once",
        "url": "http://arxiv.org/abs/2306.05422v1",
        "pub_date": "2023-06-08",
        "summary": "We present a new test-time optimization method for estimating dense and\nlong-range motion from a video sequence. Prior optical flow or particle video\ntracking algorithms typically operate within limited temporal windows,\nstruggling to track through occlusions and maintain global consistency of\nestimated motion trajectories. We propose a complete and globally consistent\nmotion representation, dubbed OmniMotion, that allows for accurate, full-length\nmotion estimation of every pixel in a video. OmniMotion represents a video\nusing a quasi-3D canonical volume and performs pixel-wise tracking via\nbijections between local and canonical space. This representation allows us to\nensure global consistency, track through occlusions, and model any combination\nof camera and object motion. Extensive evaluations on the TAP-Vid benchmark and\nreal-world footage show that our approach outperforms prior state-of-the-art\nmethods by a large margin both quantitatively and qualitatively. See our\nproject page for more results: http://omnimotion.github.io/",
        "translated": "我们提出了一种新的测试时间优化方法来估计密集和远程运动从一个视频序列。先前的光流或粒子视频跟踪算法通常在有限的时间窗内运行，努力跟踪通过遮挡和维持估计的运动轨迹的全局一致性。我们提出了一个完整的和全球一致的运动表示，称为 OmniMotion，允许准确，全长运动估计每个像素的视频。OmniMotion 使用准3D 规范体积表示视频，并通过局部和规范空间之间的双向投影执行像素级跟踪。这种表示使我们能够确保全局一致性，跟踪通过遮挡，并建模任何组合的摄像机和对象的运动。对 TAP-Vid 基准和现实世界连续镜头的广泛评估表明，我们的方法在数量和质量上都大大优于先进的方法。更多结果请参见我们的项目页面:  http://omnimotion.github.io/"
    },
    {
        "title": "Stochastic Multi-Person 3D Motion Forecasting",
        "url": "http://arxiv.org/abs/2306.05421v1",
        "pub_date": "2023-06-08",
        "summary": "This paper aims to deal with the ignored real-world complexities in prior\nwork on human motion forecasting, emphasizing the social properties of\nmulti-person motion, the diversity of motion and social interactions, and the\ncomplexity of articulated motion. To this end, we introduce a novel task of\nstochastic multi-person 3D motion forecasting. We propose a dual-level\ngenerative modeling framework that separately models independent individual\nmotion at the local level and social interactions at the global level. Notably,\nthis dual-level modeling mechanism can be achieved within a shared generative\nmodel, through introducing learnable latent codes that represent intents of\nfuture motion and switching the codes' modes of operation at different levels.\nOur framework is general; we instantiate it with different generative models,\nincluding generative adversarial networks and diffusion models, and various\nmulti-person forecasting models. Extensive experiments on CMU-Mocap, MuPoTS-3D,\nand SoMoF benchmarks show that our approach produces diverse and accurate\nmulti-person predictions, significantly outperforming the state of the art.",
        "translated": "本文针对以往人体运动预测工作中所忽视的现实世界的复杂性，强调了多人运动的社会属性、运动和社会交互的多样性以及关节运动的复杂性。为此，我们提出了一个新颖的任务随机多人三维运动预测。我们提出了一个双层生成建模框架，分别在局部层面上模拟独立的个体运动和在全球层面上模拟社会互动。值得注意的是，这种双层建模机制可以通过引入代表未来运动意图的可学习的潜在代码，以及在不同层次上切换代码的操作模式，在一个共享的生成模型内实现。我们的框架是通用的; 我们用不同的生成模型来实例化它，包括生成对抗网络和扩散模型，以及各种多人预测模型。在 CMU-Mocap、 MuPoTS-3D 和 SoMoF 基准上进行的大量实验表明，我们的方法能够产生多样化和准确的多人预测，明显优于最先进的预测。"
    },
    {
        "title": "Scaling Spherical CNNs",
        "url": "http://arxiv.org/abs/2306.05420v1",
        "pub_date": "2023-06-08",
        "summary": "Spherical CNNs generalize CNNs to functions on the sphere, by using spherical\nconvolutions as the main linear operation. The most accurate and efficient way\nto compute spherical convolutions is in the spectral domain (via the\nconvolution theorem), which is still costlier than the usual planar\nconvolutions. For this reason, applications of spherical CNNs have so far been\nlimited to small problems that can be approached with low model capacity. In\nthis work, we show how spherical CNNs can be scaled for much larger problems.\nTo achieve this, we make critical improvements including novel variants of\ncommon model components, an implementation of core operations to exploit\nhardware accelerator characteristics, and application-specific input\nrepresentations that exploit the properties of our model. Experiments show our\nlarger spherical CNNs reach state-of-the-art on several targets of the QM9\nmolecular benchmark, which was previously dominated by equivariant graph neural\nnetworks, and achieve competitive performance on multiple weather forecasting\ntasks. Our code is available at\nhttps://github.com/google-research/spherical-cnn.",
        "translated": "球面 CNN 通过球面卷积作为主要的线性运算，将 CNN 推广到球面上的函数。计算球面卷积最精确、最有效的方法是在谱域(通过卷积定理) ，这仍然比通常的平面卷积要昂贵。由于这个原因，迄今为止，球形 CNN 的应用仅限于可以用低模型容量来处理的小问题。在这项工作中，我们展示了如何球面 CNN 可以规模更大的问题。为了实现这一点，我们进行了关键的改进，包括通用模型组件的新变体、利用硬件加速器特性的核心操作的实现，以及利用模型特性的特定于应用程序的输入表示。实验表明，我们的大型球形神经网络在几个 QM9分子基准指标上达到了最先进的水平，以前主要由等变图形神经网络控制，并在多个天气预报任务上达到了具有竞争力的性能。我们的代码可以在 https://github.com/google-research/spherical-cnn 找到。"
    },
    {
        "title": "2D Supervised Monocular 3D Object Detection by Global-to-Local 3D\n  Reconstruction",
        "url": "http://arxiv.org/abs/2306.05418v1",
        "pub_date": "2023-06-08",
        "summary": "With the advent of the big model era, the demand for data has become more\nimportant. Especially in monocular 3D object detection, expensive manual\nannotations potentially limit further developments. Existing works have\ninvestigated weakly supervised algorithms with the help of LiDAR modality to\ngenerate 3D pseudo labels, which cannot be applied to ordinary videos. In this\npaper, we propose a novel paradigm, termed as BA$^2$-Det, leveraging the idea\nof global-to-local 3D reconstruction for 2D supervised monocular 3D object\ndetection. Specifically, we recover 3D structures from monocular videos by\nscene-level global reconstruction with global bundle adjustment (BA) and obtain\nobject clusters by the DoubleClustering algorithm. Learning from completely\nreconstructed objects in global BA, GBA-Learner predicts pseudo labels for\noccluded objects. Finally, we train an LBA-Learner with object-centric local BA\nto generalize the generated 3D pseudo labels to moving objects. Experiments on\nthe large-scale Waymo Open Dataset show that the performance of BA$^2$-Det is\non par with the fully-supervised BA-Det trained with 10% videos and even\noutperforms some pioneer fully-supervised methods. We also show the great\npotential of BA$^2$-Det for detecting open-set 3D objects in complex scenes.\nThe code will be made available. Project page: https://ba2det.site .",
        "translated": "随着大模型时代的到来，对数据的需求变得越来越重要。特别是在单目3D 目标检测中，昂贵的手动注释可能会限制进一步的开发。现有的研究都是利用激光雷达模态对弱监督算法进行研究，以生成不能应用于普通视频的三维伪标签。在本文中，我们提出了一个新的范例，称为 BA $^ 2 $- Det，利用全局到局部的二维监督单目三维目标检测的三维重建思想。具体来说，我们使用全局光束法平差(BA)进行场景级别的全局重建，从单目视频中恢复出三维结构，并使用双重聚类算法获得目标聚类。GBA-Learner 从全局 BA 的完全重构对象中学习，预测被遮挡对象的伪标签。最后，我们训练一个以对象为中心的 LBA 学习者，将生成的三维伪标签推广到移动对象。在大规模 Waymo Open Dataset 上的实验表明，BA $^ 2 $- Det 的性能与10% 视频训练的全监督 BA-Det 相当，甚至优于一些先进的全监督方法。我们还展示了 BA $^ 2 $- Det 在检测复杂场景中开放集合的3D 对象方面的巨大潜力。代码将可用。项目主页:  https://ba2det.site。"
    },
    {
        "title": "TopoMask: Instance-Mask-Based Formulation for the Road Topology Problem\n  via Transformer-Based Architecture",
        "url": "http://arxiv.org/abs/2306.05419v1",
        "pub_date": "2023-06-08",
        "summary": "Driving scene understanding task involves detecting static elements such as\nlanes, traffic signs, and traffic lights, and their relationships with each\nother. To facilitate the development of comprehensive scene understanding\nsolutions using multiple camera views, a new dataset called Road Genome\n(OpenLane-V2) has been released. This dataset allows for the exploration of\ncomplex road connections and situations where lane markings may be absent.\nInstead of using traditional lane markings, the lanes in this dataset are\nrepresented by centerlines, which offer a more suitable representation of lanes\nand their connections. In this study, we have introduced a new approach called\nTopoMask for predicting centerlines in road topology. Unlike existing\napproaches in the literature that rely on keypoints or parametric methods,\nTopoMask utilizes an instance-mask based formulation with a transformer-based\narchitecture and, in order to enrich the mask instances with flow information,\na direction label representation is proposed. TopoMask have ranked 4th in the\nOpenLane-V2 Score (OLS) and ranked 2nd in the F1 score of centerline prediction\nin OpenLane Topology Challenge 2023. In comparison to the current\nstate-of-the-art method, TopoNet, the proposed method has achieved similar\nperformance in Frechet-based lane detection and outperformed TopoNet in\nChamfer-based lane detection without utilizing its scene graph neural network.",
        "translated": "驾驶场景理解任务包括检测车道、交通标志、交通灯等静态要素及其相互关系。为了便于开发全面的场景理解解决方案使用多摄像机视图，一个新的数据集称为道路基因组(OpenLane-V2)已经发布。这个数据集允许探索复杂的道路连接和车道标记可能不存在的情况。该数据集中的车道由中心线表示，而不是使用传统的车道标记，中心线为车道及其连接提供了更合适的表示。在这项研究中，我们介绍了一种新的方法，称为拓扑掩模预测中心线的道路拓扑。与文献中现有的依赖关键点或参数方法的方法不同，TopoMask 利用基于实例掩模的公式和基于变压器的体系结构，为了用流量信息丰富掩模实例，提出了一种方向标签表示。TopoMask 在 OpenLane-V2得分(OLS)中排名第4，在 OpenLane 拓扑挑战赛2023的中心线预测 F1得分中排名第2。与目前最先进的 TopoNet 方法相比，该方法在基于 Frechet 的车道检测方面取得了相似的性能，在不使用场景图神经网络的情况下，在基于倒角的车道检测方面优于 TopoNet。"
    },
    {
        "title": "Leveraging Large Language Models for Scalable Vector Graphics-Driven\n  Image Understanding",
        "url": "http://arxiv.org/abs/2306.06094v1",
        "pub_date": "2023-06-09",
        "summary": "Recently, large language models (LLMs) have made significant advancements in\nnatural language understanding and generation. However, their potential in\ncomputer vision remains largely unexplored. In this paper, we introduce a new,\nexploratory approach that enables LLMs to process images using the Scalable\nVector Graphics (SVG) format. By leveraging the XML-based textual descriptions\nof SVG representations instead of raster images, we aim to bridge the gap\nbetween the visual and textual modalities, allowing LLMs to directly understand\nand manipulate images without the need for parameterized visual components. Our\nmethod facilitates simple image classification, generation, and in-context\nlearning using only LLM capabilities. We demonstrate the promise of our\napproach across discriminative and generative tasks, highlighting its (i)\nrobustness against distribution shift, (ii) substantial improvements achieved\nby tapping into the in-context learning abilities of LLMs, and (iii) image\nunderstanding and generation capabilities with human guidance. Our code, data,\nand models can be found here https://github.com/mu-cai/svg-llm.",
        "translated": "近年来，大型语言模型(LLM)在自然语言理解和生成方面取得了显著的进展。然而，它们在计算机视觉方面的潜力在很大程度上仍未得到开发。在这篇文章中，我们介绍了一种新的探索性的方法，使 LLM 能够使用可缩放向量图形(SVG)格式来处理图像。通过利用基于 XML 的 SVG 表示的文本描述而不是栅格图像，我们的目标是弥合视觉和文本模式之间的差距，允许 LLM 直接理解和操作图像，而不需要参数化的视觉组件。我们的方法有助于简单的图像分类，生成，并在上下文学习使用 LLM 的能力。我们展示了我们在歧视性和生成性任务中的方法的前景，强调了其(i)对分布转移的稳健性，(ii)通过利用 LLM 的上下文学习能力实现的实质性改进，以及(iii)图像理解和生成能力与人类指导。我们的代码、数据和模型可以在这里找到 https://github.com/mu-cai/svg-llm。"
    },
    {
        "title": "HyP-NeRF: Learning Improved NeRF Priors using a HyperNetwork",
        "url": "http://arxiv.org/abs/2306.06093v1",
        "pub_date": "2023-06-09",
        "summary": "Neural Radiance Fields (NeRF) have become an increasingly popular\nrepresentation to capture high-quality appearance and shape of scenes and\nobjects. However, learning generalizable NeRF priors over categories of scenes\nor objects has been challenging due to the high dimensionality of network\nweight space. To address the limitations of existing work on generalization,\nmulti-view consistency and to improve quality, we propose HyP-NeRF, a latent\nconditioning method for learning generalizable category-level NeRF priors using\nhypernetworks. Rather than using hypernetworks to estimate only the weights of\na NeRF, we estimate both the weights and the multi-resolution hash encodings\nresulting in significant quality gains. To improve quality even further, we\nincorporate a denoise and finetune strategy that denoises images rendered from\nNeRFs estimated by the hypernetwork and finetunes it while retaining multiview\nconsistency. These improvements enable us to use HyP-NeRF as a generalizable\nprior for multiple downstream tasks including NeRF reconstruction from\nsingle-view or cluttered scenes and text-to-NeRF. We provide qualitative\ncomparisons and evaluate HyP-NeRF on three tasks: generalization, compression,\nand retrieval, demonstrating our state-of-the-art results.",
        "translated": "神经辐射场(NeRF)已成为一种越来越流行的表示，以捕捉高品质的外观和形状的场景和对象。然而，由于网络权重空间的高维性，在场景或对象类别上学习可推广的 NRF 先验一直是一个挑战。为了解决现有工作在泛化、多视图一致性和提高质量方面的局限性，我们提出了 HyP-NeRF 方法，这是一种利用超网络学习可泛化类别级 NeRF 先验的潜在条件方法。与使用超网络估计 NERF 的权重不同，我们同时估计权重和多分辨率哈希编码，从而获得显著的质量增益。为了进一步提高图像的质量，我们引入了一种去噪和微调策略，该策略对由超网络估计的 NERF 渲染的图像进行去噪，并对其进行微调，同时保持多视图的一致性。这些改进使我们能够使用 HyP-NeRF 作为多个下游任务(包括从单视图或混乱场景的 NERF 重建和文本到 NERF)之前的推广。我们提供定性比较和评估 HyP-NeRF 的三个任务: 概括，压缩和检索，展示了我们的最先进的结果。"
    },
    {
        "title": "Realistic Saliency Guided Image Enhancement",
        "url": "http://arxiv.org/abs/2306.06092v1",
        "pub_date": "2023-06-09",
        "summary": "Common editing operations performed by professional photographers include the\ncleanup operations: de-emphasizing distracting elements and enhancing subjects.\nThese edits are challenging, requiring a delicate balance between manipulating\nthe viewer's attention while maintaining photo realism. While recent approaches\ncan boast successful examples of attention attenuation or amplification, most\nof them also suffer from frequent unrealistic edits. We propose a realism loss\nfor saliency-guided image enhancement to maintain high realism across varying\nimage types, while attenuating distractors and amplifying objects of interest.\nEvaluations with professional photographers confirm that we achieve the dual\nobjective of realism and effectiveness, and outperform the recent approaches on\ntheir own datasets, while requiring a smaller memory footprint and runtime. We\nthus offer a viable solution for automating image enhancement and photo cleanup\noperations.",
        "translated": "专业摄影师通常进行的编辑操作包括清理操作: 去强调分散注意力的元素和强化主题。这些编辑是具有挑战性的，需要在操纵观众注意力和保持照片真实感之间做出人海万花筒(电影)。虽然最近的方法可以吹嘘注意力衰减或放大的成功例子，但它们中的大多数也经常遭受不切实际的编辑。我们提出了一种显著性引导的图像增强的现实主义损失，以保持高真实度跨不同的图像类型，同时衰减干扰物和放大对象的兴趣。与专业摄影师的评估证实，我们实现了现实主义和有效性的双重目标，并优于最近的方法对自己的数据集，同时需要较小的内存占用和运行时间。因此，我们提供了一个可行的解决方案，自动图像增强和照片清理操作。"
    },
    {
        "title": "Computational Flash Photography through Intrinsics",
        "url": "http://arxiv.org/abs/2306.06089v1",
        "pub_date": "2023-06-09",
        "summary": "Flash is an essential tool as it often serves as the sole controllable light\nsource in everyday photography. However, the use of flash is a binary decision\nat the time a photograph is captured with limited control over its\ncharacteristics such as strength or color. In this work, we study the\ncomputational control of the flash light in photographs taken with or without\nflash. We present a physically motivated intrinsic formulation for flash\nphotograph formation and develop flash decomposition and generation methods for\nflash and no-flash photographs, respectively. We demonstrate that our intrinsic\nformulation outperforms alternatives in the literature and allows us to\ncomputationally control flash in in-the-wild images.",
        "translated": "闪光灯是一个必不可少的工具，因为它往往作为唯一的可控光源在日常摄影。然而，使用闪光灯是一个二元决定时，一张照片是捕捉有限的控制其特点，如强度或颜色。在这项工作中，我们研究计算控制闪光灯在照片中拍摄与否闪光灯。我们提出了一个物理动机的闪光照片形成的内在公式和发展闪光分解和闪光照片的生成方法和无闪光照片分别。我们证明，我们的内在公式优于文献中的替代品，并允许我们计算控制在野外的图像闪光灯。"
    },
    {
        "title": "SENS: Sketch-based Implicit Neural Shape Modeling",
        "url": "http://arxiv.org/abs/2306.06088v1",
        "pub_date": "2023-06-09",
        "summary": "We present SENS, a novel method for generating and editing 3D models from\nhand-drawn sketches, including those of an abstract nature. Our method allows\nusers to quickly and easily sketch a shape, and then maps the sketch into the\nlatent space of a part-aware neural implicit shape architecture. SENS analyzes\nthe sketch and encodes its parts into ViT patch encoding, then feeds them into\na transformer decoder that converts them to shape embeddings, suitable for\nediting 3D neural implicit shapes. SENS not only provides intuitive\nsketch-based generation and editing, but also excels in capturing the intent of\nthe user's sketch to generate a variety of novel and expressive 3D shapes, even\nfrom abstract sketches. We demonstrate the effectiveness of our model compared\nto the state-of-the-art using objective metric evaluation criteria and a\ndecisive user study, both indicating strong performance on sketches with a\nmedium level of abstraction. Furthermore, we showcase its intuitive\nsketch-based shape editing capabilities.",
        "translated": "我们提出了 SENS，一种新的方法生成和编辑三维模型从手绘草图，包括那些抽象的性质。该方法允许用户快速、简单地绘制形状草图，然后将草图映射到部分感知的神经隐式形状体系结构的潜在空间中。SENS 对草图进行分析，并将其部分编码为 ViT 补丁编码，然后将其输入变压器解码器，再将其转换为形状嵌入，适用于编辑三维神经隐式形状。SENS 不仅提供直观的基于草图的生成和编辑，而且擅长捕捉用户草图的意图，以生成各种新颖和富有表现力的3D 形状，甚至从抽象的草图。我们证明了我们的模型的有效性相比，国家的最先进的使用客观的度量评价标准和决定性的用户研究，两者都表明强大的表现与中等抽象水平的草图。此外，我们展示了其直观的基于草图的形状编辑能力。"
    },
    {
        "title": "Exploring the Impact of Image Resolution on Chest X-ray Classification\n  Performance",
        "url": "http://arxiv.org/abs/2306.06051v1",
        "pub_date": "2023-06-09",
        "summary": "Deep learning models for image classification have often used a resolution of\n$224\\times224$ pixels for computational reasons.\n  This study investigates the effect of image resolution on chest X-ray\nclassification performance, using the ChestX-ray14 dataset.\n  The results show that a higher image resolution, specifically\n$1024\\times1024$ pixels, has the best overall classification performance, with\na slight decline in performance between $256\\times256$ to $512\\times512$ pixels\nfor most of the pathological classes.\n  Comparison of saliency map-generated bounding boxes revealed that commonly\nused resolutions are insufficient for finding most pathologies.",
        "translated": "深度学习模型的图像分类往往使用的分辨率为 $224乘以224 $像素的计算原因。本研究使用 ChestX-ray14数据集，探讨图像分辨率对胸部 X 线分类性能的影响。结果显示，一个更高的图像分辨率，特别是 $1024乘以1024 $像素，具有最好的整体分类性能，性能略有下降，在 $256乘以256 $到 $512乘以512 $像素之间的大多数病理类。显著性图生成的边界框的比较显示，常用的分辨率不足以发现大多数病理。"
    },
    {
        "title": "How Does Fine-Tuning Impact Out-of-Distribution Detection for\n  Vision-Language Models?",
        "url": "http://arxiv.org/abs/2306.06048v1",
        "pub_date": "2023-06-09",
        "summary": "Recent large vision-language models such as CLIP have shown remarkable\nout-of-distribution (OOD) detection and generalization performance. However,\ntheir zero-shot in-distribution (ID) accuracy is often limited for downstream\ndatasets. Recent CLIP-based fine-tuning methods such as prompt learning have\ndemonstrated significant improvements in ID classification and OOD\ngeneralization where OOD labels are available. Nonetheless, it remains unclear\nwhether the model is reliable to semantic shifts without OOD labels. In this\npaper, we aim to bridge the gap and present a comprehensive study to understand\nhow fine-tuning impact OOD detection for few-shot downstream tasks. By framing\nOOD detection as multi-modal concept matching, we establish a connection\nbetween fine-tuning methods and various OOD scores. Our results suggest that a\nproper choice of OOD scores is essential for CLIP-based fine-tuning. In\nparticular, the maximum concept matching (MCM) score provides a promising\nsolution consistently. We also show that prompt learning demonstrates the\nstate-of-the-art OOD detection performance over the zero-shot counterpart.",
        "translated": "最近的大型视觉语言模型，例如 CLIP，已经显示出显著的分布外(OOD)检测和泛化性能。然而，对于下游数据集，它们的零点分布(ID)精度往往受到限制。最近基于 CLIP 的微调方法，例如快速学习，已经显示出在提供 OOD 标签的 ID 分类和 OOD 泛化方面的重大改进。尽管如此，在没有 OOD 标签的情况下，该模型对于语义转换是否可靠尚不清楚。在本文中，我们的目标是弥合差距，并提出了一个全面的研究，以了解如何微调影响面向对象的检测为少拍下游任务。通过将面向对象的检测定义为多模态概念匹配，建立了微调方法与不同面向对象得分之间的关系。我们的研究结果表明，正确选择面向对象分数对于基于 CLIP 的微调至关重要。特别是，最大概念匹配(MCM)得分一致地提供了一个有希望的解决方案。我们还表明，快速学习证明了最先进的面向对象的检测性能超过零拍对应。"
    },
    {
        "title": "GANeRF: Leveraging Discriminators to Optimize Neural Radiance Fields",
        "url": "http://arxiv.org/abs/2306.06044v1",
        "pub_date": "2023-06-09",
        "summary": "Neural Radiance Fields (NeRF) have shown impressive novel view synthesis\nresults; nonetheless, even thorough recordings yield imperfections in\nreconstructions, for instance due to poorly observed areas or minor lighting\nchanges. Our goal is to mitigate these imperfections from various sources with\na joint solution: we take advantage of the ability of generative adversarial\nnetworks (GANs) to produce realistic images and use them to enhance realism in\n3D scene reconstruction with NeRFs. To this end, we learn the patch\ndistribution of a scene using an adversarial discriminator, which provides\nfeedback to the radiance field reconstruction, thus improving realism in a\n3D-consistent fashion. Thereby, rendering artifacts are repaired directly in\nthe underlying 3D representation by imposing multi-view path rendering\nconstraints. In addition, we condition a generator with multi-resolution NeRF\nrenderings which is adversarially trained to further improve rendering quality.\nWe demonstrate that our approach significantly improves rendering quality,\ne.g., nearly halving LPIPS scores compared to Nerfacto while at the same time\nimproving PSNR by 1.4dB on the advanced indoor scenes of Tanks and Temples.",
        "translated": "神经辐射场(NeRF)已经显示出令人印象深刻的新颖视图合成结果; 然而，即使是全面的记录也会在重建中产生缺陷，例如由于观察不到的区域或微小的光线变化。我们的目标是通过一个联合的解决方案来减轻各种来源的这些缺陷: 我们利用生成对抗网络(GAN)的能力来生成逼真的图像，并使用它们来增强使用 NERF 进行3D 场景重建的逼真度。为此，我们使用一个对抗性鉴别器来学习场景的补丁分布，它为辐射场重建提供反馈，从而以一种三维一致的方式提高真实感。因此，通过施加多视图路径绘制约束，可以直接在底层3D 表示中修复绘制伪影。此外，我们使用多分辨率的 NERF 渲染来调节一个生成器，该生成器经过反向训练以进一步提高渲染质量。我们证明了我们的方法显著提高了渲染质量，例如，与 Nerfacto 相比，LPIPS 得分几乎减半，同时在坦克和神庙的高级室内场景上提高了1.4分贝的峰值信噪比。"
    },
    {
        "title": "WindowNet: Learnable Windows for Chest X-ray Classification",
        "url": "http://arxiv.org/abs/2306.06038v1",
        "pub_date": "2023-06-09",
        "summary": "Chest X-ray (CXR) images are commonly compressed to a lower resolution and\nbit depth to reduce their size, potentially altering subtle diagnostic\nfeatures.\n  Radiologists use windowing operations to enhance image contrast, but the\nimpact of such operations on CXR classification performance is unclear.\n  In this study, we show that windowing can improve CXR classification\nperformance, and propose WindowNet, a model that learns optimal window\nsettings.\n  We first investigate the impact of bit-depth on classification performance\nand find that a higher bit-depth (12-bit) leads to improved performance.\n  We then evaluate different windowing settings and show that training with a\ndistinct window generally improves pathology-wise classification performance.\n  Finally, we propose and evaluate WindowNet, a model that learns optimal\nwindow settings, and show that it significantly improves performance compared\nto the baseline model without windowing.",
        "translated": "胸部 X 线(CXR)图像通常压缩到较低的分辨率和位深度，以减少其大小，潜在地改变微妙的诊断功能。放射科医生使用窗口操作来增强图像对比度，但这种操作对 CXR 分类性能的影响尚不清楚。在本研究中，我们发现视窗可以改善 CXR 分类器的分类性能，并提出视窗网路模型，学习最佳视窗设定。我们首先研究了位深度对分类性能的影响，发现更高的位深度(12位)可以提高性能。然后，我们评估不同的窗口设置，并表明训练与不同的窗口一般提高病理分类性能。最后，我们提出并评估了 WindowNet，一个学习最佳窗口设置的模型，并且表明与没有窗口的基线模型相比，它显著地提高了性能。"
    },
    {
        "title": "DetZero: Rethinking Offboard 3D Object Detection with Long-term\n  Sequential Point Clouds",
        "url": "http://arxiv.org/abs/2306.06023v1",
        "pub_date": "2023-06-09",
        "summary": "Existing offboard 3D detectors always follow a modular pipeline design to\ntake advantage of unlimited sequential point clouds. We have found that the\nfull potential of offboard 3D detectors is not explored mainly due to two\nreasons: (1) the onboard multi-object tracker cannot generate sufficient\ncomplete object trajectories, and (2) the motion state of objects poses an\ninevitable challenge for the object-centric refining stage in leveraging the\nlong-term temporal context representation. To tackle these problems, we propose\na novel paradigm of offboard 3D object detection, named DetZero. Concretely, an\noffline tracker coupled with a multi-frame detector is proposed to focus on the\ncompleteness of generated object tracks. An attention-mechanism refining module\nis proposed to strengthen contextual information interaction across long-term\nsequential point clouds for object refining with decomposed regression methods.\nExtensive experiments on Waymo Open Dataset show our DetZero outperforms all\nstate-of-the-art onboard and offboard 3D detection methods. Notably, DetZero\nranks 1st place on Waymo 3D object detection leaderboard with 85.15 mAPH (L2)\ndetection performance. Further experiments validate the application of taking\nthe place of human labels with such high-quality results. Our empirical study\nleads to rethinking conventions and interesting findings that can guide future\nresearch on offboard 3D object detection.",
        "translated": "现有的船外3D 探测器总是遵循模块化的流水线设计，以利用无限的顺序点云。我们发现机载三维探测器的潜力没有得到充分发挥，主要有两个原因: (1)机载多目标跟踪器不能产生足够完整的目标轨迹; (2)目标的运动状态对以目标为中心的精化阶段利用长期的时间上下文表示提出了不可避免的挑战。为了解决这些问题，我们提出了一个新颖的船外3D 目标检测的范例，名为 DetZero。具体地，提出了一种耦合多帧检测器的离线跟踪器，以保证生成的目标轨迹的完整性。提出了一种基于分解回归方法的注意机制细化模块，用于加强长期序列点云间的上下文信息交互。在 Waymo 开放数据集上的大量实验表明，我们的 DetZero 优于所有最先进的船上和船外3D 检测方法。值得注意的是，DetZero 以85.15 mAPH (L2)的检测性能在 Waymo 3 d 目标检测排行榜上名列第一。进一步的实验验证了用这种高质量的结果代替人类标签的应用。我们的实证研究引导我们重新思考惯例和有趣的发现，这些发现可以指导未来关于离岸三维目标检测的研究。"
    },
    {
        "title": "No Free Lunch: The Hazards of Over-Expressive Representations in Anomaly\n  Detection",
        "url": "http://arxiv.org/abs/2306.07284v1",
        "pub_date": "2023-06-12",
        "summary": "Anomaly detection methods, powered by deep learning, have recently been\nmaking significant progress, mostly due to improved representations. It is\ntempting to hypothesize that anomaly detection can improve indefinitely by\nincreasing the scale of our networks, making their representations more\nexpressive. In this paper, we provide theoretical and empirical evidence to the\ncontrary. In fact, we empirically show cases where very expressive\nrepresentations fail to detect even simple anomalies when evaluated beyond the\nwell-studied object-centric datasets. To investigate this phenomenon, we begin\nby introducing a novel theoretical toy model for anomaly detection performance.\nThe model uncovers a fundamental trade-off between representation sufficiency\nand over-expressivity. It provides evidence for a no-free-lunch theorem in\nanomaly detection stating that increasing representation expressivity will\neventually result in performance degradation. Instead, guidance must be\nprovided to focus the representation on the attributes relevant to the\nanomalies of interest. We conduct an extensive empirical investigation\ndemonstrating that state-of-the-art representations often suffer from\nover-expressivity, failing to detect many types of anomalies. Our investigation\ndemonstrates how this over-expressivity impairs image anomaly detection in\npractical settings. We conclude with future directions for mitigating this\nissue.",
        "translated": "在深度学习的推动下，异常检测方法最近取得了重大进展，这主要归功于改进的表征。人们很容易假设，异常检测可以通过增加我们网络的规模，让它们的表现更具表现力，从而无限期地改善。在本文中，我们提供了相反的理论和经验证明。事实上，我们经验性地展示了这样的情况: 当评估超出已经研究过的以对象为中心的数据集时，非常具有表现力的表示甚至连简单的异常都检测不到。为了研究这种现象，我们首先引入了一个新的理论玩具模型，用于异常检测表演。该模型揭示了表示充分性和过度表达性之间的基本权衡。它为异常检测中的不免费午餐定理提供了证据，该定理指出，表现表达能力的提高最终将导致性能下降。相反，必须提供指导，将表示集中在与感兴趣的异常相关的属性上。我们进行了广泛的实证调查，表明最先进的表现往往受到表达过度，未能发现许多类型的异常。我们的研究证明了这种过度表现是如何在实际环境中损害图像异常检测的。最后，我们提出了缓解这一问题的未来方向。"
    },
    {
        "title": "Waffling around for Performance: Visual Classification with Random Words\n  and Broad Concepts",
        "url": "http://arxiv.org/abs/2306.07282v1",
        "pub_date": "2023-06-12",
        "summary": "The visual classification performance of vision-language models such as CLIP\ncan benefit from additional semantic knowledge, e.g. via large language models\n(LLMs) such as GPT-3. Further extending classnames with LLM-generated class\ndescriptors, e.g. ``waffle, \\textit{which has a round shape}'', or averaging\nretrieval scores over multiple such descriptors, has been shown to improve\ngeneralization performance. In this work, we study this behavior in detail and\npropose \\texttt{Waffle}CLIP, a framework for zero-shot visual classification\nwhich achieves similar performance gains on a large number of visual\nclassification tasks by simply replacing LLM-generated descriptors with random\ncharacter and word descriptors \\textbf{without} querying external models. We\nextend these results with an extensive experimental study on the impact and\nshortcomings of additional semantics introduced via LLM-generated descriptors,\nand showcase how semantic context is better leveraged by automatically querying\nLLMs for high-level concepts, while jointly resolving potential class name\nambiguities. Link to the codebase: https://github.com/ExplainableML/WaffleCLIP.",
        "translated": "可视化语言模型(如 CLIP)的可视化分类性能可以受益于额外的语义知识，例如通过大型语言模型(LLM)(如 GPT-3)。使用 LLM 生成的类描述符进一步扩展类名，例如“ waffle，texttit { which has a round form }”，或者在多个这样的描述符上平均检索得分，已经被证明可以提高泛化性能。在这项工作中，我们详细研究了这种行为，并提出了 texttt { Waffle } CLIP，一个零拍视觉分类的框架，通过简单地用随机字符和词描述符替换 LLM 生成的描述符，在大量的视觉分类任务中获得相似的性能提高。我们通过对通过 LLM 生成的描述符引入的附加语义的影响和缺点进行广泛的实验研究来扩展这些结果，并展示如何通过自动查询 LLM 来更好地利用语义上下文的高级概念，同时共同解决潜在的类名歧义。链接到代码库:  https://github.com/explainableml/waffleclip。"
    },
    {
        "title": "Controlling Text-to-Image Diffusion by Orthogonal Finetuning",
        "url": "http://arxiv.org/abs/2306.07280v1",
        "pub_date": "2023-06-12",
        "summary": "Large text-to-image diffusion models have impressive capabilities in\ngenerating photorealistic images from text prompts. How to effectively guide or\ncontrol these powerful models to perform different downstream tasks becomes an\nimportant open problem. To tackle this challenge, we introduce a principled\nfinetuning method -- Orthogonal Finetuning (OFT), for adapting text-to-image\ndiffusion models to downstream tasks. Unlike existing methods, OFT can provably\npreserve hyperspherical energy which characterizes the pairwise neuron\nrelationship on the unit hypersphere. We find that this property is crucial for\npreserving the semantic generation ability of text-to-image diffusion models.\nTo improve finetuning stability, we further propose Constrained Orthogonal\nFinetuning (COFT) which imposes an additional radius constraint to the\nhypersphere. Specifically, we consider two important finetuning text-to-image\ntasks: subject-driven generation where the goal is to generate subject-specific\nimages given a few images of a subject and a text prompt, and controllable\ngeneration where the goal is to enable the model to take in additional control\nsignals. We empirically show that our OFT framework outperforms existing\nmethods in generation quality and convergence speed.",
        "translated": "大型文本到图像的扩散模型在从文本提示生成逼真的图像方面具有令人印象深刻的能力。如何有效地引导或控制这些强大的模型来执行不同的下游任务成为一个重要的公开问题。为了解决这一问题，我们引入了一种原理性的微调方法——正交微调(OFT) ，用于调整文本到图像的扩散模型以适应下游任务。与现有的方法不同，OFT 可以证明保持超球面能量的特点是单位超球面上的成对神经元关系。我们发现这一特性对于保持文本-图像扩散模型的语义生成能力至关重要。为了提高微调的稳定性，我们进一步提出了约束正交微调(COFT) ，它对超球面增加了一个额外的半径约束。具体来说，我们考虑两个重要的文本到图像的微调任务: 主题驱动的生成，其目标是生成给定主题和文本提示的一些图像的主题特定的图像，以及可控的生成，其目标是使模型能够采取额外的控制信号。我们的实验表明，我们的 OFT 框架在生成质量和收敛速度方面优于现有的方法。"
    },
    {
        "title": "Scalable 3D Captioning with Pretrained Models",
        "url": "http://arxiv.org/abs/2306.07279v1",
        "pub_date": "2023-06-12",
        "summary": "We introduce Cap3D, an automatic approach for generating descriptive text for\n3D objects. This approach utilizes pretrained models from image captioning,\nimage-text alignment, and LLM to consolidate captions from multiple views of a\n3D asset, completely side-stepping the time-consuming and costly process of\nmanual annotation. We apply Cap3D to the recently introduced large-scale 3D\ndataset, Objaverse, resulting in 660k 3D-text pairs. Our evaluation, conducted\nusing 41k human annotations from the same dataset, demonstrates that Cap3D\nsurpasses human-authored descriptions in terms of quality, cost, and speed.\nThrough effective prompt engineering, Cap3D rivals human performance in\ngenerating geometric descriptions on 17k collected annotations from the ABO\ndataset. Finally, we finetune Text-to-3D models on Cap3D and human captions,\nand show Cap3D outperforms; and benchmark the SOTA including Point-E, Shape-E,\nand DreamFusion.",
        "translated": "我们介绍 Cap3D，一种为3D 对象生成描述性文本的自动方法。这种方法利用来自图像字幕、图像-文本对齐和 LLM 的预先训练的模型来合并来自3D 资产的多个视图的字幕，完全避开了手动注释的耗时和昂贵的过程。我们将 Cap3D 应用到最近引入的大规模3D 数据集 Objaverse 中，得到了660k 的3D 文本对。我们使用来自同一数据集的41k 人工注释进行的评估表明，Cap3D 在质量、成本和速度方面都超过了人工描述。通过有效的快速工程，Cap3D 在从 ABO 数据集中收集的17k 注释生成几何描述方面与人类的性能相当。最后，我们微调 Cap3D 和人工字幕上的文本到3D 模型，并显示 Cap3D 的优异表现; 并基准 SOTA 包括 Point-E，Shape-E 和 DreamFusion。"
    },
    {
        "title": "Transcendental Idealism of Planner: Evaluating Perception from Planning\n  Perspective for Autonomous Driving",
        "url": "http://arxiv.org/abs/2306.07276v1",
        "pub_date": "2023-06-12",
        "summary": "Evaluating the performance of perception modules in autonomous driving is one\nof the most critical tasks in developing the complex intelligent system. While\nmodule-level unit test metrics adopted from traditional computer vision tasks\nare feasible to some extent, it remains far less explored to measure the impact\nof perceptual noise on the driving quality of autonomous vehicles in a\nconsistent and holistic manner. In this work, we propose a principled framework\nthat provides a coherent and systematic understanding of the impact an error in\nthe perception module imposes on an autonomous agent's planning that actually\ncontrols the vehicle. Specifically, the planning process is formulated as\nexpected utility maximisation, where all input signals from upstream modules\njointly provide a world state description, and the planner strives for the\noptimal action by maximising the expected utility determined by both world\nstates and actions. We show that, under practical conditions, the objective\nfunction can be represented as an inner product between the world state\ndescription and the utility function in a Hilbert space. This geometric\ninterpretation enables a novel way to analyse the impact of noise in world\nstate estimation on planning and leads to a universal metric for evaluating\nperception. The whole framework resembles the idea of transcendental idealism\nin the classical philosophical literature, which gives the name to our\napproach.",
        "translated": "自主驾驶感知模块的性能评价是复杂智能系统开发的关键问题之一。传统的计算机视觉任务所采用的模块级单元测试指标在一定程度上是可行的，但是如何以一致性和整体性的方式来衡量感知噪声对自主驾驶车辆行驶质量的影响，目前尚缺乏深入的研究。在这项工作中，我们提出了一个原则性的框架，提供了一个连贯和系统的理解的影响，在感知模块的错误所施加的自主主体的规划，实际控制车辆。具体地说，规划过程被表述为期望效用最大化，其中来自上游模块的所有输入信号共同提供一个世界状态描述，规划者通过最大化由世界状态和行动决定的期望效用来争取最优行动。证明了在实际条件下，目标函数可以表示为希尔伯特空间中世界状态描述与效用函数之间的内积。这种几何解释为分析世界状态估计中噪声对规划的影响提供了一种新的方法，并导致了一种评估感知的通用度量。整个框架类似于古典哲学文献中的先验唯心主义的思想，这给我们的方法起了名字。"
    },
    {
        "title": "Reconstructing Heterogeneous Cryo-EM Molecular Structures by Decomposing\n  Them into Polymer Chains",
        "url": "http://arxiv.org/abs/2306.07274v1",
        "pub_date": "2023-06-12",
        "summary": "Cryogenic electron microscopy (cryo-EM) has transformed structural biology by\nallowing to reconstruct 3D biomolecular structures up to near-atomic\nresolution. However, the 3D reconstruction process remains challenging, as the\n3D structures may exhibit substantial shape variations, while the 2D image\nacquisition suffers from a low signal-to-noise ratio, requiring to acquire very\nlarge datasets that are time-consuming to process. Current reconstruction\nmethods are precise but computationally expensive, or faster but lack a\nphysically-plausible model of large molecular shape variations. To fill this\ngap, we propose CryoChains that encodes large deformations of biomolecules via\nrigid body transformation of their polymer instances (chains), while\nrepresenting their finer shape variations with the normal mode analysis\nframework of biophysics. Our synthetic data experiments on the human\n$\\text{GABA}_{\\text{B}}$ and heat shock protein show that CryoChains gives a\nbiophysically-grounded quantification of the heterogeneous conformations of\nbiomolecules, while reconstructing their 3D molecular structures at an improved\nresolution compared to the current fastest, interpretable deep learning method.",
        "translated": "低温电子显微镜(cryo-EM)已经改变了结构生物学，通过允许重建三维生物分子结构达到近原子分辨率。然而，三维重建过程仍然具有挑战性，因为三维结构可能会表现出大量的形状变化，而二维图像采集的信噪比较低，需要获取非常大的数据集，处理起来非常耗时。目前的重建方法是精确的，但计算昂贵，或更快，但缺乏一个物理合理的模型，大分子形状的变化。为了填补这个空白，我们提出冷冻链，通过其聚合物实例(链)的刚体转换来编码生物分子的大变形，同时用生物物理学的正常模式分析框架表示它们的更细微的形状变化。我们在人类文本{ GABA } _ { text { B }} $和热休克蛋白上的合成数据实验表明，冷冻链给出了生物分子的异质构象的生物物理基础定量，同时与目前最快，可解释的深度学习方法相比，以改进的分辨率重建它们的三维分子结构。"
    },
    {
        "title": "Zero-shot Composed Text-Image Retrieval",
        "url": "http://arxiv.org/abs/2306.07272v1",
        "pub_date": "2023-06-12",
        "summary": "In this paper, we consider the problem of composed image retrieval (CIR), it\naims to train a model that can fuse multi-modal information, e.g., text and\nimages, to accurately retrieve images that match the query, extending the\nuser's expression ability. We make the following contributions: (i) we initiate\na scalable pipeline to automatically construct datasets for training CIR model,\nby simply exploiting a large-scale dataset of image-text pairs, e.g., a subset\nof LAION-5B; (ii) we introduce a transformer-based adaptive aggregation model,\nTransAgg, which employs a simple yet efficient fusion mechanism, to adaptively\ncombine information from diverse modalities; (iii) we conduct extensive\nablation studies to investigate the usefulness of our proposed data\nconstruction procedure, and the effectiveness of core components in TransAgg;\n(iv) when evaluating on the publicly available benckmarks under the zero-shot\nscenario, i.e., training on the automatically constructed datasets, then\ndirectly conduct inference on target downstream datasets, e.g., CIRR and\nFashionIQ, our proposed approach either performs on par with or significantly\noutperforms the existing state-of-the-art (SOTA) models. Project page:\nhttps://code-kunkun.github.io/ZS-CIR/",
        "translated": "本文研究了组合图像检索(CIR)问题，旨在训练一种能够融合文本和图像等多模态信息的模型，以准确地检索与查询相匹配的图像，扩展用户的表达能力。我们做出了以下贡献: (i)我们启动了一个可伸缩的流水线，通过简单地利用图像-文本对的大规模数据集，自动构建用于 CIR 模型训练的数据集，例如。，LAION-5B 的一个子集; (ii)我们引入了一个基于变压器的自适应聚合模型 TransAgg，其采用了一个简单而有效的融合机制，以自适应地组合来自不同模式的信息; (iii)我们进行了广泛的消融研究，以调查我们提出的数据构建程序的有用性，以及 TransAgg 中核心组件的有效性; (iv)当评估在零射击情景下公开可用的基准，即。对自动构建的数据集进行训练，然后直接对目标下游数据集进行推理，例如。CIRR 和 FashionIQ，我们提出的方法要么表现与现有的最先进的(SOTA)模型相当，要么明显优于它们。项目主页:  https://code-kunkun.github.io/zs-cir/"
    },
    {
        "title": "detrex: Benchmarking Detection Transformers",
        "url": "http://arxiv.org/abs/2306.07265v2",
        "pub_date": "2023-06-12",
        "summary": "The DEtection TRansformer (DETR) algorithm has received considerable\nattention in the research community and is gradually emerging as a mainstream\napproach for object detection and other perception tasks. However, the current\nfield lacks a unified and comprehensive benchmark specifically tailored for\nDETR-based models. To address this issue, we develop a unified, highly modular,\nand lightweight codebase called detrex, which supports a majority of the\nmainstream DETR-based instance recognition algorithms, covering various\nfundamental tasks, including object detection, segmentation, and pose\nestimation. We conduct extensive experiments under detrex and perform a\ncomprehensive benchmark for DETR-based models. Moreover, we enhance the\nperformance of detection transformers through the refinement of training\nhyper-parameters, providing strong baselines for supported algorithms.We hope\nthat detrex could offer research communities a standardized and unified\nplatform to evaluate and compare different DETR-based models while fostering a\ndeeper understanding and driving advancements in DETR-based instance\nrecognition. Our code is available at https://github.com/IDEA-Research/detrex.\nThe project is currently being actively developed. We encourage the community\nto use detrex codebase for further development and contributions.",
        "translated": "检测变压器(DETR)算法已经在研究界引起了相当大的关注，并逐渐成为目标检测和其他感知任务的主流方法。然而，目前的领域缺乏一个统一和全面的基准，专门为基于 DETR 的模型。为了解决这个问题，我们开发了一个统一的、高度模块化的、轻量级的代码库 detrex，它支持大多数基于 DETR 的主流实例识别算法，涵盖了各种基本任务，包括目标检测、分割和姿态估计。我们在 detrex 下进行了广泛的实验，并对基于 DETR 的模型进行了全面的基准测试。此外，我们通过改进训练超参数来提高检测变压器的性能，为支持的算法提供强大的基线。我们希望 detrex 可以为研究团体提供一个标准化和统一的平台来评估和比较不同的基于 DETR 的模型，同时促进对基于 DETR 的实例识别的更深入的理解和推动进步。我们的代码可以在 https://github.com/idea-research/detrex 找到。该项目目前正在积极开发中。我们鼓励社区使用 detrex 代码库进行进一步的开发和贡献。"
    },
    {
        "title": "MovieFactory: Automatic Movie Creation from Text using Large Generative\n  Models for Language and Images",
        "url": "http://arxiv.org/abs/2306.07257v1",
        "pub_date": "2023-06-12",
        "summary": "In this paper, we present MovieFactory, a powerful framework to generate\ncinematic-picture (3072$\\times$1280), film-style (multi-scene), and\nmulti-modality (sounding) movies on the demand of natural languages. As the\nfirst fully automated movie generation model to the best of our knowledge, our\napproach empowers users to create captivating movies with smooth transitions\nusing simple text inputs, surpassing existing methods that produce soundless\nvideos limited to a single scene of modest quality. To facilitate this\ndistinctive functionality, we leverage ChatGPT to expand user-provided text\ninto detailed sequential scripts for movie generation. Then we bring scripts to\nlife visually and acoustically through vision generation and audio retrieval.\nTo generate videos, we extend the capabilities of a pretrained text-to-image\ndiffusion model through a two-stage process. Firstly, we employ spatial\nfinetuning to bridge the gap between the pretrained image model and the new\nvideo dataset. Subsequently, we introduce temporal learning to capture object\nmotion. In terms of audio, we leverage sophisticated retrieval models to select\nand align audio elements that correspond to the plot and visual content of the\nmovie. Extensive experiments demonstrate that our MovieFactory produces movies\nwith realistic visuals, diverse scenes, and seamlessly fitting audio, offering\nusers a novel and immersive experience. Generated samples can be found in\nYouTube or Bilibili (1080P).",
        "translated": "在本文中，我们介绍了 MovieFactory，一个强大的框架，生成电影图片(3072 $乘以1280 $) ，电影风格(多场景) ，和多模态(发声)电影的自然语言的需求。作为据我们所知的第一个完全自动化的电影生成模型，我们的方法使用户能够使用简单的文本输入创建具有平滑过渡的迷人电影，超过现有的方法，生产无声视频限制在一个质量不高的单一场景。为了促进这种独特的功能，我们利用 ChatGPT 将用户提供的文本扩展为用于电影生成的详细的顺序脚本。然后通过视觉生成和音频检索，使脚本在视觉和听觉上生动起来。为了生成视频，我们通过一个两阶段的过程扩展了预先训练的文本到图像扩散模型的能力。首先，我们采用空间微调来弥补预先训练的图像模型和新的视频数据集之间的差距。随后，我们引入时间学习来捕捉目标的运动。在音频方面，我们利用复杂的检索模型来选择和对齐与电影情节和视觉内容相对应的音频元素。大量的实验表明，我们的电影工厂生产的电影与现实的视觉效果，多样化的场景，无缝适合的音频，为用户提供新颖和身临其境的体验。生成的样本可以在 YouTube 或 Bilibili (1080P)中找到。"
    },
    {
        "title": "RB-Dust -- A Reference-based Dataset for Vision-based Dust Removal",
        "url": "http://arxiv.org/abs/2306.07244v1",
        "pub_date": "2023-06-12",
        "summary": "Dust in the agricultural landscape is a significant challenge and influences,\nfor example, the environmental perception of autonomous agricultural machines.\nImage enhancement algorithms can be used to reduce dust. However, these require\ndusty and dust-free images of the same environment for validation. In fact, to\ndate, there is no dataset that we are aware of that addresses this issue.\nTherefore, we present the agriscapes RB-Dust dataset, which is named after its\npurpose of reference-based dust removal. It is not possible to take pictures\nfrom the cabin during tillage, as this would cause shifts in the images.\nBecause of this, we built a setup from which it is possible to take images from\na stationary position close to the passing tractor. The test setup was based on\na half-sided gate through which the tractor could drive. The field tests were\ncarried out on a farm in Bavaria, Germany, during tillage. During the field\ntests, other parameters such as soil moisture and wind speed were controlled,\nas these significantly affect dust development. We validated our dataset with\ncontrast enhancement and image dehazing algorithms and analyzed the\ngeneralizability from recordings from the moving tractor. Finally, we\ndemonstrate the application of dust removal based on a high-level vision task,\nsuch as person classification. Our empirical study confirms the validity of\nRB-Dust for vision-based dust removal in agriculture.",
        "translated": "农业景观中的灰尘是一个重大的挑战和影响，例如，自主农业机械的环境感知。图像增强算法可以用来减少灰尘。然而，这些需要尘土飞扬和无尘的图像相同的环境进行验证。事实上，到目前为止，我们所知道的没有任何数据集可以解决这个问题。因此，我们提出了农业 RB 尘埃数据集，这是命名的目的是基于参考的除尘。在耕作期间不可能从机舱中拍摄照片，因为这会导致图像的变化。正因为如此，我们建立了一个设置，从中可以从一个静止的位置接近经过的拖拉机拍摄图像。测试装置是基于一个半边门，拖拉机可以通过它来驾驶。田间试验是在德国巴伐利亚州的一个农场进行的，当时正在耕作。在现场试验中，土壤水分和风速等其他参数也得到了控制，因为这些参数对粉尘的发展有重要影响。我们用对比度增强和图像去雾算法验证了我们的数据集，并分析了来自移动拖拉机的记录的普遍性。最后，我们展示了基于高层次视觉任务的除尘技术的应用，如人的分类。我们的实证研究证实了 RB 粉尘在农业视觉除尘中的有效性。"
    },
    {
        "title": "XrayGPT: Chest Radiographs Summarization using Medical Vision-Language\n  Models",
        "url": "http://arxiv.org/abs/2306.07971v1",
        "pub_date": "2023-06-13",
        "summary": "The latest breakthroughs in large vision-language models, such as Bard and\nGPT-4, have showcased extraordinary abilities in performing a wide range of\ntasks. Such models are trained on massive datasets comprising billions of\npublic image-text pairs with diverse tasks. However, their performance on\ntask-specific domains, such as radiology, is still under-investigated and\npotentially limited due to a lack of sophistication in understanding biomedical\nimages. On the other hand, conversational medical models have exhibited\nremarkable success but have mainly focused on text-based analysis. In this\npaper, we introduce XrayGPT, a novel conversational medical vision-language\nmodel that can analyze and answer open-ended questions about chest radiographs.\nSpecifically, we align both medical visual encoder (MedClip) with a fine-tuned\nlarge language model (Vicuna), using a simple linear transformation. This\nalignment enables our model to possess exceptional visual conversation\nabilities, grounded in a deep understanding of radiographs and medical domain\nknowledge. To enhance the performance of LLMs in the medical context, we\ngenerate ~217k interactive and high-quality summaries from free-text radiology\nreports. These summaries serve to enhance the performance of LLMs through the\nfine-tuning process. Our approach opens up new avenues the research for\nadvancing the automated analysis of chest radiographs. Our open-source demos,\nmodels, and instruction sets are available at:\nhttps://github.com/mbzuai-oryx/XrayGPT.",
        "translated": "在大型视觉语言模型方面的最新突破，如巴德和 GPT-4，展示了在执行广泛任务方面的非凡能力。这样的模型是在包含数十亿公共图像-文本对和不同任务的大量数据集上进行训练的。然而，由于缺乏对生物医学图像的深入理解，它们在特定任务领域(如放射学)的表现仍然没有得到充分的研究，并可能受到限制。另一方面，会话医学模式取得了显著的成功，但主要集中在基于文本的分析。本文介绍了一种新型的医学会话视觉语言模型 XrayGPT，它可以分析和回答有关胸片的开放性问题。具体来说，我们使用一个简单的线性映射，将两个医学视觉编码器(MedClip)与一个经过微调的大型语言模型(Vicuna)对齐。这种排列使我们的模型具有卓越的视觉对话能力，根据深刻的理解射线照相和医学领域的知识。为了提高 LLM 在医学环境中的性能，我们从自由文本放射学报告中生成约217k 交互式和高质量的摘要。这些总结有助于通过微调过程提高 LLM 的性能。我们的方法为推进胸片自动化分析的研究开辟了新的途径。我们的开源演示、模型和指令集可在以下 https://github.com/mbzuai-oryx/xraygpt 获得:。"
    },
    {
        "title": "GeneCIS: A Benchmark for General Conditional Image Similarity",
        "url": "http://arxiv.org/abs/2306.07969v1",
        "pub_date": "2023-06-13",
        "summary": "We argue that there are many notions of 'similarity' and that models, like\nhumans, should be able to adapt to these dynamically. This contrasts with most\nrepresentation learning methods, supervised or self-supervised, which learn a\nfixed embedding function and hence implicitly assume a single notion of\nsimilarity. For instance, models trained on ImageNet are biased towards object\ncategories, while a user might prefer the model to focus on colors, textures or\nspecific elements in the scene. In this paper, we propose the GeneCIS\n('genesis') benchmark, which measures models' ability to adapt to a range of\nsimilarity conditions. Extending prior work, our benchmark is designed for\nzero-shot evaluation only, and hence considers an open-set of similarity\nconditions. We find that baselines from powerful CLIP models struggle on\nGeneCIS and that performance on the benchmark is only weakly correlated with\nImageNet accuracy, suggesting that simply scaling existing methods is not\nfruitful. We further propose a simple, scalable solution based on automatically\nmining information from existing image-caption datasets. We find our method\noffers a substantial boost over the baselines on GeneCIS, and further improves\nzero-shot performance on related image retrieval benchmarks. In fact, though\nevaluated zero-shot, our model surpasses state-of-the-art supervised models on\nMIT-States. Project page at https://sgvaze.github.io/genecis/.",
        "translated": "我们认为有许多“相似性”的概念，模型，像人类一样，应该能够动态地适应这些概念。这与大多数有监督或自监督的表示学习方法相反，这些方法学习一个固定的嵌入函数，因此隐式地假定一个单一的相似性概念。例如，在 ImageNet 上训练的模型偏向于对象类别，而用户可能更喜欢模型关注场景中的颜色、纹理或特定元素。在这篇文章中，我们提出了 GeneCIS (“创生”)基准，它衡量模型适应一系列相似条件的能力。扩展了先前的工作，我们的基准设计只用于零点评估，因此考虑了一个开放的相似性条件集。我们发现，来自强大的 CLIP 模型的基线在 GeneCIS 上挣扎，基准的性能与 ImageNet 的准确性只有微弱的相关性，这表明简单地扩展现有方法是没有成效的。我们进一步提出了一个简单的，可扩展的解决方案的基础上自动挖掘信息从现有的图像标题数据集。我们发现我们的方法在 GeneCIS 基线上提供了实质性的改进，并且进一步提高了相关图像检索基准的零镜头性能。事实上，虽然评估的零拍摄，我们的模型超过国家的最先进的监督麻省理工学院的模型。Https://sgvaze.github.io/genecis/项目网页。"
    },
    {
        "title": "Neural Scene Chronology",
        "url": "http://arxiv.org/abs/2306.07970v1",
        "pub_date": "2023-06-13",
        "summary": "In this work, we aim to reconstruct a time-varying 3D model, capable of\nrendering photo-realistic renderings with independent control of viewpoint,\nillumination, and time, from Internet photos of large-scale landmarks. The core\nchallenges are twofold. First, different types of temporal changes, such as\nillumination and changes to the underlying scene itself (such as replacing one\ngraffiti artwork with another) are entangled together in the imagery. Second,\nscene-level temporal changes are often discrete and sporadic over time, rather\nthan continuous. To tackle these problems, we propose a new scene\nrepresentation equipped with a novel temporal step function encoding method\nthat can model discrete scene-level content changes as piece-wise constant\nfunctions over time. Specifically, we represent the scene as a space-time\nradiance field with a per-image illumination embedding, where\ntemporally-varying scene changes are encoded using a set of learned step\nfunctions. To facilitate our task of chronology reconstruction from Internet\nimagery, we also collect a new dataset of four scenes that exhibit various\nchanges over time. We demonstrate that our method exhibits state-of-the-art\nview synthesis results on this dataset, while achieving independent control of\nviewpoint, time, and illumination.",
        "translated": "在这项工作中，我们的目标是重建一个时变的三维模型，能够渲染照片真实的渲染与独立控制的观点，照明和时间，从互联网上的大规模地标照片。核心挑战有两方面。首先，不同类型的时间变化，如照明和变化的基础场景本身(如取代一个涂鸦艺术品与另一个)纠缠在一起的意象。其次，场景级别的时间变化往往是离散的，随着时间的推移是零星的，而不是连续的。为了解决这些问题，我们提出了一种新的场景表示方法，该方法配备了一种新的时间步长函数编码方法，可以将离散场景层次的内容随时间的变化建模为分段常数函数。具体地说，我们将场景表示为一个时空辐射场，每个图像都嵌入了照明，其中使用一组学习的步长函数对时变场景变化进行编码。为了方便我们的任务，从互联网图像的年表重建，我们还收集了一个新的数据集的四个场景，显示随着时间的推移各种变化。我们证明了我们的方法展示了最先进的视图合成结果在这个数据集，同时实现了独立的控制视点，时间和照明。"
    },
    {
        "title": "One-for-All: Generalized LoRA for Parameter-Efficient Fine-tuning",
        "url": "http://arxiv.org/abs/2306.07967v1",
        "pub_date": "2023-06-13",
        "summary": "We present Generalized LoRA (GLoRA), an advanced approach for universal\nparameter-efficient fine-tuning tasks. Enhancing Low-Rank Adaptation (LoRA),\nGLoRA employs a generalized prompt module to optimize pre-trained model weights\nand adjust intermediate activations, providing more flexibility and capability\nacross diverse tasks and datasets. Moreover, GLoRA facilitates efficient\nparameter adaptation by employing a scalable, modular, layer-wise structure\nsearch that learns individual adapter of each layer. Originating from a unified\nmathematical formulation, GLoRA exhibits strong transfer learning, few-shot\nlearning and domain generalization abilities, as it adjusts to new tasks\nthrough additional dimensions on weights and activations. Comprehensive\nexperiments demonstrate that GLoRA outperforms all previous methods in natural,\nspecialized, and structured benchmarks, achieving superior accuracy with fewer\nparameters and computations on various datasets. Furthermore, our structural\nre-parameterization design ensures that GLoRA incurs no extra inference cost,\nrendering it a practical solution for resource-limited applications. Code is\navailable at: https://github.com/Arnav0400/ViT-Slim/tree/master/GLoRA.",
        "translated": "提出了一种通用参数有效微调任务的先进方法——广义 LoRA (GLoRA)。增强低级适应(LoRA) ，GLoRA 采用广义提示模块优化预先训练的模型权重和调整中间激活，提供更多的灵活性和能力跨不同的任务和数据集。此外，GLoRA 通过采用可伸缩的、模块化的、分层的结构搜索，学习每一层的单个适配器，促进了有效的参数适配。起源于统一的数学公式，GLoRA 表现出强大的转移学习，少镜头学习和领域概括能力，因为它通过权重和激活的额外维度来适应新的任务。综合实验表明，GLoRA 在自然的、专门的和结构化的基准测试中优于以前的所有方法，在各种数据集上以更少的参数和计算获得更高的准确性。此外，我们的结构重新参数化设计确保了 GLoRA 不会产生额外的推理成本，从而为资源有限的应用程序提供了一个实用的解决方案。密码可于以下 https://github.com/arnav0400/vit-slim/tree/master/glora 索取:。"
    },
    {
        "title": "Parting with Misconceptions about Learning-based Vehicle Motion Planning",
        "url": "http://arxiv.org/abs/2306.07962v1",
        "pub_date": "2023-06-13",
        "summary": "The release of nuPlan marks a new era in vehicle motion planning research,\noffering the first large-scale real-world dataset and evaluation schemes\nrequiring both precise short-term planning and long-horizon ego-forecasting.\nExisting systems struggle to simultaneously meet both requirements. Indeed, we\nfind that these tasks are fundamentally misaligned and should be addressed\nindependently. We further assess the current state of closed-loop planning in\nthe field, revealing the limitations of learning-based methods in complex\nreal-world scenarios and the value of simple rule-based priors such as\ncenterline selection through lane graph search algorithms. More surprisingly,\nfor the open-loop sub-task, we observe that the best results are achieved when\nusing only this centerline as scene context (\\ie, ignoring all information\nregarding the map and other agents). Combining these insights, we propose an\nextremely simple and efficient planner which outperforms an extensive set of\ncompetitors, winning the nuPlan planning challenge 2023.",
        "translated": "NuPlan 的发布标志着汽车运动规划研究的一个新时代，它提供了第一个大规模的现实世界数据集和评估方案，既需要精确的短期规划，也需要长期的自我预测。现有系统很难同时满足这两个需求。事实上，我们发现这些任务从根本上是错位的，应该独立处理。我们进一步评估了闭环规划领域的现状，揭示了基于学习的方法在复杂现实场景中的局限性，以及基于规则的简单先验(如通过车道图搜索算法进行中心线选择)的价值。更令人惊讶的是，对于开环子任务，我们观察到只使用这个中心线作为场景上下文(即，忽略关于地图和其他代理的所有信息)可以获得最佳结果。结合这些见解，我们提出了一个非常简单和高效的规划，优于广泛的一套竞争对手，赢得了2023年的 nuPlan 规划挑战。"
    },
    {
        "title": "Hidden Biases of End-to-End Driving Models",
        "url": "http://arxiv.org/abs/2306.07957v1",
        "pub_date": "2023-06-13",
        "summary": "End-to-end driving systems have recently made rapid progress, in particular\non CARLA. Independent of their major contribution, they introduce changes to\nminor system components. Consequently, the source of improvements is unclear.\nWe identify two biases that recur in nearly all state-of-the-art methods and\nare critical for the observed progress on CARLA: (1) lateral recovery via a\nstrong inductive bias towards target point following, and (2) longitudinal\naveraging of multimodal waypoint predictions for slowing down. We investigate\nthe drawbacks of these biases and identify principled alternatives. By\nincorporating our insights, we develop TF++, a simple end-to-end method that\nranks first on the Longest6 and LAV benchmarks, gaining 14 driving score over\nthe best prior work on Longest6.",
        "translated": "端到端驱动系统最近取得了迅速的进展，特别是在 CARLA 上。与他们的主要贡献无关，他们引入了对次要系统组件的更改。因此，改进的来源尚不清楚。我们确定了几乎所有最先进的方法中都会出现的两种偏差，并且对于在 CARLA 上观察到的进展是至关重要的: (1)通过对目标点的强烈诱导性偏差进行横向恢复，以及(2)多模式路点预测的纵向平均减速。我们调查这些偏见的缺点，并确定原则的替代方案。通过结合我们的见解，我们开发了 TF + + ，一个简单的端到端方法，在 Longest6和 LAV 基准上排名第一，比 Longest6上的最佳工作得到14分。"
    },
    {
        "title": "Rerender A Video: Zero-Shot Text-Guided Video-to-Video Translation",
        "url": "http://arxiv.org/abs/2306.07954v1",
        "pub_date": "2023-06-13",
        "summary": "Large text-to-image diffusion models have exhibited impressive proficiency in\ngenerating high-quality images. However, when applying these models to video\ndomain, ensuring temporal consistency across video frames remains a formidable\nchallenge. This paper proposes a novel zero-shot text-guided video-to-video\ntranslation framework to adapt image models to videos. The framework includes\ntwo parts: key frame translation and full video translation. The first part\nuses an adapted diffusion model to generate key frames, with hierarchical\ncross-frame constraints applied to enforce coherence in shapes, textures and\ncolors. The second part propagates the key frames to other frames with\ntemporal-aware patch matching and frame blending. Our framework achieves global\nstyle and local texture temporal consistency at a low cost (without re-training\nor optimization). The adaptation is compatible with existing image diffusion\ntechniques, allowing our framework to take advantage of them, such as\ncustomizing a specific subject with LoRA, and introducing extra spatial\nguidance with ControlNet. Extensive experimental results demonstrate the\neffectiveness of our proposed framework over existing methods in rendering\nhigh-quality and temporally-coherent videos.",
        "translated": "大型文本-图像扩散模型在生成高质量图像方面表现出令人印象深刻的熟练程度。然而，在将这些模型应用于视频领域时，确保视频帧之间的时间一致性仍然是一个巨大的挑战。本文提出了一种新的零镜头文本引导的视频到视频的翻译框架，以适应图像模型的视频。该框架包括关键帧翻译和全视频翻译两部分。第一部分使用自适应扩散模型生成关键帧，并应用层次交叉帧约束来增强形状、纹理和颜色的一致性。第二部分利用时间感知的补丁匹配和帧混合技术将关键帧传播到其他帧。我们的框架以较低的成本(无需重新训练或优化)实现了全局样式和局部纹理的时间一致性。这种适应性与现有的图像扩散技术兼容，允许我们的框架利用这些技术，例如用 LoRA 定制特定的主题，以及用 ControlNet 引入额外的空间引导。大量的实验结果表明，我们提出的框架在渲染高质量和时间相干的视频现有方法的有效性。"
    },
    {
        "title": "MOFI: Learning Image Representations from Noisy Entity Annotated Images",
        "url": "http://arxiv.org/abs/2306.07952v1",
        "pub_date": "2023-06-13",
        "summary": "We present MOFI, a new vision foundation model designed to learn image\nrepresentations from noisy entity annotated images. MOFI differs from previous\nwork in two key aspects: ($i$) pre-training data, and ($ii$) training recipe.\nRegarding data, we introduce a new approach to automatically assign entity\nlabels to images from noisy image-text pairs. Our approach involves employing a\nnamed entity recognition model to extract entities from the alt-text, and then\nusing a CLIP model to select the correct entities as labels of the paired\nimage. The approach is simple, does not require costly human annotation, and\ncan be readily scaled up to billions of image-text pairs mined from the web.\nThrough this method, we have created Image-to-Entities (I2E), a new large-scale\ndataset with 1 billion images and 2 million distinct entities, covering rich\nvisual concepts in the wild. Building upon the I2E dataset, we study different\ntraining recipes, including supervised pre-training, contrastive pre-training,\nand multi-task learning. For constrastive pre-training, we treat entity names\nas free-form text, and further enrich them with entity descriptions.\nExperiments show that supervised pre-training with large-scale fine-grained\nentity labels is highly effective for image retrieval tasks, and multi-task\ntraining further improves the performance. The final MOFI model achieves 86.66%\nmAP on the challenging GPR1200 dataset, surpassing the previous\nstate-of-the-art performance of 72.19% from OpenAI's CLIP model. Further\nexperiments on zero-shot and linear probe image classification also show that\nMOFI outperforms a CLIP model trained on the original image-text data,\ndemonstrating the effectiveness of the I2E dataset in learning strong image\nrepresentations.",
        "translated": "提出了一种新的视觉基础模型 MOFI，该模型旨在从噪声实体注释图像中学习图像表示。MOFI 与以往的工作有两个关键方面的不同: ($i $)培训前数据和($ii $)培训配方。对于数据，我们引入了一种新的方法来自动分配实体标签图像噪声的图像-文本对。我们的方法包括使用命名实体识别模型从替代文本中提取实体，然后使用 CLIP 模型选择正确的实体作为配对图像的标签。这种方法很简单，不需要昂贵的人工注释，而且可以很容易地扩大到从网络中挖掘出的数十亿图像-文本对。通过这种方法，我们创建了图像到实体(I2E) ，这是一个新的大规模数据集，包含10亿张图像和200万个不同的实体，覆盖了丰富的野外视觉概念。在 I2E 数据集的基础上，我们研究了不同的训练方法，包括监督预训练、对比预训练和多任务学习。在对比预训练中，我们将实体名称视为自由形式的文本，并用实体描述进一步丰富实体名称。实验表明，基于大规模细粒度实体标签的监督预训练对图像检索任务具有很高的效果，多任务训练进一步提高了性能。最终的 MOFI 模型在具有挑战性的 GPR1200数据集上达到了86.66% 的 mAP，超过了之前 OpenAI 的 CLIP 模型72.19% 的最先进性能。进一步的零拍和线性探针图像分类实验也表明，MOFI 优于对原始图像-文本数据进行训练的 CLIP 模型，证明了 I2E 数据集在学习强图像表示方面的有效性。"
    },
    {
        "title": "Continuous Cost Aggregation for Dual-Pixel Disparity Extraction",
        "url": "http://arxiv.org/abs/2306.07921v1",
        "pub_date": "2023-06-13",
        "summary": "Recent works have shown that depth information can be obtained from\nDual-Pixel (DP) sensors. A DP arrangement provides two views in a single shot,\nthus resembling a stereo image pair with a tiny baseline. However, the\ndifferent point spread function (PSF) per view, as well as the small disparity\nrange, makes the use of typical stereo matching algorithms problematic. To\naddress the above shortcomings, we propose a Continuous Cost Aggregation (CCA)\nscheme within a semi-global matching framework that is able to provide accurate\ncontinuous disparities from DP images. The proposed algorithm fits parabolas to\nmatching costs and aggregates parabola coefficients along image paths. The\naggregation step is performed subject to a quadratic constraint that not only\nenforces the disparity smoothness but also maintains the quadratic form of the\ntotal costs. This gives rise to an inherently efficient disparity propagation\nscheme with a pixel-wise minimization in closed-form. Furthermore, the\ncontinuous form allows for a robust multi-scale aggregation that better\ncompensates for the varying PSF. Experiments on DP data from both DSLR and\nphone cameras show that the proposed scheme attains state-of-the-art\nperformance in DP disparity estimation.",
        "translated": "最近的工作表明，深度信息可以从双像素(DP)传感器。DP 排列在一个镜头中提供两个视图，因此类似于具有微小基线的立体图像对。然而，不同的点扩展函数(PSF)每个视图，以及小的视差范围，使得使用典型的立体匹配算法存在问题。针对上述缺点，我们提出了一种半全局匹配框架下的连续成本聚合(CCA)方案，该方案能够提供与 DP 图像的精确连续差异。该算法通过拟合抛物线来匹配代价，并将抛物线系数沿图像路径聚集起来。聚合步骤在二次约束条件下进行，二次约束条件不仅保证了视差平滑性，而且保持了总成本的二次形式。这就产生了一种内在有效的视差传播方案，该方案具有闭合形式的像素级最小化。此外，连续形式允许一个鲁棒的多尺度聚合，更好地补偿变化的 PSF。对数码单反和手机摄像机数据进行的实验表明，该方法在视差估计方面达到了先进水平。"
    },
    {
        "title": "Image Captioners Are Scalable Vision Learners Too",
        "url": "http://arxiv.org/abs/2306.07915v1",
        "pub_date": "2023-06-13",
        "summary": "Contrastive pretraining on image-text pairs from the web is one of the most\npopular large-scale pretraining strategies for vision backbones, especially in\nthe context of large multimodal models. At the same time, image captioning on\nthis type of data is commonly considered an inferior pretraining strategy. In\nthis paper, we perform a fair comparison of these two pretraining strategies,\ncarefully matching training data, compute, and model capacity. Using a standard\nencoder-decoder transformer, we find that captioning alone is surprisingly\neffective: on classification tasks, captioning produces vision encoders\ncompetitive with contrastively pretrained encoders, while surpassing them on\nvision &amp; language tasks. We further analyze the effect of the model\narchitecture and scale, as well as the pretraining data on the representation\nquality, and find that captioning exhibits the same or better scaling behavior\nalong these axes. Overall our results show that plain image captioning is a\nmore powerful pretraining strategy than was previously believed.",
        "translated": "基于网络图像-文本对的对比预训练是目前最流行的大规模视觉骨干预训练策略之一，尤其是在大型多模态模型中。同时，对这类数据的图像字幕通常被认为是一种劣质的预训练策略。在本文中，我们对这两种预训练策略进行了公平的比较，仔细匹配训练数据，计算和模型能力。使用一个标准的编码器-解码器转换器，我们发现字幕单独是令人惊讶的有效: 在分类任务，字幕产生的视觉编码器与对比预先训练的编码器竞争，同时在视觉和语言任务超过他们。我们进一步分析了模型结构和尺度，以及预训练数据对表示质量的影响，发现字幕沿着这些轴表现出相同或更好的尺度行为。总的来说，我们的结果表明，平面图像字幕是一个更强大的预训练策略比以前认为。"
    },
    {
        "title": "Seeing the World through Your Eyes",
        "url": "http://arxiv.org/abs/2306.09348v1",
        "pub_date": "2023-06-15",
        "summary": "The reflective nature of the human eye is an underappreciated source of\ninformation about what the world around us looks like. By imaging the eyes of a\nmoving person, we can collect multiple views of a scene outside the camera's\ndirect line of sight through the reflections in the eyes. In this paper, we\nreconstruct a 3D scene beyond the camera's line of sight using portrait images\ncontaining eye reflections. This task is challenging due to 1) the difficulty\nof accurately estimating eye poses and 2) the entangled appearance of the eye\niris and the scene reflections. Our method jointly refines the cornea poses,\nthe radiance field depicting the scene, and the observer's eye iris texture. We\nfurther propose a simple regularization prior on the iris texture pattern to\nimprove reconstruction quality. Through various experiments on synthetic and\nreal-world captures featuring people with varied eye colors, we demonstrate the\nfeasibility of our approach to recover 3D scenes using eye reflections.",
        "translated": "人类眼睛的反射性本质是一个被低估的信息来源，关于我们周围的世界是什么样子。通过对移动人物的眼睛进行成像，我们可以通过眼睛的反射来收集摄像机直接视线以外的场景的多种视图。在本文中，我们利用包含眼睛反射的人像图像重建摄像机视线以外的三维场景。这项任务是具有挑战性的，因为1)准确估计眼睛的姿势和2)眼睛虹膜的纠缠外观和场景反射的困难。我们的方法共同提炼角膜的姿态，光场描述的场景，和观察者的眼睛虹膜纹理。我们进一步提出了一种简单的正则化先验的虹膜纹理模式，以提高重建质量。通过对不同眼睛颜色的人的合成和真实世界捕捉的各种实验，我们证明了我们的方法的可行性恢复三维场景使用眼睛的反射。"
    },
    {
        "title": "UrbanIR: Large-Scale Urban Scene Inverse Rendering from a Single Video",
        "url": "http://arxiv.org/abs/2306.09349v1",
        "pub_date": "2023-06-15",
        "summary": "We show how to build a model that allows realistic, free-viewpoint renderings\nof a scene under novel lighting conditions from video. Our method -- UrbanIR:\nUrban Scene Inverse Rendering -- computes an inverse graphics representation\nfrom the video. UrbanIR jointly infers shape, albedo, visibility, and sun and\nsky illumination from a single video of unbounded outdoor scenes with unknown\nlighting. UrbanIR uses videos from cameras mounted on cars (in contrast to many\nviews of the same points in typical NeRF-style estimation). As a result,\nstandard methods produce poor geometry estimates (for example, roofs), and\nthere are numerous ''floaters''. Errors in inverse graphics inference can\nresult in strong rendering artifacts. UrbanIR uses novel losses to control\nthese and other sources of error. UrbanIR uses a novel loss to make very good\nestimates of shadow volumes in the original scene. The resulting\nrepresentations facilitate controllable editing, delivering photorealistic\nfree-viewpoint renderings of relit scenes and inserted objects. Qualitative\nevaluation demonstrates strong improvements over the state-of-the-art.",
        "translated": "我们展示了如何建立一个模型，允许现实，自由视点渲染一个场景下新的照明条件从视频。我们的方法—— UrbanIR: 城市场景反向渲染——从视频中计算一个反向图形表示。UrbanIR 共同推断的形状，反照率，能见度，以及太阳和天空照明从一个单一的视频无界户外场景与未知的照明。UrbanIR 使用安装在汽车上的摄像头拍摄的视频(与典型的 NERF 风格估算中的许多同一点的视图形成对比)。因此，标准的方法产生了糟糕的几何估计(例如，屋顶) ，并且存在大量的“浮动”。反向图形推理中的错误可能导致强烈的渲染伪影。UrbanIR 使用新的损失来控制这些和其他误差来源。UrbanIR 使用一种新颖的损失来对原始场景中的阴影体积做出很好的估计。由此产生的表示方便可控的编辑，提供重现场景和插入对象的真实感自由视点渲染。定性评价表明，与最先进的技术相比，有了很大的改进。"
    },
    {
        "title": "Rosetta Neurons: Mining the Common Units in a Model Zoo",
        "url": "http://arxiv.org/abs/2306.09346v1",
        "pub_date": "2023-06-15",
        "summary": "Do different neural networks, trained for various vision tasks, share some\ncommon representations?\n  In this paper, we demonstrate the existence of common features we call\n\"Rosetta Neurons\" across a range of models with different architectures,\ndifferent tasks (generative and discriminative), and different types of\nsupervision (class-supervised, text-supervised, self-supervised). We present an\nalgorithm for mining a dictionary of Rosetta Neurons across several popular\nvision models:\n  Class Supervised-ResNet50, DINO-ResNet50, DINO-ViT, MAE, CLIP-ResNet50,\nBigGAN, StyleGAN-2, StyleGAN-XL.\n  Our findings suggest that certain visual concepts and structures are\ninherently embedded in the natural world and can be learned by different models\nregardless of the specific task or architecture, and without the use of\nsemantic labels. We can visualize shared concepts directly due to generative\nmodels included in our analysis. The Rosetta Neurons facilitate model-to-model\ntranslation enabling various inversion-based manipulations, including\ncross-class alignments, shifting, zooming, and more, without the need for\nspecialized training.",
        "translated": "不同的神经网络，训练不同的视觉任务，共享一些共同的表征？在本文中，我们证明了存在的共同特征，我们称之为“罗塞塔神经元”跨一系列的模型，不同的架构，不同的任务(生成和区分) ，以及不同类型的监督(类监督，文本监督，自我监督)。我们提出了一个算法挖掘罗塞塔神经元字典跨几个流行的视觉模型: 类监督-ResNet50，DINO-ResNet50，DINO-ViT，MAE，CLIP-ResNet50，BigGAN，StyleGAN-2，StyleGAN-XL。我们的研究结果表明，某些视觉概念和结构固有地嵌入在自然世界中，可以通过不同的模型学习，不管具体的任务或架构，而不使用语义标签。由于分析中包含了生成模型，我们可以直接将共享的概念可视化。Rosetta 神经元促进模型到模型的转换，使各种基于倒置的操作成为可能，包括跨类比对、移位、缩放等等，而不需要专门的训练。"
    },
    {
        "title": "Segment Any Point Cloud Sequences by Distilling Vision Foundation Models",
        "url": "http://arxiv.org/abs/2306.09347v1",
        "pub_date": "2023-06-15",
        "summary": "Recent advancements in vision foundation models (VFMs) have opened up new\npossibilities for versatile and efficient visual perception. In this work, we\nintroduce Seal, a novel framework that harnesses VFMs for segmenting diverse\nautomotive point cloud sequences. Seal exhibits three appealing properties: i)\nScalability: VFMs are directly distilled into point clouds, eliminating the\nneed for annotations in either 2D or 3D during pretraining. ii) Consistency:\nSpatial and temporal relationships are enforced at both the camera-to-LiDAR and\npoint-to-segment stages, facilitating cross-modal representation learning. iii)\nGeneralizability: Seal enables knowledge transfer in an off-the-shelf manner to\ndownstream tasks involving diverse point clouds, including those from\nreal/synthetic, low/high-resolution, large/small-scale, and clean/corrupted\ndatasets. Extensive experiments conducted on eleven different point cloud\ndatasets showcase the effectiveness and superiority of Seal. Notably, Seal\nachieves a remarkable 45.0% mIoU on nuScenes after linear probing, surpassing\nrandom initialization by 36.9% mIoU and outperforming prior arts by 6.1% mIoU.\nMoreover, Seal demonstrates significant performance gains over existing methods\nacross 20 different few-shot fine-tuning tasks on all eleven tested point cloud\ndatasets.",
        "translated": "最近视觉基础模型(vision foundation model，VFM)的进步为多功能和高效的视知觉开辟了新的可能性。在这项工作中，我们介绍了密封，一个新的框架，利用 VFM 分割不同的汽车点云序列。玺展示了三个吸引人的特性: i)可伸缩性: VFM 被直接提炼成点云，在预训练期间不再需要2D 或3D 的注释。一致性: 在相机到激光雷达和点到片段阶段，空间和时间关系都得到了强化，促进了跨模态表示学习。Iii)概括性: 密封使得知识能够以现成的方式转移到涉及不同点云的下游任务，包括来自真实/合成、低/高分辨率、大/小规模和清洁/损坏数据集的任务。在十一个不同的点云数据集上进行的大量实验表明了海豹的有效性和优越性。值得注意的是，希尔在线性探测后在 nuScenes 上达到了显着的45.0% mIoU，比随机初始化高出36.9% mIoU，比现有技术高出6.1% mIoU。此外，在所有11个测试点云数据集上，在20个不同的小镜头微调任务上，Seal 显示了比现有方法显著的性能提高。"
    },
    {
        "title": "Evaluating Data Attribution for Text-to-Image Models",
        "url": "http://arxiv.org/abs/2306.09345v1",
        "pub_date": "2023-06-15",
        "summary": "While large text-to-image models are able to synthesize \"novel\" images, these\nimages are necessarily a reflection of the training data. The problem of data\nattribution in such models -- which of the images in the training set are most\nresponsible for the appearance of a given generated image -- is a difficult yet\nimportant one. As an initial step toward this problem, we evaluate attribution\nthrough \"customization\" methods, which tune an existing large-scale model\ntoward a given exemplar object or style. Our key insight is that this allows us\nto efficiently create synthetic images that are computationally influenced by\nthe exemplar by construction. With our new dataset of such exemplar-influenced\nimages, we are able to evaluate various data attribution algorithms and\ndifferent possible feature spaces. Furthermore, by training on our dataset, we\ncan tune standard models, such as DINO, CLIP, and ViT, toward the attribution\nproblem. Even though the procedure is tuned towards small exemplar sets, we\nshow generalization to larger sets. Finally, by taking into account the\ninherent uncertainty of the problem, we can assign soft attribution scores over\na set of training images.",
        "translated": "虽然大型文本-图像模型能够合成“新颖”图像，但这些图像必然是训练数据的反映。这类模型中的数据归属问题——训练集中的图像中哪一个对生成的图像的外观负有最大责任——是一个困难而又重要的问题。作为解决这个问题的第一步，我们通过“定制”方法评估属性，这些方法将现有的大规模模型调优到给定的示例对象或样式。我们的主要见解是，这使我们能够有效地创建合成图像，这些图像在计算上受到构造样本的影响。我们的新数据集，这样的样本影响的图像，我们能够评估各种数据归属算法和不同的可能的特征空间。此外，通过对数据集进行训练，我们可以针对属性问题调优标准模型，如 DINO、 CLIP 和 ViT。尽管这个过程是针对小范例集进行调整的，但是我们展示了对大范例集的泛化。最后，通过考虑问题固有的不确定性，我们可以在一组训练图像上分配软归因得分。"
    },
    {
        "title": "DreamSim: Learning New Dimensions of Human Visual Similarity using\n  Synthetic Data",
        "url": "http://arxiv.org/abs/2306.09344v1",
        "pub_date": "2023-06-15",
        "summary": "Current perceptual similarity metrics operate at the level of pixels and\npatches. These metrics compare images in terms of their low-level colors and\ntextures, but fail to capture mid-level similarities and differences in image\nlayout, object pose, and semantic content. In this paper, we develop a\nperceptual metric that assesses images holistically. Our first step is to\ncollect a new dataset of human similarity judgments over image pairs that are\nalike in diverse ways. Critical to this dataset is that judgments are nearly\nautomatic and shared by all observers. To achieve this we use recent\ntext-to-image models to create synthetic pairs that are perturbed along various\ndimensions. We observe that popular perceptual metrics fall short of explaining\nour new data, and we introduce a new metric, DreamSim, tuned to better align\nwith human perception. We analyze how our metric is affected by different\nvisual attributes, and find that it focuses heavily on foreground objects and\nsemantic content while also being sensitive to color and layout. Notably,\ndespite being trained on synthetic data, our metric generalizes to real images,\ngiving strong results on retrieval and reconstruction tasks. Furthermore, our\nmetric outperforms both prior learned metrics and recent large vision models on\nthese tasks.",
        "translated": "当前的感知相似性度量工作在像素和斑块的水平。这些度量标准根据图像的低层颜色和纹理对图像进行比较，但无法捕捉图像布局、对象姿势和语义内容的中层相似性和差异性。在本文中，我们开发了一个感知度量全面评估图像。我们的第一步是收集一个新的数据集，这个数据集包含了人类对不同方式相似的图像对的相似性判断。这个数据集的关键是，判断几乎是自动的，所有观察者共享。为了实现这一点，我们使用最近的文本到图像模型来创建沿着不同维度扰动的合成对。我们观察到，流行的感知指标不足以解释我们的新数据，我们引入了一个新的指标，DreamSim，调整以更好地与人类的感知一致。我们分析了我们的度量是如何受到不同的视觉属性的影响，发现它主要集中在前景对象和语义内容，同时也对颜色和布局敏感。值得注意的是，尽管在合成数据上进行了训练，我们的度量方法还是对真实图像进行了推广，在检索和重建任务上给出了强有力的结果。此外，我们的度量在这些任务上优于先前的学习度量和最近的大型视觉模型。"
    },
    {
        "title": "PaReprop: Fast Parallelized Reversible Backpropagation",
        "url": "http://arxiv.org/abs/2306.09342v1",
        "pub_date": "2023-06-15",
        "summary": "The growing size of datasets and deep learning models has made faster and\nmemory-efficient training crucial. Reversible transformers have recently been\nintroduced as an exciting new method for extremely memory-efficient training,\nbut they come with an additional computation overhead of activation\nre-computation in the backpropagation phase. We present PaReprop, a fast\nParallelized Reversible Backpropagation algorithm that parallelizes the\nadditional activation re-computation overhead in reversible training with the\ngradient computation itself in backpropagation phase. We demonstrate the\neffectiveness of the proposed PaReprop algorithm through extensive benchmarking\nacross model families (ViT, MViT, Swin and RoBERTa), data modalities (Vision &amp;\nNLP), model sizes (from small to giant), and training batch sizes. Our\nempirical results show that PaReprop achieves up to 20% higher training\nthroughput than vanilla reversible training, largely mitigating the theoretical\noverhead of 25% lower throughput from activation recomputation in reversible\ntraining. Project page: https://tylerzhu.com/pareprop.",
        "translated": "不断增长的数据集和深度学习模型使得更快、更有效的训练变得至关重要。可逆变压器作为一种新兴的高效存储器训练方法，近年来得到了广泛的应用，但是在反向传播阶段，这种方法需要增加激活重计算的计算开销。本文提出了一种快速并行化可逆反向传播算法，它将可逆训练中的附加激活重计算开销与反向传播阶段的梯度计算本身并行化。我们通过跨模型家族(ViT，MViT，Swin 和 RoBERTa) ，数据模式(Vision & NLP) ，模型大小(从小到大)和训练批量大小的广泛基准测试来证明所提出的 PaReprop 算法的有效性。我们的实验结果表明，PaReprop 比普通可逆训练提高了20% 的训练吞吐量，大大降低了可逆训练中激活重计算吞吐量降低25% 的理论开销。项目主页:  https://tylerzhu.com/pareprop。"
    },
    {
        "title": "Human Preference Score v2: A Solid Benchmark for Evaluating Human\n  Preferences of Text-to-Image Synthesis",
        "url": "http://arxiv.org/abs/2306.09341v1",
        "pub_date": "2023-06-15",
        "summary": "Recent text-to-image generative models can generate high-fidelity images from\ntext inputs, but the quality of these generated images cannot be accurately\nevaluated by existing evaluation metrics. To address this issue, we introduce\nHuman Preference Dataset v2 (HPD v2), a large-scale dataset that captures human\npreferences on images from a wide range of sources. HPD v2 comprises 798,090\nhuman preference choices on 430,060 pairs of images, making it the largest\ndataset of its kind. The text prompts and images are deliberately collected to\neliminate potential bias, which is a common issue in previous datasets. By\nfine-tuning CLIP on HPD v2, we obtain Human Preference Score v2 (HPS v2), a\nscoring model that can more accurately predict text-generated images' human\npreferences. Our experiments demonstrate that HPS v2 generalizes better than\nprevious metrics across various image distributions and is responsive to\nalgorithmic improvements of text-to-image generative models, making it a\npreferable evaluation metric for these models. We also investigate the design\nof the evaluation prompts for text-to-image generative models, to make the\nevaluation stable, fair and easy-to-use. Finally, we establish a benchmark for\ntext-to-image generative models using HPS v2, which includes a set of recent\ntext-to-image models from the academia, community and industry. The code and\ndataset is / will be available at https://github.com/tgxs002/HPSv2.",
        "translated": "最近的文本到图像的生成模型可以从文本输入生成高保真度的图像，但是这些生成的图像的质量不能被现有的评估指标准确地评估。为了解决这个问题，我们引入了人类偏好数据集 v2(HPD v2) ，这是一个大规模的数据集，可以捕获来自各种来源的图像上的人类偏好。HPD v2包含430,060对图像的798,090个人类偏好选项，使其成为同类图像中最大的数据集。文本提示和图像是有意收集的，以消除潜在的偏见，这是一个共同的问题在以前的数据集。通过对 HPD v2上的 CLIP 进行微调，我们得到了 Human Preferences Score v2(HPS v2) ，这是一个可以更准确地预测文本生成图像的人类偏好的评分模型。我们的实验表明，HPS v2在不同的图像分布中比以前的度量方法更好地推广，并且对文本到图像生成模型的算法改进作出响应，使其成为这些模型的一个更好的评估度量。研究了文本-图像生成模型中评价提示的设计，使评价结果稳定、公平、易用。最后，我们使用 HPS v2为文本到图像的生成模型建立了一个基准，其中包括来自学术界、社区和行业的一组最新的文本到图像模型。代码和数据集可在 https://github.com/tgxs002/hpsv2获得。"
    },
    {
        "title": "Understanding Optimization of Deep Learning",
        "url": "http://arxiv.org/abs/2306.09338v1",
        "pub_date": "2023-06-15",
        "summary": "This article provides a comprehensive understanding of optimization in deep\nlearning, with a primary focus on the challenges of gradient vanishing and\ngradient exploding, which normally lead to diminished model representational\nability and training instability, respectively. We analyze these two challenges\nthrough several strategic measures, including the improvement of gradient flow\nand the imposition of constraints on a network's Lipschitz constant. To help\nunderstand the current optimization methodologies, we categorize them into two\nclasses: explicit optimization and implicit optimization. Explicit optimization\nmethods involve direct manipulation of optimizer parameters, including weight,\ngradient, learning rate, and weight decay. Implicit optimization methods, by\ncontrast, focus on improving the overall landscape of a network by enhancing\nits modules, such as residual shortcuts, normalization methods, attention\nmechanisms, and activations. In this article, we provide an in-depth analysis\nof these two optimization classes and undertake a thorough examination of the\nJacobian matrices and the Lipschitz constants of many widely used deep learning\nmodules, highlighting existing issues as well as potential improvements.\nMoreover, we also conduct a series of analytical experiments to substantiate\nour theoretical discussions. This article does not aim to propose a new\noptimizer or network. Rather, our intention is to present a comprehensive\nunderstanding of optimization in deep learning. We hope that this article will\nassist readers in gaining a deeper insight in this field and encourages the\ndevelopment of more robust, efficient, and high-performing models.",
        "translated": "本文对深度学习中的优化问题进行了全面的理解，重点讨论了梯度消失和梯度爆炸对深度学习的挑战，这些挑战通常分别导致模型表示能力和训练不稳定性的降低。我们通过改进梯度流和对网络的 Lipschitz 常数施加约束等策略措施来分析这两个挑战。为了帮助理解当前的优化方法，我们将它们分为两类: 显式优化和隐式优化。显式优化方法包括对优化器参数的直接操作，包括权重、梯度、学习率和权重衰减。相比之下，隐式优化方法侧重于通过增强网络模块(如剩余捷径、归一化方法、注意力机制和激活)来改善网络的整体景观。在本文中，我们对这两个优化类进行了深入的分析，并对许多广泛使用的深度学习模块的雅可比矩阵和 Lipschitz 常数进行了彻底的检查，突出了存在的问题和潜在的改进。此外，我们还进行了一系列的分析实验，以证实我们的理论讨论。本文并不打算提出一种新的优化器或网络。相反，我们的目的是提出一个全面的理解优化在深度学习。我们希望本文能够帮助读者更深入地了解这个领域，并鼓励开发更健壮、高效和高性能的模型。"
    },
    {
        "title": "Generative Proxemics: A Prior for 3D Social Interaction from Images",
        "url": "http://arxiv.org/abs/2306.09337v1",
        "pub_date": "2023-06-15",
        "summary": "Social interaction is a fundamental aspect of human behavior and\ncommunication. The way individuals position themselves in relation to others,\nalso known as proxemics, conveys social cues and affects the dynamics of social\ninteraction. We present a novel approach that learns a 3D proxemics prior of\ntwo people in close social interaction. Since collecting a large 3D dataset of\ninteracting people is a challenge, we rely on 2D image collections where social\ninteractions are abundant. We achieve this by reconstructing pseudo-ground\ntruth 3D meshes of interacting people from images with an optimization approach\nusing existing ground-truth contact maps. We then model the proxemics using a\nnovel denoising diffusion model called BUDDI that learns the joint distribution\nof two people in close social interaction directly in the SMPL-X parameter\nspace. Sampling from our generative proxemics model produces realistic 3D human\ninteractions, which we validate through a user study. Additionally, we\nintroduce a new optimization method that uses the diffusion prior to\nreconstruct two people in close proximity from a single image without any\ncontact annotation. Our approach recovers more accurate and plausible 3D social\ninteractions from noisy initial estimates and outperforms state-of-the-art\nmethods. See our project site for code, data, and model:\nmuelea.github.io/buddi.",
        "translated": "社会互动是人类行为和交流的一个基本方面。个体将自己与他人关系定位的方式，也被称为近似学，传达社会线索并影响社会互动的动态。我们提出了一个新颖的方法，学习一个三维的前两个人在密切的社会互动。由于收集大量的人际互动的3D 数据集是一个挑战，我们依赖于社会互动非常丰富的2D 图像收集。我们通过使用现有的地面-真相接触图，利用优化方法从图像中重建相互作用的人的伪地面真相3D 网格来实现这一点。然后，我们使用一种新的去噪扩散模型 BUDDI 模型，在 SMPL-X 参数空间中直接学习密切社会交往中两个人的联合分布。从我们的生成近似模型抽样生成真实的3D 人类交互，我们通过用户研究验证。此外，我们引入了一种新的优化方法，使用扩散之前重建两个人在接近的一个单一的图像没有任何接触注释。我们的方法从嘈杂的初始估计中恢复更准确和合理的3D 社会互动，并且优于最先进的方法。有关代码、数据和模型，请参阅我们的项目站点:  muelea.github.io/buddi。"
    },
    {
        "title": "Coaching a Teachable Student",
        "url": "http://arxiv.org/abs/2306.10014v1",
        "pub_date": "2023-06-16",
        "summary": "We propose a novel knowledge distillation framework for effectively teaching\na sensorimotor student agent to drive from the supervision of a privileged\nteacher agent. Current distillation for sensorimotor agents methods tend to\nresult in suboptimal learned driving behavior by the student, which we\nhypothesize is due to inherent differences between the input, modeling\ncapacity, and optimization processes of the two agents. We develop a novel\ndistillation scheme that can address these limitations and close the gap\nbetween the sensorimotor agent and its privileged teacher. Our key insight is\nto design a student which learns to align their input features with the\nteacher's privileged Bird's Eye View (BEV) space. The student then can benefit\nfrom direct supervision by the teacher over the internal representation\nlearning. To scaffold the difficult sensorimotor learning task, the student\nmodel is optimized via a student-paced coaching mechanism with various\nauxiliary supervision. We further propose a high-capacity imitation learned\nprivileged agent that surpasses prior privileged agents in CARLA and ensures\nthe student learns safe driving behavior. Our proposed sensorimotor agent\nresults in a robust image-based behavior cloning agent in CARLA, improving over\ncurrent models by over 20.6% in driving score without requiring LiDAR,\nhistorical observations, ensemble of models, on-policy data aggregation or\nreinforcement learning.",
        "translated": "我们提出了一个新的知识提取框架，有效地教学感觉运动学生代理驱动从特权教师代理的监督。目前对于感觉运动代理方法的精馏往往导致学生学习驾驶行为的次优化，我们假设这是由于两个代理的输入、建模能力和优化过程之间的固有差异。我们开发了一个新颖的蒸馏方案，可以解决这些局限性，并缩小之间的差距，感觉运动代理及其特权教师。我们的主要见解是设计一个学生，学会调整他们的输入功能与教师的特权鸟瞰(BEV)空间。这样，学生就可以从教师对内部表征学习的直接监督中受益。为了构建高难度感觉运动学习任务的框架，通过多种辅助监控的学生节奏辅导机制对学生模型进行优化。我们进一步提出了一个高容量的模仿学习特权代理，超过以往的特权代理在 CARLA，并确保学生学习安全驾驶行为。我们提出的感知运动代理在 CARLA 中产生了一个强大的基于图像的行为克隆代理，在不需要激光雷达、历史观测、模型集成、政策数据聚合或强化学习的情况下，驱动分数比目前的模型提高了20.6% 以上。"
    },
    {
        "title": "PanoOcc: Unified Occupancy Representation for Camera-based 3D Panoptic\n  Segmentation",
        "url": "http://arxiv.org/abs/2306.10013v1",
        "pub_date": "2023-06-16",
        "summary": "Comprehensive modeling of the surrounding 3D world is key to the success of\nautonomous driving. However, existing perception tasks like object detection,\nroad structure segmentation, depth &amp; elevation estimation, and open-set object\nlocalization each only focus on a small facet of the holistic 3D scene\nunderstanding task. This divide-and-conquer strategy simplifies the algorithm\ndevelopment procedure at the cost of losing an end-to-end unified solution to\nthe problem. In this work, we address this limitation by studying camera-based\n3D panoptic segmentation, aiming to achieve a unified occupancy representation\nfor camera-only 3D scene understanding. To achieve this, we introduce a novel\nmethod called PanoOcc, which utilizes voxel queries to aggregate spatiotemporal\ninformation from multi-frame and multi-view images in a coarse-to-fine scheme,\nintegrating feature learning and scene representation into a unified occupancy\nrepresentation. We have conducted extensive ablation studies to verify the\neffectiveness and efficiency of the proposed method. Our approach achieves new\nstate-of-the-art results for camera-based semantic segmentation and panoptic\nsegmentation on the nuScenes dataset. Furthermore, our method can be easily\nextended to dense occupancy prediction and has shown promising performance on\nthe Occ3D benchmark. The code will be released at\nhttps://github.com/Robertwyq/PanoOcc.",
        "translated": "周围三维世界的综合建模是自主驾驶成功的关键。然而，现有的感知任务，比如目标检测、道路结构分割、深度和高度估计以及开放式对象定位，都只关注于整体三维场景理解任务的一小部分。这种分而治之的策略简化了算法开发过程，但代价是失去了问题的端到端统一解。在这项工作中，我们通过研究基于摄像机的三维全景分割来解决这一局限性，目的是实现一个统一的占用表示，用于只有摄像机的三维场景理解。为了实现这一目标，我们引入了一种新的方法 PanoOcc，该方法利用体素查询从多帧多视图图像中粗到精地聚合时空信息，将特征学习和场景表示融合到一个统一的占用表示中。我们进行了广泛的消融研究，以验证所提出的方法的有效性和效率。我们的方法在 nuScenes 数据集上实现了基于摄像机的语义分割和全景分割的最新结果。此外，我们的方法可以很容易地扩展到密集的占用率预测，并显示了良好的性能在 Occ3D 基准。密码会在 https://github.com/robertwyq/panoocc 公布。"
    },
    {
        "title": "MagicBrush: A Manually Annotated Dataset for Instruction-Guided Image\n  Editing",
        "url": "http://arxiv.org/abs/2306.10012v1",
        "pub_date": "2023-06-16",
        "summary": "Text-guided image editing is widely needed in daily life, ranging from\npersonal use to professional applications such as Photoshop. However, existing\nmethods are either zero-shot or trained on an automatically synthesized\ndataset, which contains a high volume of noise. Thus, they still require lots\nof manual tuning to produce desirable outcomes in practice. To address this\nissue, we introduce MagicBrush (https://osu-nlp-group.github.io/MagicBrush/),\nthe first large-scale, manually annotated dataset for instruction-guided real\nimage editing that covers diverse scenarios: single-turn, multi-turn,\nmask-provided, and mask-free editing. MagicBrush comprises over 10K manually\nannotated triples (source image, instruction, target image), which supports\ntrainining large-scale text-guided image editing models. We fine-tune\nInstructPix2Pix on MagicBrush and show that the new model can produce much\nbetter images according to human evaluation. We further conduct extensive\nexperiments to evaluate current image editing baselines from multiple\ndimensions including quantitative, qualitative, and human evaluations. The\nresults reveal the challenging nature of our dataset and the gap between\ncurrent baselines and real-world editing needs.",
        "translated": "从个人使用到 Photoshop 等专业应用，文本引导图像编辑在日常生活中被广泛需要。然而，现有的方法要么是零拍摄，要么是在一个自动合成的数据集上训练，这个数据集包含了大量的噪声。因此，它们仍然需要大量的手动调优，以便在实践中产生理想的结果。为了解决这个问题，我们引入了 MagicBrush ( https://osu-nlp-group.github.io/MagicBrush/) ，这是第一个用于指令引导的真实图像编辑的大规模手动注释数据集，涵盖了不同的场景: 单转、多转、掩码提供和无掩码编辑。MagicBrush 包含超过10K 的手动注释三元组(源图像、指令、目标图像) ，支持训练大规模的文本引导图像编辑模型。我们在 MagicBrush 上对 DirectPix2Pix 进行了微调，结果表明新的模型可以根据人的评价产生更好的图像。我们进一步进行广泛的实验，以评估目前的图像编辑基线从多个维度，包括定量，定性和人类的评价。结果揭示了我们的数据集的挑战性，以及当前基线和现实世界编辑需求之间的差距。"
    },
    {
        "title": "CLIP2Protect: Protecting Facial Privacy using Text-Guided Makeup via\n  Adversarial Latent Search",
        "url": "http://arxiv.org/abs/2306.10008v1",
        "pub_date": "2023-06-16",
        "summary": "The success of deep learning based face recognition systems has given rise to\nserious privacy concerns due to their ability to enable unauthorized tracking\nof users in the digital world. Existing methods for enhancing privacy fail to\ngenerate naturalistic images that can protect facial privacy without\ncompromising user experience. We propose a novel two-step approach for facial\nprivacy protection that relies on finding adversarial latent codes in the\nlow-dimensional manifold of a pretrained generative model. The first step\ninverts the given face image into the latent space and finetunes the generative\nmodel to achieve an accurate reconstruction of the given image from its latent\ncode. This step produces a good initialization, aiding the generation of\nhigh-quality faces that resemble the given identity. Subsequently, user-defined\nmakeup text prompts and identity-preserving regularization are used to guide\nthe search for adversarial codes in the latent space. Extensive experiments\ndemonstrate that faces generated by our approach have stronger black-box\ntransferability with an absolute gain of 12.06% over the state-of-the-art\nfacial privacy protection approach under the face verification task. Finally,\nwe demonstrate the effectiveness of the proposed approach for commercial face\nrecognition systems. Our code is available at\nhttps://github.com/fahadshamshad/Clip2Protect.",
        "translated": "基于深度学习的人脸识别系统的成功引起了严重的隐私问题，因为它们能够在数字世界中对用户进行未经授权的跟踪。现有的增强隐私的方法无法生成自然的图像，从而在不损害用户体验的情况下保护面部隐私。我们提出了一种新颖的两步法来保护面部隐私，这种方法依赖于在预先训练好的生成模型的低维流形中发现对手的潜在代码。第一步将给定的人脸图像反转到潜在空间，并对生成模型进行微调，以便从潜在代码实现给定图像的准确重建。这一步产生了良好的初始化，有助于生成类似于给定身份的高质量面孔。然后，利用自定义化妆文本提示和身份保持正则化来引导在潜在空间中搜索对手代码。大量实验表明，该方法生成的人脸具有更强的黑盒可转移性，在人脸验证任务下比最先进的面部隐私保护方法的绝对增益为12.06% 。最后，我们证明了该方法在商业人脸识别系统中的有效性。我们的代码可以在 https://github.com/fahadshamshad/clip2protect 找到。"
    },
    {
        "title": "Robot Learning with Sensorimotor Pre-training",
        "url": "http://arxiv.org/abs/2306.10007v1",
        "pub_date": "2023-06-16",
        "summary": "We present a self-supervised sensorimotor pre-training approach for robotics.\nOur model, called RPT, is a Transformer that operates on sequences of\nsensorimotor tokens. Given a sequence of camera images, proprioceptive robot\nstates, and past actions, we encode the interleaved sequence into tokens, mask\nout a random subset, and train a model to predict the masked-out content. We\nhypothesize that if the robot can predict the missing content it has acquired a\ngood model of the physical world that can enable it to act. RPT is designed to\noperate on latent visual representations which makes prediction tractable,\nenables scaling to 10x larger models, and 10 Hz inference on a real robot. To\nevaluate our approach, we collect a dataset of 20,000 real-world trajectories\nover 9 months using a combination of motion planning and model-based grasping\nalgorithms. We find that pre-training on this data consistently outperforms\ntraining from scratch, leads to 2x improvements in the block stacking task, and\nhas favorable scaling properties.",
        "translated": "我们提出了一种自我监督的感觉运动机器人预训练方法。我们的模型，称为 RPT，是一个变压器，操作感应运动令牌序列。给定一个摄像机图像序列、本体感知机器人状态和过去的动作，我们将交织序列编码成标记，掩盖掉一个随机子集，并训练一个模型来预测掩盖掉的内容。我们假设，如果机器人能够预测丢失的内容，它已经获得了一个良好的物理世界模型，使它能够采取行动。RPT 被设计用于对潜在的视觉表征进行操作，这使得预测变得易于处理，能够在真实的机器人上扩展到10倍大的模型和10赫兹的推理。为了评估我们的方法，我们使用运动规划和基于模型的抓取算法的组合，在9个月内收集了20,000个真实世界轨迹的数据集。我们发现，对这些数据的预训练始终优于从头开始的训练，导致块堆叠任务的2倍改进，并具有良好的缩放性能。"
    },
    {
        "title": "Unsupervised Learning of Style-Aware Facial Animation from Real Acting\n  Performances",
        "url": "http://arxiv.org/abs/2306.10006v1",
        "pub_date": "2023-06-16",
        "summary": "This paper presents a novel approach for text/speech-driven animation of a\nphoto-realistic head model based on blend-shape geometry, dynamic textures, and\nneural rendering. Training a VAE for geometry and texture yields a parametric\nmodel for accurate capturing and realistic synthesis of facial expressions from\na latent feature vector. Our animation method is based on a conditional CNN\nthat transforms text or speech into a sequence of animation parameters. In\ncontrast to previous approaches, our animation model learns\ndisentangling/synthesizing different acting-styles in an unsupervised manner,\nrequiring only phonetic labels that describe the content of training sequences.\nFor realistic real-time rendering, we train a U-Net that refines\nrasterization-based renderings by computing improved pixel colors and a\nforeground matte. We compare our framework qualitatively/quantitatively against\nrecent methods for head modeling as well as facial animation and evaluate the\nperceived rendering/animation quality in a user-study, which indicates large\nimprovements compared to state-of-the-art approaches",
        "translated": "提出了一种基于混合形状几何、动态纹理和神经绘制的真实感头部模型文本/语音驱动动画方法。训练几何和纹理的 VAE 产生一个参数模型，以准确捕捉和真实合成的面部表情从潜在的特征向量。我们的动画方法是基于条件细胞神经网络，转换成一系列的动画参数的文本或语音。与以前的方法相比，我们的动画模型学习在一个无监督的方式解开/综合不同的表演风格，只需要语音标签，描述训练序列的内容。为了实现真实的实时渲染，我们训练了一个 U-Net，它通过计算改进的像素颜色和前景哑光来改进基于栅格化的渲染。我们将我们的框架与最近的头部建模以及面部动画方法进行定性/定量比较，并评估用户研究中的感知渲染/动画质量，这表明与最先进的方法相比有很大的改进"
    },
    {
        "title": "C2F2NeUS: Cascade Cost Frustum Fusion for High Fidelity and\n  Generalizable Neural Surface Reconstruction",
        "url": "http://arxiv.org/abs/2306.10003v1",
        "pub_date": "2023-06-16",
        "summary": "There is an emerging effort to combine the two popular technical paths, i.e.,\nthe multi-view stereo (MVS) and neural implicit surface (NIS), in scene\nreconstruction from sparse views. In this paper, we introduce a novel\nintegration scheme that combines the multi-view stereo with neural signed\ndistance function representations, which potentially overcomes the limitations\nof both methods. MVS uses per-view depth estimation and cross-view fusion to\ngenerate accurate surface, while NIS relies on a common coordinate volume.\nBased on this, we propose to construct per-view cost frustum for finer geometry\nestimation, and then fuse cross-view frustums and estimate the implicit signed\ndistance functions to tackle noise and hole issues. We further apply a cascade\nfrustum fusion strategy to effectively captures global-local information and\nstructural consistency. Finally, we apply cascade sampling and a\npseudo-geometric loss to foster stronger integration between the two\narchitectures. Extensive experiments demonstrate that our method reconstructs\nrobust surfaces and outperforms existing state-of-the-art methods.",
        "translated": "将多视点立体声(MVS)和神经隐式曲面(NIS)这两种常用的技术路径结合起来进行稀疏视图场景重建是一种新兴的研究方向。本文提出了一种新的积分方法，将多视点立体视觉与神经元符号距离函数表示相结合，克服了两种方法的局限性。MVS 使用单视深度估计和交叉视图融合来生成精确的表面，而 NIS 则依赖于一个共同的坐标体积。在此基础上，我们提出构造视图代价平台进行精细几何估计，然后融合横视图平台和估计隐式符号距离函数，以解决噪声和孔问题。我们进一步应用级联平截体融合策略来有效地捕获全局-局部信息和结构一致性。最后，我们应用级联抽样和伪几何损失来促进两个架构之间更强的整合。大量的实验表明，我们的方法重建鲁棒的表面和优于现有的国家的最先进的方法。"
    },
    {
        "title": "Group Orthogonalization Regularization For Vision Models Adaptation and\n  Robustness",
        "url": "http://arxiv.org/abs/2306.10001v1",
        "pub_date": "2023-06-16",
        "summary": "As neural networks become deeper, the redundancy within their parameters\nincreases. This phenomenon has led to several methods that attempt to reduce\nthe correlation between convolutional filters. We propose a computationally\nefficient regularization technique that encourages orthonormality between\ngroups of filters within the same layer. Our experiments show that when\nincorporated into recent adaptation methods for diffusion models and vision\ntransformers (ViTs), this regularization improves performance on downstream\ntasks. We further show improved robustness when group orthogonality is enforced\nduring adversarial training. Our code is available at\nhttps://github.com/YoavKurtz/GOR.",
        "translated": "随着神经网络变得越来越深入，其参数中的冗余度也越来越大。这种现象导致了一些方法，试图降低卷积滤波器之间的相关性。我们提出了一个计算有效的正则化技术，鼓励在同一层滤波器群之间的正交性。我们的实验表明，当融入到扩散模型和视觉变换器(ViTs)的最新自适应方法中时，这种正则化改善了下游任务的性能。我们进一步表明，在对抗性训练中，当群体正交性被强制时，鲁棒性得到改善。我们的代码可以在 https://github.com/yoavkurtz/gor 找到。"
    },
    {
        "title": "SLACK: Stable Learning of Augmentations with Cold-start and KL\n  regularization",
        "url": "http://arxiv.org/abs/2306.09998v1",
        "pub_date": "2023-06-16",
        "summary": "Data augmentation is known to improve the generalization capabilities of\nneural networks, provided that the set of transformations is chosen with care,\na selection often performed manually. Automatic data augmentation aims at\nautomating this process. However, most recent approaches still rely on some\nprior information; they start from a small pool of manually-selected default\ntransformations that are either used to pretrain the network or forced to be\npart of the policy learned by the automatic data augmentation algorithm. In\nthis paper, we propose to directly learn the augmentation policy without\nleveraging such prior knowledge. The resulting bilevel optimization problem\nbecomes more challenging due to the larger search space and the inherent\ninstability of bilevel optimization algorithms. To mitigate these issues (i) we\nfollow a successive cold-start strategy with a Kullback-Leibler regularization,\nand (ii) we parameterize magnitudes as continuous distributions. Our approach\nleads to competitive results on standard benchmarks despite a more challenging\nsetting, and generalizes beyond natural images.",
        "translated": "众所周知，数据增强可以提高神经网络的泛化能力，只要谨慎地选择转换集合，这种选择通常是手动执行的。自动数据增强旨在实现这一过程的自动化。然而，最近的大多数方法仍然依赖于一些先前的信息; 它们从一小组手动选择的默认转换开始，这些转换要么用于预训练网络，要么被迫成为自动数据增强算法所学习的策略的一部分。在本文中，我们建议直接学习增强策略，而不利用这种先验知识。由于双最佳化问题优化算法的搜索空间更大，而且固有的不稳定性，因此产生的双层优化变得更具挑战性。为了减轻这些问题(i)我们遵循一个连续的冷启动策略与 Kullback-Leibler 正则化，和(ii)我们参数化大小作为连续分布。我们的方法导致竞争结果的标准基准，尽管更具挑战性的设置，并概括超越自然的图像。"
    },
    {
        "title": "Investigating Prompting Techniques for Zero- and Few-Shot Visual\n  Question Answering",
        "url": "http://arxiv.org/abs/2306.09996v1",
        "pub_date": "2023-06-16",
        "summary": "Visual question answering (VQA) is a challenging task that requires the\nability to comprehend and reason with visual information. While recent\nvision-language models have made strides, they continue to struggle with\nzero-shot VQA, particularly in handling complex compositional questions and\nadapting to new domains i.e. knowledge-based reasoning. This paper explores the\nuse of various prompting strategies, focusing on the BLIP2 model, to enhance\nzero-shot VQA performance. We conduct a comprehensive investigation across\nseveral VQA datasets, examining the effectiveness of different question\ntemplates, the role of few-shot exemplars, the impact of chain-of-thought (CoT)\nreasoning, and the benefits of incorporating image captions as additional\nvisual cues. Despite the varied outcomes, our findings demonstrate that\ncarefully designed question templates and the integration of additional visual\ncues, like image captions, can contribute to improved VQA performance,\nespecially when used in conjunction with few-shot examples. However, we also\nidentify a limitation in the use of chain-of-thought rationalization, which\nnegatively affects VQA accuracy. Our study thus provides critical insights into\nthe potential of prompting for improving zero-shot VQA performance.",
        "translated": "视觉问题回答是一项具有挑战性的任务，需要对视觉信息进行理解和推理。虽然最近的视觉语言模型已经取得了长足的进步，但是它们仍然在与零射击 VQA 作斗争，特别是在处理复杂的组合问题和适应新的领域，即基于知识的推理方面。本文探讨了各种激励策略的使用，重点是 BLIP2模型，以提高零拍 VQA 的性能。我们对几个 VQA 数据集进行了全面的调查，检查了不同问题模板的有效性，少拍范例的作用，思维链(CoT)推理的影响，以及将图像标题作为额外的视觉线索的好处。尽管结果各不相同，但我们的研究结果表明，精心设计的问题模板和整合额外的视觉线索，如图像标题，可以有助于改善 VQA 的性能，特别是当与少数镜头的例子一起使用时。然而，我们也发现了思维链合理化使用的局限性，这对 VQA 的准确性产生了负面影响。因此，我们的研究提供了关键的见解，促进提高零拍 VQA 性能的潜力。"
    },
    {
        "title": "Learning Profitable NFT Image Diffusions via Multiple Visual-Policy\n  Guided Reinforcement Learning",
        "url": "http://arxiv.org/abs/2306.11731v1",
        "pub_date": "2023-06-20",
        "summary": "We study the task of generating profitable Non-Fungible Token (NFT) images\nfrom user-input texts. Recent advances in diffusion models have shown great\npotential for image generation. However, existing works can fall short in\ngenerating visually-pleasing and highly-profitable NFT images, mainly due to\nthe lack of 1) plentiful and fine-grained visual attribute prompts for an NFT\nimage, and 2) effective optimization metrics for generating high-quality NFT\nimages. To solve these challenges, we propose a Diffusion-based generation\nframework with Multiple Visual-Policies as rewards (i.e., Diffusion-MVP) for\nNFT images. The proposed framework consists of a large language model (LLM), a\ndiffusion-based image generator, and a series of visual rewards by design.\nFirst, the LLM enhances a basic human input (such as \"panda\") by generating\nmore comprehensive NFT-style prompts that include specific visual attributes,\nsuch as \"panda with Ninja style and green background.\" Second, the\ndiffusion-based image generator is fine-tuned using a large-scale NFT dataset\nto capture fine-grained image styles and accessory compositions of popular NFT\nelements. Third, we further propose to utilize multiple visual-policies as\noptimization goals, including visual rarity levels, visual aesthetic scores,\nand CLIP-based text-image relevances. This design ensures that our proposed\nDiffusion-MVP is capable of minting NFT images with high visual quality and\nmarket value. To facilitate this research, we have collected the largest\npublicly available NFT image dataset to date, consisting of 1.5 million\nhigh-quality images with corresponding texts and market values. Extensive\nexperiments including objective evaluations and user studies demonstrate that\nour framework can generate NFT images showing more visually engaging elements\nand higher market value, compared with SOTA approaches.",
        "translated": "我们研究了从用户输入的文本中生成有利可图的非替代令牌(NFT)图像的任务。扩散模型的最新进展显示了图像生成的巨大潜力。然而，现有的作品在生成视觉上令人满意且高利润的非功能性傅里叶变换(NFT)图像方面存在不足，这主要是由于缺乏1)丰富和细粒度的非功能性傅里叶变换图像的视觉属性提示，以及2)生成高质量非功能性傅里叶变换图像的有效优化指标。为了解决这些问题，我们提出了一个基于扩散的生成框架，该框架使用多种视觉策略作为 NFT 图像的奖励(即扩散 MVP)。该框架由一个大型语言模型(LLM)、一个基于扩散的图像生成器和一系列设计的视觉奖励组成。首先，LLM 通过生成更全面的 NFT 风格的提示，包括特定的视觉属性，如“忍者风格和绿色背景的熊猫”，增强了基本的人类输入(如“熊猫”)其次，使用大规模的 NFT 数据集对基于扩散的图像生成器进行微调，以捕获流行的 NFT 元件的细粒度图像样式和附件组成。第三，我们进一步提出利用多个视觉策略作为优化目标，包括视觉稀有度水平，视觉审美评分，以及基于 CLIP 的文本图像相关性。这种设计确保了我们提出的扩散 MVP 能够生成具有高视觉质量和市场价值的 NFT 图像。为了促进这项研究，我们已经收集了迄今为止最大的公开可用的 NFT 图像数据集，包括150万高质量的图像与相应的文本和市场价值。包括客观评价和用户研究在内的大量实验表明，与 SOTA 方法相比，我们的框架可以生成更具视觉吸引力的元素和更高的市场价值的 NFT 图像。"
    },
    {
        "title": "Segment Anything Model (SAM) for Radiation Oncology",
        "url": "http://arxiv.org/abs/2306.11730v1",
        "pub_date": "2023-06-20",
        "summary": "In this study, we evaluate the performance of the Segment Anything Model\n(SAM) model in clinical radiotherapy. We collected real clinical cases from\nfour regions at the Mayo Clinic: prostate, lung, gastrointestinal, and head \\&amp;\nneck, which are typical treatment sites in radiation oncology. For each case,\nwe selected the OARs of concern in radiotherapy planning and compared the Dice\nand Jaccard outcomes between clinical manual delineation, automatic\nsegmentation using SAM's \"segment anything\" mode, and automatic segmentation\nusing SAM with box prompt. Our results indicate that SAM performs better in\nautomatic segmentation for the prostate and lung regions, while its performance\nin the gastrointestinal and head \\&amp; neck regions was relatively inferior. When\nconsidering the size of the organ and the clarity of its boundary, SAM displays\nbetter performance for larger organs with clear boundaries, such as the lung\nand liver, and worse for smaller organs with unclear boundaries, like the\nparotid and cochlea. These findings align with the generally accepted\nvariations in difficulty level associated with manual delineation of different\norgans at different sites in clinical radiotherapy. Given that SAM, a single\ntrained model, could handle the delineation of OARs in four regions, these\nresults also demonstrate SAM's robust generalization capabilities in automatic\nsegmentation for radiotherapy, i.e., achieving delineation of different\nradiotherapy OARs using a generic automatic segmentation model. SAM's\ngeneralization capabilities across different regions make it technically\nfeasible to develop a generic model for automatic segmentation in radiotherapy.",
        "translated": "在这项研究中，我们评估分段任意模型(SAM)在临床放射治疗中的性能。我们从梅奥诊所的四个地区收集了真实的临床病例: 前列腺、肺、胃肠道和头颈部，这是放射肿瘤学的典型治疗部位。对于每种情况，我们选择放疗计划中关注的 OAR，并比较临床手动描绘，使用 SAM 的“分割任何东西”模式的自动分割以及使用 SAM 和框提示的自动分割之间的 Dice 和 Jaccard 结果。我们的研究结果表明，SAM 在前列腺和肺部区域的自动分割方面表现较好，而在胃肠和头颈部区域的表现相对较差。当考虑器官的大小和其边界的清晰度时，SAM 对于边界清晰的较大器官(如肺和肝)表现出更好的性能，对于边界不清楚的较小器官(如腮腺和耳蜗)表现更差。这些发现与临床放射治疗中在不同部位手工描绘不同器官的难度水平的普遍接受的变化相一致。鉴于 SAM 是一个单一的训练模型，可以处理四个区域中 OAR 的描绘，这些结果也证明了 SAM 在放射治疗自动分割方面的强大推广能力，即使用通用的自动分割模型来实现不同放射治疗 OAR 的描绘。SAM 在不同区域的推广能力使得开发放射治疗中自动分割的通用模型在技术上是可行的。"
    },
    {
        "title": "Dense Video Object Captioning from Disjoint Supervision",
        "url": "http://arxiv.org/abs/2306.11729v1",
        "pub_date": "2023-06-20",
        "summary": "We propose a new task and model for dense video object captioning --\ndetecting, tracking, and captioning trajectories of all objects in a video.\nThis task unifies spatial and temporal understanding of the video, and requires\nfine-grained language description. Our model for dense video object captioning\nis trained end-to-end and consists of different modules for spatial\nlocalization, tracking, and captioning. As such, we can train our model with a\nmixture of disjoint tasks, and leverage diverse, large-scale datasets which\nsupervise different parts of our model. This results in noteworthy zero-shot\nperformance. Moreover, by finetuning a model from this initialization, we can\nfurther improve our performance, surpassing strong image-based baselines by a\nsignificant margin. Although we are not aware of other work performing this\ntask, we are able to repurpose existing video grounding datasets for our task,\nnamely VidSTG and VLN. We show our task is more general than grounding, and\nmodels trained on our task can directly be applied to grounding by finding the\nbounding box with the maximum likelihood of generating the query sentence. Our\nmodel outperforms dedicated, state-of-the-art models for spatial grounding on\nboth VidSTG and VLN.",
        "translated": "我们提出了一个新的任务和模型密集视频对象字幕-检测，跟踪和字幕轨迹的所有对象在视频。该任务将视频的时空理解统一起来，要求对视频进行细粒度的语言描述。我们的密集视频对象字幕模型是端到端训练的，由不同的空间定位、跟踪和字幕模块组成。因此，我们可以使用不相关任务的混合来训练模型，并利用不同的、大规模的数据集来监督模型的不同部分。这导致了值得注意的零射击性能。此外，通过从这个初始化微调模型，我们可以进一步提高我们的性能，大大超过强大的基于图像的基线。虽然我们不知道其他工作执行这项任务，我们能够重新利用现有的视频接地数据集为我们的任务，即 VidSTG 和 VLN。我们表明我们的任务是更一般的比接地，模型训练在我们的任务可以直接应用到接地，通过找到最大可能性生成查询句子的边界框。我们的模型在 VidSTG 和 VLN 上的空间接地性能优于专用的、最先进的模型。"
    },
    {
        "title": "How can objects help action recognition?",
        "url": "http://arxiv.org/abs/2306.11726v1",
        "pub_date": "2023-06-20",
        "summary": "Current state-of-the-art video models process a video clip as a long sequence\nof spatio-temporal tokens. However, they do not explicitly model objects, their\ninteractions across the video, and instead process all the tokens in the video.\nIn this paper, we investigate how we can use knowledge of objects to design\nbetter video models, namely to process fewer tokens and to improve recognition\naccuracy. This is in contrast to prior works which either drop tokens at the\ncost of accuracy, or increase accuracy whilst also increasing the computation\nrequired. First, we propose an object-guided token sampling strategy that\nenables us to retain a small fraction of the input tokens with minimal impact\non accuracy. And second, we propose an object-aware attention module that\nenriches our feature representation with object information and improves\noverall accuracy. Our resulting framework achieves better performance when\nusing fewer tokens than strong baselines. In particular, we match our baseline\nwith 30%, 40%, and 60% of the input tokens on SomethingElse,\nSomething-something v2, and Epic-Kitchens, respectively. When we use our model\nto process the same number of tokens as our baseline, we improve by 0.6 to 4.2\npoints on these datasets.",
        "translated": "当前最先进的视频模型将视频剪辑作为一长串时空令牌进行处理。但是，它们不显式地建模对象、它们在视频中的交互，而是处理视频中的所有令牌。本文研究了如何利用对象的知识来设计更好的视频模型，即处理更少的标记和提高识别准确率。这与以前的工作形成了鲜明的对比，以前的工作要么以牺牲精度为代价丢弃令牌，要么在提高精度的同时增加所需的计算量。首先，我们提出了一个对象引导的令牌采样策略，使我们能够保留输入令牌的一小部分，对准确性的影响最小。其次，提出了一种对象感知的注意模块，该模块充实了对象信息的特征表示，提高了整体的准确性。当使用的令牌比强基线少时，我们得到的框架获得了更好的性能。具体来说，我们分别使用30% 、40% 和60% 的输入令牌来匹配我们的基线; 这些令牌分别位于 Something Else、 Something-something v2和 Epic-Kitchens 上。当我们使用模型处理与基线相同数量的令牌时，我们在这些数据集上提高了0.6到4.2个点。"
    },
    {
        "title": "Low-complexity Multidimensional DCT Approximations",
        "url": "http://arxiv.org/abs/2306.11724v1",
        "pub_date": "2023-06-20",
        "summary": "In this paper, we introduce low-complexity multidimensional discrete cosine\ntransform (DCT) approximations. Three dimensional DCT (3D DCT) approximations\nare formalized in terms of high-order tensor theory. The formulation is\nextended to higher dimensions with arbitrary lengths. Several multiplierless\n$8\\times 8\\times 8$ approximate methods are proposed and the computational\ncomplexity is discussed for the general multidimensional case. The proposed\nmethods complexity cost was assessed, presenting considerably lower arithmetic\noperations when compared with the exact 3D DCT. The proposed approximations\nwere embedded into 3D DCT-based video coding scheme and a modified quantization\nstep was introduced. The simulation results showed that the approximate 3D DCT\ncoding methods offer almost identical output visual quality when compared with\nexact 3D DCT scheme. The proposed 3D approximations were also employed as a\ntool for visual tracking. The approximate 3D DCT-based proposed system performs\nsimilarly to the original exact 3D DCT-based method. In general, the suggested\nmethods showed competitive performance at a considerably lower computational\ncost.",
        "translated": "在这篇文章中，我们介绍了低复杂度的多维离散余弦变换(dCT)近似。利用高阶张量理论对三维 DCT (3D DCT)近似进行了形式化处理。将公式推广到具有任意长度的较高维数。提出了几种无乘法 $8乘以8乘以8的近似方法，并讨论了一般多维情况下的计算复杂性。提出的方法复杂度成本进行了评估，提出了相当低的算术运算相比，精确的三维离散余弦变换。在基于三维离散余弦变换(DCT)的视频编码方案中，引入了一种改进的量化步骤。仿真结果表明，与精确的三维 DCT 方案相比，近似的三维 DCT 编码方法提供了几乎相同的输出视觉质量。提出的三维近似也被用作视觉跟踪的工具。基于近似三维离散余弦变换的系统与基于精确三维离散余弦变换的方法性能相似。一般来说，建议的方法显示了竞争性能在一个相当低的计算成本。"
    },
    {
        "title": "Diffusion with Forward Models: Solving Stochastic Inverse Problems\n  Without Direct Supervision",
        "url": "http://arxiv.org/abs/2306.11719v1",
        "pub_date": "2023-06-20",
        "summary": "Denoising diffusion models are a powerful type of generative models used to\ncapture complex distributions of real-world signals. However, their\napplicability is limited to scenarios where training samples are readily\navailable, which is not always the case in real-world applications. For\nexample, in inverse graphics, the goal is to generate samples from a\ndistribution of 3D scenes that align with a given image, but ground-truth 3D\nscenes are unavailable and only 2D images are accessible. To address this\nlimitation, we propose a novel class of denoising diffusion probabilistic\nmodels that learn to sample from distributions of signals that are never\ndirectly observed. Instead, these signals are measured indirectly through a\nknown differentiable forward model, which produces partial observations of the\nunknown signal. Our approach involves integrating the forward model directly\ninto the denoising process. This integration effectively connects the\ngenerative modeling of observations with the generative modeling of the\nunderlying signals, allowing for end-to-end training of a conditional\ngenerative model over signals. During inference, our approach enables sampling\nfrom the distribution of underlying signals that are consistent with a given\npartial observation. We demonstrate the effectiveness of our method on three\nchallenging computer vision tasks. For instance, in the context of inverse\ngraphics, our model enables direct sampling from the distribution of 3D scenes\nthat align with a single 2D input image.",
        "translated": "去噪扩散模型是一种强大的生成模型，用于捕获复杂的分布的真实世界的信号。然而，它们的适用性仅限于训练样本容易获得的场景，而在实际应用中并非总是如此。例如，在反向图形中，目标是从与给定图像对齐的3D 场景分布中生成样本，但是实地3D 场景不可用，只能访问2D 图像。为了解决这一局限性，我们提出了一类新的去噪扩散概率模型，它学习从从未直接观测到的信号分布中采样。相反，这些信号是间接测量通过一个已知的可微正演模型，产生未知信号的部分观察。我们的方法包括将正向模型直接集成到去噪过程中。这种集成有效地将观测的生成建模与基础信号的生成建模联系起来，允许对信号进行端到端的条件生成模型训练。在推理过程中，我们的方法能够从与给定的部分观测结果一致的潜在信号的分布中进行采样。我们证明了我们的方法在三个具有挑战性的计算机视觉任务的有效性。例如，在反向图形的背景下，我们的模型支持从与单个2D 输入图像对齐的3D 场景分布中直接采样。"
    },
    {
        "title": "Meta-Analysis of Transfer Learning for Segmentation of Brain Lesions",
        "url": "http://arxiv.org/abs/2306.11714v1",
        "pub_date": "2023-06-20",
        "summary": "A major challenge in stroke research and stroke recovery predictions is the\ndetermination of a stroke lesion's extent and its impact on relevant brain\nsystems. Manual segmentation of stroke lesions from 3D magnetic resonance (MR)\nimaging volumes, the current gold standard, is not only very time-consuming,\nbut its accuracy highly depends on the operator's experience. As a result,\nthere is a need for a fully automated segmentation method that can efficiently\nand objectively measure lesion extent and the impact of each lesion to predict\nimpairment and recovery potential which might be beneficial for clinical,\ntranslational, and research settings. We have implemented and tested a fully\nautomatic method for stroke lesion segmentation which was developed using eight\ndifferent 2D-model architectures trained via transfer learning (TL) and mixed\ndata approaches. Additionally, the final prediction was made using a novel\nensemble method involving stacking and agreement window. Our novel method was\nevaluated in a novel in-house dataset containing 22 T1w brain MR images, which\nwere challenging in various perspectives, but mostly because they included T1w\nMR images from the subacute (which typically less well defined T1 lesions) and\nchronic stroke phase (which typically means well defined T1-lesions).\nCross-validation results indicate that our new method can efficiently and\nautomatically segment lesions fast and with high accuracy compared to ground\ntruth. In addition to segmentation, we provide lesion volume and weighted\nlesion load of relevant brain systems based on the lesions' overlap with a\ncanonical structural motor system that stretches from the cortical motor region\nto the lowest end of the brain stem.",
        "translated": "中风研究和中风恢复预测的一个主要挑战是确定中风病变的程度及其对相关脑系统的影响。根据目前的黄金标准，手工从三维磁共振(MR)成像体积中分割中风病灶不仅非常耗时，而且其准确性在很大程度上取决于操作者的经验。因此，需要一种完全自动化的分割方法，可以有效和客观地测量病变程度和每个病变的影响，以预测损伤和恢复潜力，这可能有利于临床，翻译和研究环境。我们已经实现并测试了一种全自动的脑卒中病灶分割方法，该方法使用了八种不同的2D 模型结构，通过转移学习(TL)和混合数据方法进行训练。此外，采用了一种新的集合方法，包括叠加和协调窗口，进行了最终的预测。我们的新方法在包含22个 T1w 脑 MR 图像的新型内部数据集中进行了评估，这些图像在各个方面都具有挑战性，但主要是因为它们包括来自亚急性(通常不太明确的 T1病变)和慢性中风阶段(通常意味着明确的 T1病变)的 T1w MR 图像。交叉验证结果表明，我们的新方法可以有效和自动分割病变快速和高准确性相比，地面真相。除了分割之外，我们基于病变与从皮层运动区域延伸到脑干最低端的规范结构运动系统的重叠，提供相关脑系统的病变体积和加权病变负荷。"
    },
    {
        "title": "Data-Driven but Privacy-Conscious: Pedestrian Dataset De-identification\n  via Full-Body Person Synthesis",
        "url": "http://arxiv.org/abs/2306.11710v1",
        "pub_date": "2023-06-20",
        "summary": "The advent of data-driven technology solutions is accompanied by an\nincreasing concern with data privacy. This is of particular importance for\nhuman-centered image recognition tasks, such as pedestrian detection,\nre-identification, and tracking. To highlight the importance of privacy issues\nand motivate future research, we motivate and introduce the Pedestrian Dataset\nDe-Identification (PDI) task. PDI evaluates the degree of de-identification and\ndownstream task training performance for a given de-identification method. As a\nfirst baseline, we propose IncogniMOT, a two-stage full-body de-identification\npipeline based on image synthesis via generative adversarial networks. The\nfirst stage replaces target pedestrians with synthetic identities. To improve\ndownstream task performance, we then apply stage two, which blends and adapts\nthe synthetic image parts into the data. To demonstrate the effectiveness of\nIncogniMOT, we generate a fully de-identified version of the MOT17 pedestrian\ntracking dataset and analyze its application as training data for pedestrian\nre-identification, detection, and tracking models. Furthermore, we show how our\ndata is able to narrow the synthetic-to-real performance gap in a\nprivacy-conscious manner.",
        "translated": "数据驱动技术解决方案的出现伴随着对数据隐私的日益关注。这对于以人为中心的图像识别任务尤其重要，例如行人检测、重新识别和跟踪。为了突出隐私问题的重要性和激励未来的研究，我们激励和介绍行人数据集去识别(PDI)任务。PDI 对给定的去识别方法的去识别程度和下游任务训练绩效进行评估。作为第一个基线，我们提出了 IncognMOT，一个基于图像合成通过生成对抗网络的两阶段全身去识别流水线。第一阶段用合成身份替换目标行人。为了提高下游任务的性能，我们接着应用第二阶段，将合成图像部分混合并适应到数据中。为了证明 IncognMOT 的有效性，我们生成了一个完全去识别的版本的 MOT17行人跟踪数据集，并分析其作为行人再识别，检测和跟踪模型的训练数据的应用。此外，我们还展示了我们的数据如何能够以一种注重隐私的方式缩小综合性能与实际性能之间的差距。"
    },
    {
        "title": "GenPlot: Increasing the Scale and Diversity of Chart Derendering Data",
        "url": "http://arxiv.org/abs/2306.11699v1",
        "pub_date": "2023-06-20",
        "summary": "Vertical bars, horizontal bars, dot, scatter, and line plots provide a\ndiverse set of visualizations to represent data. To understand these plots, one\nmust be able to recognize textual components, locate data points in a plot, and\nprocess diverse visual contexts to extract information. In recent works such as\nPix2Struct, Matcha, and Deplot, OCR-free chart-to-text translation has achieved\nstate-of-the-art results on visual language tasks. These results outline the\nimportance of chart-derendering as a pre-training objective, yet existing\ndatasets provide a fixed set of training examples. In this paper, we propose\nGenPlot; a plot generator that can generate billions of additional plots for\nchart-derendering using synthetic data.",
        "translated": "垂直条、水平条、点、散点和线条图提供了一组不同的可视化表示数据。为了理解这些情节，人们必须能够识别文本组成部分，在一个情节中定位数据点，并处理不同的视觉上下文来提取信息。在最近的作品，如 Pix2Struct，Matcha，和 Deplot，无 OCR 的图表到文本的翻译已经取得了最先进的成果，在视觉语言的任务。这些结果概述了图表去渲染作为一个预训练目标的重要性，然而现有的数据集提供了一组固定的训练例子。在本文中，我们提出了 GenPlot; 一个图形生成器，它可以生成数十亿个额外的图形，用于使用合成数据进行图表去渲染。"
    },
    {
        "title": "SkyGPT: Probabilistic Short-term Solar Forecasting Using Synthetic Sky\n  Videos from Physics-constrained VideoGPT",
        "url": "http://arxiv.org/abs/2306.11682v1",
        "pub_date": "2023-06-20",
        "summary": "In recent years, deep learning-based solar forecasting using all-sky images\nhas emerged as a promising approach for alleviating uncertainty in PV power\ngeneration. However, the stochastic nature of cloud movement remains a major\nchallenge for accurate and reliable solar forecasting. With the recent advances\nin generative artificial intelligence, the synthesis of visually plausible yet\ndiversified sky videos has potential for aiding in forecasts. In this study, we\nintroduce \\emph{SkyGPT}, a physics-informed stochastic video prediction model\nthat is able to generate multiple possible future images of the sky with\ndiverse cloud motion patterns, by using past sky image sequences as input.\nExtensive experiments and comparison with benchmark video prediction models\ndemonstrate the effectiveness of the proposed model in capturing cloud dynamics\nand generating future sky images with high realism and diversity. Furthermore,\nwe feed the generated future sky images from the video prediction models for\n15-minute-ahead probabilistic solar forecasting for a 30-kW roof-top PV system,\nand compare it with an end-to-end deep learning baseline model SUNSET and a\nsmart persistence model. Better PV output prediction reliability and sharpness\nis observed by using the predicted sky images generated with SkyGPT compared\nwith other benchmark models, achieving a continuous ranked probability score\n(CRPS) of 2.81 (13\\% better than SUNSET and 23\\% better than smart persistence)\nand a Winkler score of 26.70 for the test set. Although an arbitrary number of\nfutures can be generated from a historical sky image sequence, the results\nsuggest that 10 future scenarios is a good choice that balances probabilistic\nsolar forecasting performance and computational cost.",
        "translated": "近年来，基于全天空图像的深度学习太阳能预测已经成为缓解光伏发电不确定性的一种有前途的方法。然而，云层移动的随机性仍然是准确和可靠的太阳预报的主要挑战。随着生成性人工智能的最新进展，合成视觉上似是而非但多样化的天空视频具有帮助预报的潜力。在这项研究中，我们介绍了一个基于物理信息的随机视频预测模型，它能够以过去的天空图像序列作为输入，生成多个具有不同云运动模式的未来天空图像。大量的实验和与基准视频预测模型的比较表明，该模型在捕捉云动态和生成具有高真实度和多样性的未来天空图像方面是有效的。此外，我们为一个30千瓦的屋顶光伏系统提供了15分钟提前概率太阳预测的视频预测模型生成的未来天空图像，并将其与端到端深度学习基线模型 SUNSET 和智能持久性模型进行比较。通过使用用 SkyGPT 生成的预测天空图像与其他基准模型相比，观察到更好的 PV 输出预测可靠性和清晰度，实现了2.81的连续排名概率评分(CRPS)(比 SUNSET 高13% ，比智能持久性高23%)和26.70的 Winkler 评分测试集。虽然任意数量的未来可以从历史的天空图像序列生成，结果表明，10个未来情景是一个很好的选择，平衡了概率太阳预报性能和计算成本。"
    },
    {
        "title": "Benchmarking and Analyzing 3D-aware Image Synthesis with a Modularized\n  Codebase",
        "url": "http://arxiv.org/abs/2306.12423v1",
        "pub_date": "2023-06-21",
        "summary": "Despite the rapid advance of 3D-aware image synthesis, existing studies\nusually adopt a mixture of techniques and tricks, leaving it unclear how each\npart contributes to the final performance in terms of generality. Following the\nmost popular and effective paradigm in this field, which incorporates a neural\nradiance field (NeRF) into the generator of a generative adversarial network\n(GAN), we build a well-structured codebase, dubbed Carver, through modularizing\nthe generation process. Such a design allows researchers to develop and replace\neach module independently, and hence offers an opportunity to fairly compare\nvarious approaches and recognize their contributions from the module\nperspective. The reproduction of a range of cutting-edge algorithms\ndemonstrates the availability of our modularized codebase. We also perform a\nvariety of in-depth analyses, such as the comparison across different types of\npoint feature, the necessity of the tailing upsampler in the generator, the\nreliance on the camera pose prior, etc., which deepen our understanding of\nexisting methods and point out some further directions of the research work. We\nrelease code and models at https://github.com/qiuyu96/Carver to facilitate the\ndevelopment and evaluation of this field.",
        "translated": "尽管3D 感知图像合成技术发展迅速，但现有的研究通常采用技术和技巧相结合的方法，因此不清楚每个部分如何在通用性方面对最终性能做出贡献。遵循这个领域中最流行和有效的范例，将神经辐射场(NeRF)合并到生成对抗网络(GAN)的生成器中，我们通过模块化生成过程建立了一个结构良好的代码库，称为 Carver。这种设计使研究人员能够独立地开发和替换每个模块，从而提供了一个公平地比较各种方法并从模块的角度认识它们的贡献的机会。一系列尖端算法的再现证明了我们的模块化代码库的可用性。我们还进行了各种深入的分析，如不同类型的点特征的比较，在生成器中尾随上采样器的必要性，对摄像机姿态的依赖，等等，这加深了我们对现有方法的理解，并指出了一些进一步的研究工作方向。我们 https://github.com/qiuyu96/carver 发布代码和模型，以促进这一领域的发展和评估。"
    },
    {
        "title": "VisoGender: A dataset for benchmarking gender bias in image-text pronoun\n  resolution",
        "url": "http://arxiv.org/abs/2306.12424v1",
        "pub_date": "2023-06-21",
        "summary": "We introduce VisoGender, a novel dataset for benchmarking gender bias in\nvision-language models. We focus on occupation-related gender biases, inspired\nby Winograd and Winogender schemas, where each image is associated with a\ncaption containing a pronoun relationship of subjects and objects in the scene.\nVisoGender is balanced by gender representation in professional roles,\nsupporting bias evaluation in two ways: i) resolution bias, where we evaluate\nthe difference between gender resolution accuracies for men and women and ii)\nretrieval bias, where we compare ratios of male and female professionals\nretrieved for a gender-neutral search query. We benchmark several\nstate-of-the-art vision-language models and find that they lack the reasoning\nabilities to correctly resolve gender in complex scenes. While the direction\nand magnitude of gender bias depends on the task and the model being evaluated,\ncaptioning models generally are more accurate and less biased than CLIP-like\nmodels. Dataset and code are available at https://github.com/oxai/visogender",
        "translated": "我们介绍了视觉性别，一个新的数据集的基准性别偏见的视觉语言模型。我们重点关注与职业相关的性别偏见，灵感来自 Winograd 和 Winosex 模式，其中每个图像都与一个包含场景中主体和客体的代词关系的标题相关联。Viso 性别通过专业角色中的性别代表性来平衡，支持偏倚评估有两种方式: i)解析偏倚，其中我们评估男性和女性性别解析准确性之间的差异以及 ii)检索偏倚，其中我们比较检索的男性和女性专业人员的比例进行性别中立的搜索查询。我们基准的几个国家的最先进的视觉语言模型，发现他们缺乏推理能力，以正确解决性别在复杂的场景。虽然性别偏见的方向和程度取决于被评估的任务和模型，但字幕模型一般比 CLIP 类模型更准确，偏见更少。数据集和代码可在 https://github.com/oxai/visogender 下载"
    },
    {
        "title": "DreamTime: An Improved Optimization Strategy for Text-to-3D Content\n  Creation",
        "url": "http://arxiv.org/abs/2306.12422v1",
        "pub_date": "2023-06-21",
        "summary": "Text-to-image diffusion models pre-trained on billions of image-text pairs\nhave recently enabled text-to-3D content creation by optimizing a randomly\ninitialized Neural Radiance Fields (NeRF) with score distillation. However, the\nresultant 3D models exhibit two limitations: (a) quality concerns such as\nsaturated color and the Janus problem; (b) extremely low diversity comparing to\ntext-guided image synthesis. In this paper, we show that the conflict between\nNeRF optimization process and uniform timestep sampling in score distillation\nis the main reason for these limitations. To resolve this conflict, we propose\nto prioritize timestep sampling with monotonically non-increasing functions,\nwhich aligns NeRF optimization with the sampling process of diffusion model.\nExtensive experiments show that our simple redesign significantly improves\ntext-to-3D content creation with higher quality and diversity.",
        "translated": "在数十亿图像-文本对上预先训练的文本-图像扩散模型最近通过优化随机初始化的神经辐射场(NeRF)来实现文本到3D 内容的创建。然而，由此产生的三维模型表现出两个局限性: (a)质量问题，如饱和颜色和 Janus 问题; (b)与文本引导的图像合成相比，极低的多样性。本文指出，NERF 优化过程与均匀时间步长抽样在分数精馏中的冲突是造成这些局限性的主要原因。为了解决这一矛盾，我们提出了时间步长采样的优先级与单调非增长函数，调整 NERF 优化与扩散模型的采样过程。大量的实验表明，我们简单的重新设计显著提高了文本到3D 内容的创建，具有更高的质量和多样性。"
    },
    {
        "title": "Multi-Task Consistency for Active Learning",
        "url": "http://arxiv.org/abs/2306.12398v1",
        "pub_date": "2023-06-21",
        "summary": "Learning-based solutions for vision tasks require a large amount of labeled\ntraining data to ensure their performance and reliability. In single-task\nvision-based settings, inconsistency-based active learning has proven to be\neffective in selecting informative samples for annotation. However, there is a\nlack of research exploiting the inconsistency between multiple tasks in\nmulti-task networks. To address this gap, we propose a novel multi-task active\nlearning strategy for two coupled vision tasks: object detection and semantic\nsegmentation. Our approach leverages the inconsistency between them to identify\ninformative samples across both tasks. We propose three constraints that\nspecify how the tasks are coupled and introduce a method for determining the\npixels belonging to the object detected by a bounding box, to later quantify\nthe constraints as inconsistency scores. To evaluate the effectiveness of our\napproach, we establish multiple baselines for multi-task active learning and\nintroduce a new metric, mean Detection Segmentation Quality (mDSQ), tailored\nfor the multi-task active learning comparison that addresses the performance of\nboth tasks. We conduct extensive experiments on the nuImages and A9 datasets,\ndemonstrating that our approach outperforms existing state-of-the-art methods\nby up to 3.4% mDSQ on nuImages. Our approach achieves 95% of the fully-trained\nperformance using only 67% of the available data, corresponding to 20% fewer\nlabels compared to random selection and 5% fewer labels compared to\nstate-of-the-art selection strategy. Our code will be made publicly available\nafter the review process.",
        "translated": "基于学习的视觉任务解决方案需要大量的标记训练数据，以确保其性能和可靠性。在基于单任务视觉的环境中，基于不一致性的主动学习在选择信息样本进行注释时被证明是有效的。然而，针对多任务网络中多任务间不一致性的研究还很少。为了解决这个问题，我们提出了一个新的多任务主动学习策略，用于两个耦合的视觉任务: 目标检测和语义分割。我们的方法利用它们之间的不一致性来识别两个任务之间的信息样本。我们提出了三个约束，指定如何耦合的任务，并引入了一种方法来确定像素属于对象检测的边界框，以后量化的不一致性分数的约束。为了评估我们的方法的有效性，我们建立了多任务主动学习的多个基线，并引入了一个新的度量，平均检测分割质量(mDSQ) ，专为多任务主动学习比较，解决两个任务的性能。我们在 nuImages 和 A9数据集上进行了广泛的实验，证明我们的方法比现有的最先进的方法在 nuImages 上的性能高出3.4% mDSQ。我们的方法仅使用67% 的可用数据就实现了95% 的全训练性能，与随机选择相比，相应的标签减少了20% ，与最先进的选择策略相比，标签减少了5% 。我们的代码将在评审过程之后公开发布。"
    },
    {
        "title": "M-VAAL: Multimodal Variational Adversarial Active Learning for\n  Downstream Medical Image Analysis Tasks",
        "url": "http://arxiv.org/abs/2306.12376v1",
        "pub_date": "2023-06-21",
        "summary": "Acquiring properly annotated data is expensive in the medical field as it\nrequires experts, time-consuming protocols, and rigorous validation. Active\nlearning attempts to minimize the need for large annotated samples by actively\nsampling the most informative examples for annotation. These examples\ncontribute significantly to improving the performance of supervised machine\nlearning models, and thus, active learning can play an essential role in\nselecting the most appropriate information in deep learning-based diagnosis,\nclinical assessments, and treatment planning. Although some existing works have\nproposed methods for sampling the best examples for annotation in medical image\nanalysis, they are not task-agnostic and do not use multimodal auxiliary\ninformation in the sampler, which has the potential to increase robustness.\nTherefore, in this work, we propose a Multimodal Variational Adversarial Active\nLearning (M-VAAL) method that uses auxiliary information from additional\nmodalities to enhance the active sampling. We applied our method to two\ndatasets: i) brain tumor segmentation and multi-label classification using the\nBraTS2018 dataset, and ii) chest X-ray image classification using the\nCOVID-QU-Ex dataset. Our results show a promising direction toward\ndata-efficient learning under limited annotations.",
        "translated": "获取正确注释的数据在医学领域是昂贵的，因为它需要专家、耗时的协议和严格的验证。主动学习试图通过主动抽样最具信息量的注释示例来最小化对大型注释示例的需求。这些例子有助于提高监督式学习模型的性能，因此，在基于深度学习的诊断、临床评估和治疗计划中，主动学习可以在选择最合适的信息方面发挥重要作用。虽然现有的一些工作已经提出了医学图像分析中注释的最佳例子的抽样方法，但是它们不是任务不可知的，并且在采样器中不使用多模态辅助信息，这有可能提高鲁棒性。因此，在本研究中，我们提出一个多模态变分对抗主动学习(M-VAAL)方法，利用附加模态的辅助信息来增强主动抽样。我们将我们的方法应用于两个数据集: i)使用 BraTS2018数据集的脑肿瘤分割和多标记分类，以及 ii)使用 COVID-QU-Ex 数据集的胸部 X 射线图像分类。我们的研究结果为有限注释下的数据高效学习指明了一个有希望的方向。"
    },
    {
        "title": "Attention Hybrid Variational Net for Accelerated MRI Reconstruction",
        "url": "http://arxiv.org/abs/2306.12365v1",
        "pub_date": "2023-06-21",
        "summary": "The application of compressed sensing (CS)-enabled data reconstruction for\naccelerating magnetic resonance imaging (MRI) remains a challenging problem.\nThis is due to the fact that the information lost in k-space from the\nacceleration mask makes it difficult to reconstruct an image similar to the\nquality of a fully sampled image. Multiple deep learning-based structures have\nbeen proposed for MRI reconstruction using CS, both in the k-space and image\ndomains as well as using unrolled optimization methods. However, the drawback\nof these structures is that they are not fully utilizing the information from\nboth domains (k-space and image). Herein, we propose a deep learning-based\nattention hybrid variational network that performs learning in both the k-space\nand image domain. We evaluate our method on a well-known open-source MRI\ndataset and a clinical MRI dataset of patients diagnosed with strokes from our\ninstitution to demonstrate the performance of our network. In addition to\nquantitative evaluation, we undertook a blinded comparison of image quality\nacross networks performed by a subspecialty trained radiologist. Overall, we\ndemonstrate that our network achieves a superior performance among others under\nmultiple reconstruction tasks.",
        "translated": "应用压缩感知重建技术加速磁共振成像仍然是一个具有挑战性的问题。这是由于加速掩模在 k 空间中丢失的信息使得重建一幅类似于完全采样图像质量的图像变得困难。基于深度学习的结构已经被提出用于基于 CS 的 MRI 重建，包括在 k 空间和图像域以及使用展开优化方法。然而，这些结构的缺点是它们没有充分利用来自两个域(k 空间和图像)的信息。在这里，我们提出了一种基于深度学习的注意力混合变分网络，该网络在 k 空间和图像领域都可以进行学习。我们评估我们的方法在一个众所周知的开源 MRI 数据集和临床 MRI 数据集的患者诊断为中风从我们的机构，以证明我们的网络的表现。除了定量评估，我们进行了盲目的比较图像质量的网络进行了专业培训的放射科医生。总的来说，我们证明了我们的网络在多重重构任务下取得了比其他网络更好的性能。"
    },
    {
        "title": "Geometric Pooling: maintaining more useful information",
        "url": "http://arxiv.org/abs/2306.12341v1",
        "pub_date": "2023-06-21",
        "summary": "Graph Pooling technology plays an important role in graph node classification\ntasks. Sorting pooling technologies maintain large-value units for pooling\ngraphs of varying sizes. However, by analyzing the statistical characteristic\nof activated units after pooling, we found that a large number of units dropped\nby sorting pooling are negative-value units that contain useful information and\ncan contribute considerably to the final decision. To maintain more useful\ninformation, a novel pooling technology, called Geometric Pooling (GP), was\nproposed to contain the unique node features with negative values by measuring\nthe similarity of all node features. We reveal the effectiveness of GP from the\nentropy reduction view. The experiments were conducted on TUdatasets to show\nthe effectiveness of GP. The results showed that the proposed GP outperforms\nthe SOTA graph pooling technologies by 1%\\sim5% with fewer parameters.",
        "translated": "图池技术在图节点分类任务中起着重要作用。排序池技术为不同大小的池图维护大值单元。然而，通过分析池化后活化单元的统计特征，我们发现，由于排序池化而丢失的大量单元都是负值单元，它们包含有用的信息，对最终决策有很大的贡献。为了维护更多有用的信息，提出了一种新的池技术，称为几何池(GP) ，通过测量所有节点特征的相似度来包含负值的唯一节点特征。我们从熵约简的角度揭示了 GP 的有效性。在 TU 数据集上进行了实验，验证了 GP 算法的有效性。结果表明，在参数较少的情况下，所提出的 GP 比 SOTA 图池技术的性能提高了1% ，达到5% 。"
    },
    {
        "title": "Dynamic Implicit Image Function for Efficient Arbitrary-Scale Image\n  Representation",
        "url": "http://arxiv.org/abs/2306.12321v1",
        "pub_date": "2023-06-21",
        "summary": "Recent years have witnessed the remarkable success of implicit neural\nrepresentation methods. The recent work Local Implicit Image Function (LIIF)\nhas achieved satisfactory performance for continuous image representation,\nwhere pixel values are inferred from a neural network in a continuous spatial\ndomain. However, the computational cost of such implicit arbitrary-scale\nsuper-resolution (SR) methods increases rapidly as the scale factor increases,\nwhich makes arbitrary-scale SR time-consuming. In this paper, we propose\nDynamic Implicit Image Function (DIIF), which is a fast and efficient method to\nrepresent images with arbitrary resolution. Instead of taking an image\ncoordinate and the nearest 2D deep features as inputs to predict its pixel\nvalue, we propose a coordinate grouping and slicing strategy, which enables the\nneural network to perform decoding from coordinate slices to pixel value\nslices. We further propose a Coarse-to-Fine Multilayer Perceptron (C2F-MLP) to\nperform decoding with dynamic coordinate slicing, where the number of\ncoordinates in each slice varies as the scale factor varies. With dynamic\ncoordinate slicing, DIIF significantly reduces the computational cost when\nencountering arbitrary-scale SR. Experimental results demonstrate that DIIF can\nbe integrated with implicit arbitrary-scale SR methods and achieves SOTA SR\nperformance with significantly superior computational efficiency, thereby\nopening a path for real-time arbitrary-scale image representation. Our code can\nbe found at https://github.com/HeZongyao/DIIF.",
        "translated": "近年来，内隐神经表征方法取得了显著的成功。最近的工作局部隐式图像函数(LIIF)已经取得了令人满意的性能连续图像表示，其中像素值推断从一个连续的空间域的神经网络。然而，这种隐式任意尺度超分辨(SR)方法的计算量随着尺度因子的增加而迅速增加，这使得任意尺度超分辨方法的计算量大大增加。本文提出了动态隐式图像函数(DIIF) ，这是一种快速有效的表示任意分辨率图像的方法。提出了一种坐标分组和切片策略，使神经网络能够从坐标切片到像素值切片进行解码，而不是以图像坐标和最近的二维深度特征作为输入来预测像素值。我们进一步提出了一个粗到精的多层感知机(C2F-MLP)来执行动态坐标切片的解码，其中每个切片中的坐标数随比例因子的变化而变化。在动态坐标切片的情况下，DIIF 可以显著降低任意尺度 SR 的计算量。实验结果表明，DIIF 可以与隐式任意尺度 SR 方法相结合，以显著优越的计算效率实现 SOTA SR 性能，从而为实时任意尺度图像表示开辟了一条途径。我们的代码可以在 https://github.com/hezongyao/diif 找到。"
    },
    {
        "title": "StarVQA+: Co-training Space-Time Attention for Video Quality Assessment",
        "url": "http://arxiv.org/abs/2306.12298v1",
        "pub_date": "2023-06-21",
        "summary": "Self-attention based Transformer has achieved great success in many computer\nvision tasks. However, its application to video quality assessment (VQA) has\nnot been satisfactory so far. Evaluating the quality of in-the-wild videos is\nchallenging due to the unknown of pristine reference and shooting distortion.\nThis paper presents a co-trained Space-Time Attention network for the VQA\nproblem, termed StarVQA+. Specifically, we first build StarVQA+ by alternately\nconcatenating the divided space-time attention. Then, to facilitate the\ntraining of StarVQA+, we design a vectorized regression loss by encoding the\nmean opinion score (MOS) to the probability vector and embedding a special\ntoken as the learnable variable of MOS, leading to better fitting of human's\nrating process. Finally, to solve the data hungry problem with Transformer, we\npropose to co-train the spatial and temporal attention weights using both\nimages and videos. Various experiments are conducted on the de-facto\nin-the-wild video datasets, including LIVE-Qualcomm, LIVE-VQC, KoNViD-1k,\nYouTube-UGC, LSVQ, LSVQ-1080p, and DVL2021. Experimental results demonstrate\nthe superiority of the proposed StarVQA+ over the state-of-the-art.",
        "translated": "基于自注意的变压器在许多计算机视觉任务中取得了巨大的成功。然而，它在视频质量评价(VQA)中的应用却不尽如人意。由于未知的原始参考和拍摄失真，评估野外视频的质量是具有挑战性的。针对 VQA 问题，本文提出了一种联合训练的空时注意网络，命名为 StarVQA + 。具体来说，我们首先通过交替连接分割的时空注意力来构建 StarVQA + 。然后，为了便于 StarVQA + 的训练，我们设计了一个向量化回归损失，将平均意见分数(MOS)编码到概率向量中，并嵌入一个特殊的标记作为 MOS 的可学习变量，从而更好地拟合人类的评分过程。最后，为了解决变压器的数据饥饿问题，我们提出了利用图像和视频共同训练空间和时间注意权重的方法。在事实上的野外视频数据集上进行了各种实验，包括 LIVE-Qualcomm，LIVE-VQC，KoNViD-1k，YouTube-UGC，LSVQ，LSVQ-1080p 和 DVL2021。实验结果表明，所提出的 StarVQA + 相对于最新技术具有优越性。"
    },
    {
        "title": "Wildfire Detection Via Transfer Learning: A Survey",
        "url": "http://arxiv.org/abs/2306.12276v1",
        "pub_date": "2023-06-21",
        "summary": "This paper surveys different publicly available neural network models used\nfor detecting wildfires using regular visible-range cameras which are placed on\nhilltops or forest lookout towers. The neural network models are pre-trained on\nImageNet-1K and fine-tuned on a custom wildfire dataset. The performance of\nthese models is evaluated on a diverse set of wildfire images, and the survey\nprovides useful information for those interested in using transfer learning for\nwildfire detection. Swin Transformer-tiny has the highest AUC value but\nConvNext-tiny detects all the wildfire events and has the lowest false alarm\nrate in our dataset.",
        "translated": "本文调查了不同的公开可用的神经网络模型用于探测野火使用常规的可见距离摄像机放置在山顶或森林瞭望塔。神经网络模型在 ImageNet-1K 上进行预训练，并在自定义野火数据集上进行微调。这些模型的性能评估在一个不同的野火图像集，调查为那些有兴趣使用传递学习野火探测提供了有用的信息。微型变压器具有最高的 AUC 值，但是容量下一个-微型检测所有的野火事件，并具有最低的虚警率在我们的数据集。"
    },
    {
        "title": "Squeeze, Recover and Relabel: Dataset Condensation at ImageNet Scale\n  From A New Perspective",
        "url": "http://arxiv.org/abs/2306.13092v1",
        "pub_date": "2023-06-22",
        "summary": "We present a new dataset condensation framework termed Squeeze, Recover and\nRelabel (SRe$^2$L) that decouples the bilevel optimization of model and\nsynthetic data during training, to handle varying scales of datasets, model\narchitectures and image resolutions for effective dataset condensation. The\nproposed method demonstrates flexibility across diverse dataset scales and\nexhibits multiple advantages in terms of arbitrary resolutions of synthesized\nimages, low training cost and memory consumption with high-resolution training,\nand the ability to scale up to arbitrary evaluation network architectures.\nExtensive experiments are conducted on Tiny-ImageNet and full ImageNet-1K\ndatasets. Under 50 IPC, our approach achieves the highest 42.5% and 60.8%\nvalidation accuracy on Tiny-ImageNet and ImageNet-1K, outperforming all\nprevious state-of-the-art methods by margins of 14.5% and 32.9%, respectively.\nOur approach also outperforms MTT by approximately 52$\\times$ (ConvNet-4) and\n16$\\times$ (ResNet-18) faster in speed with less memory consumption of\n11.6$\\times$ and 6.4$\\times$ during data synthesis. Our code and condensed\ndatasets of 50, 200 IPC with 4K recovery budget are available at\nhttps://zeyuanyin.github.io/projects/SRe2L/.",
        "translated": ""
    },
    {
        "title": "Evading Forensic Classifiers with Attribute-Conditioned Adversarial\n  Faces",
        "url": "http://arxiv.org/abs/2306.13091v1",
        "pub_date": "2023-06-22",
        "summary": "The ability of generative models to produce highly realistic synthetic face\nimages has raised security and ethical concerns. As a first line of defense\nagainst such fake faces, deep learning based forensic classifiers have been\ndeveloped. While these forensic models can detect whether a face image is\nsynthetic or real with high accuracy, they are also vulnerable to adversarial\nattacks. Although such attacks can be highly successful in evading detection by\nforensic classifiers, they introduce visible noise patterns that are detectable\nthrough careful human scrutiny. Additionally, these attacks assume access to\nthe target model(s) which may not always be true. Attempts have been made to\ndirectly perturb the latent space of GANs to produce adversarial fake faces\nthat can circumvent forensic classifiers. In this work, we go one step further\nand show that it is possible to successfully generate adversarial fake faces\nwith a specified set of attributes (e.g., hair color, eye size, race, gender,\netc.). To achieve this goal, we leverage the state-of-the-art generative model\nStyleGAN with disentangled representations, which enables a range of\nmodifications without leaving the manifold of natural images. We propose a\nframework to search for adversarial latent codes within the feature space of\nStyleGAN, where the search can be guided either by a text prompt or a reference\nimage. We also propose a meta-learning based optimization strategy to achieve\ntransferable performance on unknown target models. Extensive experiments\ndemonstrate that the proposed approach can produce semantically manipulated\nadversarial fake faces, which are true to the specified attribute set and can\nsuccessfully fool forensic face classifiers, while remaining undetectable by\nhumans. Code: https://github.com/koushiksrivats/face_attribute_attack.",
        "translated": ""
    },
    {
        "title": "PromptIR: Prompting for All-in-One Blind Image Restoration",
        "url": "http://arxiv.org/abs/2306.13090v1",
        "pub_date": "2023-06-22",
        "summary": "Image restoration involves recovering a high-quality clean image from its\ndegraded version. Deep learning-based methods have significantly improved image\nrestoration performance, however, they have limited generalization ability to\ndifferent degradation types and levels. This restricts their real-world\napplication since it requires training individual models for each specific\ndegradation and knowing the input degradation type to apply the relevant model.\nWe present a prompt-based learning approach, PromptIR, for All-In-One image\nrestoration that can effectively restore images from various types and levels\nof degradation. In particular, our method uses prompts to encode\ndegradation-specific information, which is then used to dynamically guide the\nrestoration network. This allows our method to generalize to different\ndegradation types and levels, while still achieving state-of-the-art results on\nimage denoising, deraining, and dehazing. Overall, PromptIR offers a generic\nand efficient plugin module with few lightweight prompts that can be used to\nrestore images of various types and levels of degradation with no prior\ninformation on the corruptions present in the image. Our code and pretrained\nmodels are available here: https://github.com/va1shn9v/PromptIR",
        "translated": ""
    },
    {
        "title": "Continuous Layout Editing of Single Images with Diffusion Models",
        "url": "http://arxiv.org/abs/2306.13078v1",
        "pub_date": "2023-06-22",
        "summary": "Recent advancements in large-scale text-to-image diffusion models have\nenabled many applications in image editing. However, none of these methods have\nbeen able to edit the layout of single existing images. To address this gap, we\npropose the first framework for layout editing of a single image while\npreserving its visual properties, thus allowing for continuous editing on a\nsingle image. Our approach is achieved through two key modules. First, to\npreserve the characteristics of multiple objects within an image, we\ndisentangle the concepts of different objects and embed them into separate\ntextual tokens using a novel method called masked textual inversion. Next, we\npropose a training-free optimization method to perform layout control for a\npre-trained diffusion model, which allows us to regenerate images with learned\nconcepts and align them with user-specified layouts. As the first framework to\nedit the layout of existing images, we demonstrate that our method is effective\nand outperforms other baselines that were modified to support this task. Our\ncode will be freely available for public use upon acceptance.",
        "translated": ""
    },
    {
        "title": "Semi-automated extraction of research topics and trends from NCI funding\n  in radiological sciences from 2000-2020",
        "url": "http://arxiv.org/abs/2306.13075v1",
        "pub_date": "2023-06-22",
        "summary": "Investigators, funders, and the public desire knowledge on topics and trends\nin publicly funded research but current efforts in manual categorization are\nlimited in scale and understanding. We developed a semi-automated approach to\nextract and name research topics, and applied this to \\$1.9B of NCI funding\nover 21 years in the radiological sciences to determine micro- and macro-scale\nresearch topics and funding trends. Our method relies on sequential clustering\nof existing biomedical-based word embeddings, naming using subject matter\nexperts, and visualization to discover trends at a macroscopic scale above\nindividual topics. We present results using 15 and 60 cluster topics, where we\nfound that 2D projection of grant embeddings reveals two dominant axes:\nphysics-biology and therapeutic-diagnostic. For our dataset, we found that\nfunding for therapeutics- and physics-based research have outpaced diagnostics-\nand biology-based research, respectively. We hope these results may (1) give\ninsight to funders on the appropriateness of their funding allocation, (2)\nassist investigators in contextualizing their work and explore neighboring\nresearch domains, and (3) allow the public to review where their tax dollars\nare being allocated.",
        "translated": ""
    },
    {
        "title": "Iterative Scale-Up ExpansionIoU and Deep Features Association for\n  Multi-Object Tracking in Sports",
        "url": "http://arxiv.org/abs/2306.13074v1",
        "pub_date": "2023-06-22",
        "summary": "Multi-object tracking algorithms have made significant advancements due to\nthe recent developments in object detection. However, most existing methods\nprimarily focus on tracking pedestrians or vehicles, which exhibit relatively\nsimple and regular motion patterns. Consequently, there is a scarcity of\nalgorithms that address the tracking of targets with irregular or non-linear\nmotion, such as multi-athlete tracking. Furthermore, popular tracking\nalgorithms often rely on the Kalman filter for object motion modeling, which\nfails to track objects when their motion contradicts the linear motion\nassumption of the Kalman filter. Due to this reason, we proposed a novel online\nand robust multi-object tracking approach, named Iterative Scale-Up\nExpansionIoU and Deep Features for multi-object tracking. Unlike conventional\nmethods, we abandon the use of the Kalman filter and propose utilizing the\niterative scale-up expansion IoU. This approach achieves superior tracking\nperformance without requiring additional training data or adopting a more\nrobust detector, all while maintaining a lower computational cost compared to\nother appearance-based methods. Our proposed method demonstrates remarkable\neffectiveness in tracking irregular motion objects, achieving a score of 75.3%\nin HOTA. It outperforms all state-of-the-art online tracking algorithms on the\nSportsMOT dataset, covering various kinds of sport scenarios.",
        "translated": ""
    },
    {
        "title": "Deep Metric Learning with Soft Orthogonal Proxies",
        "url": "http://arxiv.org/abs/2306.13055v1",
        "pub_date": "2023-06-22",
        "summary": "Deep Metric Learning (DML) models rely on strong representations and\nsimilarity-based measures with specific loss functions. Proxy-based losses have\nshown great performance compared to pair-based losses in terms of convergence\nspeed. However, proxies that are assigned to different classes may end up being\nclosely located in the embedding space and hence having a hard time to\ndistinguish between positive and negative items. Alternatively, they may become\nhighly correlated and hence provide redundant information with the model. To\naddress these issues, we propose a novel approach that introduces Soft\nOrthogonality (SO) constraint on proxies. The constraint ensures the proxies to\nbe as orthogonal as possible and hence control their positions in the embedding\nspace. Our approach leverages Data-Efficient Image Transformer (DeiT) as an\nencoder to extract contextual features from images along with a DML objective.\nThe objective is made of the Proxy Anchor loss along with the SO\nregularization. We evaluate our method on four public benchmarks for\ncategory-level image retrieval and demonstrate its effectiveness with\ncomprehensive experimental results and ablation studies. Our evaluations\ndemonstrate the superiority of our proposed approach over state-of-the-art\nmethods by a significant margin.",
        "translated": ""
    },
    {
        "title": "AugDMC: Data Augmentation Guided Deep Multiple Clustering",
        "url": "http://arxiv.org/abs/2306.13023v1",
        "pub_date": "2023-06-22",
        "summary": "Clustering aims to group similar objects together while separating dissimilar\nones apart. Thereafter, structures hidden in data can be identified to help\nunderstand data in an unsupervised manner. Traditional clustering methods such\nas k-means provide only a single clustering for one data set. Deep clustering\nmethods such as auto-encoder based clustering methods have shown a better\nperformance, but still provide a single clustering. However, a given dataset\nmight have multiple clustering structures and each represents a unique\nperspective of the data. Therefore, some multiple clustering methods have been\ndeveloped to discover multiple independent structures hidden in data. Although\ndeep multiple clustering methods provide better performance, how to efficiently\ncapture the alternative perspectives in data is still a problem. In this paper,\nwe propose AugDMC, a novel data Augmentation guided Deep Multiple Clustering\nmethod, to tackle the challenge. Specifically, AugDMC leverages data\naugmentations to automatically extract features related to a certain aspect of\nthe data using a self-supervised prototype-based representation learning, where\ndifferent aspects of the data can be preserved under different data\naugmentations. Moreover, a stable optimization strategy is proposed to\nalleviate the unstable problem from different augmentations. Thereafter,\nmultiple clusterings based on different aspects of the data can be obtained.\nExperimental results on three real-world datasets compared with\nstate-of-the-art methods validate the effectiveness of the proposed method.",
        "translated": ""
    },
    {
        "title": "Toward Automated Detection of Microbleeds with Anatomical Scale\n  Localization: A Complete Clinical Diagnosis Support Using Deep Learning",
        "url": "http://arxiv.org/abs/2306.13020v1",
        "pub_date": "2023-06-22",
        "summary": "Cerebral Microbleeds (CMBs) are chronic deposits of small blood products in\nthe brain tissues, which have explicit relation to various cerebrovascular\ndiseases depending on their anatomical location, including cognitive decline,\nintracerebral hemorrhage, and cerebral infarction. However, manual detection of\nCMBs is a time-consuming and error-prone process because of their sparse and\ntiny structural properties. The detection of CMBs is commonly affected by the\npresence of many CMB mimics that cause a high false-positive rate (FPR), such\nas calcification and pial vessels. This paper proposes a novel 3D deep learning\nframework that does not only detect CMBs but also inform their anatomical\nlocation in the brain (i.e., lobar, deep, and infratentorial regions). For the\nCMB detection task, we propose a single end-to-end model by leveraging the\nU-Net as a backbone with Region Proposal Network (RPN). To significantly reduce\nthe FPs within the same single model, we develop a new scheme, containing\nFeature Fusion Module (FFM) that detects small candidates utilizing contextual\ninformation and Hard Sample Prototype Learning (HSPL) that mines CMB mimics and\ngenerates additional loss term called concentration loss using Convolutional\nPrototype Learning (CPL). The anatomical localization task does not only tell\nto which region the CMBs belong but also eliminate some FPs from the detection\ntask by utilizing anatomical information. The results show that the proposed\nRPN that utilizes the FFM and HSPL outperforms the vanilla RPN and achieves a\nsensitivity of 94.66% vs. 93.33% and an average number of false positives per\nsubject (FPavg) of 0.86 vs. 14.73. Also, the anatomical localization task\nfurther improves the detection performance by reducing the FPavg to 0.56 while\nmaintaining the sensitivity of 94.66%.",
        "translated": ""
    },
    {
        "title": "Affine Correspondences between Multi-Camera Systems for Relative Pose\n  Estimation",
        "url": "http://arxiv.org/abs/2306.12996v1",
        "pub_date": "2023-06-22",
        "summary": "We present a novel method to compute the relative pose of multi-camera\nsystems using two affine correspondences (ACs). Existing solutions to the\nmulti-camera relative pose estimation are either restricted to special cases of\nmotion, have too high computational complexity, or require too many point\ncorrespondences (PCs). Thus, these solvers impede an efficient or accurate\nrelative pose estimation when applying RANSAC as a robust estimator. This paper\nshows that the 6DOF relative pose estimation problem using ACs permits a\nfeasible minimal solution, when exploiting the geometric constraints between\nACs and multi-camera systems using a special parameterization. We present a\nproblem formulation based on two ACs that encompass two common types of ACs\nacross two views, i.e., inter-camera and intra-camera. Moreover, the framework\nfor generating the minimal solvers can be extended to solve various relative\npose estimation problems, e.g., 5DOF relative pose estimation with known\nrotation angle prior. Experiments on both virtual and real multi-camera systems\nprove that the proposed solvers are more efficient than the state-of-the-art\nalgorithms, while resulting in a better relative pose accuracy. Source code is\navailable at https://github.com/jizhaox/relpose-mcs-depth.",
        "translated": ""
    },
    {
        "title": "ProRes: Exploring Degradation-aware Visual Prompt for Universal Image\n  Restoration",
        "url": "http://arxiv.org/abs/2306.13653v1",
        "pub_date": "2023-06-23",
        "summary": "Image restoration aims to reconstruct degraded images, e.g., denoising or\ndeblurring. Existing works focus on designing task-specific methods and there\nare inadequate attempts at universal methods. However, simply unifying multiple\ntasks into one universal architecture suffers from uncontrollable and undesired\npredictions. To address those issues, we explore prompt learning in universal\narchitectures for image restoration tasks. In this paper, we present\nDegradation-aware Visual Prompts, which encode various types of image\ndegradation, e.g., noise and blur, into unified visual prompts. These\ndegradation-aware prompts provide control over image processing and allow\nweighted combinations for customized image restoration. We then leverage\ndegradation-aware visual prompts to establish a controllable and universal\nmodel for image restoration, called ProRes, which is applicable to an extensive\nrange of image restoration tasks. ProRes leverages the vanilla Vision\nTransformer (ViT) without any task-specific designs. Furthermore, the\npre-trained ProRes can easily adapt to new tasks through efficient prompt\ntuning with only a few images. Without bells and whistles, ProRes achieves\ncompetitive performance compared to task-specific methods and experiments can\ndemonstrate its ability for controllable restoration and adaptation for new\ntasks. The code and models will be released in\n\\url{https://github.com/leonmakise/ProRes}.",
        "translated": ""
    },
    {
        "title": "LightGlue: Local Feature Matching at Light Speed",
        "url": "http://arxiv.org/abs/2306.13643v1",
        "pub_date": "2023-06-23",
        "summary": "We introduce LightGlue, a deep neural network that learns to match local\nfeatures across images. We revisit multiple design decisions of SuperGlue, the\nstate of the art in sparse matching, and derive simple but effective\nimprovements. Cumulatively, they make LightGlue more efficient - in terms of\nboth memory and computation, more accurate, and much easier to train. One key\nproperty is that LightGlue is adaptive to the difficulty of the problem: the\ninference is much faster on image pairs that are intuitively easy to match, for\nexample because of a larger visual overlap or limited appearance change. This\nopens up exciting prospects for deploying deep matchers in latency-sensitive\napplications like 3D reconstruction. The code and trained models are publicly\navailable at https://github.com/cvg/LightGlue.",
        "translated": ""
    },
    {
        "title": "OpenMask3D: Open-Vocabulary 3D Instance Segmentation",
        "url": "http://arxiv.org/abs/2306.13631v1",
        "pub_date": "2023-06-23",
        "summary": "We introduce the task of open-vocabulary 3D instance segmentation.\nTraditional approaches for 3D instance segmentation largely rely on existing 3D\nannotated datasets, which are restricted to a closed-set of object categories.\nThis is an important limitation for real-life applications where one might need\nto perform tasks guided by novel, open-vocabulary queries related to objects\nfrom a wide variety. Recently, open-vocabulary 3D scene understanding methods\nhave emerged to address this problem by learning queryable features per each\npoint in the scene. While such a representation can be directly employed to\nperform semantic segmentation, existing methods have limitations in their\nability to identify object instances. In this work, we address this limitation,\nand propose OpenMask3D, which is a zero-shot approach for open-vocabulary 3D\ninstance segmentation. Guided by predicted class-agnostic 3D instance masks,\nour model aggregates per-mask features via multi-view fusion of CLIP-based\nimage embeddings. We conduct experiments and ablation studies on the ScanNet200\ndataset to evaluate the performance of OpenMask3D, and provide insights about\nthe open-vocabulary 3D instance segmentation task. We show that our approach\noutperforms other open-vocabulary counterparts, particularly on the long-tail\ndistribution. Furthermore, OpenMask3D goes beyond the limitations of\nclose-vocabulary approaches, and enables the segmentation of object instances\nbased on free-form queries describing object properties such as semantics,\ngeometry, affordances, and material properties.",
        "translated": ""
    },
    {
        "title": "Machine Learning methods for simulating particle response in the Zero\n  Degree Calorimeter at the ALICE experiment, CERN",
        "url": "http://arxiv.org/abs/2306.13606v1",
        "pub_date": "2023-06-23",
        "summary": "Currently, over half of the computing power at CERN GRID is used to run High\nEnergy Physics simulations. The recent updates at the Large Hadron Collider\n(LHC) create the need for developing more efficient simulation methods. In\nparticular, there exists a demand for a fast simulation of the neutron Zero\nDegree Calorimeter, where existing Monte Carlo-based methods impose a\nsignificant computational burden. We propose an alternative approach to the\nproblem that leverages machine learning. Our solution utilises neural network\nclassifiers and generative models to directly simulate the response of the\ncalorimeter. In particular, we examine the performance of variational\nautoencoders and generative adversarial networks, expanding the GAN\narchitecture by an additional regularisation network and a simple, yet\neffective postprocessing step. Our approach increases the simulation speed by 2\norders of magnitude while maintaining the high fidelity of the simulation.",
        "translated": ""
    },
    {
        "title": "A Semi-Paired Approach For Label-to-Image Translation",
        "url": "http://arxiv.org/abs/2306.13585v1",
        "pub_date": "2023-06-23",
        "summary": "Data efficiency, or the ability to generalize from a few labeled data,\nremains a major challenge in deep learning. Semi-supervised learning has\nthrived in traditional recognition tasks alleviating the need for large amounts\nof labeled data, yet it remains understudied in image-to-image translation\n(I2I) tasks. In this work, we introduce the first semi-supervised (semi-paired)\nframework for label-to-image translation, a challenging subtask of I2I which\ngenerates photorealistic images from semantic label maps. In the semi-paired\nsetting, the model has access to a small set of paired data and a larger set of\nunpaired images and labels. Instead of using geometrical transformations as a\npretext task like previous works, we leverage an input reconstruction task by\nexploiting the conditional discriminator on the paired data as a reverse\ngenerator. We propose a training algorithm for this shared network, and we\npresent a rare classes sampling algorithm to focus on under-represented\nclasses. Experiments on 3 standard benchmarks show that the proposed model\noutperforms state-of-the-art unsupervised and semi-supervised approaches, as\nwell as some fully supervised approaches while using a much smaller number of\npaired samples.",
        "translated": ""
    },
    {
        "title": "Penalty Gradient Normalization for Generative Adversarial Networks",
        "url": "http://arxiv.org/abs/2306.13576v1",
        "pub_date": "2023-06-23",
        "summary": "In this paper, we propose a novel normalization method called penalty\ngradient normalization (PGN) to tackle the training instability of Generative\nAdversarial Networks (GANs) caused by the sharp gradient space. Unlike existing\nwork such as gradient penalty and spectral normalization, the proposed PGN only\nimposes a penalty gradient norm constraint on the discriminator function, which\nincreases the capacity of the discriminator. Moreover, the proposed penalty\ngradient normalization can be applied to different GAN architectures with\nlittle modification. Extensive experiments on three datasets show that GANs\ntrained with penalty gradient normalization outperform existing methods in\nterms of both Frechet Inception and Distance and Inception Score.",
        "translated": ""
    },
    {
        "title": "The MI-Motion Dataset and Benchmark for 3D Multi-Person Motion\n  Prediction",
        "url": "http://arxiv.org/abs/2306.13566v1",
        "pub_date": "2023-06-23",
        "summary": "3D multi-person motion prediction is a challenging task that involves\nmodeling individual behaviors and interactions between people. Despite the\nemergence of approaches for this task, comparing them is difficult due to the\nlack of standardized training settings and benchmark datasets. In this paper,\nwe introduce the Multi-Person Interaction Motion (MI-Motion) Dataset, which\nincludes skeleton sequences of multiple individuals collected by motion capture\nsystems and refined and synthesized using a game engine. The dataset contains\n167k frames of interacting people's skeleton poses and is categorized into 5\ndifferent activity scenes. To facilitate research in multi-person motion\nprediction, we also provide benchmarks to evaluate the performance of\nprediction methods in three settings: short-term, long-term, and\nultra-long-term prediction. Additionally, we introduce a novel baseline\napproach that leverages graph and temporal convolutional networks, which has\ndemonstrated competitive results in multi-person motion prediction. We believe\nthat the proposed MI-Motion benchmark dataset and baseline will facilitate\nfuture research in this area, ultimately leading to better understanding and\nmodeling of multi-person interactions.",
        "translated": ""
    },
    {
        "title": "Estimating Residential Solar Potential Using Aerial Data",
        "url": "http://arxiv.org/abs/2306.13564v1",
        "pub_date": "2023-06-23",
        "summary": "Project Sunroof estimates the solar potential of residential buildings using\nhigh quality aerial data. That is, it estimates the potential solar energy (and\nassociated financial savings) that can be captured by buildings if solar panels\nwere to be installed on their roofs. Unfortunately its coverage is limited by\nthe lack of high resolution digital surface map (DSM) data. We present a deep\nlearning approach that bridges this gap by enhancing widely available\nlow-resolution data, thereby dramatically increasing the coverage of Sunroof.\nWe also present some ongoing efforts to potentially improve accuracy even\nfurther by replacing certain algorithmic components of the Sunroof processing\npipeline with deep learning.",
        "translated": ""
    },
    {
        "title": "FPGA Implementation of Convolutional Neural Network for Real-Time\n  Handwriting Recognition",
        "url": "http://arxiv.org/abs/2306.13557v2",
        "pub_date": "2023-06-23",
        "summary": "Machine Learning (ML) has recently been a skyrocketing field in Computer\nScience. As computer hardware engineers, we are enthusiastic about hardware\nimplementations of popular software ML architectures to optimize their\nperformance, reliability, and resource usage. In this project, we designed a\nhighly-configurable, real-time device for recognizing handwritten letters and\ndigits using an Altera DE1 FPGA Kit. We followed various engineering standards,\nincluding IEEE-754 32-bit Floating-Point Standard, Video Graphics Array (VGA)\ndisplay protocol, Universal Asynchronous Receiver-Transmitter (UART) protocol,\nand Inter-Integrated Circuit (I2C) protocols to achieve the project goals.\nThese significantly improved our design in compatibility, reusability, and\nsimplicity in verifications. Following these standards, we designed a 32-bit\nfloating-point (FP) instruction set architecture (ISA). We developed a 5-stage\nRISC processor in System Verilog to manage image processing, matrix\nmultiplications, ML classifications, and user interfaces. Three different ML\narchitectures were implemented and evaluated on our design: Linear\nClassification (LC), a 784-64-10 fully connected neural network (NN), and a\nLeNet-like Convolutional Neural Network (CNN) with ReLU activation layers and\n36 classes (10 for the digits and 26 for the case-insensitive letters). The\ntraining processes were done in Python scripts, and the resulting kernels and\nweights were stored in hex files and loaded into the FPGA's SRAM units.\nConvolution, pooling, data management, and various other ML features were\nguided by firmware in our custom assembly language. This paper documents the\nhigh-level design block diagrams, interfaces between each System Verilog\nmodule, implementation details of our software and firmware components, and\nfurther discussions on potential impacts.",
        "translated": ""
    },
    {
        "title": "A Survey on Multimodal Large Language Models",
        "url": "http://arxiv.org/abs/2306.13549v1",
        "pub_date": "2023-06-23",
        "summary": "Multimodal Large Language Model (MLLM) recently has been a new rising\nresearch hotspot, which uses powerful Large Language Models (LLMs) as a brain\nto perform multimodal tasks. The surprising emergent capabilities of MLLM, such\nas writing stories based on images and OCR-free math reasoning, are rare in\ntraditional methods, suggesting a potential path to artificial general\nintelligence. In this paper, we aim to trace and summarize the recent progress\nof MLLM. First of all, we present the formulation of MLLM and delineate its\nrelated concepts. Then, we discuss the key techniques and applications,\nincluding Multimodal Instruction Tuning (M-IT), Multimodal In-Context Learning\n(M-ICL), Multimodal Chain of Thought (M-CoT), and LLM-Aided Visual Reasoning\n(LAVR). Finally, we discuss existing challenges and point out promising\nresearch directions. In light of the fact that the era of MLLM has only just\nbegun, we will keep updating this survey and hope it can inspire more research.\nAn associated GitHub link collecting the latest papers is available at\nhttps://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models.",
        "translated": ""
    },
    {
        "title": "FunQA: Towards Surprising Video Comprehension",
        "url": "http://arxiv.org/abs/2306.14899v1",
        "pub_date": "2023-06-26",
        "summary": "Surprising videos, e.g., funny clips, creative performances, or visual\nillusions, attract significant attention. Enjoyment of these videos is not\nsimply a response to visual stimuli; rather, it hinges on the human capacity to\nunderstand (and appreciate) commonsense violations depicted in these videos. We\nintroduce FunQA, a challenging video question answering (QA) dataset\nspecifically designed to evaluate and enhance the depth of video reasoning\nbased on counter-intuitive and fun videos. Unlike most video QA benchmarks\nwhich focus on less surprising contexts, e.g., cooking or instructional videos,\nFunQA covers three previously unexplored types of surprising videos: 1)\nHumorQA, 2) CreativeQA, and 3) MagicQA. For each subset, we establish rigorous\nQA tasks designed to assess the model's capability in counter-intuitive\ntimestamp localization, detailed video description, and reasoning around\ncounter-intuitiveness. We also pose higher-level tasks, such as attributing a\nfitting and vivid title to the video, and scoring the video creativity. In\ntotal, the FunQA benchmark consists of 312K free-text QA pairs derived from\n4.3K video clips, spanning a total of 24 video hours. Extensive experiments\nwith existing VideoQA models reveal significant performance gaps for the FunQA\nvideos across spatial-temporal reasoning, visual-centered reasoning, and\nfree-text generation.",
        "translated": ""
    },
    {
        "title": "Large Multimodal Models: Notes on CVPR 2023 Tutorial",
        "url": "http://arxiv.org/abs/2306.14895v1",
        "pub_date": "2023-06-26",
        "summary": "This tutorial note summarizes the presentation on ``Large Multimodal Models:\nTowards Building and Surpassing Multimodal GPT-4'', a part of CVPR 2023\ntutorial on ``Recent Advances in Vision Foundation Models''. The tutorial\nconsists of three parts. We first introduce the background on recent GPT-like\nlarge models for vision-and-language modeling to motivate the research in\ninstruction-tuned large multimodal models (LMMs). As a pre-requisite, we\ndescribe the basics of instruction-tuning in large language models, which is\nfurther extended to the multimodal space. Lastly, we illustrate how to build\nthe minimum prototype of multimodal GPT-4 like models with the open-source\nresource, and review the recently emerged topics.",
        "translated": ""
    },
    {
        "title": "RVT: Robotic View Transformer for 3D Object Manipulation",
        "url": "http://arxiv.org/abs/2306.14896v1",
        "pub_date": "2023-06-26",
        "summary": "For 3D object manipulation, methods that build an explicit 3D representation\nperform better than those relying only on camera images. But using explicit 3D\nrepresentations like voxels comes at large computing cost, adversely affecting\nscalability. In this work, we propose RVT, a multi-view transformer for 3D\nmanipulation that is both scalable and accurate. Some key features of RVT are\nan attention mechanism to aggregate information across views and re-rendering\nof the camera input from virtual views around the robot workspace. In\nsimulations, we find that a single RVT model works well across 18 RLBench tasks\nwith 249 task variations, achieving 26% higher relative success than the\nexisting state-of-the-art method (PerAct). It also trains 36X faster than\nPerAct for achieving the same performance and achieves 2.3X the inference speed\nof PerAct. Further, RVT can perform a variety of manipulation tasks in the real\nworld with just a few ($\\sim$10) demonstrations per task. Visual results, code,\nand trained model are provided at https://robotic-view-transformer.github.io/.",
        "translated": ""
    },
    {
        "title": "Fuzzy-Conditioned Diffusion and Diffusion Projection Attention Applied\n  to Facial Image Correction",
        "url": "http://arxiv.org/abs/2306.14891v1",
        "pub_date": "2023-06-26",
        "summary": "Image diffusion has recently shown remarkable performance in image synthesis\nand implicitly as an image prior. Such a prior has been used with conditioning\nto solve the inpainting problem, but only supporting binary user-based\nconditioning. We derive a fuzzy-conditioned diffusion, where implicit diffusion\npriors can be exploited with controllable strength. Our fuzzy conditioning can\nbe applied pixel-wise, enabling the modification of different image components\nto varying degrees. Additionally, we propose an application to facial image\ncorrection, where we combine our fuzzy-conditioned diffusion with\ndiffusion-derived attention maps. Our map estimates the degree of anomaly, and\nwe obtain it by projecting on the diffusion space. We show how our approach\nalso leads to interpretable and autonomous facial image correction.",
        "translated": ""
    },
    {
        "title": "Domain-Scalable Unpaired Image Translation via Latent Space Anchoring",
        "url": "http://arxiv.org/abs/2306.14879v1",
        "pub_date": "2023-06-26",
        "summary": "Unpaired image-to-image translation (UNIT) aims to map images between two\nvisual domains without paired training data. However, given a UNIT model\ntrained on certain domains, it is difficult for current methods to incorporate\nnew domains because they often need to train the full model on both existing\nand new domains. To address this problem, we propose a new domain-scalable UNIT\nmethod, termed as latent space anchoring, which can be efficiently extended to\nnew visual domains and does not need to fine-tune encoders and decoders of\nexisting domains. Our method anchors images of different domains to the same\nlatent space of frozen GANs by learning lightweight encoder and regressor\nmodels to reconstruct single-domain images. In the inference phase, the learned\nencoders and decoders of different domains can be arbitrarily combined to\ntranslate images between any two domains without fine-tuning. Experiments on\nvarious datasets show that the proposed method achieves superior performance on\nboth standard and domain-scalable UNIT tasks in comparison with the\nstate-of-the-art methods.",
        "translated": ""
    },
    {
        "title": "Restart Sampling for Improving Generative Processes",
        "url": "http://arxiv.org/abs/2306.14878v1",
        "pub_date": "2023-06-26",
        "summary": "Generative processes that involve solving differential equations, such as\ndiffusion models, frequently necessitate balancing speed and quality. ODE-based\nsamplers are fast but plateau in performance while SDE-based samplers deliver\nhigher sample quality at the cost of increased sampling time. We attribute this\ndifference to sampling errors: ODE-samplers involve smaller discretization\nerrors while stochasticity in SDE contracts accumulated errors. Based on these\nfindings, we propose a novel sampling algorithm called Restart in order to\nbetter balance discretization errors and contraction. The sampling method\nalternates between adding substantial noise in additional forward steps and\nstrictly following a backward ODE. Empirically, Restart sampler surpasses\nprevious SDE and ODE samplers in both speed and accuracy. Restart not only\noutperforms the previous best SDE results, but also accelerates the sampling\nspeed by 10-fold / 2-fold on CIFAR-10 / ImageNet $64 \\times 64$. In addition,\nit attains significantly better sample quality than ODE samplers within\ncomparable sampling times. Moreover, Restart better balances text-image\nalignment/visual quality versus diversity than previous samplers in the\nlarge-scale text-to-image Stable Diffusion model pre-trained on LAION $512\n\\times 512$. Code is available at\nhttps://github.com/Newbeeer/diffusion_restart_sampling",
        "translated": ""
    },
    {
        "title": "A Fully Unsupervised Instance Segmentation Technique for White Blood\n  Cell Images",
        "url": "http://arxiv.org/abs/2306.14875v1",
        "pub_date": "2023-06-26",
        "summary": "White blood cells, also known as leukocytes are group of heterogeneously\nnucleated cells which act as salient immune system cells. These are originated\nin the bone marrow and are found in blood, plasma, and lymph tissues.\nLeukocytes kill the bacteria, virus and other kind of pathogens which invade\nhuman body through phagocytosis that in turn results immunity. Detection of a\nwhite blood cell count can reveal camouflaged infections and warn doctors about\nchronic medical conditions such as autoimmune diseases, immune deficiencies,\nand blood disorders. Segmentation plays an important role in identification of\nwhite blood cells (WBC) from microscopic image analysis. The goal of\nsegmentation in a microscopic image is to divide the image into different\ndistinct regions. In our paper, we tried to propose a novel instance\nsegmentation method for segmenting the WBCs containing both the nucleus and the\ncytoplasm, from bone marrow images.",
        "translated": ""
    },
    {
        "title": "ViNT: A Foundation Model for Visual Navigation",
        "url": "http://arxiv.org/abs/2306.14846v1",
        "pub_date": "2023-06-26",
        "summary": "General-purpose pre-trained models (\"foundation models\") have enabled\npractitioners to produce generalizable solutions for individual machine\nlearning problems with datasets that are significantly smaller than those\nrequired for learning from scratch. Such models are typically trained on large\nand diverse datasets with weak supervision, consuming much more training data\nthan is available for any individual downstream application. In this paper, we\ndescribe the Visual Navigation Transformer (ViNT), a foundation model that aims\nto bring the success of general-purpose pre-trained models to vision-based\nrobotic navigation. ViNT is trained with a general goal-reaching objective that\ncan be used with any navigation dataset, and employs a flexible\nTransformer-based architecture to learn navigational affordances and enable\nefficient adaptation to a variety of downstream navigational tasks. ViNT is\ntrained on a number of existing navigation datasets, comprising hundreds of\nhours of robotic navigation from a variety of different robotic platforms, and\nexhibits positive transfer, outperforming specialist models trained on singular\ndatasets. ViNT can be augmented with diffusion-based subgoal proposals to\nexplore novel environments, and can solve kilometer-scale navigation problems\nwhen equipped with long-range heuristics. ViNT can also be adapted to novel\ntask specifications with a technique inspired by prompt-tuning, where the goal\nencoder is replaced by an encoding of another task modality (e.g., GPS\nwaypoints or routing commands) embedded into the same space of goal tokens.\nThis flexibility and ability to accommodate a variety of downstream problem\ndomains establishes ViNT as an effective foundation model for mobile robotics.\nFor videos, code, and model checkpoints, see our project page at\nhttps://visualnav-transformer.github.io.",
        "translated": ""
    },
    {
        "title": "A Flyweight CNN with Adaptive Decoder for Schistosoma mansoni Egg\n  Detection",
        "url": "http://arxiv.org/abs/2306.14840v1",
        "pub_date": "2023-06-26",
        "summary": "Schistosomiasis mansoni is an endemic parasitic disease in more than seventy\ncountries, whose diagnosis is commonly performed by visually counting the\nparasite eggs in microscopy images of fecal samples. State-of-the-art (SOTA)\nobject detection algorithms are based on heavyweight neural networks,\nunsuitable for automating the diagnosis in the laboratory routine. We\ncircumvent the problem by presenting a flyweight Convolutional Neural Network\n(CNN) that weighs thousands of times less than SOTA object detectors. The\nkernels in our approach are learned layer-by-layer from attention regions\nindicated by user-drawn scribbles on very few training images. Representative\nkernels are visually identified and selected to improve performance with\nreduced computational cost. Another innovation is a single-layer adaptive\ndecoder whose convolutional weights are automatically defined for each image\non-the-fly. The experiments show that our CNN can outperform three SOTA\nbaselines according to five measures, being also suitable for CPU execution in\nthe laboratory routine, processing approximately four images a second for each\navailable thread.",
        "translated": ""
    },
    {
        "title": "Kosmos-2: Grounding Multimodal Large Language Models to the World",
        "url": "http://arxiv.org/abs/2306.14824v2",
        "pub_date": "2023-06-26",
        "summary": "We introduce Kosmos-2, a Multimodal Large Language Model (MLLM), enabling new\ncapabilities of perceiving object descriptions (e.g., bounding boxes) and\ngrounding text to the visual world. Specifically, we represent refer\nexpressions as links in Markdown, i.e., ``[text span](bounding boxes)'', where\nobject descriptions are sequences of location tokens. Together with multimodal\ncorpora, we construct large-scale data of grounded image-text pairs (called\nGrIT) to train the model. In addition to the existing capabilities of MLLMs\n(e.g., perceiving general modalities, following instructions, and performing\nin-context learning), Kosmos-2 integrates the grounding capability into\ndownstream applications. We evaluate Kosmos-2 on a wide range of tasks,\nincluding (i) multimodal grounding, such as referring expression comprehension,\nand phrase grounding, (ii) multimodal referring, such as referring expression\ngeneration, (iii) perception-language tasks, and (iv) language understanding\nand generation. This work lays out the foundation for the development of\nEmbodiment AI and sheds light on the big convergence of language, multimodal\nperception, action, and world modeling, which is a key step toward artificial\ngeneral intelligence. Data, demo, and pretrained models are available at\nhttps://aka.ms/kosmos-2.",
        "translated": ""
    },
    {
        "title": "Symphonize 3D Semantic Scene Completion with Contextual Instance Queries",
        "url": "http://arxiv.org/abs/2306.15670v1",
        "pub_date": "2023-06-27",
        "summary": "3D Semantic Scene Completion (SSC) has emerged as a nascent and pivotal task\nfor autonomous driving, as it involves predicting per-voxel occupancy within a\n3D scene from partial LiDAR or image inputs. Existing methods primarily focus\non the voxel-wise feature aggregation, while neglecting the instance-centric\nsemantics and broader context. In this paper, we present a novel paradigm\ntermed Symphonies (Scene-from-Insts) for SSC, which completes the scene volume\nfrom a sparse set of instance queries derived from the input with context\nawareness. By incorporating the queries as the instance feature representations\nwithin the scene, Symphonies dynamically encodes the instance-centric semantics\nto interact with the image and volume features while avoiding the dense\nvoxel-wise modeling. Simultaneously, it orchestrates a more comprehensive\nunderstanding of the scenario by capturing context throughout the entire scene,\ncontributing to alleviating the geometric ambiguity derived from occlusion and\nperspective errors. Symphonies achieves a state-of-the-art result of 13.02 mIoU\non the challenging SemanticKITTI dataset, outperforming existing methods and\nshowcasing the promising advancements of the paradigm. The code is available at\n\\url{https://github.com/hustvl/Symphonies}.",
        "translated": ""
    },
    {
        "title": "Detector-Free Structure from Motion",
        "url": "http://arxiv.org/abs/2306.15669v1",
        "pub_date": "2023-06-27",
        "summary": "We propose a new structure-from-motion framework to recover accurate camera\nposes and point clouds from unordered images. Traditional SfM systems typically\nrely on the successful detection of repeatable keypoints across multiple views\nas the first step, which is difficult for texture-poor scenes, and poor\nkeypoint detection may break down the whole SfM system. We propose a new\ndetector-free SfM framework to draw benefits from the recent success of\ndetector-free matchers to avoid the early determination of keypoints, while\nsolving the multi-view inconsistency issue of detector-free matchers.\nSpecifically, our framework first reconstructs a coarse SfM model from\nquantized detector-free matches. Then, it refines the model by a novel\niterative refinement pipeline, which iterates between an attention-based\nmulti-view matching module to refine feature tracks and a geometry refinement\nmodule to improve the reconstruction accuracy. Experiments demonstrate that the\nproposed framework outperforms existing detector-based SfM systems on common\nbenchmark datasets. We also collect a texture-poor SfM dataset to demonstrate\nthe capability of our framework to reconstruct texture-poor scenes. Based on\nthis framework, we take $\\textit{first place}$ in Image Matching Challenge\n2023.",
        "translated": ""
    },
    {
        "title": "Physion++: Evaluating Physical Scene Understanding that Requires Online\n  Inference of Different Physical Properties",
        "url": "http://arxiv.org/abs/2306.15668v1",
        "pub_date": "2023-06-27",
        "summary": "General physical scene understanding requires more than simply localizing and\nrecognizing objects -- it requires knowledge that objects can have different\nlatent properties (e.g., mass or elasticity), and that those properties affect\nthe outcome of physical events. While there has been great progress in physical\nand video prediction models in recent years, benchmarks to test their\nperformance typically do not require an understanding that objects have\nindividual physical properties, or at best test only those properties that are\ndirectly observable (e.g., size or color). This work proposes a novel dataset\nand benchmark, termed Physion++, that rigorously evaluates visual physical\nprediction in artificial systems under circumstances where those predictions\nrely on accurate estimates of the latent physical properties of objects in the\nscene. Specifically, we test scenarios where accurate prediction relies on\nestimates of properties such as mass, friction, elasticity, and deformability,\nand where the values of those properties can only be inferred by observing how\nobjects move and interact with other objects or fluids. We evaluate the\nperformance of a number of state-of-the-art prediction models that span a\nvariety of levels of learning vs. built-in knowledge, and compare that\nperformance to a set of human predictions. We find that models that have been\ntrained using standard regimes and datasets do not spontaneously learn to make\ninferences about latent properties, but also that models that encode objectness\nand physical states tend to make better predictions. However, there is still a\nhuge gap between all models and human performance, and all models' predictions\ncorrelate poorly with those made by humans, suggesting that no state-of-the-art\nmodel is learning to make physical predictions in a human-like way. Project\npage: https://dingmyu.github.io/physion_v2/",
        "translated": ""
    },
    {
        "title": "PoseDiffusion: Solving Pose Estimation via Diffusion-aided Bundle\n  Adjustment",
        "url": "http://arxiv.org/abs/2306.15667v2",
        "pub_date": "2023-06-27",
        "summary": "Camera pose estimation is a long-standing computer vision problem that to\ndate often relies on classical methods, such as handcrafted keypoint matching,\nRANSAC and bundle adjustment. In this paper, we propose to formulate the\nStructure from Motion (SfM) problem inside a probabilistic diffusion framework,\nmodelling the conditional distribution of camera poses given input images. This\nnovel view of an old problem has several advantages. (i) The nature of the\ndiffusion framework mirrors the iterative procedure of bundle adjustment. (ii)\nThe formulation allows a seamless integration of geometric constraints from\nepipolar geometry. (iii) It excels in typically difficult scenarios such as\nsparse views with wide baselines. (iv) The method can predict intrinsics and\nextrinsics for an arbitrary amount of images. We demonstrate that our method\nPoseDiffusion significantly improves over the classic SfM pipelines and the\nlearned approaches on two real-world datasets. Finally, it is observed that our\nmethod can generalize across datasets without further training. Project page:\nhttps://posediffusion.github.io/",
        "translated": ""
    },
    {
        "title": "Measured Albedo in the Wild: Filling the Gap in Intrinsics Evaluation",
        "url": "http://arxiv.org/abs/2306.15662v1",
        "pub_date": "2023-06-27",
        "summary": "Intrinsic image decomposition and inverse rendering are long-standing\nproblems in computer vision. To evaluate albedo recovery, most algorithms\nreport their quantitative performance with a mean Weighted Human Disagreement\nRate (WHDR) metric on the IIW dataset. However, WHDR focuses only on relative\nalbedo values and often fails to capture overall quality of the albedo. In\norder to comprehensively evaluate albedo, we collect a new dataset, Measured\nAlbedo in the Wild (MAW), and propose three new metrics that complement WHDR:\nintensity, chromaticity and texture metrics. We show that existing algorithms\noften improve WHDR metric but perform poorly on other metrics. We then finetune\ndifferent algorithms on our MAW dataset to significantly improve the quality of\nthe reconstructed albedo both quantitatively and qualitatively. Since the\nproposed intensity, chromaticity, and texture metrics and the WHDR are all\ncomplementary we further introduce a relative performance measure that captures\naverage performance. By analysing existing algorithms we show that there is\nsignificant room for improvement. Our dataset and evaluation metrics will\nenable researchers to develop algorithms that improve albedo reconstruction.\nCode and Data available at: https://measuredalbedo.github.io/",
        "translated": ""
    },
    {
        "title": "CLIPA-v2: Scaling CLIP Training with 81.1% Zero-shot ImageNet Accuracy\n  within a \\$10,000 Budget; An Extra \\$4,000 Unlocks 81.8% Accuracy",
        "url": "http://arxiv.org/abs/2306.15658v1",
        "pub_date": "2023-06-27",
        "summary": "The recent work CLIPA presents an inverse scaling law for CLIP training --\nwhereby the larger the image/text encoders used, the shorter the sequence\nlength of image/text tokens that can be applied in training. This finding\nenables us to train high-performance CLIP models with significantly reduced\ncomputations. Building upon this work, we hereby present CLIPA-v2 with two key\ncontributions. Technically, we find this inverse scaling law is also applicable\nin the finetuning stage, enabling further reduction in computational needs.\nEmpirically, we explore CLIPA at scale, extending the experiments up to the\nH/14 model with ~13B image-text pairs seen during training.\n  Our results are exciting -- by only allocating a budget of \\$10,000, our CLIP\nmodel achieves an impressive zero-shot ImageNet accuracy of 81.1%, surpassing\nthe prior best CLIP model (from OpenCLIP, 80.1%) by 1.0% and meanwhile reducing\nthe computational cost by ~39X. Moreover, with an additional investment of\n$4,000, we can further elevate the zero-shot ImageNet accuracy to 81.8%. Our\ncode and models are available at https://github.com/UCSC-VLAA/CLIPA.",
        "translated": ""
    },
    {
        "title": "Machine-learning based noise characterization and correction on neutral\n  atoms NISQ devices",
        "url": "http://arxiv.org/abs/2306.15628v1",
        "pub_date": "2023-06-27",
        "summary": "Neutral atoms devices represent a promising technology that uses optical\ntweezers to geometrically arrange atoms and modulated laser pulses to control\nthe quantum states. A neutral atoms Noisy Intermediate Scale Quantum (NISQ)\ndevice is developed by Pasqal with rubidium atoms that will allow to work with\nup to 100 qubits. All NISQ devices are affected by noise that have an impact on\nthe computations results. Therefore it is important to better understand and\ncharacterize the noise sources and possibly to correct them. Here, two\napproaches are proposed to characterize and correct noise parameters on neutral\natoms NISQ devices. In particular the focus is on Pasqal devices and Machine\nLearning (ML) techniques are adopted to pursue those objectives. To\ncharacterize the noise parameters, several ML models are trained, using as\ninput only the measurements of the final quantum state of the atoms, to predict\nlaser intensity fluctuation and waist, temperature and false positive and\nnegative measurement rate. Moreover, an analysis is provided with the scaling\non the number of atoms in the system and on the number of measurements used as\ninput. Also, we compare on real data the values predicted with ML with the a\npriori estimated parameters. Finally, a Reinforcement Learning (RL) framework\nis employed to design a pulse in order to correct the effect of the noise in\nthe measurements. It is expected that the analysis performed in this work will\nbe useful for a better understanding of the quantum dynamic in neutral atoms\ndevices and for the widespread adoption of this class of NISQ devices.",
        "translated": ""
    },
    {
        "title": "SCENEREPLICA: Benchmarking Real-World Robot Manipulation by Creating\n  Reproducible Scenes",
        "url": "http://arxiv.org/abs/2306.15620v1",
        "pub_date": "2023-06-27",
        "summary": "We present a new reproducible benchmark for evaluating robot manipulation in\nthe real world, specifically focusing on pick-and-place. Our benchmark uses the\nYCB objects, a commonly used dataset in the robotics community, to ensure that\nour results are comparable to other studies. Additionally, the benchmark is\ndesigned to be easily reproducible in the real world, making it accessible to\nresearchers and practitioners. We also provide our experimental results and\nanalyzes for model-based and model-free 6D robotic grasping on the benchmark,\nwhere representative algorithms are evaluated for object perception, grasping\nplanning, and motion planning. We believe that our benchmark will be a valuable\ntool for advancing the field of robot manipulation. By providing a standardized\nevaluation framework, researchers can more easily compare different techniques\nand algorithms, leading to faster progress in developing robot manipulation\nmethods.",
        "translated": ""
    },
    {
        "title": "Rethinking Cross-Entropy Loss for Stereo Matching Networks",
        "url": "http://arxiv.org/abs/2306.15612v1",
        "pub_date": "2023-06-27",
        "summary": "Despite the great success of deep learning in stereo matching, recovering\naccurate and clearly-contoured disparity map is still challenging. Currently,\nL1 loss and cross-entropy loss are the two most widely used loss functions for\ntraining the stereo matching networks. Comparing with the former, the latter\ncan usually achieve better results thanks to its direct constraint to the the\ncost volume. However, how to generate reasonable ground-truth distribution for\nthis loss function remains largely under exploited. Existing works assume\nuni-modal distributions around the ground-truth for all of the pixels, which\nignores the fact that the edge pixels may have multi-modal distributions. In\nthis paper, we first experimentally exhibit the importance of correct edge\nsupervision to the overall disparity accuracy. Then a novel adaptive\nmulti-modal cross-entropy loss which encourages the network to generate\ndifferent distribution patterns for edge and non-edge pixels is proposed. We\nfurther optimize the disparity estimator in the inference stage to alleviate\nthe bleeding and misalignment artifacts at the edge. Our method is generic and\ncan help classic stereo matching models regain competitive performance. GANet\ntrained by our loss ranks 1st on the KITTI 2015 and 2012 benchmarks and\noutperforms state-of-the-art methods by a large margin. Meanwhile, our method\nalso exhibits superior cross-domain generalization ability and outperforms\nexisting generalization-specialized methods on four popular real-world\ndatasets.",
        "translated": ""
    },
    {
        "title": "Recurrent Neural Network-coupled SPAD TCSPC System for Real-time\n  Fluorescence Lifetime Imaging",
        "url": "http://arxiv.org/abs/2306.15599v1",
        "pub_date": "2023-06-27",
        "summary": "Fluorescence lifetime imaging (FLI) has been receiving increased attention in\nrecent years as a powerful imaging technique in biological and medical\nresearch. However, existing FLI systems often suffer from a tradeoff between\nprocessing speed, accuracy, and robustness. In this paper, we propose a SPAD\nTCSPC system coupled to a recurrent neural network (RNN) for FLI that\naccurately estimates on the fly fluorescence lifetime directly from raw\ntimestamps instead of histograms, which drastically reduces the data transfer\nrate and hardware resource utilization. We train two variants of the RNN on a\nsynthetic dataset and compare the results to those obtained using the\ncenter-of-mass method (CMM) and least squares fitting (LS fitting) methods. The\nresults demonstrate that two RNN variants, gated recurrent unit (GRU) and long\nshort-term memory (LSTM), are comparable to CMM and LS fitting in terms of\naccuracy and outperform CMM and LS fitting by a large margin in the presence of\nbackground noise. We also look at the Cramer-Rao lower bound and detailed\nanalysis showed that the RNN models are close to the theoretical optima. The\nanalysis of experimental data shows that our model, which is purely trained on\nsynthetic datasets, works well on real-world data. We build a FLI microscope\nsetup for evaluation based on Piccolo, a 32$\\times$32 SPAD sensor developed in\nour lab. Four quantized GRU cores, capable of processing up to 4 million\nphotons per second, are deployed on a Xilinx Kintex-7 FPGA. Powered by the GRU,\nthe FLI setup can retrieve real-time fluorescence lifetime images at up to 10\nframes per second. The proposed FLI system is promising for many important\nbiomedical applications, ranging from biological imaging of fast-moving cells\nto fluorescence-assisted diagnosis and surgery.",
        "translated": ""
    },
    {
        "title": "On Practical Aspects of Aggregation Defenses against Data Poisoning\n  Attacks",
        "url": "http://arxiv.org/abs/2306.16415v1",
        "pub_date": "2023-06-28",
        "summary": "The increasing access to data poses both opportunities and risks in deep\nlearning, as one can manipulate the behaviors of deep learning models with\nmalicious training samples. Such attacks are known as data poisoning. Recent\nadvances in defense strategies against data poisoning have highlighted the\neffectiveness of aggregation schemes in achieving state-of-the-art results in\ncertified poisoning robustness. However, the practical implications of these\napproaches remain unclear. Here we focus on Deep Partition Aggregation, a\nrepresentative aggregation defense, and assess its practical aspects, including\nefficiency, performance, and robustness. For evaluations, we use ImageNet\nresized to a resolution of 64 by 64 to enable evaluations at a larger scale\nthan previous ones. Firstly, we demonstrate a simple yet practical approach to\nscaling base models, which improves the efficiency of training and inference\nfor aggregation defenses. Secondly, we provide empirical evidence supporting\nthe data-to-complexity ratio, i.e. the ratio between the data set size and\nsample complexity, as a practical estimation of the maximum number of base\nmodels that can be deployed while preserving accuracy. Last but not least, we\npoint out how aggregation defenses boost poisoning robustness empirically\nthrough the poisoning overfitting phenomenon, which is the key underlying\nmechanism for the empirical poisoning robustness of aggregations. Overall, our\nfindings provide valuable insights for practical implementations of aggregation\ndefenses to mitigate the threat of data poisoning.",
        "translated": ""
    },
    {
        "title": "MultiZoo &amp; MultiBench: A Standardized Toolkit for Multimodal Deep\n  Learning",
        "url": "http://arxiv.org/abs/2306.16413v1",
        "pub_date": "2023-06-28",
        "summary": "Learning multimodal representations involves integrating information from\nmultiple heterogeneous sources of data. In order to accelerate progress towards\nunderstudied modalities and tasks while ensuring real-world robustness, we\nrelease MultiZoo, a public toolkit consisting of standardized implementations\nof &gt; 20 core multimodal algorithms and MultiBench, a large-scale benchmark\nspanning 15 datasets, 10 modalities, 20 prediction tasks, and 6 research areas.\nTogether, these provide an automated end-to-end machine learning pipeline that\nsimplifies and standardizes data loading, experimental setup, and model\nevaluation. To enable holistic evaluation, we offer a comprehensive methodology\nto assess (1) generalization, (2) time and space complexity, and (3) modality\nrobustness. MultiBench paves the way towards a better understanding of the\ncapabilities and limitations of multimodal models, while ensuring ease of use,\naccessibility, and reproducibility. Our toolkits are publicly available, will\nbe regularly updated, and welcome inputs from the community.",
        "translated": ""
    },
    {
        "title": "Towards Language Models That Can See: Computer Vision Through the LENS\n  of Natural Language",
        "url": "http://arxiv.org/abs/2306.16410v1",
        "pub_date": "2023-06-28",
        "summary": "We propose LENS, a modular approach for tackling computer vision problems by\nleveraging the power of large language models (LLMs). Our system uses a\nlanguage model to reason over outputs from a set of independent and highly\ndescriptive vision modules that provide exhaustive information about an image.\nWe evaluate the approach on pure computer vision settings such as zero- and\nfew-shot object recognition, as well as on vision and language problems. LENS\ncan be applied to any off-the-shelf LLM and we find that the LLMs with LENS\nperform highly competitively with much bigger and much more sophisticated\nsystems, without any multimodal training whatsoever. We open-source our code at\nhttps://github.com/ContextualAI/lens and provide an interactive demo.",
        "translated": ""
    },
    {
        "title": "Theater Aid System for the Visually Impaired Through Transfer Learning\n  of Spatio-Temporal Graph Convolution Networks",
        "url": "http://arxiv.org/abs/2306.16357v1",
        "pub_date": "2023-06-28",
        "summary": "The aim of this research is to recognize human actions performed on stage to\naid visually impaired and blind individuals. To achieve this, we have created a\ntheatre human action recognition system that uses skeleton data captured by\ndepth image as input. We collected new samples of human actions in a theatre\nenvironment, and then tested the transfer learning technique with three\npre-trained Spatio-Temporal Graph Convolution Networks for skeleton-based human\naction recognition: the spatio-temporal graph convolution network, the\ntwo-stream adaptive graph convolution network, and the multi-scale disentangled\nunified graph convolution network. We selected the NTU-RGBD human action\nbenchmark as the source domain and used our collected dataset as the target\ndomain. We analyzed the transferability of the pre-trained models and proposed\ntwo configurations to apply and adapt the transfer learning technique to the\ndiversity between the source and target domains. The use of transfer learning\nhelped to improve the performance of the human action system within the context\nof theatre. The results indicate that Spatio-Temporal Graph Convolution\nNetworks is positively transferred, and there was an improvement in performance\ncompared to the baseline without transfer learning.",
        "translated": ""
    },
    {
        "title": "DiffComplete: Diffusion-based Generative 3D Shape Completion",
        "url": "http://arxiv.org/abs/2306.16329v1",
        "pub_date": "2023-06-28",
        "summary": "We introduce a new diffusion-based approach for shape completion on 3D range\nscans. Compared with prior deterministic and probabilistic methods, we strike a\nbalance between realism, multi-modality, and high fidelity. We propose\nDiffComplete by casting shape completion as a generative task conditioned on\nthe incomplete shape. Our key designs are two-fold. First, we devise a\nhierarchical feature aggregation mechanism to inject conditional features in a\nspatially-consistent manner. So, we can capture both local details and broader\ncontexts of the conditional inputs to control the shape completion. Second, we\npropose an occupancy-aware fusion strategy in our model to enable the\ncompletion of multiple partial shapes and introduce higher flexibility on the\ninput conditions. DiffComplete sets a new SOTA performance (e.g., 40% decrease\non l_1 error) on two large-scale 3D shape completion benchmarks. Our completed\nshapes not only have a realistic outlook compared with the deterministic\nmethods but also exhibit high similarity to the ground truths compared with the\nprobabilistic alternatives. Further, DiffComplete has strong generalizability\non objects of entirely unseen classes for both synthetic and real data,\neliminating the need for model re-training in various applications.",
        "translated": ""
    },
    {
        "title": "DoseDiff: Distance-aware Diffusion Model for Dose Prediction in\n  Radiotherapy",
        "url": "http://arxiv.org/abs/2306.16324v1",
        "pub_date": "2023-06-28",
        "summary": "Treatment planning is a critical component of the radiotherapy workflow,\ntypically carried out by a medical physicist using a time-consuming\ntrial-and-error manner. Previous studies have proposed knowledge-based or deep\nlearning-based methods for predicting dose distribution maps to assist medical\nphysicists in improving the efficiency of treatment planning. However, these\ndose prediction methods usuallylack the effective utilization of distance\ninformation between surrounding tissues andtargets or organs-at-risk (OARs).\nMoreover, they are poor in maintaining the distribution characteristics of ray\npaths in the predicted dose distribution maps, resulting in a loss of valuable\ninformation obtained by medical physicists. In this paper, we propose a\ndistance-aware diffusion model (DoseDiff) for precise prediction of dose\ndistribution. We define dose prediction as a sequence of denoising steps,\nwherein the predicted dose distribution map is generated with the conditions of\nthe CT image and signed distance maps (SDMs). The SDMs are obtained by a\ndistance transformation from the masks of targets or OARs, which provide the\ndistance information from each pixel in the image to the outline of the targets\nor OARs. Besides, we propose a multiencoder and multi-scale fusion network\n(MMFNet) that incorporates a multi-scale fusion and a transformer-based fusion\nmodule to enhance information fusion between the CT image and SDMs at the\nfeature level. Our model was evaluated on two datasets collected from patients\nwith breast cancer and nasopharyngeal cancer, respectively. The results\ndemonstrate that our DoseDiff outperforms the state-of-the-art dose prediction\nmethods in terms of both quantitative and visual quality.",
        "translated": ""
    },
    {
        "title": "Point2Point : A Framework for Efficient Deep Learning on Hilbert sorted\n  Point Clouds with applications in Spatio-Temporal Occupancy Prediction",
        "url": "http://arxiv.org/abs/2306.16306v1",
        "pub_date": "2023-06-28",
        "summary": "The irregularity and permutation invariance of point cloud data pose\nchallenges for effective learning. Conventional methods for addressing this\nissue involve converting raw point clouds to intermediate representations such\nas 3D voxel grids or range images. While such intermediate representations\nsolve the problem of permutation invariance, they can result in significant\nloss of information. Approaches that do learn on raw point clouds either have\ntrouble in resolving neighborhood relationships between points or are too\ncomplicated in their formulation. In this paper, we propose a novel approach to\nrepresenting point clouds as a locality preserving 1D ordering induced by the\nHilbert space-filling curve. We also introduce Point2Point, a neural\narchitecture that can effectively learn on Hilbert-sorted point clouds. We show\nthat Point2Point shows competitive performance on point cloud segmentation and\ngeneration tasks. Finally, we show the performance of Point2Point on\nSpatio-temporal Occupancy prediction from Point clouds.",
        "translated": ""
    },
    {
        "title": "Generalizing Surgical Instruments Segmentation to Unseen Domains with\n  One-to-Many Synthesis",
        "url": "http://arxiv.org/abs/2306.16285v1",
        "pub_date": "2023-06-28",
        "summary": "Despite their impressive performance in various surgical scene understanding\ntasks, deep learning-based methods are frequently hindered from deploying to\nreal-world surgical applications for various causes. Particularly, data\ncollection, annotation, and domain shift in-between sites and patients are the\nmost common obstacles. In this work, we mitigate data-related issues by\nefficiently leveraging minimal source images to generate synthetic surgical\ninstrument segmentation datasets and achieve outstanding generalization\nperformance on unseen real domains. Specifically, in our framework, only one\nbackground tissue image and at most three images of each foreground instrument\nare taken as the seed images. These source images are extensively transformed\nand employed to build up the foreground and background image pools, from which\nrandomly sampled tissue and instrument images are composed with multiple\nblending techniques to generate new surgical scene images. Besides, we\nintroduce hybrid training-time augmentations to diversify the training data\nfurther. Extensive evaluation on three real-world datasets, i.e., Endo2017,\nEndo2018, and RoboTool, demonstrates that our one-to-many synthetic surgical\ninstruments datasets generation and segmentation framework can achieve\nencouraging performance compared with training with real data. Notably, on the\nRoboTool dataset, where a more significant domain gap exists, our framework\nshows its superiority of generalization by a considerable margin. We expect\nthat our inspiring results will attract research attention to improving model\ngeneralization with data synthesizing.",
        "translated": ""
    },
    {
        "title": "RSPrompter: Learning to Prompt for Remote Sensing Instance Segmentation\n  based on Visual Foundation Model",
        "url": "http://arxiv.org/abs/2306.16269v1",
        "pub_date": "2023-06-28",
        "summary": "Leveraging vast training data (SA-1B), the foundation Segment Anything Model\n(SAM) proposed by Meta AI Research exhibits remarkable generalization and\nzero-shot capabilities. Nonetheless, as a category-agnostic instance\nsegmentation method, SAM heavily depends on prior manual guidance involving\npoints, boxes, and coarse-grained masks. Additionally, its performance on\nremote sensing image segmentation tasks has yet to be fully explored and\ndemonstrated. In this paper, we consider designing an automated instance\nsegmentation approach for remote sensing images based on the SAM foundation\nmodel, incorporating semantic category information. Inspired by prompt\nlearning, we propose a method to learn the generation of appropriate prompts\nfor SAM input. This enables SAM to produce semantically discernible\nsegmentation results for remote sensing images, which we refer to as\nRSPrompter. We also suggest several ongoing derivatives for instance\nsegmentation tasks, based on recent developments in the SAM community, and\ncompare their performance with RSPrompter. Extensive experimental results on\nthe WHU building, NWPU VHR-10, and SSDD datasets validate the efficacy of our\nproposed method. Our code is accessible at\n\\url{https://kyanchen.github.io/RSPrompter}.",
        "translated": ""
    },
    {
        "title": "Land Cover Segmentation with Sparse Annotations from Sentinel-2 Imagery",
        "url": "http://arxiv.org/abs/2306.16252v1",
        "pub_date": "2023-06-28",
        "summary": "Land cover (LC) segmentation plays a critical role in various applications,\nincluding environmental analysis and natural disaster management. However,\ngenerating accurate LC maps is a complex and time-consuming task that requires\nthe expertise of multiple annotators and regular updates to account for\nenvironmental changes. In this work, we introduce SPADA, a framework for fuel\nmap delineation that addresses the challenges associated with LC segmentation\nusing sparse annotations and domain adaptation techniques for semantic\nsegmentation. Performance evaluations using reliable ground truths, such as\nLUCAS and Urban Atlas, demonstrate the technique's effectiveness. SPADA\noutperforms state-of-the-art semantic segmentation approaches as well as\nthird-party products, achieving a mean Intersection over Union (IoU) score of\n42.86 and an F1 score of 67.93 on Urban Atlas and LUCAS, respectively.",
        "translated": ""
    },
    {
        "title": "An Efficient General-Purpose Modular Vision Model via Multi-Task\n  Heterogeneous Training",
        "url": "http://arxiv.org/abs/2306.17165v1",
        "pub_date": "2023-06-29",
        "summary": "We present a model that can perform multiple vision tasks and can be adapted\nto other downstream tasks efficiently. Despite considerable progress in\nmulti-task learning, most efforts focus on learning from multi-label data: a\nsingle image set with multiple task labels. Such multi-label data sets are\nrare, small, and expensive. We say heterogeneous to refer to image sets with\ndifferent task labels, or to combinations of single-task datasets. Few have\nexplored training on such heterogeneous datasets. General-purpose vision models\nare still dominated by single-task pretraining, and it remains unclear how to\nscale up multi-task models by leveraging mainstream vision datasets designed\nfor different purposes. The challenges lie in managing large intrinsic\ndifferences among vision tasks, including data distribution, architectures,\ntask-specific modules, dataset scales, and sampling strategies. To address\nthese challenges, we propose to modify and scale up mixture-of-experts (MoE)\nvision transformers, so that they can simultaneously learn classification,\ndetection, and segmentation on diverse mainstream vision datasets including\nImageNet, COCO, and ADE20K. Our approach achieves comparable results to\nsingle-task state-of-the-art models and demonstrates strong generalization on\ndownstream tasks. Due to its emergent modularity, this general-purpose model\ndecomposes into high-performing components, efficiently adapting to downstream\ntasks. We can fine-tune it with fewer training parameters, fewer model\nparameters, and less computation. Additionally, its modularity allows for easy\nexpansion in continual-learning-without-forgetting scenarios. Finally, these\nfunctions can be controlled and combined to meet various demands of downstream\ntasks.",
        "translated": ""
    },
    {
        "title": "Generate Anything Anywhere in Any Scene",
        "url": "http://arxiv.org/abs/2306.17154v1",
        "pub_date": "2023-06-29",
        "summary": "Text-to-image diffusion models have attracted considerable interest due to\ntheir wide applicability across diverse fields. However, challenges persist in\ncreating controllable models for personalized object generation. In this paper,\nwe first identify the entanglement issues in existing personalized generative\nmodels, and then propose a straightforward and efficient data augmentation\ntraining strategy that guides the diffusion model to focus solely on object\nidentity. By inserting the plug-and-play adapter layers from a pre-trained\ncontrollable diffusion model, our model obtains the ability to control the\nlocation and size of each generated personalized object. During inference, we\npropose a regionally-guided sampling technique to maintain the quality and\nfidelity of the generated images. Our method achieves comparable or superior\nfidelity for personalized objects, yielding a robust, versatile, and\ncontrollable text-to-image diffusion model that is capable of generating\nrealistic and personalized images. Our approach demonstrates significant\npotential for various applications, such as those in art, entertainment, and\nadvertising design.",
        "translated": ""
    },
    {
        "title": "Filtered-Guided Diffusion: Fast Filter Guidance for Black-Box Diffusion\n  Models",
        "url": "http://arxiv.org/abs/2306.17141v1",
        "pub_date": "2023-06-29",
        "summary": "Recent advances in diffusion-based generative models have shown incredible\npromise for Image-to-Image translation and editing. Most recent work in this\nspace relies on additional training or architecture-specific adjustments to the\ndiffusion process. In this work, we show that much of this low-level control\ncan be achieved without additional training or any access to features of the\ndiffusion model. Our method simply applies a filter to the input of each\ndiffusion step based on the output of the previous step in an adaptive manner.\nNotably, this approach does not depend on any specific architecture or sampler\nand can be done without access to internal features of the network, making it\neasy to combine with other techniques, samplers, and diffusion architectures.\nFurthermore, it has negligible cost to performance, and allows for more\ncontinuous adjustment of guidance strength than other approaches. We show FGD\noffers a fast and strong baseline that is competitive with recent\narchitecture-dependent approaches. Furthermore, FGD can also be used as a\nsimple add-on to enhance the structural guidance of other state-of-the-art I2I\nmethods. Finally, our derivation of this method helps to understand the impact\nof self attention, a key component of other recent architecture-specific I2I\napproaches, in a more architecture-independent way. Project page:\nhttps://github.com/jaclyngu/FilteredGuidedDiffusion",
        "translated": ""
    },
    {
        "title": "ID-Pose: Sparse-view Camera Pose Estimation by Inverting Diffusion\n  Models",
        "url": "http://arxiv.org/abs/2306.17140v1",
        "pub_date": "2023-06-29",
        "summary": "Given sparse views of an object, estimating their camera poses is a\nlong-standing and intractable problem. We harness the pre-trained diffusion\nmodel of novel views conditioned on viewpoints (Zero-1-to-3). We present\nID-Pose which inverses the denoising diffusion process to estimate the relative\npose given two input images. ID-Pose adds a noise on one image, and predicts\nthe noise conditioned on the other image and a decision variable for the pose.\nThe prediction error is used as the objective to find the optimal pose with the\ngradient descent method. ID-Pose can handle more than two images and estimate\neach of the poses with multiple image pairs from triangular relationships.\nID-Pose requires no training and generalizes to real-world images. We conduct\nexperiments using high-quality real-scanned 3D objects, where ID-Pose\nsignificantly outperforms state-of-the-art methods.",
        "translated": ""
    },
    {
        "title": "PVP: Personalized Video Prior for Editable Dynamic Portraits using\n  StyleGAN",
        "url": "http://arxiv.org/abs/2306.17123v1",
        "pub_date": "2023-06-29",
        "summary": "Portrait synthesis creates realistic digital avatars which enable users to\ninteract with others in a compelling way. Recent advances in StyleGAN and its\nextensions have shown promising results in synthesizing photorealistic and\naccurate reconstruction of human faces. However, previous methods often focus\non frontal face synthesis and most methods are not able to handle large head\nrotations due to the training data distribution of StyleGAN. In this work, our\ngoal is to take as input a monocular video of a face, and create an editable\ndynamic portrait able to handle extreme head poses. The user can create novel\nviewpoints, edit the appearance, and animate the face. Our method utilizes\npivotal tuning inversion (PTI) to learn a personalized video prior from a\nmonocular video sequence. Then we can input pose and expression coefficients to\nMLPs and manipulate the latent vectors to synthesize different viewpoints and\nexpressions of the subject. We also propose novel loss functions to further\ndisentangle pose and expression in the latent space. Our algorithm shows much\nbetter performance over previous approaches on monocular video datasets, and it\nis also capable of running in real-time at 54 FPS on an RTX 3080.",
        "translated": ""
    },
    {
        "title": "Learning Nuclei Representations with Masked Image Modelling",
        "url": "http://arxiv.org/abs/2306.17116v1",
        "pub_date": "2023-06-29",
        "summary": "Masked image modelling (MIM) is a powerful self-supervised representation\nlearning paradigm, whose potential has not been widely demonstrated in medical\nimage analysis. In this work, we show the capacity of MIM to capture rich\nsemantic representations of Haemotoxylin &amp; Eosin (H&amp;E)-stained images at the\nnuclear level. Inspired by Bidirectional Encoder representation from Image\nTransformers (BEiT), we split the images into smaller patches and generate\ncorresponding discrete visual tokens. In addition to the regular grid-based\npatches, typically used in visual Transformers, we introduce patches of\nindividual cell nuclei. We propose positional encoding of the irregular\ndistribution of these structures within an image. We pre-train the model in a\nself-supervised manner on H&amp;E-stained whole-slide images of diffuse large\nB-cell lymphoma, where cell nuclei have been segmented. The pre-training\nobjective is to recover the original discrete visual tokens of the masked image\non the one hand, and to reconstruct the visual tokens of the masked object\ninstances on the other. Coupling these two pre-training tasks allows us to\nbuild powerful, context-aware representations of nuclei. Our model generalizes\nwell and can be fine-tuned on downstream classification tasks, achieving\nimproved cell classification accuracy on PanNuke dataset by more than 5%\ncompared to current instance segmentation methods.",
        "translated": ""
    },
    {
        "title": "Michelangelo: Conditional 3D Shape Generation based on Shape-Image-Text\n  Aligned Latent Representation",
        "url": "http://arxiv.org/abs/2306.17115v1",
        "pub_date": "2023-06-29",
        "summary": "We present a novel alignment-before-generation approach to tackle the\nchallenging task of generating general 3D shapes based on 2D images or texts.\nDirectly learning a conditional generative model from images or texts to 3D\nshapes is prone to producing inconsistent results with the conditions because\n3D shapes have an additional dimension whose distribution significantly differs\nfrom that of 2D images and texts. To bridge the domain gap among the three\nmodalities and facilitate multi-modal-conditioned 3D shape generation, we\nexplore representing 3D shapes in a shape-image-text-aligned space. Our\nframework comprises two models: a Shape-Image-Text-Aligned Variational\nAuto-Encoder (SITA-VAE) and a conditional Aligned Shape Latent Diffusion Model\n(ASLDM). The former model encodes the 3D shapes into the shape latent space\naligned to the image and text and reconstructs the fine-grained 3D neural\nfields corresponding to given shape embeddings via the transformer-based\ndecoder. The latter model learns a probabilistic mapping function from the\nimage or text space to the latent shape space. Our extensive experiments\ndemonstrate that our proposed approach can generate higher-quality and more\ndiverse 3D shapes that better semantically conform to the visual or textural\nconditional inputs, validating the effectiveness of the\nshape-image-text-aligned space for cross-modality 3D shape generation.",
        "translated": ""
    },
    {
        "title": "LLaVAR: Enhanced Visual Instruction Tuning for Text-Rich Image\n  Understanding",
        "url": "http://arxiv.org/abs/2306.17107v1",
        "pub_date": "2023-06-29",
        "summary": "Instruction tuning unlocks the superior capability of Large Language Models\n(LLM) to interact with humans. Furthermore, recent instruction-following\ndatasets include images as visual inputs, collecting responses for image-based\ninstructions. However, visual instruction-tuned models cannot comprehend\ntextual details within images well. This work enhances the current visual\ninstruction tuning pipeline with text-rich images (e.g., movie posters, book\ncovers, etc.). Specifically, we first use publicly available OCR tools to\ncollect results on 422K text-rich images from the LAION dataset. Moreover, we\nprompt text-only GPT-4 with recognized texts and image captions to generate 16K\nconversations, each containing question-answer pairs for text-rich images. By\ncombining our collected data with previous multi-modal instruction-following\ndata, our model, LLaVAR, substantially improves the LLaVA model's capability on\ntext-based VQA datasets (up to 20% accuracy improvement) while achieving an\naccuracy of 91.42% on ScienceQA. The GPT-4-based instruction-following\nevaluation also demonstrates the improvement of our model on both natural\nimages and text-rich images. Through qualitative analysis, LLaVAR shows\npromising interaction (e.g., reasoning, writing, and elaboration) skills with\nhumans based on the latest real-world online content that combines text and\nimages. We make our code/data/models publicly available at\nhttps://llavar.github.io/.",
        "translated": ""
    },
    {
        "title": "Deep Ensemble for Rotorcraft Attitude Prediction",
        "url": "http://arxiv.org/abs/2306.17104v1",
        "pub_date": "2023-06-29",
        "summary": "Historically, the rotorcraft community has experienced a higher fatal\naccident rate than other aviation segments, including commercial and general\naviation. Recent advancements in artificial intelligence (AI) and the\napplication of these technologies in different areas of our lives are both\nintriguing and encouraging. When developed appropriately for the aviation\ndomain, AI techniques provide an opportunity to help design systems that can\naddress rotorcraft safety challenges. Our recent work demonstrated that AI\nalgorithms could use video data from onboard cameras and correctly identify\ndifferent flight parameters from cockpit gauges, e.g., indicated airspeed.\nThese AI-based techniques provide a potentially cost-effective solution,\nespecially for small helicopter operators, to record the flight state\ninformation and perform post-flight analyses. We also showed that carefully\ndesigned and trained AI systems could accurately predict rotorcraft attitude\n(i.e., pitch and yaw) from outside scenes (images or video data). Ordinary\noff-the-shelf video cameras were installed inside the rotorcraft cockpit to\nrecord the outside scene, including the horizon. The AI algorithm could\ncorrectly identify rotorcraft attitude at an accuracy in the range of 80\\%. In\nthis work, we combined five different onboard camera viewpoints to improve\nattitude prediction accuracy to 94\\%. In this paper, five onboard camera views\nincluded the pilot windshield, co-pilot windshield, pilot Electronic Flight\nInstrument System (EFIS) display, co-pilot EFIS display, and the attitude\nindicator gauge. Using video data from each camera view, we trained various\nconvolutional neural networks (CNNs), which achieved prediction accuracy in the\nrange of 79\\% % to 90\\% %. We subsequently ensembled the learned knowledge from\nall CNNs and achieved an ensembled accuracy of 93.3\\%.",
        "translated": ""
    },
    {
        "title": "The Importance of Robust Features in Mitigating Catastrophic Forgetting",
        "url": "http://arxiv.org/abs/2306.17091v1",
        "pub_date": "2023-06-29",
        "summary": "Continual learning (CL) is an approach to address catastrophic forgetting,\nwhich refers to forgetting previously learned knowledge by neural networks when\ntrained on new tasks or data distributions. The adversarial robustness has\ndecomposed features into robust and non-robust types and demonstrated that\nmodels trained on robust features significantly enhance adversarial robustness.\nHowever, no study has been conducted on the efficacy of robust features from\nthe lens of the CL model in mitigating catastrophic forgetting in CL. In this\npaper, we introduce the CL robust dataset and train four baseline models on\nboth the standard and CL robust datasets. Our results demonstrate that the CL\nmodels trained on the CL robust dataset experienced less catastrophic\nforgetting of the previously learned tasks than when trained on the standard\ndataset. Our observations highlight the significance of the features provided\nto the underlying CL models, showing that CL robust features can alleviate\ncatastrophic forgetting.",
        "translated": ""
    },
    {
        "title": "Hardwiring ViT Patch Selectivity into CNNs using Patch Mixing",
        "url": "http://arxiv.org/abs/2306.17848v1",
        "pub_date": "2023-06-30",
        "summary": "Vision transformers (ViTs) have significantly changed the computer vision\nlandscape and have periodically exhibited superior performance in vision tasks\ncompared to convolutional neural networks (CNNs). Although the jury is still\nout on which model type is superior, each has unique inductive biases that\nshape their learning and generalization performance. For example, ViTs have\ninteresting properties with respect to early layer non-local feature\ndependence, as well as self-attention mechanisms which enhance learning\nflexibility, enabling them to ignore out-of-context image information more\neffectively. We hypothesize that this power to ignore out-of-context\ninformation (which we name $\\textit{patch selectivity}$), while integrating\nin-context information in a non-local manner in early layers, allows ViTs to\nmore easily handle occlusion. In this study, our aim is to see whether we can\nhave CNNs $\\textit{simulate}$ this ability of patch selectivity by effectively\nhardwiring this inductive bias using Patch Mixing data augmentation, which\nconsists of inserting patches from another image onto a training image and\ninterpolating labels between the two image classes. Specifically, we use Patch\nMixing to train state-of-the-art ViTs and CNNs, assessing its impact on their\nability to ignore out-of-context patches and handle natural occlusions. We find\nthat ViTs do not improve nor degrade when trained using Patch Mixing, but CNNs\nacquire new capabilities to ignore out-of-context information and improve on\nocclusion benchmarks, leaving us to conclude that this training method is a way\nof simulating in CNNs the abilities that ViTs already possess. We will release\nour Patch Mixing implementation and proposed datasets for public use. Project\npage: https://arielnlee.github.io/PatchMixing/",
        "translated": ""
    },
    {
        "title": "Magic123: One Image to High-Quality 3D Object Generation Using Both 2D\n  and 3D Diffusion Priors",
        "url": "http://arxiv.org/abs/2306.17843v1",
        "pub_date": "2023-06-30",
        "summary": "We present Magic123, a two-stage coarse-to-fine approach for high-quality,\ntextured 3D meshes generation from a single unposed image in the wild using\nboth2D and 3D priors. In the first stage, we optimize a neural radiance field\nto produce a coarse geometry. In the second stage, we adopt a memory-efficient\ndifferentiable mesh representation to yield a high-resolution mesh with a\nvisually appealing texture. In both stages, the 3D content is learned through\nreference view supervision and novel views guided by a combination of 2D and 3D\ndiffusion priors. We introduce a single trade-off parameter between the 2D and\n3D priors to control exploration (more imaginative) and exploitation (more\nprecise) of the generated geometry. Additionally, we employ textual inversion\nand monocular depth regularization to encourage consistent appearances across\nviews and to prevent degenerate solutions, respectively. Magic123 demonstrates\na significant improvement over previous image-to-3D techniques, as validated\nthrough extensive experiments on synthetic benchmarks and diverse real-world\nimages. Our code, models, and generated 3D assets are available at\nhttps://github.com/guochengqian/Magic123.",
        "translated": ""
    },
    {
        "title": "SPAE: Semantic Pyramid AutoEncoder for Multimodal Generation with Frozen\n  LLMs",
        "url": "http://arxiv.org/abs/2306.17842v2",
        "pub_date": "2023-06-30",
        "summary": "In this work, we introduce Semantic Pyramid AutoEncoder (SPAE) for enabling\nfrozen LLMs to perform both understanding and generation tasks involving\nnon-linguistic modalities such as images or videos. SPAE converts between raw\npixels and interpretable lexical tokens (or words) extracted from the LLM's\nvocabulary. The resulting tokens capture both the semantic meaning and the\nfine-grained details needed for visual reconstruction, effectively translating\nthe visual content into a language comprehensible to the LLM, and empowering it\nto perform a wide array of multimodal tasks. Our approach is validated through\nin-context learning experiments with frozen PaLM 2 and GPT 3.5 on a diverse set\nof image understanding and generation tasks. Our method marks the first\nsuccessful attempt to enable a frozen LLM to generate image content while\nsurpassing state-of-the-art performance in image understanding tasks, under the\nsame setting, by over 25%.",
        "translated": ""
    },
    {
        "title": "Federated Ensemble YOLOv5 - A Better Generalized Object Detection\n  Algorithm",
        "url": "http://arxiv.org/abs/2306.17829v1",
        "pub_date": "2023-06-30",
        "summary": "Federated learning (FL) has gained significant traction as a\nprivacy-preserving algorithm, but the underlying resembles of federated\nlearning algorithm like Federated averaging (FED Avg) or Federated SGD (FED\nSGD) to ensemble learning algorithms has not been fully explored. The purpose\nof this paper is to examine the application of FL to object detection as a\nmethod to enhance generalizability, and to compare its performance against a\ncentralized training approach for an object detection algorithm. Specifically,\nwe investigate the performance of a YOLOv5 model trained using FL across\nmultiple clients and employ a random sampling strategy without replacement, so\neach client holds a portion of the same dataset used for centralized training.\nOur experimental results showcase the superior efficiency of the FL object\ndetector's global model in generating accurate bounding boxes for unseen\nobjects, with the test set being a mixture of objects from two distinct clients\nnot represented in the training dataset. These findings suggest that FL can be\nviewed from an ensemble algorithm perspective, akin to a synergistic blend of\nBagging and Boosting techniques. As a result, FL can be seen not only as a\nmethod to enhance privacy, but also as a method to enhance the performance of a\nmachine learning model.",
        "translated": ""
    },
    {
        "title": "Stay on topic with Classifier-Free Guidance",
        "url": "http://arxiv.org/abs/2306.17806v1",
        "pub_date": "2023-06-30",
        "summary": "Classifier-Free Guidance (CFG) has recently emerged in text-to-image\ngeneration as a lightweight technique to encourage prompt-adherence in\ngenerations. In this work, we demonstrate that CFG can be used broadly as an\ninference-time technique in pure language modeling. We show that CFG (1)\nimproves the performance of Pythia, GPT-2 and LLaMA-family models across an\narray of tasks: Q\\&amp;A, reasoning, code generation, and machine translation,\nachieving SOTA on LAMBADA with LLaMA-7B over PaLM-540B; (2) brings improvements\nequivalent to a model with twice the parameter-count; (3) can stack alongside\nother inference-time methods like Chain-of-Thought and Self-Consistency,\nyielding further improvements in difficult tasks; (4) can be used to increase\nthe faithfulness and coherence of assistants in challenging form-driven and\ncontent-driven prompts: in a human evaluation we show a 75\\% preference for\nGPT4All using CFG over baseline.",
        "translated": ""
    },
    {
        "title": "Look, Remember and Reason: Visual Reasoning with Grounded Rationales",
        "url": "http://arxiv.org/abs/2306.17778v1",
        "pub_date": "2023-06-30",
        "summary": "Large language models have recently shown human level performance on a\nvariety of reasoning tasks. However, the ability of these models to perform\ncomplex visual reasoning has not been studied in detail yet. A key challenge in\nmany visual reasoning tasks is that the visual information needs to be tightly\nintegrated in the reasoning process. We propose to address this challenge by\ndrawing inspiration from human visual problem solving which depends on a\nvariety of low-level visual capabilities. It can often be cast as the three\nstep-process of ``Look, Remember, Reason'': visual information is incrementally\nextracted using low-level visual routines in a step-by-step fashion until a\nfinal answer is reached. We follow the same paradigm to enable existing large\nlanguage models, with minimal changes to the architecture, to solve visual\nreasoning problems. To this end, we introduce rationales over the visual input\nthat allow us to integrate low-level visual capabilities, such as object\nrecognition and tracking, as surrogate tasks. We show competitive performance\non diverse visual reasoning tasks from the CLEVR, CATER, and ACRE datasets over\nstate-of-the-art models designed specifically for these tasks.",
        "translated": ""
    },
    {
        "title": "MTR++: Multi-Agent Motion Prediction with Symmetric Scene Modeling and\n  Guided Intention Querying",
        "url": "http://arxiv.org/abs/2306.17770v1",
        "pub_date": "2023-06-30",
        "summary": "Motion prediction is crucial for autonomous driving systems to understand\ncomplex driving scenarios and make informed decisions. However, this task is\nchallenging due to the diverse behaviors of traffic participants and complex\nenvironmental contexts. In this paper, we propose Motion TRansformer (MTR)\nframeworks to address these challenges. The initial MTR framework utilizes a\ntransformer encoder-decoder structure with learnable intention queries,\nenabling efficient and accurate prediction of future trajectories. By\ncustomizing intention queries for distinct motion modalities, MTR improves\nmultimodal motion prediction while reducing reliance on dense goal candidates.\nThe framework comprises two essential processes: global intention localization,\nidentifying the agent's intent to enhance overall efficiency, and local\nmovement refinement, adaptively refining predicted trajectories for improved\naccuracy. Moreover, we introduce an advanced MTR++ framework, extending the\ncapability of MTR to simultaneously predict multimodal motion for multiple\nagents. MTR++ incorporates symmetric context modeling and mutually-guided\nintention querying modules to facilitate future behavior interaction among\nmultiple agents, resulting in scene-compliant future trajectories. Extensive\nexperimental results demonstrate that the MTR framework achieves\nstate-of-the-art performance on the highly-competitive motion prediction\nbenchmarks, while the MTR++ framework surpasses its precursor, exhibiting\nenhanced performance and efficiency in predicting accurate multimodal future\ntrajectories for multiple agents.",
        "translated": ""
    },
    {
        "title": "FlipNeRF: Flipped Reflection Rays for Few-shot Novel View Synthesis",
        "url": "http://arxiv.org/abs/2306.17723v1",
        "pub_date": "2023-06-30",
        "summary": "Neural Radiance Field (NeRF) has been a mainstream in novel view synthesis\nwith its remarkable quality of rendered images and simple architecture.\nAlthough NeRF has been developed in various directions improving continuously\nits performance, the necessity of a dense set of multi-view images still exists\nas a stumbling block to progress for practical application. In this work, we\npropose FlipNeRF, a novel regularization method for few-shot novel view\nsynthesis by utilizing our proposed flipped reflection rays. The flipped\nreflection rays are explicitly derived from the input ray directions and\nestimated normal vectors, and play a role of effective additional training rays\nwhile enabling to estimate more accurate surface normals and learn the 3D\ngeometry effectively. Since the surface normal and the scene depth are both\nderived from the estimated densities along a ray, the accurate surface normal\nleads to more exact depth estimation, which is a key factor for few-shot novel\nview synthesis. Furthermore, with our proposed Uncertainty-aware Emptiness Loss\nand Bottleneck Feature Consistency Loss, FlipNeRF is able to estimate more\nreliable outputs with reducing floating artifacts effectively across the\ndifferent scene structures, and enhance the feature-level consistency between\nthe pair of the rays cast toward the photo-consistent pixels without any\nadditional feature extractor, respectively. Our FlipNeRF achieves the SOTA\nperformance on the multiple benchmarks across all the scenarios.",
        "translated": ""
    },
    {
        "title": "Exploration and Exploitation of Unlabeled Data for Open-Set\n  Semi-Supervised Learning",
        "url": "http://arxiv.org/abs/2306.17699v1",
        "pub_date": "2023-06-30",
        "summary": "In this paper, we address a complex but practical scenario in semi-supervised\nlearning (SSL) named open-set SSL, where unlabeled data contain both\nin-distribution (ID) and out-of-distribution (OOD) samples. Unlike previous\nmethods that only consider ID samples to be useful and aim to filter out OOD\nones completely during training, we argue that the exploration and exploitation\nof both ID and OOD samples can benefit SSL. To support our claim, i) we propose\na prototype-based clustering and identification algorithm that explores the\ninherent similarity and difference among samples at feature level and\neffectively cluster them around several predefined ID and OOD prototypes,\nthereby enhancing feature learning and facilitating ID/OOD identification; ii)\nwe propose an importance-based sampling method that exploits the difference in\nimportance of each ID and OOD sample to SSL, thereby reducing the sampling bias\nand improving the training. Our proposed method achieves state-of-the-art in\nseveral challenging benchmarks, and improves upon existing SSL methods even\nwhen ID samples are totally absent in unlabeled data.",
        "translated": ""
    },
    {
        "title": "Multimodal Prompt Retrieval for Generative Visual Question Answering",
        "url": "http://arxiv.org/abs/2306.17675v1",
        "pub_date": "2023-06-30",
        "summary": "Recent years have witnessed impressive results of pre-trained vision-language\nmodels on knowledge-intensive tasks such as visual question answering (VQA).\nDespite the recent advances in VQA, existing methods mainly adopt a\ndiscriminative formulation that predicts answers within a pre-defined label\nset, leading to easy overfitting on low-resource domains with limited labeled\ndata (e.g., medicine) and poor generalization under domain shift to another\ndataset. To tackle this limitation, we propose a novel generative model\nenhanced by multimodal prompt retrieval (MPR) that integrates retrieved prompts\nand multimodal features to generate answers in free text. Our generative model\nenables rapid zero-shot dataset adaptation to unseen data distributions and\nopen-set answer labels across datasets. Our experiments on medical VQA tasks\nshow that MPR outperforms its non-retrieval counterpart by up to 30% accuracy\npoints in a few-shot domain adaptation setting.",
        "translated": ""
    },
    {
        "title": "Real-time Monocular Full-body Capture in World Space via Sequential\n  Proxy-to-Motion Learning",
        "url": "http://arxiv.org/abs/2307.01200v1",
        "pub_date": "2023-07-03",
        "summary": "Learning-based approaches to monocular motion capture have recently shown\npromising results by learning to regress in a data-driven manner. However, due\nto the challenges in data collection and network designs, it remains\nchallenging for existing solutions to achieve real-time full-body capture while\nbeing accurate in world space. In this work, we contribute a sequential\nproxy-to-motion learning scheme together with a proxy dataset of 2D skeleton\nsequences and 3D rotational motions in world space. Such proxy data enables us\nto build a learning-based network with accurate full-body supervision while\nalso mitigating the generalization issues. For more accurate and physically\nplausible predictions, a contact-aware neural motion descent module is proposed\nin our network so that it can be aware of foot-ground contact and motion\nmisalignment with the proxy observations. Additionally, we share the body-hand\ncontext information in our network for more compatible wrist poses recovery\nwith the full-body model. With the proposed learning-based solution, we\ndemonstrate the first real-time monocular full-body capture system with\nplausible foot-ground contact in world space. More video results can be found\nat our project page: https://liuyebin.com/proxycap.",
        "translated": ""
    },
    {
        "title": "NeuBTF: Neural fields for BTF encoding and transfer",
        "url": "http://arxiv.org/abs/2307.01199v1",
        "pub_date": "2023-07-03",
        "summary": "Neural material representations are becoming a popular way to represent\nmaterials for rendering. They are more expressive than analytic models and\noccupy less memory than tabulated BTFs. However, existing neural materials are\nimmutable, meaning that their output for a certain query of UVs, camera, and\nlight vector is fixed once they are trained. While this is practical when there\nis no need to edit the material, it can become very limiting when the fragment\nof the material used for training is too small or not tileable, which\nfrequently happens when the material has been captured with a\ngonioreflectometer. In this paper, we propose a novel neural material\nrepresentation which jointly tackles the problems of BTF compression, tiling,\nand extrapolation. At test time, our method uses a guidance image as input to\ncondition the neural BTF to the structural features of this input image. Then,\nthe neural BTF can be queried as a regular BTF using UVs, camera, and light\nvectors. Every component in our framework is purposefully designed to maximize\nBTF encoding quality at minimal parameter count and computational complexity,\nachieving competitive compression rates compared with previous work. We\ndemonstrate the results of our method on a variety of synthetic and captured\nmaterials, showing its generality and capacity to learn to represent many\noptical properties.",
        "translated": ""
    },
    {
        "title": "Segment Anything Meets Point Tracking",
        "url": "http://arxiv.org/abs/2307.01197v1",
        "pub_date": "2023-07-03",
        "summary": "The Segment Anything Model (SAM) has established itself as a powerful\nzero-shot image segmentation model, employing interactive prompts such as\npoints to generate masks. This paper presents SAM-PT, a method extending SAM's\ncapability to tracking and segmenting anything in dynamic videos. SAM-PT\nleverages robust and sparse point selection and propagation techniques for mask\ngeneration, demonstrating that a SAM-based segmentation tracker can yield\nstrong zero-shot performance across popular video object segmentation\nbenchmarks, including DAVIS, YouTube-VOS, and MOSE. Compared to traditional\nobject-centric mask propagation strategies, we uniquely use point propagation\nto exploit local structure information that is agnostic to object semantics. We\nhighlight the merits of point-based tracking through direct evaluation on the\nzero-shot open-world Unidentified Video Objects (UVO) benchmark. To further\nenhance our approach, we utilize K-Medoids clustering for point initialization\nand track both positive and negative points to clearly distinguish the target\nobject. We also employ multiple mask decoding passes for mask refinement and\ndevise a point re-initialization strategy to improve tracking accuracy. Our\ncode integrates different point trackers and video segmentation benchmarks and\nwill be released at https://github.com/SysCV/sam-pt.",
        "translated": ""
    },
    {
        "title": "SAMAug: Point Prompt Augmentation for Segment Anything Model",
        "url": "http://arxiv.org/abs/2307.01187v1",
        "pub_date": "2023-07-03",
        "summary": "This paper introduces SAMAug, a novel visual point augmentation method for\nthe Segment Anything Model (SAM) that enhances interactive image segmentation\nperformance. SAMAug generates augmented point prompts to provide more\ninformation to SAM. From the initial point prompt, SAM produces the initial\nmask, which is then fed into our proposed SAMAug to generate augmented point\nprompts. By incorporating these extra points, SAM can generate augmented\nsegmentation masks based on the augmented point prompts and the initial prompt,\nresulting in improved segmentation performance. We evaluate four point\naugmentation techniques: random selection, maximum difference entropy, maximum\ndistance, and a saliency model. Experiments on the COCO, Fundus, and Chest\nX-ray datasets demonstrate that SAMAug can boost SAM's segmentation results,\nespecially using the maximum distance and saliency model methods. SAMAug\nunderscores the potential of visual prompt engineering to advance interactive\ncomputer vision models.",
        "translated": ""
    },
    {
        "title": "Investigating Data Memorization in 3D Latent Diffusion Models for\n  Medical Image Synthesis",
        "url": "http://arxiv.org/abs/2307.01148v1",
        "pub_date": "2023-07-03",
        "summary": "Generative latent diffusion models have been established as state-of-the-art\nin data generation. One promising application is generation of realistic\nsynthetic medical imaging data for open data sharing without compromising\npatient privacy. Despite the promise, the capacity of such models to memorize\nsensitive patient training data and synthesize samples showing high resemblance\nto training data samples is relatively unexplored. Here, we assess the\nmemorization capacity of 3D latent diffusion models on photon-counting coronary\ncomputed tomography angiography and knee magnetic resonance imaging datasets.\nTo detect potential memorization of training samples, we utilize\nself-supervised models based on contrastive learning. Our results suggest that\nsuch latent diffusion models indeed memorize training data, and there is a dire\nneed for devising strategies to mitigate memorization.",
        "translated": ""
    },
    {
        "title": "AVSegFormer: Audio-Visual Segmentation with Transformer",
        "url": "http://arxiv.org/abs/2307.01146v2",
        "pub_date": "2023-07-03",
        "summary": "The combination of audio and vision has long been a topic of interest in the\nmulti-modal community. Recently, a new audio-visual segmentation (AVS) task has\nbeen introduced, aiming to locate and segment the sounding objects in a given\nvideo. This task demands audio-driven pixel-level scene understanding for the\nfirst time, posing significant challenges. In this paper, we propose\nAVSegFormer, a novel framework for AVS tasks that leverages the transformer\narchitecture. Specifically, we introduce audio queries and learnable queries\ninto the transformer decoder, enabling the network to selectively attend to\ninterested visual features. Besides, we present an audio-visual mixer, which\ncan dynamically adjust visual features by amplifying relevant and suppressing\nirrelevant spatial channels. Additionally, we devise an intermediate mask loss\nto enhance the supervision of the decoder, encouraging the network to produce\nmore accurate intermediate predictions. Extensive experiments demonstrate that\nAVSegFormer achieves state-of-the-art results on the AVS benchmark. The code is\navailable at https://github.com/vvvb-github/AVSegFormer.",
        "translated": ""
    },
    {
        "title": "SCITUNE: Aligning Large Language Models with Scientific Multimodal\n  Instructions",
        "url": "http://arxiv.org/abs/2307.01139v1",
        "pub_date": "2023-07-03",
        "summary": "Instruction finetuning is a popular paradigm to align large language models\n(LLM) with human intent. Despite its popularity, this idea is less explored in\nimproving the LLMs to align existing foundation models with scientific\ndisciplines, concepts and goals. In this work, we present SciTune as a tuning\nframework to improve the ability of LLMs to follow scientific multimodal\ninstructions. To test our methodology, we use a human-generated scientific\ninstruction tuning dataset and train a large multimodal model LLaMA-SciTune\nthat connects a vision encoder and LLM for science-focused visual and language\nunderstanding. In comparison to the models that are finetuned with machine\ngenerated data only, LLaMA-SciTune surpasses human performance on average and\nin many sub-categories on the ScienceQA benchmark.",
        "translated": ""
    },
    {
        "title": "Cross-modality Attention Adapter: A Glioma Segmentation Fine-tuning\n  Method for SAM Using Multimodal Brain MR Images",
        "url": "http://arxiv.org/abs/2307.01124v1",
        "pub_date": "2023-07-03",
        "summary": "According to the 2021 World Health Organization (WHO) Classification scheme\nfor gliomas, glioma segmentation is a very important basis for diagnosis and\ngenotype prediction. In general, 3D multimodal brain MRI is an effective\ndiagnostic tool. In the past decade, there has been an increase in the use of\nmachine learning, particularly deep learning, for medical images processing.\nThanks to the development of foundation models, models pre-trained with\nlarge-scale datasets have achieved better results on a variety of tasks.\nHowever, for medical images with small dataset sizes, deep learning methods\nstruggle to achieve better results on real-world image datasets. In this paper,\nwe propose a cross-modality attention adapter based on multimodal fusion to\nfine-tune the foundation model to accomplish the task of glioma segmentation in\nmultimodal MRI brain images with better results. The effectiveness of the\nproposed method is validated via our private glioma data set from the First\nAffiliated Hospital of Zhengzhou University (FHZU) in Zhengzhou, China. Our\nproposed method is superior to current state-of-the-art methods with a Dice of\n88.38% and Hausdorff distance of 10.64, thereby exhibiting a 4% increase in\nDice to segment the glioma region for glioma treatment.",
        "translated": ""
    },
    {
        "title": "Artifacts Mapping: Multi-Modal Semantic Mapping for Object Detection and\n  3D Localization",
        "url": "http://arxiv.org/abs/2307.01121v1",
        "pub_date": "2023-07-03",
        "summary": "Geometric navigation is nowadays a well-established field of robotics and the\nresearch focus is shifting towards higher-level scene understanding, such as\nSemantic Mapping. When a robot needs to interact with its environment, it must\nbe able to comprehend the contextual information of its surroundings. This work\nfocuses on classifying and localising objects within a map, which is under\nconstruction (SLAM) or already built. To further explore this direction, we\npropose a framework that can autonomously detect and localize predefined\nobjects in a known environment using a multi-modal sensor fusion approach\n(combining RGB and depth data from an RGB-D camera and a lidar). The framework\nconsists of three key elements: understanding the environment through RGB data,\nestimating depth through multi-modal sensor fusion, and managing artifacts\n(i.e., filtering and stabilizing measurements). The experiments show that the\nproposed framework can accurately detect 98% of the objects in the real sample\nenvironment, without post-processing, while 85% and 80% of the objects were\nmapped using the single RGBD camera or RGB + lidar setup respectively. The\ncomparison with single-sensor (camera or lidar) experiments is performed to\nshow that sensor fusion allows the robot to accurately detect near and far\nobstacles, which would have been noisy or imprecise in a purely visual or\nlaser-based approach.",
        "translated": ""
    },
    {
        "title": "MeT: A Graph Transformer for Semantic Segmentation of 3D Meshes",
        "url": "http://arxiv.org/abs/2307.01115v1",
        "pub_date": "2023-07-03",
        "summary": "Polygonal meshes have become the standard for discretely approximating 3D\nshapes, thanks to their efficiency and high flexibility in capturing\nnon-uniform shapes. This non-uniformity, however, leads to irregularity in the\nmesh structure, making tasks like segmentation of 3D meshes particularly\nchallenging. Semantic segmentation of 3D mesh has been typically addressed\nthrough CNN-based approaches, leading to good accuracy. Recently, transformers\nhave gained enough momentum both in NLP and computer vision fields, achieving\nperformance at least on par with CNN models, supporting the long-sought\narchitecture universalism. Following this trend, we propose a transformer-based\nmethod for semantic segmentation of 3D mesh motivated by a better modeling of\nthe graph structure of meshes, by means of global attention mechanisms. In\norder to address the limitations of standard transformer architectures in\nmodeling relative positions of non-sequential data, as in the case of 3D\nmeshes, as well as in capturing the local context, we perform positional\nencoding by means the Laplacian eigenvectors of the adjacency matrix, replacing\nthe traditional sinusoidal positional encodings, and by introducing\nclustering-based features into the self-attention and cross-attention\noperators. Experimental results, carried out on three sets of the Shape COSEG\nDataset, on the human segmentation dataset proposed in Maron et al., 2017 and\non the ShapeNet benchmark, show how the proposed approach yields\nstate-of-the-art performance on semantic segmentation of 3D meshes.",
        "translated": ""
    },
    {
        "title": "Building Cooperative Embodied Agents Modularly with Large Language\n  Models",
        "url": "http://arxiv.org/abs/2307.02485v1",
        "pub_date": "2023-07-05",
        "summary": "Large Language Models (LLMs) have demonstrated impressive planning abilities\nin single-agent embodied tasks across various domains. However, their capacity\nfor planning and communication in multi-agent cooperation remains unclear, even\nthough these are crucial skills for intelligent embodied agents. In this paper,\nwe present a novel framework that utilizes LLMs for multi-agent cooperation and\ntests it in various embodied environments. Our framework enables embodied\nagents to plan, communicate, and cooperate with other embodied agents or humans\nto accomplish long-horizon tasks efficiently. We demonstrate that recent LLMs,\nsuch as GPT-4, can surpass strong planning-based methods and exhibit emergent\neffective communication using our framework without requiring fine-tuning or\nfew-shot prompting. We also discover that LLM-based agents that communicate in\nnatural language can earn more trust and cooperate more effectively with\nhumans. Our research underscores the potential of LLMs for embodied AI and lays\nthe foundation for future research in multi-agent cooperation. Videos can be\nfound on the project website https://vis-www.cs.umass.edu/Co-LLM-Agents/.",
        "translated": ""
    },
    {
        "title": "A Dataset of Inertial Measurement Units for Handwritten English\n  Alphabets",
        "url": "http://arxiv.org/abs/2307.02480v1",
        "pub_date": "2023-07-05",
        "summary": "This paper presents an end-to-end methodology for collecting datasets to\nrecognize handwritten English alphabets by utilizing Inertial Measurement Units\n(IMUs) and leveraging the diversity present in the Indian writing style. The\nIMUs are utilized to capture the dynamic movement patterns associated with\nhandwriting, enabling more accurate recognition of alphabets. The Indian\ncontext introduces various challenges due to the heterogeneity in writing\nstyles across different regions and languages. By leveraging this diversity,\nthe collected dataset and the collection system aim to achieve higher\nrecognition accuracy. Some preliminary experimental results demonstrate the\neffectiveness of the dataset in accurately recognizing handwritten English\nalphabet in the Indian context. This research can be extended and contributes\nto the field of pattern recognition and offers valuable insights for developing\nimproved systems for handwriting recognition, particularly in diverse\nlinguistic and cultural contexts.",
        "translated": ""
    },
    {
        "title": "What Matters in Training a GPT4-Style Language Model with Multimodal\n  Inputs?",
        "url": "http://arxiv.org/abs/2307.02469v1",
        "pub_date": "2023-07-05",
        "summary": "Recent advancements in Large Language Models (LLMs) such as GPT4 have\ndisplayed exceptional multi-modal capabilities in following open-ended\ninstructions given images. However, the performance of these models heavily\nrelies on design choices such as network structures, training data, and\ntraining strategies, and these choices have not been extensively discussed in\nthe literature, making it difficult to quantify progress in this field. To\naddress this issue, this paper presents a systematic and comprehensive study,\nquantitatively and qualitatively, on training such models. We implement over 20\nvariants with controlled settings. Concretely, for network structures, we\ncompare different LLM backbones and model designs. For training data, we\ninvestigate the impact of data and sampling strategies. For instructions, we\nexplore the influence of diversified prompts on the instruction-following\nability of the trained models. For benchmarks, we contribute the first, to our\nbest knowledge, comprehensive evaluation set including both image and video\ntasks through crowd-sourcing. Based on our findings, we present Lynx, which\nperforms the most accurate multi-modal understanding while keeping the best\nmulti-modal generation ability compared to existing open-sourced GPT4-style\nmodels.",
        "translated": ""
    },
    {
        "title": "Large-scale Detection of Marine Debris in Coastal Areas with Sentinel-2",
        "url": "http://arxiv.org/abs/2307.02465v1",
        "pub_date": "2023-07-05",
        "summary": "Detecting and quantifying marine pollution and macro-plastics is an\nincreasingly pressing ecological issue that directly impacts ecology and human\nhealth. Efforts to quantify marine pollution are often conducted with sparse\nand expensive beach surveys, which are difficult to conduct on a large scale.\nHere, remote sensing can provide reliable estimates of plastic pollution by\nregularly monitoring and detecting marine debris in coastal areas.\nMedium-resolution satellite data of coastal areas is readily available and can\nbe leveraged to detect aggregations of marine debris containing plastic litter.\nIn this work, we present a detector for marine debris built on a deep\nsegmentation model that outputs a probability for marine debris at the pixel\nlevel. We train this detector with a combination of annotated datasets of\nmarine debris and evaluate it on specifically selected test sites where it is\nhighly probable that plastic pollution is present in the detected marine\ndebris. We demonstrate quantitatively and qualitatively that a deep learning\nmodel trained on this dataset issued from multiple sources outperforms existing\ndetection models trained on previous datasets by a large margin. Our\nexperiments show, consistent with the principles of data-centric AI, that this\nperformance is due to our particular dataset design with extensive sampling of\nnegative examples and label refinements rather than depending on the particular\ndeep learning model. We hope to accelerate advances in the large-scale\nautomated detection of marine debris, which is a step towards quantifying and\nmonitoring marine litter with remote sensing at global scales, and release the\nmodel weights and training source code under\nhttps://github.com/marccoru/marinedebrisdetector",
        "translated": ""
    },
    {
        "title": "AxonCallosumEM Dataset: Axon Semantic Segmentation of Whole Corpus\n  Callosum cross section from EM Images",
        "url": "http://arxiv.org/abs/2307.02464v1",
        "pub_date": "2023-07-05",
        "summary": "The electron microscope (EM) remains the predominant technique for\nelucidating intricate details of the animal nervous system at the nanometer\nscale. However, accurately reconstructing the complex morphology of axons and\nmyelin sheaths poses a significant challenge. Furthermore, the absence of\npublicly available, large-scale EM datasets encompassing complete cross\nsections of the corpus callosum, with dense ground truth segmentation for axons\nand myelin sheaths, hinders the advancement and evaluation of holistic corpus\ncallosum reconstructions. To surmount these obstacles, we introduce the\nAxonCallosumEM dataset, comprising a 1.83 times 5.76mm EM image captured from\nthe corpus callosum of the Rett Syndrome (RTT) mouse model, which entail\nextensive axon bundles. We meticulously proofread over 600,000 patches at a\nresolution of 1024 times 1024, thus providing a comprehensive ground truth for\nmyelinated axons and myelin sheaths. Additionally, we extensively annotated\nthree distinct regions within the dataset for the purposes of training,\ntesting, and validation. Utilizing this dataset, we develop a fine-tuning\nmethodology that adapts Segment Anything Model (SAM) to EM images segmentation\ntasks, called EM-SAM, enabling outperforms other state-of-the-art methods.\nFurthermore, we present the evaluation results of EM-SAM as a baseline.",
        "translated": ""
    },
    {
        "title": "Expert-Agnostic Ultrasound Image Quality Assessment using Deep\n  Variational Clustering",
        "url": "http://arxiv.org/abs/2307.02462v1",
        "pub_date": "2023-07-05",
        "summary": "Ultrasound imaging is a commonly used modality for several diagnostic and\ntherapeutic procedures. However, the diagnosis by ultrasound relies heavily on\nthe quality of images assessed manually by sonographers, which diminishes the\nobjectivity of the diagnosis and makes it operator-dependent. The supervised\nlearning-based methods for automated quality assessment require manually\nannotated datasets, which are highly labour-intensive to acquire. These\nultrasound images are low in quality and suffer from noisy annotations caused\nby inter-observer perceptual variations, which hampers learning efficiency. We\npropose an UnSupervised UltraSound image Quality assessment Network, US2QNet,\nthat eliminates the burden and uncertainty of manual annotations. US2QNet uses\nthe variational autoencoder embedded with the three modules, pre-processing,\nclustering and post-processing, to jointly enhance, extract, cluster and\nvisualize the quality feature representation of ultrasound images. The\npre-processing module uses filtering of images to point the network's attention\ntowards salient quality features, rather than getting distracted by noise.\nPost-processing is proposed for visualizing the clusters of feature\nrepresentations in 2D space. We validated the proposed framework for quality\nassessment of the urinary bladder ultrasound images. The proposed framework\nachieved 78% accuracy and superior performance to state-of-the-art clustering\nmethods.",
        "translated": ""
    },
    {
        "title": "Performance Scaling via Optimal Transport: Enabling Data Selection from\n  Partially Revealed Sources",
        "url": "http://arxiv.org/abs/2307.02460v1",
        "pub_date": "2023-07-05",
        "summary": "Traditionally, data selection has been studied in settings where all samples\nfrom prospective sources are fully revealed to a machine learning developer.\nHowever, in practical data exchange scenarios, data providers often reveal only\na limited subset of samples before an acquisition decision is made. Recently,\nthere have been efforts to fit scaling laws that predict model performance at\nany size and data source composition using the limited available samples.\nHowever, these scaling functions are black-box, computationally expensive to\nfit, highly susceptible to overfitting, or/and difficult to optimize for data\nselection. This paper proposes a framework called &lt;projektor&gt;, which predicts\nmodel performance and supports data selection decisions based on partial\nsamples of prospective data sources. Our approach distinguishes itself from\nexisting work by introducing a novel *two-stage* performance inference process.\nIn the first stage, we leverage the Optimal Transport distance to predict the\nmodel's performance for any data mixture ratio within the range of disclosed\ndata sizes. In the second stage, we extrapolate the performance to larger\nundisclosed data sizes based on a novel parameter-free mapping technique\ninspired by neural scaling laws. We further derive an efficient gradient-based\nmethod to select data sources based on the projected model performance.\nEvaluation over a diverse range of applications demonstrates that &lt;projektor&gt;\nsignificantly improves existing performance scaling approaches in terms of both\nthe accuracy of performance inference and the computation costs associated with\nconstructing the performance predictor. Also, &lt;projektor&gt; outperforms by a wide\nmargin in data selection effectiveness compared to a range of other\noff-the-shelf solutions.",
        "translated": ""
    },
    {
        "title": "DeSRA: Detect and Delete the Artifacts of GAN-based Real-World\n  Super-Resolution Models",
        "url": "http://arxiv.org/abs/2307.02457v1",
        "pub_date": "2023-07-05",
        "summary": "Image super-resolution (SR) with generative adversarial networks (GAN) has\nachieved great success in restoring realistic details. However, it is notorious\nthat GAN-based SR models will inevitably produce unpleasant and undesirable\nartifacts, especially in practical scenarios. Previous works typically suppress\nartifacts with an extra loss penalty in the training phase. They only work for\nin-distribution artifact types generated during training. When applied in\nreal-world scenarios, we observe that those improved methods still generate\nobviously annoying artifacts during inference. In this paper, we analyze the\ncause and characteristics of the GAN artifacts produced in unseen test data\nwithout ground-truths. We then develop a novel method, namely, DeSRA, to Detect\nand then Delete those SR Artifacts in practice. Specifically, we propose to\nmeasure a relative local variance distance from MSE-SR results and GAN-SR\nresults, and locate the problematic areas based on the above distance and\nsemantic-aware thresholds. After detecting the artifact regions, we develop a\nfinetune procedure to improve GAN-based SR models with a few samples, so that\nthey can deal with similar types of artifacts in more unseen real data.\nEquipped with our DeSRA, we can successfully eliminate artifacts from inference\nand improve the ability of SR models to be applied in real-world scenarios. The\ncode will be available at https://github.com/TencentARC/DeSRA.",
        "translated": ""
    },
    {
        "title": "LLCaps: Learning to Illuminate Low-Light Capsule Endoscopy with Curved\n  Wavelet Attention and Reverse Diffusion",
        "url": "http://arxiv.org/abs/2307.02452v1",
        "pub_date": "2023-07-05",
        "summary": "Wireless capsule endoscopy (WCE) is a painless and non-invasive diagnostic\ntool for gastrointestinal (GI) diseases. However, due to GI anatomical\nconstraints and hardware manufacturing limitations, WCE vision signals may\nsuffer from insufficient illumination, leading to a complicated screening and\nexamination procedure. Deep learning-based low-light image enhancement (LLIE)\nin the medical field gradually attracts researchers. Given the exuberant\ndevelopment of the denoising diffusion probabilistic model (DDPM) in computer\nvision, we introduce a WCE LLIE framework based on the multi-scale\nconvolutional neural network (CNN) and reverse diffusion process. The\nmulti-scale design allows models to preserve high-resolution representation and\ncontext information from low-resolution, while the curved wavelet attention\n(CWA) block is proposed for high-frequency and local feature learning.\nFurthermore, we combine the reverse diffusion procedure to further optimize the\nshallow output and generate the most realistic image. The proposed method is\ncompared with ten state-of-the-art (SOTA) LLIE methods and significantly\noutperforms quantitatively and qualitatively. The superior performance on GI\ndisease segmentation further demonstrates the clinical potential of our\nproposed model. Our code is publicly accessible.",
        "translated": ""
    },
    {
        "title": "Base Layer Efficiency in Scalable Human-Machine Coding",
        "url": "http://arxiv.org/abs/2307.02430v1",
        "pub_date": "2023-07-05",
        "summary": "A basic premise in scalable human-machine coding is that the base layer is\nintended for automated machine analysis and is therefore more compressible than\nthe same content would be for human viewing. Use cases for such coding include\nvideo surveillance and traffic monitoring, where the majority of the content\nwill never be seen by humans. Therefore, base layer efficiency is of paramount\nimportance because the system would most frequently operate at the base-layer\nrate. In this paper, we analyze the coding efficiency of the base layer in a\nstate-of-the-art scalable human-machine image codec, and show that it can be\nimproved. In particular, we demonstrate that gains of 20-40% in BD-Rate\ncompared to the currently best results on object detection and instance\nsegmentation are possible.",
        "translated": ""
    },
    {
        "title": "Synthesizing Artistic Cinemagraphs from Text",
        "url": "http://arxiv.org/abs/2307.03190v1",
        "pub_date": "2023-07-06",
        "summary": "We introduce Artistic Cinemagraph, a fully automated method for creating\ncinemagraphs from text descriptions - an especially challenging task when\nprompts feature imaginary elements and artistic styles, given the complexity of\ninterpreting the semantics and motions of these images. Existing single-image\nanimation methods fall short on artistic inputs, and recent text-based video\nmethods frequently introduce temporal inconsistencies, struggling to keep\ncertain regions static. To address these challenges, we propose an idea of\nsynthesizing image twins from a single text prompt - a pair of an artistic\nimage and its pixel-aligned corresponding natural-looking twin. While the\nartistic image depicts the style and appearance detailed in our text prompt,\nthe realistic counterpart greatly simplifies layout and motion analysis.\nLeveraging existing natural image and video datasets, we can accurately segment\nthe realistic image and predict plausible motion given the semantic\ninformation. The predicted motion can then be transferred to the artistic image\nto create the final cinemagraph. Our method outperforms existing approaches in\ncreating cinemagraphs for natural landscapes as well as artistic and\nother-worldly scenes, as validated by automated metrics and user studies.\nFinally, we demonstrate two extensions: animating existing paintings and\ncontrolling motion directions using text.",
        "translated": ""
    },
    {
        "title": "IPO-LDM: Depth-aided 360-degree Indoor RGB Panorama Outpainting via\n  Latent Diffusion Model",
        "url": "http://arxiv.org/abs/2307.03177v1",
        "pub_date": "2023-07-06",
        "summary": "Generating complete 360-degree panoramas from narrow field of view images is\nongoing research as omnidirectional RGB data is not readily available. Existing\nGAN-based approaches face some barriers to achieving higher quality output, and\nhave poor generalization performance over different mask types. In this paper,\nwe present our 360-degree indoor RGB panorama outpainting model using latent\ndiffusion models (LDM), called IPO-LDM. We introduce a new bi-modal latent\ndiffusion structure that utilizes both RGB and depth panoramic data during\ntraining, but works surprisingly well to outpaint normal depth-free RGB images\nduring inference. We further propose a novel technique of introducing\nprogressive camera rotations during each diffusion denoising step, which leads\nto substantial improvement in achieving panorama wraparound consistency.\nResults show that our IPO-LDM not only significantly outperforms\nstate-of-the-art methods on RGB panorama outpainting, but can also produce\nmultiple and diverse well-structured results for different types of masks.",
        "translated": ""
    },
    {
        "title": "Push Past Green: Learning to Look Behind Plant Foliage by Moving It",
        "url": "http://arxiv.org/abs/2307.03175v1",
        "pub_date": "2023-07-06",
        "summary": "Autonomous agriculture applications (e.g., inspection, phenotyping, plucking\nfruits) require manipulating the plant foliage to look behind the leaves and\nthe branches. Partial visibility, extreme clutter, thin structures, and unknown\ngeometry and dynamics for plants make such manipulation challenging. We tackle\nthese challenges through data-driven methods. We use self-supervision to train\nSRPNet, a neural network that predicts what space is revealed on execution of a\ncandidate action on a given plant. We use SRPNet with the cross-entropy method\nto predict actions that are effective at revealing space beneath plant foliage.\nFurthermore, as SRPNet does not just predict how much space is revealed but\nalso where it is revealed, we can execute a sequence of actions that\nincrementally reveal more and more space beneath the plant foliage. We\nexperiment with a synthetic (vines) and a real plant (Dracaena) on a physical\ntest-bed across 5 settings including 2 settings that test generalization to\nnovel plant configurations. Our experiments reveal the effectiveness of our\noverall method, PPG, over a competitive hand-crafted exploration method, and\nthe effectiveness of SRPNet over a hand-crafted dynamics model and relevant\nablations.",
        "translated": ""
    },
    {
        "title": "VideoGLUE: Video General Understanding Evaluation of Foundation Models",
        "url": "http://arxiv.org/abs/2307.03166v1",
        "pub_date": "2023-07-06",
        "summary": "We evaluate existing foundation models video understanding capabilities using\na carefully designed experiment protocol consisting of three hallmark tasks\n(action recognition, temporal localization, and spatiotemporal localization),\neight datasets well received by the community, and four adaptation methods\ntailoring a foundation model (FM) for a downstream task. Moreover, we propose a\nscalar VideoGLUE score (VGS) to measure an FMs efficacy and efficiency when\nadapting to general video understanding tasks. Our main findings are as\nfollows. First, task-specialized models significantly outperform the six FMs\nstudied in this work, in sharp contrast to what FMs have achieved in natural\nlanguage and image understanding. Second,video-native FMs, whose pretraining\ndata contains the video modality, are generally better than image-native FMs in\nclassifying motion-rich videos, localizing actions in time, and understanding a\nvideo of more than one action. Third, the video-native FMs can perform well on\nvideo tasks under light adaptations to downstream tasks(e.g., freezing the FM\nbackbones), while image-native FMs win in full end-to-end finetuning. The first\ntwo observations reveal the need and tremendous opportunities to conduct\nresearch on video-focused FMs, and the last confirms that both tasks and\nadaptation methods matter when it comes to the evaluation of FMs.",
        "translated": ""
    },
    {
        "title": "Can Domain Adaptation Improve Accuracy and Fairness of Skin Lesion\n  Classification?",
        "url": "http://arxiv.org/abs/2307.03157v1",
        "pub_date": "2023-07-06",
        "summary": "Deep learning-based diagnostic system has demonstrated potential in\nclassifying skin cancer conditions when labeled training example are abundant.\nHowever, skin lesion analysis often suffers from a scarcity of labeled data,\nhindering the development of an accurate and reliable diagnostic system. In\nthis work, we leverage multiple skin lesion datasets and investigate the\nfeasibility of various unsupervised domain adaptation (UDA) methods in binary\nand multi-class skin lesion classification. In particular, we assess three UDA\ntraining schemes: single-, combined-, and multi-source. Our experiment results\nshow that UDA is effective in binary classification, with further improvement\nbeing observed when imbalance is mitigated. In multi-class task, its\nperformance is less prominent, and imbalance problem again needs to be\naddressed to achieve above-baseline accuracy. Through our quantitative\nanalysis, we find that the test error of multi-class tasks is strongly\ncorrelated with label shift, and feature-level UDA methods have limitations\nwhen handling imbalanced datasets. Finally, our study reveals that UDA can\neffectively reduce bias against minority groups and promote fairness, even\nwithout the explicit use of fairness-focused techniques.",
        "translated": ""
    },
    {
        "title": "MultiVENT: Multilingual Videos of Events with Aligned Natural Text",
        "url": "http://arxiv.org/abs/2307.03153v1",
        "pub_date": "2023-07-06",
        "summary": "Everyday news coverage has shifted from traditional broadcasts towards a wide\nrange of presentation formats such as first-hand, unedited video footage.\nDatasets that reflect the diverse array of multimodal, multilingual news\nsources available online could be used to teach models to benefit from this\nshift, but existing news video datasets focus on traditional news broadcasts\nproduced for English-speaking audiences. We address this limitation by\nconstructing MultiVENT, a dataset of multilingual, event-centric videos\ngrounded in text documents across five target languages. MultiVENT includes\nboth news broadcast videos and non-professional event footage, which we use to\nanalyze the state of online news videos and how they can be leveraged to build\nrobust, factually accurate models. Finally, we provide a model for complex,\nmultilingual video retrieval to serve as a baseline for information retrieval\nusing MultiVENT.",
        "translated": ""
    },
    {
        "title": "Topology-Aware Loss for Aorta and Great Vessel Segmentation in Computed\n  Tomography Images",
        "url": "http://arxiv.org/abs/2307.03137v1",
        "pub_date": "2023-07-06",
        "summary": "Segmentation networks are not explicitly imposed to learn global invariants\nof an image, such as the shape of an object and the geometry between multiple\nobjects, when they are trained with a standard loss function. On the other\nhand, incorporating such invariants into network training may help improve\nperformance for various segmentation tasks when they are the intrinsic\ncharacteristics of the objects to be segmented. One example is segmentation of\naorta and great vessels in computed tomography (CT) images where vessels are\nfound in a particular geometry in the body due to the human anatomy and they\nmostly seem as round objects on a 2D CT image. This paper addresses this issue\nby introducing a new topology-aware loss function that penalizes topology\ndissimilarities between the ground truth and prediction through persistent\nhomology. Different from the previously suggested segmentation network designs,\nwhich apply the threshold filtration on a likelihood function of the prediction\nmap and the Betti numbers of the ground truth, this paper proposes to apply the\nVietoris-Rips filtration to obtain persistence diagrams of both ground truth\nand prediction maps and calculate the dissimilarity with the Wasserstein\ndistance between the corresponding persistence diagrams. The use of this\nfiltration has advantage of modeling shape and geometry at the same time, which\nmay not happen when the threshold filtration is applied. Our experiments on\n4327 CT images of 24 subjects reveal that the proposed topology-aware loss\nfunction leads to better results than its counterparts, indicating the\neffectiveness of this use.",
        "translated": ""
    },
    {
        "title": "Distilling Large Vision-Language Model with Out-of-Distribution\n  Generalizability",
        "url": "http://arxiv.org/abs/2307.03135v1",
        "pub_date": "2023-07-06",
        "summary": "Large vision-language models have achieved outstanding performance, but their\nsize and computational requirements make their deployment on\nresource-constrained devices and time-sensitive tasks impractical. Model\ndistillation, the process of creating smaller, faster models that maintain the\nperformance of larger models, is a promising direction towards the solution.\nThis paper investigates the distillation of visual representations in large\nteacher vision-language models into lightweight student models using a small-\nor mid-scale dataset. Notably, this study focuses on open-vocabulary\nout-of-distribution (OOD) generalization, a challenging problem that has been\noverlooked in previous model distillation literature. We propose two principles\nfrom vision and language modality perspectives to enhance student's OOD\ngeneralization: (1) by better imitating teacher's visual representation space,\nand carefully promoting better coherence in vision-language alignment with the\nteacher; (2) by enriching the teacher's language representations with\ninformative and finegrained semantic attributes to effectively distinguish\nbetween different labels. We propose several metrics and conduct extensive\nexperiments to investigate their techniques. The results demonstrate\nsignificant improvements in zero-shot and few-shot student performance on\nopen-vocabulary out-of-distribution classification, highlighting the\neffectiveness of our proposed approaches. Our code will be released at\nhttps://github.com/xuanlinli17/large_vlm_distillation_ood",
        "translated": ""
    },
    {
        "title": "Benchmarking Test-Time Adaptation against Distribution Shifts in Image\n  Classification",
        "url": "http://arxiv.org/abs/2307.03133v1",
        "pub_date": "2023-07-06",
        "summary": "Test-time adaptation (TTA) is a technique aimed at enhancing the\ngeneralization performance of models by leveraging unlabeled samples solely\nduring prediction. Given the need for robustness in neural network systems when\nfaced with distribution shifts, numerous TTA methods have recently been\nproposed. However, evaluating these methods is often done under different\nsettings, such as varying distribution shifts, backbones, and designing\nscenarios, leading to a lack of consistent and fair benchmarks to validate\ntheir effectiveness. To address this issue, we present a benchmark that\nsystematically evaluates 13 prominent TTA methods and their variants on five\nwidely used image classification datasets: CIFAR-10-C, CIFAR-100-C, ImageNet-C,\nDomainNet, and Office-Home. These methods encompass a wide range of adaptation\nscenarios (e.g. online adaptation v.s. offline adaptation, instance adaptation\nv.s. batch adaptation v.s. domain adaptation). Furthermore, we explore the\ncompatibility of different TTA methods with diverse network backbones. To\nimplement this benchmark, we have developed a unified framework in PyTorch,\nwhich allows for consistent evaluation and comparison of the TTA methods across\nthe different datasets and network architectures. By establishing this\nbenchmark, we aim to provide researchers and practitioners with a reliable\nmeans of assessing and comparing the effectiveness of TTA methods in improving\nmodel robustness and generalization performance. Our code is available at\nhttps://github.com/yuyongcan/Benchmark-TTA.",
        "translated": ""
    },
    {
        "title": "T-MARS: Improving Visual Representations by Circumventing Text Feature\n  Learning",
        "url": "http://arxiv.org/abs/2307.03132v1",
        "pub_date": "2023-07-06",
        "summary": "Large web-sourced multimodal datasets have powered a slew of new methods for\nlearning general-purpose visual representations, advancing the state of the art\nin computer vision and revolutionizing zero- and few-shot recognition. One\ncrucial decision facing practitioners is how, if at all, to curate these\never-larger datasets. For example, the creators of the LAION-5B dataset chose\nto retain only image-caption pairs whose CLIP similarity score exceeded a\ndesignated threshold. In this paper, we propose a new state-of-the-art data\nfiltering approach motivated by our observation that nearly 40% of LAION's\nimages contain text that overlaps significantly with the caption. Intuitively,\nsuch data could be wasteful as it incentivizes models to perform optical\ncharacter recognition rather than learning visual features. However, naively\nremoving all such data could also be wasteful, as it throws away images that\ncontain visual features (in addition to overlapping text). Our simple and\nscalable approach, T-MARS (Text Masking and Re-Scoring), filters out only those\npairs where the text dominates the remaining visual features -- by first\nmasking out the text and then filtering out those with a low CLIP similarity\nscore of the masked image. Experimentally, T-MARS outperforms the top-ranked\nmethod on the \"medium scale\" of DataComp (a data filtering benchmark) by a\nmargin of 6.5% on ImageNet and 4.7% on VTAB. Additionally, our systematic\nevaluation on various data pool sizes from 2M to 64M shows that the accuracy\ngains enjoyed by T-MARS linearly increase as data and compute are scaled\nexponentially. Code is available at https://github.com/locuslab/T-MARS.",
        "translated": ""
    },
    {
        "title": "Training Ensembles with Inliers and Outliers for Semi-supervised Active\n  Learning",
        "url": "http://arxiv.org/abs/2307.03741v1",
        "pub_date": "2023-07-07",
        "summary": "Deep active learning in the presence of outlier examples poses a realistic\nyet challenging scenario. Acquiring unlabeled data for annotation requires a\ndelicate balance between avoiding outliers to conserve the annotation budget\nand prioritizing useful inlier examples for effective training. In this work,\nwe present an approach that leverages three highly synergistic components,\nwhich are identified as key ingredients: joint classifier training with inliers\nand outliers, semi-supervised learning through pseudo-labeling, and model\nensembling. Our work demonstrates that ensembling significantly enhances the\naccuracy of pseudo-labeling and improves the quality of data acquisition. By\nenabling semi-supervision through the joint training process, where outliers\nare properly handled, we observe a substantial boost in classifier accuracy\nthrough the use of all available unlabeled examples. Notably, we reveal that\nthe integration of joint training renders explicit outlier detection\nunnecessary; a conventional component for acquisition in prior work. The three\nkey components align seamlessly with numerous existing approaches. Through\nempirical evaluations, we showcase that their combined use leads to a\nperformance increase. Remarkably, despite its simplicity, our proposed approach\noutperforms all other methods in terms of performance. Code:\nhttps://github.com/vladan-stojnic/active-outliers",
        "translated": ""
    },
    {
        "title": "INT-FP-QSim: Mixed Precision and Formats For Large Language Models and\n  Vision Transformers",
        "url": "http://arxiv.org/abs/2307.03712v1",
        "pub_date": "2023-07-07",
        "summary": "The recent rise of large language models (LLMs) has resulted in increased\nefforts towards running LLMs at reduced precision. Running LLMs at lower\nprecision supports resource constraints and furthers their democratization,\nenabling users to run billion-parameter LLMs on their personal devices. To\nsupplement this ongoing effort, we propose INT-FP-QSim: an open-source\nsimulator that enables flexible evaluation of LLMs and vision transformers at\nvarious numerical precisions and formats. INT-FP-QSim leverages existing\nopen-source repositories such as TensorRT, QPytorch and AIMET for a combined\nsimulator that supports various floating point and integer formats. With the\nhelp of our simulator, we survey the impact of different numerical formats on\nthe performance of LLMs and vision transformers at 4-bit weights and 4-bit or\n8-bit activations. We also compare recently proposed methods like Adaptive\nBlock Floating Point, SmoothQuant, GPTQ and RPTQ on the model performances. We\nhope INT-FP-QSim will enable researchers to flexibly simulate models at various\nprecisions to support further research in quantization of LLMs and vision\ntransformers.",
        "translated": ""
    },
    {
        "title": "Equivariant Single View Pose Prediction Via Induced and Restricted\n  Representations",
        "url": "http://arxiv.org/abs/2307.03704v1",
        "pub_date": "2023-07-07",
        "summary": "Learning about the three-dimensional world from two-dimensional images is a\nfundamental problem in computer vision. An ideal neural network architecture\nfor such tasks would leverage the fact that objects can be rotated and\ntranslated in three dimensions to make predictions about novel images. However,\nimposing SO(3)-equivariance on two-dimensional inputs is difficult because the\ngroup of three-dimensional rotations does not have a natural action on the\ntwo-dimensional plane. Specifically, it is possible that an element of SO(3)\nwill rotate an image out of plane. We show that an algorithm that learns a\nthree-dimensional representation of the world from two dimensional images must\nsatisfy certain geometric consistency properties which we formulate as\nSO(2)-equivariance constraints. We use the induced and restricted\nrepresentations of SO(2) on SO(3) to construct and classify architectures which\nsatisfy these geometric consistency constraints. We prove that any architecture\nwhich respects said consistency constraints can be realized as an instance of\nour construction. We show that three previously proposed neural architectures\nfor 3D pose prediction are special cases of our construction. We propose a new\nalgorithm that is a learnable generalization of previously considered methods.\nWe test our architecture on three pose predictions task and achieve SOTA\nresults on both the PASCAL3D+ and SYMSOL pose estimation tasks.",
        "translated": ""
    },
    {
        "title": "Motion Magnification in Robotic Sonography: Enabling Pulsation-Aware\n  Artery Segmentation",
        "url": "http://arxiv.org/abs/2307.03698v1",
        "pub_date": "2023-07-07",
        "summary": "Ultrasound (US) imaging is widely used for diagnosing and monitoring arterial\ndiseases, mainly due to the advantages of being non-invasive, radiation-free,\nand real-time. In order to provide additional information to assist clinicians\nin diagnosis, the tubular structures are often segmented from US images. To\nimprove the artery segmentation accuracy and stability during scans, this work\npresents a novel pulsation-assisted segmentation neural network (PAS-NN) by\nexplicitly taking advantage of the cardiac-induced motions. Motion\nmagnification techniques are employed to amplify the subtle motion within the\nfrequency band of interest to extract the pulsation signals from sequential US\nimages. The extracted real-time pulsation information can help to locate the\narteries on cross-section US images; therefore, we explicitly integrated the\npulsation into the proposed PAS-NN as attention guidance. Notably, a robotic\narm is necessary to provide stable movement during US imaging since magnifying\nthe target motions from the US images captured along a scan path is not\nmanually feasible due to the hand tremor. To validate the proposed robotic US\nsystem for imaging arteries, experiments are carried out on volunteers' carotid\nand radial arteries. The results demonstrated that the PAS-NN could achieve\ncomparable results as state-of-the-art on carotid and can effectively improve\nthe segmentation performance for small vessels (radial artery).",
        "translated": ""
    },
    {
        "title": "Detecting the Sensing Area of A Laparoscopic Probe in Minimally Invasive\n  Cancer Surgery",
        "url": "http://arxiv.org/abs/2307.03662v1",
        "pub_date": "2023-07-07",
        "summary": "In surgical oncology, it is challenging for surgeons to identify lymph nodes\nand completely resect cancer even with pre-operative imaging systems like PET\nand CT, because of the lack of reliable intraoperative visualization tools.\nEndoscopic radio-guided cancer detection and resection has recently been\nevaluated whereby a novel tethered laparoscopic gamma detector is used to\nlocalize a preoperatively injected radiotracer. This can both enhance the\nendoscopic imaging and complement preoperative nuclear imaging data. However,\ngamma activity visualization is challenging to present to the operator because\nthe probe is non-imaging and it does not visibly indicate the activity\norigination on the tissue surface. Initial failed attempts used segmentation or\ngeometric methods, but led to the discovery that it could be resolved by\nleveraging high-dimensional image features and probe position information. To\ndemonstrate the effectiveness of this solution, we designed and implemented a\nsimple regression network that successfully addressed the problem. To further\nvalidate the proposed solution, we acquired and publicly released two datasets\ncaptured using a custom-designed, portable stereo laparoscope system. Through\nintensive experimentation, we demonstrated that our method can successfully and\neffectively detect the sensing area, establishing a new performance benchmark.\nCode and data are available at\nhttps://github.com/br0202/Sensing_area_detection.git",
        "translated": ""
    },
    {
        "title": "Robust Human Detection under Visual Degradation via Thermal and mmWave\n  Radar Fusion",
        "url": "http://arxiv.org/abs/2307.03623v1",
        "pub_date": "2023-07-07",
        "summary": "The majority of human detection methods rely on the sensor using visible\nlights (e.g., RGB cameras) but such sensors are limited in scenarios with\ndegraded vision conditions. In this paper, we present a multimodal human\ndetection system that combines portable thermal cameras and single-chip mmWave\nradars. To mitigate the noisy detection features caused by the low contrast of\nthermal cameras and the multi-path noise of radar point clouds, we propose a\nBayesian feature extractor and a novel uncertainty-guided fusion method that\nsurpasses a variety of competing methods, either single-modal or multi-modal.\nWe evaluate the proposed method on real-world data collection and demonstrate\nthat our approach outperforms the state-of-the-art methods by a large margin.",
        "translated": ""
    },
    {
        "title": "Depth Estimation Analysis of Orthogonally Divergent Fisheye Cameras with\n  Distortion Removal",
        "url": "http://arxiv.org/abs/2307.03602v1",
        "pub_date": "2023-07-07",
        "summary": "Stereo vision systems have become popular in computer vision applications,\nsuch as 3D reconstruction, object tracking, and autonomous navigation. However,\ntraditional stereo vision systems that use rectilinear lenses may not be\nsuitable for certain scenarios due to their limited field of view. This has led\nto the popularity of vision systems based on one or multiple fisheye cameras in\ndifferent orientations, which can provide a field of view of 180x180 degrees or\nmore. However, fisheye cameras introduce significant distortion at the edges\nthat affects the accuracy of stereo matching and depth estimation. To overcome\nthese limitations, this paper proposes a method for distortion-removal and\ndepth estimation analysis for stereovision system using orthogonally divergent\nfisheye cameras (ODFC). The proposed method uses two virtual pinhole cameras\n(VPC), each VPC captures a small portion of the original view and presents it\nwithout any lens distortions, emulating the behavior of a pinhole camera. By\ncarefully selecting the captured regions, it is possible to create a stereo\npair using two VPCs. The performance of the proposed method is evaluated in\nboth simulation using virtual environment and experiments using real cameras\nand their results compared to stereo cameras with parallel optical axes. The\nresults demonstrate the effectiveness of the proposed method in terms of\ndistortion removal and depth estimation accuracy.",
        "translated": ""
    },
    {
        "title": "GPT4RoI: Instruction Tuning Large Language Model on Region-of-Interest",
        "url": "http://arxiv.org/abs/2307.03601v1",
        "pub_date": "2023-07-07",
        "summary": "Instruction tuning large language model (LLM) on image-text pairs has\nachieved unprecedented vision-language multimodal abilities. However, their\nvision-language alignments are only built on image-level, the lack of\nregion-level alignment limits their advancements to fine-grained multimodal\nunderstanding. In this paper, we propose instruction tuning on\nregion-of-interest. The key design is to reformulate the bounding box as the\nformat of spatial instruction. The interleaved sequences of visual features\nextracted by the spatial instruction and the language embedding are input to\nLLM, and trained on the transformed region-text data in instruction tuning\nformat. Our region-level vision-language model, termed as GPT4RoI, brings brand\nnew conversational and interactive experience beyond image-level understanding.\n(1) Controllability: Users can interact with our model by both language and\nspatial instructions to flexibly adjust the detail level of the question. (2)\nCapacities: Our model supports not only single-region spatial instruction but\nalso multi-region. This unlocks more region-level multimodal capacities such as\ndetailed region caption and complex region reasoning. (3) Composition: Any\noff-the-shelf object detector can be a spatial instruction provider so as to\nmine informative object attributes from our model, like color, shape, material,\naction, relation to other objects, etc. The code, data, and demo can be found\nat https://github.com/jshilong/GPT4RoI.",
        "translated": ""
    },
    {
        "title": "VesselVAE: Recursive Variational Autoencoders for 3D Blood Vessel\n  Synthesis",
        "url": "http://arxiv.org/abs/2307.03592v1",
        "pub_date": "2023-07-07",
        "summary": "We present a data-driven generative framework for synthesizing blood vessel\n3D geometry. This is a challenging task due to the complexity of vascular\nsystems, which are highly variating in shape, size, and structure. Existing\nmodel-based methods provide some degree of control and variation in the\nstructures produced, but fail to capture the diversity of actual anatomical\ndata. We developed VesselVAE, a recursive variational Neural Network that fully\nexploits the hierarchical organization of the vessel and learns a\nlow-dimensional manifold encoding branch connectivity along with geometry\nfeatures describing the target surface. After training, the VesselVAE latent\nspace can be sampled to generate new vessel geometries. To the best of our\nknowledge, this work is the first to utilize this technique for synthesizing\nblood vessels. We achieve similarities of synthetic and real data for radius\n(.97), length (.95), and tortuosity (.96). By leveraging the power of deep\nneural networks, we generate 3D models of blood vessels that are both accurate\nand diverse, which is crucial for medical and surgical training, hemodynamic\nsimulations, and many other purposes.",
        "translated": ""
    },
    {
        "title": "Unsupervised Segmentation of Fetal Brain MRI using Deep Learning\n  Cascaded Registration",
        "url": "http://arxiv.org/abs/2307.03579v1",
        "pub_date": "2023-07-07",
        "summary": "Accurate segmentation of fetal brain magnetic resonance images is crucial for\nanalyzing fetal brain development and detecting potential neurodevelopmental\nabnormalities. Traditional deep learning-based automatic segmentation, although\neffective, requires extensive training data with ground-truth labels, typically\nproduced by clinicians through a time-consuming annotation process. To overcome\nthis challenge, we propose a novel unsupervised segmentation method based on\nmulti-atlas segmentation, that accurately segments multiple tissues without\nrelying on labeled data for training. Our method employs a cascaded deep\nlearning network for 3D image registration, which computes small, incremental\ndeformations to the moving image to align it precisely with the fixed image.\nThis cascaded network can then be used to register multiple annotated images\nwith the image to be segmented, and combine the propagated labels to form a\nrefined segmentation. Our experiments demonstrate that the proposed cascaded\narchitecture outperforms the state-of-the-art registration methods that were\ntested. Furthermore, the derived segmentation method achieves similar\nperformance and inference time to nnU-Net while only using a small subset of\nannotated data for the multi-atlas segmentation task and none for training the\nnetwork. Our pipeline for registration and multi-atlas segmentation is publicly\navailable at https://github.com/ValBcn/CasReg.",
        "translated": ""
    },
    {
        "title": "Semantic-SAM: Segment and Recognize Anything at Any Granularity",
        "url": "http://arxiv.org/abs/2307.04767v1",
        "pub_date": "2023-07-10",
        "summary": "In this paper, we introduce Semantic-SAM, a universal image segmentation\nmodel to enable segment and recognize anything at any desired granularity. Our\nmodel offers two key advantages: semantic-awareness and granularity-abundance.\nTo achieve semantic-awareness, we consolidate multiple datasets across three\ngranularities and introduce decoupled classification for objects and parts.\nThis allows our model to capture rich semantic information. For the\nmulti-granularity capability, we propose a multi-choice learning scheme during\ntraining, enabling each click to generate masks at multiple levels that\ncorrespond to multiple ground-truth masks. Notably, this work represents the\nfirst attempt to jointly train a model on SA-1B, generic, and part segmentation\ndatasets. Experimental results and visualizations demonstrate that our model\nsuccessfully achieves semantic-awareness and granularity-abundance.\nFurthermore, combining SA-1B training with other segmentation tasks, such as\npanoptic and part segmentation, leads to performance improvements. We will\nprovide code and a demo for further exploration and evaluation.",
        "translated": ""
    },
    {
        "title": "Learning Spatial Features from Audio-Visual Correspondence in Egocentric\n  Videos",
        "url": "http://arxiv.org/abs/2307.04760v1",
        "pub_date": "2023-07-10",
        "summary": "We propose a self-supervised method for learning representations based on\nspatial audio-visual correspondences in egocentric videos. In particular, our\nmethod leverages a masked auto-encoding framework to synthesize masked binaural\naudio through the synergy of audio and vision, thereby learning useful spatial\nrelationships between the two modalities. We use our pretrained features to\ntackle two downstream video tasks requiring spatial understanding in social\nscenarios: active speaker detection and spatial audio denoising. We show\nthrough extensive experiments that our features are generic enough to improve\nover multiple state-of-the-art baselines on two public challenging egocentric\nvideo datasets, EgoCom and EasyCom. Project:\nhttp://vision.cs.utexas.edu/projects/ego_av_corr.",
        "translated": ""
    },
    {
        "title": "Shelving, Stacking, Hanging: Relational Pose Diffusion for Multi-modal\n  Rearrangement",
        "url": "http://arxiv.org/abs/2307.04751v1",
        "pub_date": "2023-07-10",
        "summary": "We propose a system for rearranging objects in a scene to achieve a desired\nobject-scene placing relationship, such as a book inserted in an open slot of a\nbookshelf. The pipeline generalizes to novel geometries, poses, and layouts of\nboth scenes and objects, and is trained from demonstrations to operate directly\non 3D point clouds. Our system overcomes challenges associated with the\nexistence of many geometrically-similar rearrangement solutions for a given\nscene. By leveraging an iterative pose de-noising training procedure, we can\nfit multi-modal demonstration data and produce multi-modal outputs while\nremaining precise and accurate. We also show the advantages of conditioning on\nrelevant local geometric features while ignoring irrelevant global structure\nthat harms both generalization and precision. We demonstrate our approach on\nthree distinct rearrangement tasks that require handling multi-modality and\ngeneralization over object shape and pose in both simulation and the real\nworld. Project website, code, and videos:\nhttps://anthonysimeonov.github.io/rpdiff-multi-modal/",
        "translated": ""
    },
    {
        "title": "Divide, Evaluate, and Refine: Evaluating and Improving Text-to-Image\n  Alignment with Iterative VQA Feedback",
        "url": "http://arxiv.org/abs/2307.04749v1",
        "pub_date": "2023-07-10",
        "summary": "The field of text-conditioned image generation has made unparalleled progress\nwith the recent advent of latent diffusion models. While remarkable, as the\ncomplexity of given text input increases, the state-of-the-art diffusion models\nmay still fail in generating images which accurately convey the semantics of\nthe given prompt. Furthermore, it has been observed that such misalignments are\noften left undetected by pretrained multi-modal models such as CLIP. To address\nthese problems, in this paper we explore a simple yet effective decompositional\napproach towards both evaluation and improvement of text-to-image alignment. In\nparticular, we first introduce a Decompositional-Alignment-Score which given a\ncomplex prompt decomposes it into a set of disjoint assertions. The alignment\nof each assertion with generated images is then measured using a VQA model.\nFinally, alignment scores for different assertions are combined aposteriori to\ngive the final text-to-image alignment score. Experimental analysis reveals\nthat the proposed alignment metric shows significantly higher correlation with\nhuman ratings as opposed to traditional CLIP, BLIP scores. Furthermore, we also\nfind that the assertion level alignment scores provide a useful feedback which\ncan then be used in a simple iterative procedure to gradually increase the\nexpression of different assertions in the final image outputs. Human user\nstudies indicate that the proposed approach surpasses previous state-of-the-art\nby 8.7% in overall text-to-image alignment accuracy. Project page for our paper\nis available at https://1jsingh.github.io/divide-evaluate-and-refine",
        "translated": ""
    },
    {
        "title": "AnimateDiff: Animate Your Personalized Text-to-Image Diffusion Models\n  without Specific Tuning",
        "url": "http://arxiv.org/abs/2307.04725v1",
        "pub_date": "2023-07-10",
        "summary": "With the advance of text-to-image models (e.g., Stable Diffusion) and\ncorresponding personalization techniques such as DreamBooth and LoRA, everyone\ncan manifest their imagination into high-quality images at an affordable cost.\nSubsequently, there is a great demand for image animation techniques to further\ncombine generated static images with motion dynamics. In this report, we\npropose a practical framework to animate most of the existing personalized\ntext-to-image models once and for all, saving efforts in model-specific tuning.\nAt the core of the proposed framework is to insert a newly initialized motion\nmodeling module into the frozen text-to-image model and train it on video clips\nto distill reasonable motion priors. Once trained, by simply injecting this\nmotion modeling module, all personalized versions derived from the same base\nT2I readily become text-driven models that produce diverse and personalized\nanimated images. We conduct our evaluation on several public representative\npersonalized text-to-image models across anime pictures and realistic\nphotographs, and demonstrate that our proposed framework helps these models\ngenerate temporally smooth animation clips while preserving the domain and\ndiversity of their outputs. Code and pre-trained weights will be publicly\navailable at https://animatediff.github.io/ .",
        "translated": ""
    },
    {
        "title": "CVPR MultiEarth 2023 Deforestation Estimation\n  Challenge:SpaceVision4Amazon",
        "url": "http://arxiv.org/abs/2307.04715v1",
        "pub_date": "2023-07-10",
        "summary": "In this paper, we present a deforestation estimation method based on\nattention guided UNet architecture using Electro-Optical (EO) and Synthetic\nAperture Radar (SAR) satellite imagery. For optical images, Landsat-8 and for\nSAR imagery, Sentinel-1 data have been used to train and validate the proposed\nmodel. Due to the unavailability of temporally and spatially collocated data,\nindividual model has been trained for each sensor. During training time\nLandsat-8 model achieved training and validation pixel accuracy of 93.45% and\nSentinel-2 model achieved 83.87% pixel accuracy. During the test set\nevaluation, the model achieved pixel accuracy of 84.70% with F1-Score of 0.79\nand IoU of 0.69.",
        "translated": ""
    },
    {
        "title": "FreeDrag: Point Tracking is Not You Need for Interactive Point-based\n  Image Editing",
        "url": "http://arxiv.org/abs/2307.04684v1",
        "pub_date": "2023-07-10",
        "summary": "To serve the intricate and varied demands of image editing, precise and\nflexible manipulation of image content is indispensable. Recently, DragGAN has\nachieved impressive editing results through point-based manipulation. However,\nwe have observed that DragGAN struggles with miss tracking, where DragGAN\nencounters difficulty in effectively tracking the desired handle points, and\nambiguous tracking, where the tracked points are situated within other regions\nthat bear resemblance to the handle points. To deal with the above issues, we\npropose FreeDrag, which adopts a feature-oriented approach to free the burden\non point tracking within the point-oriented methodology of DragGAN. The\nFreeDrag incorporates adaptive template features, line search, and fuzzy\nlocalization techniques to perform stable and efficient point-based image\nediting. Extensive experiments demonstrate that our method is superior to the\nDragGAN and enables stable point-based editing in challenging scenarios with\nsimilar structures, fine details, or under multi-point targets.",
        "translated": ""
    },
    {
        "title": "Joint Salient Object Detection and Camouflaged Object Detection via\n  Uncertainty-aware Learning",
        "url": "http://arxiv.org/abs/2307.04651v1",
        "pub_date": "2023-07-10",
        "summary": "Salient objects attract human attention and usually stand out clearly from\ntheir surroundings. In contrast, camouflaged objects share similar colors or\ntextures with the environment. In this case, salient objects are typically\nnon-camouflaged, and camouflaged objects are usually not salient. Due to this\ninherent contradictory attribute, we introduce an uncertainty-aware learning\npipeline to extensively explore the contradictory information of salient object\ndetection (SOD) and camouflaged object detection (COD) via data-level and\ntask-wise contradiction modeling. We first exploit the dataset correlation of\nthese two tasks and claim that the easy samples in the COD dataset can serve as\nhard samples for SOD to improve the robustness of the SOD model. Based on the\nassumption that these two models should lead to activation maps highlighting\ndifferent regions of the same input image, we further introduce a contrastive\nmodule with a joint-task contrastive learning framework to explicitly model the\ncontradictory attributes of these two tasks. Different from conventional\nintra-task contrastive learning for unsupervised representation learning, our\ncontrastive module is designed to model the task-wise correlation, leading to\ncross-task representation learning. To better understand the two tasks from the\nperspective of uncertainty, we extensively investigate the uncertainty\nestimation techniques for modeling the main uncertainties of the two tasks,\nnamely task uncertainty (for SOD) and data uncertainty (for COD), and aiming to\neffectively estimate the challenging regions for each task to achieve\ndifficulty-aware learning. Experimental results on benchmark datasets\ndemonstrate that our solution leads to both state-of-the-art performance and\ninformative uncertainty estimation.",
        "translated": ""
    },
    {
        "title": "Multimodal brain age estimation using interpretable adaptive\n  population-graph learning",
        "url": "http://arxiv.org/abs/2307.04639v1",
        "pub_date": "2023-07-10",
        "summary": "Brain age estimation is clinically important as it can provide valuable\ninformation in the context of neurodegenerative diseases such as Alzheimer's.\nPopulation graphs, which include multimodal imaging information of the subjects\nalong with the relationships among the population, have been used in literature\nalong with Graph Convolutional Networks (GCNs) and have proved beneficial for a\nvariety of medical imaging tasks. A population graph is usually static and\nconstructed manually using non-imaging information. However, graph construction\nis not a trivial task and might significantly affect the performance of the\nGCN, which is inherently very sensitive to the graph structure. In this work,\nwe propose a framework that learns a population graph structure optimized for\nthe downstream task. An attention mechanism assigns weights to a set of imaging\nand non-imaging features (phenotypes), which are then used for edge extraction.\nThe resulting graph is used to train the GCN. The entire pipeline can be\ntrained end-to-end. Additionally, by visualizing the attention weights that\nwere the most important for the graph construction, we increase the\ninterpretability of the graph. We use the UK Biobank, which provides a large\nvariety of neuroimaging and non-imaging phenotypes, to evaluate our method on\nbrain age regression and classification. The proposed method outperforms\ncompeting static graph approaches and other state-of-the-art adaptive methods.\nWe further show that the assigned attention scores indicate that there are both\nimaging and non-imaging phenotypes that are informative for brain age\nestimation and are in agreement with the relevant literature.",
        "translated": ""
    },
    {
        "title": "Weakly-supervised positional contrastive learning: application to\n  cirrhosis classification",
        "url": "http://arxiv.org/abs/2307.04617v1",
        "pub_date": "2023-07-10",
        "summary": "Large medical imaging datasets can be cheaply and quickly annotated with\nlow-confidence, weak labels (e.g., radiological scores). Access to\nhigh-confidence labels, such as histology-based diagnoses, is rare and costly.\nPretraining strategies, like contrastive learning (CL) methods, can leverage\nunlabeled or weakly-annotated datasets. These methods typically require large\nbatch sizes, which poses a difficulty in the case of large 3D images at full\nresolution, due to limited GPU memory. Nevertheless, volumetric positional\ninformation about the spatial context of each 2D slice can be very important\nfor some medical applications. In this work, we propose an efficient\nweakly-supervised positional (WSP) contrastive learning strategy where we\nintegrate both the spatial context of each 2D slice and a weak label via a\ngeneric kernel-based loss function. We illustrate our method on cirrhosis\nprediction using a large volume of weakly-labeled images, namely radiological\nlow-confidence annotations, and small strongly-labeled (i.e., high-confidence)\ndatasets. The proposed model improves the classification AUC by 5% with respect\nto a baseline model on our internal dataset, and by 26% on the public LIHC\ndataset from the Cancer Genome Atlas. The code is available at:\nhttps://github.com/Guerbet-AI/wsp-contrastive.",
        "translated": ""
    },
    {
        "title": "Differentiable Blocks World: Qualitative 3D Decomposition by Rendering\n  Primitives",
        "url": "http://arxiv.org/abs/2307.05473v1",
        "pub_date": "2023-07-11",
        "summary": "Given a set of calibrated images of a scene, we present an approach that\nproduces a simple, compact, and actionable 3D world representation by means of\n3D primitives. While many approaches focus on recovering high-fidelity 3D\nscenes, we focus on parsing a scene into mid-level 3D representations made of a\nsmall set of textured primitives. Such representations are interpretable, easy\nto manipulate and suited for physics-based simulations. Moreover, unlike\nexisting primitive decomposition methods that rely on 3D input data, our\napproach operates directly on images through differentiable rendering.\nSpecifically, we model primitives as textured superquadric meshes and optimize\ntheir parameters from scratch with an image rendering loss. We highlight the\nimportance of modeling transparency for each primitive, which is critical for\noptimization and also enables handling varying numbers of primitives. We show\nthat the resulting textured primitives faithfully reconstruct the input images\nand accurately model the visible 3D points, while providing amodal shape\ncompletions of unseen object regions. We compare our approach to the state of\nthe art on diverse scenes from DTU, and demonstrate its robustness on real-life\ncaptures from BlendedMVS and Nerfstudio. We also showcase how our results can\nbe used to effortlessly edit a scene or perform physical simulations. Code and\nvideo results are available at https://www.tmonnier.com/DBW .",
        "translated": ""
    },
    {
        "title": "Scale Alone Does not Improve Mechanistic Interpretability in Vision\n  Models",
        "url": "http://arxiv.org/abs/2307.05471v1",
        "pub_date": "2023-07-11",
        "summary": "In light of the recent widespread adoption of AI systems, understanding the\ninternal information processing of neural networks has become increasingly\ncritical. Most recently, machine vision has seen remarkable progress by scaling\nneural networks to unprecedented levels in dataset and model size. We here ask\nwhether this extraordinary increase in scale also positively impacts the field\nof mechanistic interpretability. In other words, has our understanding of the\ninner workings of scaled neural networks improved as well? We here use a\npsychophysical paradigm to quantify mechanistic interpretability for a diverse\nsuite of models and find no scaling effect for interpretability - neither for\nmodel nor dataset size. Specifically, none of the nine investigated\nstate-of-the-art models are easier to interpret than the GoogLeNet model from\nalmost a decade ago. Latest-generation vision models appear even less\ninterpretable than older architectures, hinting at a regression rather than\nimprovement, with modern models sacrificing interpretability for accuracy.\nThese results highlight the need for models explicitly designed to be\nmechanistically interpretable and the need for more helpful interpretability\nmethods to increase our understanding of networks at an atomic level. We\nrelease a dataset containing more than 120'000 human responses from our\npsychophysical evaluation of 767 units across nine models. This dataset is\nmeant to facilitate research on automated instead of human-based\ninterpretability evaluations that can ultimately be leveraged to directly\noptimize the mechanistic interpretability of models.",
        "translated": ""
    },
    {
        "title": "My3DGen: Building Lightweight Personalized 3D Generative Model",
        "url": "http://arxiv.org/abs/2307.05468v2",
        "pub_date": "2023-07-11",
        "summary": "Our paper presents My3DGen, a practical system for creating a personalized\nand lightweight 3D generative prior using as few as 10 images. My3DGen can\nreconstruct multi-view consistent images from an input test image, and generate\nnovel appearances by interpolating between any two images of the same\nindividual. While recent studies have demonstrated the effectiveness of\npersonalized generative priors in producing high-quality 2D portrait\nreconstructions and syntheses, to the best of our knowledge, we are the first\nto develop a personalized 3D generative prior. Instead of fine-tuning a large\npre-trained generative model with millions of parameters to achieve\npersonalization, we propose a parameter-efficient approach. Our method involves\nutilizing a pre-trained model with fixed weights as a generic prior, while\ntraining a separate personalized prior through low-rank decomposition of the\nweights in each convolution and fully connected layer. However,\nparameter-efficient few-shot fine-tuning on its own often leads to overfitting.\nTo address this, we introduce a regularization technique based on symmetry of\nhuman faces. This regularization enforces that novel view renderings of a\ntraining sample, rendered from symmetric poses, exhibit the same identity. By\nincorporating this symmetry prior, we enhance the quality of reconstruction and\nsynthesis, particularly for non-frontal (profile) faces. Our final system\ncombines low-rank fine-tuning with symmetry regularization and significantly\nsurpasses the performance of pre-trained models, e.g. EG3D. It introduces only\napproximately 0.6 million additional parameters per identity compared to 31\nmillion for full finetuning of the original model. As a result, our system\nachieves a 50-fold reduction in model size without sacrificing the quality of\nthe generated 3D faces. Code will be available at our project page:\nhttps://luchaoqi.github.io/my3dgen.",
        "translated": ""
    },
    {
        "title": "EgoVLPv2: Egocentric Video-Language Pre-training with Fusion in the\n  Backbone",
        "url": "http://arxiv.org/abs/2307.05463v1",
        "pub_date": "2023-07-11",
        "summary": "Video-language pre-training (VLP) has become increasingly important due to\nits ability to generalize to various vision and language tasks. However,\nexisting egocentric VLP frameworks utilize separate video and language encoders\nand learn task-specific cross-modal information only during fine-tuning,\nlimiting the development of a unified system. In this work, we introduce the\nsecond generation of egocentric video-language pre-training (EgoVLPv2), a\nsignificant improvement from the previous generation, by incorporating\ncross-modal fusion directly into the video and language backbones. EgoVLPv2\nlearns strong video-text representation during pre-training and reuses the\ncross-modal attention modules to support different downstream tasks in a\nflexible and efficient manner, reducing fine-tuning costs. Moreover, our\nproposed fusion in the backbone strategy is more lightweight and\ncompute-efficient than stacking additional fusion-specific layers. Extensive\nexperiments on a wide range of VL tasks demonstrate the effectiveness of\nEgoVLPv2 by achieving consistent state-of-the-art performance over strong\nbaselines across all downstream. Our project page can be found at\nhttps://shramanpramanick.github.io/EgoVLPv2/.",
        "translated": ""
    },
    {
        "title": "Efficient 3D Articulated Human Generation with Layered Surface Volumes",
        "url": "http://arxiv.org/abs/2307.05462v1",
        "pub_date": "2023-07-11",
        "summary": "Access to high-quality and diverse 3D articulated digital human assets is\ncrucial in various applications, ranging from virtual reality to social\nplatforms. Generative approaches, such as 3D generative adversarial networks\n(GANs), are rapidly replacing laborious manual content creation tools. However,\nexisting 3D GAN frameworks typically rely on scene representations that\nleverage either template meshes, which are fast but offer limited quality, or\nvolumes, which offer high capacity but are slow to render, thereby limiting the\n3D fidelity in GAN settings. In this work, we introduce layered surface volumes\n(LSVs) as a new 3D object representation for articulated digital humans. LSVs\nrepresent a human body using multiple textured mesh layers around a\nconventional template. These layers are rendered using alpha compositing with\nfast differentiable rasterization, and they can be interpreted as a volumetric\nrepresentation that allocates its capacity to a manifold of finite thickness\naround the template. Unlike conventional single-layer templates that struggle\nwith representing fine off-surface details like hair or accessories, our\nsurface volumes naturally capture such details. LSVs can be articulated, and\nthey exhibit exceptional efficiency in GAN settings, where a 2D generator\nlearns to synthesize the RGBA textures for the individual layers. Trained on\nunstructured, single-view 2D image datasets, our LSV-GAN generates high-quality\nand view-consistent 3D articulated digital humans without the need for\nview-inconsistent 2D upsampling networks.",
        "translated": ""
    },
    {
        "title": "Bio-Inspired Night Image Enhancement Based on Contrast Enhancement and\n  Denoising",
        "url": "http://arxiv.org/abs/2307.05447v1",
        "pub_date": "2023-07-11",
        "summary": "Due to the low accuracy of object detection and recognition in many\nintelligent surveillance systems at nighttime, the quality of night images is\ncrucial. Compared with the corresponding daytime image, nighttime image is\ncharacterized as low brightness, low contrast and high noise. In this paper, a\nbio-inspired image enhancement algorithm is proposed to convert a low\nilluminance image to a brighter and clear one. Different from existing\nbio-inspired algorithm, the proposed method doesn't use any training sequences,\nwe depend on a novel chain of contrast enhancement and denoising algorithms\nwithout using any forms of recursive functions. Our method can largely improve\nthe brightness and contrast of night images, besides, suppress noise. Then we\nimplement on real experiment, and simulation experiment to test our algorithms.\nBoth results show the advantages of proposed algorithm over contrast pair,\nMeylan and Retinex.",
        "translated": ""
    },
    {
        "title": "3D detection of roof sections from a single satellite image and\n  application to LOD2-building reconstruction",
        "url": "http://arxiv.org/abs/2307.05409v1",
        "pub_date": "2023-07-11",
        "summary": "Reconstructing urban areas in 3D out of satellite raster images has been a\nlong-standing and challenging goal of both academical and industrial research.\nThe rare methods today achieving this objective at a Level Of Details $2$ rely\non procedural approaches based on geometry, and need stereo images and/or LIDAR\ndata as input. We here propose a method for urban 3D reconstruction named\nKIBS(\\textit{Keypoints Inference By Segmentation}), which comprises two novel\nfeatures: i) a full deep learning approach for the 3D detection of the roof\nsections, and ii) only one single (non-orthogonal) satellite raster image as\nmodel input. This is achieved in two steps: i) by a Mask R-CNN model performing\na 2D segmentation of the buildings' roof sections, and after blending these\nlatter segmented pixels within the RGB satellite raster image, ii) by another\nidentical Mask R-CNN model inferring the heights-to-ground of the roof\nsections' corners via panoptic segmentation, unto full 3D reconstruction of the\nbuildings and city. We demonstrate the potential of the KIBS method by\nreconstructing different urban areas in a few minutes, with a Jaccard index for\nthe 2D segmentation of individual roof sections of $88.55\\%$ and $75.21\\%$ on\nour two data sets resp., and a height's mean error of such correctly segmented\npixels for the 3D reconstruction of $1.60$ m and $2.06$ m on our two data sets\nresp., hence within the LOD2 precision range.",
        "translated": ""
    },
    {
        "title": "On the Vulnerability of DeepFake Detectors to Attacks Generated by\n  Denoising Diffusion Models",
        "url": "http://arxiv.org/abs/2307.05397v1",
        "pub_date": "2023-07-11",
        "summary": "The detection of malicious Deepfakes is a constantly evolving problem, that\nrequires continuous monitoring of detectors, to ensure they are able to detect\nimage manipulations generated by the latest emerging models. In this paper, we\npresent a preliminary study that investigates the vulnerability of single-image\nDeepfake detectors to attacks created by a representative of the newest\ngeneration of generative methods, i.e. Denoising Diffusion Models (DDMs). Our\nexperiments are run on FaceForensics++, a commonly used benchmark dataset,\nconsisting of Deepfakes generated with various techniques for face swapping and\nface reenactment. The analysis shows, that reconstructing existing Deepfakes\nwith only one denoising diffusion step significantly decreases the accuracy of\nall tested detectors, without introducing visually perceptible image changes.",
        "translated": ""
    },
    {
        "title": "Handwritten Text Recognition Using Convolutional Neural Network",
        "url": "http://arxiv.org/abs/2307.05396v1",
        "pub_date": "2023-07-11",
        "summary": "OCR (Optical Character Recognition) is a technology that offers comprehensive\nalphanumeric recognition of handwritten and printed characters at electronic\nspeed by merely scanning the document. Recently, the understanding of visual\ndata has been termed Intelligent Character Recognition (ICR). Intelligent\nCharacter Recognition (ICR) is the OCR module that can convert scans of\nhandwritten or printed characters into ASCII text. ASCII data is the standard\nformat for data encoding in electronic communication. ASCII assigns standard\nnumeric values to letters, numeral, symbols, white-spaces and other characters.\nIn more technical terms, OCR is the process of using an electronic device to\ntransform 2-Dimensional textual information into machine-encoded text. Anything\nthat contains text both machine written or handwritten can be scanned either\nthrough a scanner or just simply a picture of the text is enough for the\nrecognition system to distinguish the text. The goal of this papers is to show\nthe results of a Convolutional Neural Network model which has been trained on\nNational Institute of Science and Technology (NIST) dataset containing over a\n100,000 images. The network learns from the features extracted from the images\nand use it to generate the probability of each class to which the picture\nbelongs to. We have achieved an accuracy of 90.54% with a loss of 2.53%.",
        "translated": ""
    },
    {
        "title": "Self-supervised adversarial masking for 3D point cloud representation\n  learning",
        "url": "http://arxiv.org/abs/2307.05325v1",
        "pub_date": "2023-07-11",
        "summary": "Self-supervised methods have been proven effective for learning deep\nrepresentations of 3D point cloud data. Although recent methods in this domain\noften rely on random masking of inputs, the results of this approach can be\nimproved. We introduce PointCAM, a novel adversarial method for learning a\nmasking function for point clouds. Our model utilizes a self-distillation\nframework with an online tokenizer for 3D point clouds. Compared to previous\ntechniques that optimize patch-level and object-level objectives, we postulate\napplying an auxiliary network that learns how to select masks instead of\nchoosing them randomly. Our results show that the learned masking function\nachieves state-of-the-art or competitive performance on various downstream\ntasks. The source code is available at https://github.com/szacho/pointcam.",
        "translated": ""
    },
    {
        "title": "Neural Free-Viewpoint Relighting for Glossy Indirect Illumination",
        "url": "http://arxiv.org/abs/2307.06335v1",
        "pub_date": "2023-07-12",
        "summary": "Precomputed Radiance Transfer (PRT) remains an attractive solution for\nreal-time rendering of complex light transport effects such as glossy global\nillumination. After precomputation, we can relight the scene with new\nenvironment maps while changing viewpoint in real-time. However, practical PRT\nmethods are usually limited to low-frequency spherical harmonic lighting.\nAll-frequency techniques using wavelets are promising but have so far had\nlittle practical impact. The curse of dimensionality and much higher data\nrequirements have typically limited them to relighting with fixed view or only\ndirect lighting with triple product integrals. In this paper, we demonstrate a\nhybrid neural-wavelet PRT solution to high-frequency indirect illumination,\nincluding glossy reflection, for relighting with changing view. Specifically,\nwe seek to represent the light transport function in the Haar wavelet basis.\nFor global illumination, we learn the wavelet transport using a small\nmulti-layer perceptron (MLP) applied to a feature field as a function of\nspatial location and wavelet index, with reflected direction and material\nparameters being other MLP inputs. We optimize/learn the feature field\n(compactly represented by a tensor decomposition) and MLP parameters from\nmultiple images of the scene under different lighting and viewing conditions.\nWe demonstrate real-time (512 x 512 at 24 FPS, 800 x 600 at 13 FPS) precomputed\nrendering of challenging scenes involving view-dependent reflections and even\ncaustics.",
        "translated": ""
    },
    {
        "title": "Deep Learning of Crystalline Defects from TEM images: A Solution for the\n  Problem of \"Never Enough Training Data\"",
        "url": "http://arxiv.org/abs/2307.06322v1",
        "pub_date": "2023-07-12",
        "summary": "Crystalline defects, such as line-like dislocations, play an important role\nfor the performance and reliability of many metallic devices. Their interaction\nand evolution still poses a multitude of open questions to materials science\nand materials physics. In-situ TEM experiments can provide important insights\ninto how dislocations behave and move. During such experiments, the dislocation\nmicrostructure is captured in form of videos. The analysis of individual video\nframes can provide useful insights but is limited by the capabilities of\nautomated identification, digitization, and quantitative extraction of the\ndislocations as curved objects. The vast amount of data also makes manual\nannotation very time consuming, thereby limiting the use of Deep\nLearning-based, automated image analysis and segmentation of the dislocation\nmicrostructure. In this work, a parametric model for generating synthetic\ntraining data for segmentation of dislocations is developed. Even though domain\nscientists might dismiss synthetic training images sometimes as too artificial,\nour findings show that they can result in superior performance, particularly\nregarding the generalizing of the Deep Learning models with respect to\ndifferent microstructures and imaging conditions. Additionally, we propose an\nenhanced deep learning method optimized for segmenting overlapping or\nintersecting dislocation lines. Upon testing this framework on four distinct\nreal datasets, we find that our synthetic training data are able to yield\nhigh-quality results also on real images-even more so if fine-tune on a few\nreal images was done.",
        "translated": ""
    },
    {
        "title": "Correlation-Aware Mutual Learning for Semi-supervised Medical Image\n  Segmentation",
        "url": "http://arxiv.org/abs/2307.06312v1",
        "pub_date": "2023-07-12",
        "summary": "Semi-supervised learning has become increasingly popular in medical image\nsegmentation due to its ability to leverage large amounts of unlabeled data to\nextract additional information. However, most existing semi-supervised\nsegmentation methods only focus on extracting information from unlabeled data,\ndisregarding the potential of labeled data to further improve the performance\nof the model. In this paper, we propose a novel Correlation Aware Mutual\nLearning (CAML) framework that leverages labeled data to guide the extraction\nof information from unlabeled data. Our approach is based on a mutual learning\nstrategy that incorporates two modules: the Cross-sample Mutual Attention\nModule (CMA) and the Omni-Correlation Consistency Module (OCC). The CMA module\nestablishes dense cross-sample correlations among a group of samples, enabling\nthe transfer of label prior knowledge to unlabeled data. The OCC module\nconstructs omni-correlations between the unlabeled and labeled datasets and\nregularizes dual models by constraining the omni-correlation matrix of each\nsub-model to be consistent. Experiments on the Atrial Segmentation Challenge\ndataset demonstrate that our proposed approach outperforms state-of-the-art\nmethods, highlighting the effectiveness of our framework in medical image\nsegmentation tasks. The codes, pre-trained weights, and data are publicly\navailable.",
        "translated": ""
    },
    {
        "title": "Facial Reenactment Through a Personalized Generator",
        "url": "http://arxiv.org/abs/2307.06307v1",
        "pub_date": "2023-07-12",
        "summary": "In recent years, the role of image generative models in facial reenactment\nhas been steadily increasing. Such models are usually subject-agnostic and\ntrained on domain-wide datasets. The appearance of the reenacted individual is\nlearned from a single image, and hence, the entire breadth of the individual's\nappearance is not entirely captured, leading these methods to resort to\nunfaithful hallucination. Thanks to recent advancements, it is now possible to\ntrain a personalized generative model tailored specifically to a given\nindividual. In this paper, we propose a novel method for facial reenactment\nusing a personalized generator. We train the generator using frames from a\nshort, yet varied, self-scan video captured using a simple commodity camera.\nImages synthesized by the personalized generator are guaranteed to preserve\nidentity. The premise of our work is that the task of reenactment is thus\nreduced to accurately mimicking head poses and expressions. To this end, we\nlocate the desired frames in the latent space of the personalized generator\nusing carefully designed latent optimization. Through extensive evaluation, we\ndemonstrate state-of-the-art performance for facial reenactment. Furthermore,\nwe show that since our reenactment takes place in a semantic latent space, it\ncan be semantically edited and stylized in post-processing.",
        "translated": ""
    },
    {
        "title": "Patch n' Pack: NaViT, a Vision Transformer for any Aspect Ratio and\n  Resolution",
        "url": "http://arxiv.org/abs/2307.06304v1",
        "pub_date": "2023-07-12",
        "summary": "The ubiquitous and demonstrably suboptimal choice of resizing images to a\nfixed resolution before processing them with computer vision models has not yet\nbeen successfully challenged. However, models such as the Vision Transformer\n(ViT) offer flexible sequence-based modeling, and hence varying input sequence\nlengths. We take advantage of this with NaViT (Native Resolution ViT) which\nuses sequence packing during training to process inputs of arbitrary\nresolutions and aspect ratios. Alongside flexible model usage, we demonstrate\nimproved training efficiency for large-scale supervised and contrastive\nimage-text pretraining. NaViT can be efficiently transferred to standard tasks\nsuch as image and video classification, object detection, and semantic\nsegmentation and leads to improved results on robustness and fairness\nbenchmarks. At inference time, the input resolution flexibility can be used to\nsmoothly navigate the test-time cost-performance trade-off. We believe that\nNaViT marks a departure from the standard, CNN-designed, input and modelling\npipeline used by most computer vision models, and represents a promising\ndirection for ViTs.",
        "translated": ""
    },
    {
        "title": "Improved Real-time Image Smoothing with Weak Structures Preserved and\n  High-contrast Details Removed",
        "url": "http://arxiv.org/abs/2307.06298v1",
        "pub_date": "2023-07-12",
        "summary": "Image smoothing is by reducing pixel-wise gradients to smooth out details. As\nexisting methods always rely on gradients to determine smoothing manners, it is\ndifficult to distinguish structures and details to handle distinctively due to\nthe overlapped ranges of gradients for structures and details. Thus, it is\nstill challenging to achieve high-quality results, especially on preserving\nweak structures and removing high-contrast details. In this paper, we address\nthis challenge by improving the real-time optimization-based method via\niterative least squares (called ILS). We observe that 1) ILS uses gradients as\nthe independent variable in its penalty function for determining smoothing\nmanners, and 2) the framework of ILS can still work for image smoothing when we\nuse some values instead of gradients in the penalty function. Thus,\ncorresponding to the properties of pixels on structures or not, we compute some\nvalues to use in the penalty function to determine smoothing manners, and so we\ncan handle structures and details distinctively, no matter whether their\ngradients are high or low. As a result, we can conveniently remove\nhigh-contrast details while preserving weak structures. Moreover, such values\ncan be adjusted to accelerate optimization computation, so that we can use\nfewer iterations than the original ILS method for efficiency. This also reduces\nthe changes onto structures to help structure preservation. Experimental\nresults show our advantages over existing methods on efficiency and quality.",
        "translated": ""
    },
    {
        "title": "MMBench: Is Your Multi-modal Model an All-around Player?",
        "url": "http://arxiv.org/abs/2307.06281v1",
        "pub_date": "2023-07-12",
        "summary": "Large vision-language models have recently achieved remarkable progress,\nexhibiting great perception and reasoning abilities concerning visual\ninformation. However, how to effectively evaluate these large vision-language\nmodels remains a major obstacle, hindering future model development.\nTraditional benchmarks like VQAv2 or COCO Caption provide quantitative\nperformance measurements but suffer from a lack of fine-grained ability\nassessment and non-robust evaluation metrics. Recent subjective benchmarks,\nsuch as OwlEval, offer comprehensive evaluations of a model's abilities by\nincorporating human labor, but they are not scalable and display significant\nbias. In response to these challenges, we propose MMBench, a novel\nmulti-modality benchmark. MMBench methodically develops a comprehensive\nevaluation pipeline, primarily comprised of two elements. The first element is\na meticulously curated dataset that surpasses existing similar benchmarks in\nterms of the number and variety of evaluation questions and abilities. The\nsecond element introduces a novel CircularEval strategy and incorporates the\nuse of ChatGPT. This implementation is designed to convert free-form\npredictions into pre-defined choices, thereby facilitating a more robust\nevaluation of the model's predictions. MMBench is a systematically-designed\nobjective benchmark for robustly evaluating the various abilities of\nvision-language models. We hope MMBench will assist the research community in\nbetter evaluating their models and encourage future advancements in this\ndomain. Project page: https://opencompass.org.cn/mmbench.",
        "translated": ""
    },
    {
        "title": "Stochastic Light Field Holography",
        "url": "http://arxiv.org/abs/2307.06277v1",
        "pub_date": "2023-07-12",
        "summary": "The Visual Turing Test is the ultimate goal to evaluate the realism of\nholographic displays. Previous studies have focused on addressing challenges\nsuch as limited \\'etendue and image quality over a large focal volume, but they\nhave not investigated the effect of pupil sampling on the viewing experience in\nfull 3D holograms. In this work, we tackle this problem with a novel hologram\ngeneration algorithm motivated by matching the projection operators of\nincoherent Light Field and coherent Wigner Function light transport. To this\nend, we supervise hologram computation using synthesized photographs, which are\nrendered on-the-fly using Light Field refocusing from stochastically sampled\npupil states during optimization. The proposed method produces holograms with\ncorrect parallax and focus cues, which are important for passing the Visual\nTuring Test. We validate that our approach compares favorably to\nstate-of-the-art CGH algorithms that use Light Field and Focal Stack\nsupervision. Our experiments demonstrate that our algorithm significantly\nimproves the realism of the viewing experience for a variety of different pupil\nstates.",
        "translated": ""
    },
    {
        "title": "Exposing the Fake: Effective Diffusion-Generated Images Detection",
        "url": "http://arxiv.org/abs/2307.06272v1",
        "pub_date": "2023-07-12",
        "summary": "Image synthesis has seen significant advancements with the advent of\ndiffusion-based generative models like Denoising Diffusion Probabilistic Models\n(DDPM) and text-to-image diffusion models. Despite their efficacy, there is a\ndearth of research dedicated to detecting diffusion-generated images, which\ncould pose potential security and privacy risks. This paper addresses this gap\nby proposing a novel detection method called Stepwise Error for\nDiffusion-generated Image Detection (SeDID). Comprising statistical-based\n$\\text{SeDID}_{\\text{Stat}}$ and neural network-based\n$\\text{SeDID}_{\\text{NNs}}$, SeDID exploits the unique attributes of diffusion\nmodels, namely deterministic reverse and deterministic denoising computation\nerrors. Our evaluations demonstrate SeDID's superior performance over existing\nmethods when applied to diffusion models. Thus, our work makes a pivotal\ncontribution to distinguishing diffusion model-generated images, marking a\nsignificant step in the domain of artificial intelligence security.",
        "translated": ""
    },
    {
        "title": "UGCANet: A Unified Global Context-Aware Transformer-based Network with\n  Feature Alignment for Endoscopic Image Analysis",
        "url": "http://arxiv.org/abs/2307.06260v1",
        "pub_date": "2023-07-12",
        "summary": "Gastrointestinal endoscopy is a medical procedure that utilizes a flexible\ntube equipped with a camera and other instruments to examine the digestive\ntract. This minimally invasive technique allows for diagnosing and managing\nvarious gastrointestinal conditions, including inflammatory bowel disease,\ngastrointestinal bleeding, and colon cancer. The early detection and\nidentification of lesions in the upper gastrointestinal tract and the\nidentification of malignant polyps that may pose a risk of cancer development\nare critical components of gastrointestinal endoscopy's diagnostic and\ntherapeutic applications. Therefore, enhancing the detection rates of\ngastrointestinal disorders can significantly improve a patient's prognosis by\nincreasing the likelihood of timely medical intervention, which may prolong the\npatient's lifespan and improve overall health outcomes. This paper presents a\nnovel Transformer-based deep neural network designed to perform multiple tasks\nsimultaneously, thereby enabling accurate identification of both upper\ngastrointestinal tract lesions and colon polyps. Our approach proposes a unique\nglobal context-aware module and leverages the powerful MiT backbone, along with\na feature alignment block, to enhance the network's representation capability.\nThis novel design leads to a significant improvement in performance across\nvarious endoscopic diagnosis tasks. Extensive experiments demonstrate the\nsuperior performance of our method compared to other state-of-the-art\napproaches.",
        "translated": ""
    },
    {
        "title": "HyperDreamBooth: HyperNetworks for Fast Personalization of Text-to-Image\n  Models",
        "url": "http://arxiv.org/abs/2307.06949v1",
        "pub_date": "2023-07-13",
        "summary": "Personalization has emerged as a prominent aspect within the field of\ngenerative AI, enabling the synthesis of individuals in diverse contexts and\nstyles, while retaining high-fidelity to their identities. However, the process\nof personalization presents inherent challenges in terms of time and memory\nrequirements. Fine-tuning each personalized model needs considerable GPU time\ninvestment, and storing a personalized model per subject can be demanding in\nterms of storage capacity. To overcome these challenges, we propose\nHyperDreamBooth-a hypernetwork capable of efficiently generating a small set of\npersonalized weights from a single image of a person. By composing these\nweights into the diffusion model, coupled with fast finetuning, HyperDreamBooth\ncan generate a person's face in various contexts and styles, with high subject\ndetails while also preserving the model's crucial knowledge of diverse styles\nand semantic modifications. Our method achieves personalization on faces in\nroughly 20 seconds, 25x faster than DreamBooth and 125x faster than Textual\nInversion, using as few as one reference image, with the same quality and style\ndiversity as DreamBooth. Also our method yields a model that is 10000x smaller\nthan a normal DreamBooth model. Project page: https://hyperdreambooth.github.io",
        "translated": ""
    },
    {
        "title": "Self-regulating Prompts: Foundational Model Adaptation without\n  Forgetting",
        "url": "http://arxiv.org/abs/2307.06948v1",
        "pub_date": "2023-07-13",
        "summary": "Prompt learning has emerged as an efficient alternative for fine-tuning\nfoundational models, such as CLIP, for various downstream tasks. Conventionally\ntrained using the task-specific objective, i.e., cross-entropy loss, prompts\ntend to overfit downstream data distributions and find it challenging to\ncapture task-agnostic general features from the frozen CLIP. This leads to the\nloss of the model's original generalization capability. To address this issue,\nour work introduces a self-regularization framework for prompting called\nPromptSRC (Prompting with Self-regulating Constraints). PromptSRC guides the\nprompts to optimize for both task-specific and task-agnostic general\nrepresentations using a three-pronged approach by: (a) regulating {prompted}\nrepresentations via mutual agreement maximization with the frozen model, (b)\nregulating with self-ensemble of prompts over the training trajectory to encode\ntheir complementary strengths, and (c) regulating with textual diversity to\nmitigate sample diversity imbalance with the visual branch. To the best of our\nknowledge, this is the first regularization framework for prompt learning that\navoids overfitting by jointly attending to pre-trained model features, the\ntraining trajectory during prompting, and the textual diversity. PromptSRC\nexplicitly steers the prompts to learn a representation space that maximizes\nperformance on downstream tasks without compromising CLIP generalization. We\nperform extensive experiments on 4 benchmarks where PromptSRC overall performs\nfavorably well compared to the existing methods. Our code and pre-trained\nmodels are publicly available at: https://github.com/muzairkhattak/PromptSRC.",
        "translated": ""
    },
    {
        "title": "Video-FocalNets: Spatio-Temporal Focal Modulation for Video Action\n  Recognition",
        "url": "http://arxiv.org/abs/2307.06947v1",
        "pub_date": "2023-07-13",
        "summary": "Recent video recognition models utilize Transformer models for long-range\nspatio-temporal context modeling. Video transformer designs are based on\nself-attention that can model global context at a high computational cost. In\ncomparison, convolutional designs for videos offer an efficient alternative but\nlack long-range dependency modeling. Towards achieving the best of both\ndesigns, this work proposes Video-FocalNet, an effective and efficient\narchitecture for video recognition that models both local and global contexts.\nVideo-FocalNet is based on a spatio-temporal focal modulation architecture that\nreverses the interaction and aggregation steps of self-attention for better\nefficiency. Further, the aggregation step and the interaction step are both\nimplemented using efficient convolution and element-wise multiplication\noperations that are computationally less expensive than their self-attention\ncounterparts on video representations. We extensively explore the design space\nof focal modulation-based spatio-temporal context modeling and demonstrate our\nparallel spatial and temporal encoding design to be the optimal choice.\nVideo-FocalNets perform favorably well against the state-of-the-art\ntransformer-based models for video recognition on three large-scale datasets\n(Kinetics-400, Kinetics-600, and SS-v2) at a lower computational cost. Our\ncode/models are released at https://github.com/TalalWasim/Video-FocalNets.",
        "translated": ""
    },
    {
        "title": "InternVid: A Large-scale Video-Text Dataset for Multimodal Understanding\n  and Generation",
        "url": "http://arxiv.org/abs/2307.06942v1",
        "pub_date": "2023-07-13",
        "summary": "This paper introduces InternVid, a large-scale video-centric multimodal\ndataset that enables learning powerful and transferable video-text\nrepresentations for multimodal understanding and generation. The InternVid\ndataset contains over 7 million videos lasting nearly 760K hours, yielding 234M\nvideo clips accompanied by detailed descriptions of total 4.1B words. Our core\ncontribution is to develop a scalable approach to autonomously build a\nhigh-quality video-text dataset with large language models (LLM), thereby\nshowcasing its efficacy in learning video-language representation at scale.\nSpecifically, we utilize a multi-scale approach to generate video-related\ndescriptions. Furthermore, we introduce ViCLIP, a video-text representation\nlearning model based on ViT-L. Learned on InternVid via contrastive learning,\nthis model demonstrates leading zero-shot action recognition and competitive\nvideo retrieval performance. Beyond basic video understanding tasks like\nrecognition and retrieval, our dataset and model have broad applications. They\nare particularly beneficial for generating interleaved video-text data for\nlearning a video-centric dialogue system, advancing video-to-text and\ntext-to-video generation research. These proposed resources provide a tool for\nresearchers and practitioners interested in multimodal video understanding and\ngeneration.",
        "translated": ""
    },
    {
        "title": "On the Connection between Game-Theoretic Feature Attributions and\n  Counterfactual Explanations",
        "url": "http://arxiv.org/abs/2307.06941v1",
        "pub_date": "2023-07-13",
        "summary": "Explainable Artificial Intelligence (XAI) has received widespread interest in\nrecent years, and two of the most popular types of explanations are feature\nattributions, and counterfactual explanations. These classes of approaches have\nbeen largely studied independently and the few attempts at reconciling them\nhave been primarily empirical. This work establishes a clear theoretical\nconnection between game-theoretic feature attributions, focusing on but not\nlimited to SHAP, and counterfactuals explanations. After motivating operative\nchanges to Shapley values based feature attributions and counterfactual\nexplanations, we prove that, under conditions, they are in fact equivalent. We\nthen extend the equivalency result to game-theoretic solution concepts beyond\nShapley values. Moreover, through the analysis of the conditions of such\nequivalence, we shed light on the limitations of naively using counterfactual\nexplanations to provide feature importances. Experiments on three datasets\nquantitatively show the difference in explanations at every stage of the\nconnection between the two approaches and corroborate the theoretical findings.",
        "translated": ""
    },
    {
        "title": "Animate-A-Story: Storytelling with Retrieval-Augmented Video Generation",
        "url": "http://arxiv.org/abs/2307.06940v1",
        "pub_date": "2023-07-13",
        "summary": "Generating videos for visual storytelling can be a tedious and complex\nprocess that typically requires either live-action filming or graphics\nanimation rendering. To bypass these challenges, our key idea is to utilize the\nabundance of existing video clips and synthesize a coherent storytelling video\nby customizing their appearances. We achieve this by developing a framework\ncomprised of two functional modules: (i) Motion Structure Retrieval, which\nprovides video candidates with desired scene or motion context described by\nquery texts, and (ii) Structure-Guided Text-to-Video Synthesis, which generates\nplot-aligned videos under the guidance of motion structure and text prompts.\nFor the first module, we leverage an off-the-shelf video retrieval system and\nextract video depths as motion structure. For the second module, we propose a\ncontrollable video generation model that offers flexible controls over\nstructure and characters. The videos are synthesized by following the\nstructural guidance and appearance instruction. To ensure visual consistency\nacross clips, we propose an effective concept personalization approach, which\nallows the specification of the desired character identities through text\nprompts. Extensive experiments demonstrate that our approach exhibits\nsignificant advantages over various existing baselines.",
        "translated": ""
    },
    {
        "title": "mBLIP: Efficient Bootstrapping of Multilingual Vision-LLMs",
        "url": "http://arxiv.org/abs/2307.06930v1",
        "pub_date": "2023-07-13",
        "summary": "Modular vision-language models (Vision-LLMs) align pretrained image encoders\nwith (pretrained) large language models (LLMs), representing a computationally\nmuch more efficient alternative to end-to-end training of large vision-language\nmodels from scratch, which is prohibitively expensive for most. Vision-LLMs\ninstead post-hoc condition LLMs to `understand' the output of an image encoder.\nWith the abundance of readily available high-quality English image-text data as\nwell as monolingual English LLMs, the research focus has been on English-only\nVision-LLMs. Multilingual vision-language models are still predominantly\nobtained via expensive end-to-end pretraining, resulting in comparatively\nsmaller models, trained on limited multilingual image data supplemented with\ntext-only multilingual corpora. In this work, we present mBLIP, the first\nmultilingual Vision-LLM, which we obtain in a computationally efficient manner\n-- on consumer hardware using only a few million training examples -- by\nleveraging a pretrained multilingual LLM. To this end, we \\textit{re-align} an\nimage encoder previously tuned to an English LLM to a new, multilingual LLM --\nfor this, we leverage multilingual data from a mix of vision-and-language\ntasks, which we obtain by machine-translating high-quality English data to 95\nlanguages. On the IGLUE benchmark, mBLIP yields results competitive with\nstate-of-the-art models. Moreover, in image captioning on XM3600, mBLIP\n(zero-shot) even outperforms PaLI-X (a model with 55B parameters). Compared to\nthese very large multilingual vision-language models trained from scratch, we\nobtain mBLIP by training orders of magnitude fewer parameters on magnitudes\nless data. We release our model and code at\n\\url{https://github.com/gregor-ge/mBLIP}.",
        "translated": ""
    },
    {
        "title": "Domain-Agnostic Tuning-Encoder for Fast Personalization of Text-To-Image\n  Models",
        "url": "http://arxiv.org/abs/2307.06925v1",
        "pub_date": "2023-07-13",
        "summary": "Text-to-image (T2I) personalization allows users to guide the creative image\ngeneration process by combining their own visual concepts in natural language\nprompts. Recently, encoder-based techniques have emerged as a new effective\napproach for T2I personalization, reducing the need for multiple images and\nlong training times. However, most existing encoders are limited to a\nsingle-class domain, which hinders their ability to handle diverse concepts. In\nthis work, we propose a domain-agnostic method that does not require any\nspecialized dataset or prior information about the personalized concepts. We\nintroduce a novel contrastive-based regularization technique to maintain high\nfidelity to the target concept characteristics while keeping the predicted\nembeddings close to editable regions of the latent space, by pushing the\npredicted tokens toward their nearest existing CLIP tokens. Our experimental\nresults demonstrate the effectiveness of our approach and show how the learned\ntokens are more semantic than tokens predicted by unregularized models. This\nleads to a better representation that achieves state-of-the-art performance\nwhile being more flexible than previous methods.",
        "translated": ""
    },
    {
        "title": "Uncovering Unique Concept Vectors through Latent Space Decomposition",
        "url": "http://arxiv.org/abs/2307.06913v1",
        "pub_date": "2023-07-13",
        "summary": "Interpreting the inner workings of deep learning models is crucial for\nestablishing trust and ensuring model safety. Concept-based explanations have\nemerged as a superior approach that is more interpretable than feature\nattribution estimates such as pixel saliency. However, defining the concepts\nfor the interpretability analysis biases the explanations by the user's\nexpectations on the concepts. To address this, we propose a novel post-hoc\nunsupervised method that automatically uncovers the concepts learned by deep\nmodels during training. By decomposing the latent space of a layer in singular\nvectors and refining them by unsupervised clustering, we uncover concept\nvectors aligned with directions of high variance that are relevant to the model\nprediction, and that point to semantically distinct concepts. Our extensive\nexperiments reveal that the majority of our concepts are readily understandable\nto humans, exhibit coherency, and bear relevance to the task at hand. Moreover,\nwe showcase the practical utility of our method in dataset exploration, where\nour concept vectors successfully identify outlier training samples affected by\nvarious confounding factors. This novel exploration technique has remarkable\nversatility to data types and model architectures and it will facilitate the\nidentification of biases and the discovery of sources of error within training\ndata.",
        "translated": ""
    },
    {
        "title": "LVLane: Deep Learning for Lane Detection and Classification in\n  Challenging Conditions",
        "url": "http://arxiv.org/abs/2307.06853v1",
        "pub_date": "2023-07-13",
        "summary": "Lane detection plays a pivotal role in the field of autonomous vehicles and\nadvanced driving assistant systems (ADAS). Over the years, numerous algorithms\nhave emerged, spanning from rudimentary image processing techniques to\nsophisticated deep neural networks. The performance of deep learning-based\nmodels is highly dependent on the quality of their training data. Consequently,\nthese models often experience a decline in performance when confronted with\nchallenging scenarios such as extreme lighting conditions, partially visible\nlane markings, and sparse lane markings like Botts' dots. To address this, we\npresent an end-to-end lane detection and classification system based on deep\nlearning methodologies. In our study, we introduce a unique dataset\nmeticulously curated to encompass scenarios that pose significant challenges\nfor state-of-the-art (SOTA) models. Through fine-tuning selected models, we aim\nto achieve enhanced localization accuracy. Moreover, we propose a CNN-based\nclassification branch, seamlessly integrated with the detector, facilitating\nthe identification of distinct lane types. This architecture enables informed\nlane-changing decisions and empowers more resilient ADAS capabilities. We also\ninvestigate the effect of using mixed precision training and testing on\ndifferent models and batch sizes. Experimental evaluations conducted on the\nwidely-used TuSimple dataset, Caltech lane dataset, and our LVLane dataset\ndemonstrate the effectiveness of our model in accurately detecting and\nclassifying lanes amidst challenging scenarios. Our method achieves\nstate-of-the-art classification results on the TuSimple dataset. The code of\nthe work will be published upon the acceptance of the paper.",
        "translated": ""
    },
    {
        "title": "NIFTY: Neural Object Interaction Fields for Guided Human Motion\n  Synthesis",
        "url": "http://arxiv.org/abs/2307.07511v1",
        "pub_date": "2023-07-14",
        "summary": "We address the problem of generating realistic 3D motions of humans\ninteracting with objects in a scene. Our key idea is to create a neural\ninteraction field attached to a specific object, which outputs the distance to\nthe valid interaction manifold given a human pose as input. This interaction\nfield guides the sampling of an object-conditioned human motion diffusion\nmodel, so as to encourage plausible contacts and affordance semantics. To\nsupport interactions with scarcely available data, we propose an automated\nsynthetic data pipeline. For this, we seed a pre-trained motion model, which\nhas priors for the basics of human movement, with interaction-specific anchor\nposes extracted from limited motion capture data. Using our guided diffusion\nmodel trained on generated synthetic data, we synthesize realistic motions for\nsitting and lifting with several objects, outperforming alternative approaches\nin terms of motion quality and successful action completion. We call our\nframework NIFTY: Neural Interaction Fields for Trajectory sYnthesis.",
        "translated": ""
    },
    {
        "title": "Brain Tumor Detection using Convolutional Neural Networks with Skip\n  Connections",
        "url": "http://arxiv.org/abs/2307.07503v1",
        "pub_date": "2023-07-14",
        "summary": "In this paper, we present different architectures of Convolutional Neural\nNetworks (CNN) to analyze and classify the brain tumors into benign and\nmalignant types using the Magnetic Resonance Imaging (MRI) technique. Different\nCNN architecture optimization techniques such as widening and deepening of the\nnetwork and adding skip connections are applied to improve the accuracy of the\nnetwork. Results show that a subset of these techniques can judiciously be used\nto outperform a baseline CNN model used for the same purpose.",
        "translated": ""
    },
    {
        "title": "TALL: Thumbnail Layout for Deepfake Video Detection",
        "url": "http://arxiv.org/abs/2307.07494v1",
        "pub_date": "2023-07-14",
        "summary": "The growing threats of deepfakes to society and cybersecurity have raised\nenormous public concerns, and increasing efforts have been devoted to this\ncritical topic of deepfake video detection. Existing video methods achieve good\nperformance but are computationally intensive. This paper introduces a simple\nyet effective strategy named Thumbnail Layout (TALL), which transforms a video\nclip into a pre-defined layout to realize the preservation of spatial and\ntemporal dependencies. Specifically, consecutive frames are masked in a fixed\nposition in each frame to improve generalization, then resized to sub-images\nand rearranged into a pre-defined layout as the thumbnail. TALL is\nmodel-agnostic and extremely simple by only modifying a few lines of code.\nInspired by the success of vision transformers, we incorporate TALL into Swin\nTransformer, forming an efficient and effective method TALL-Swin. Extensive\nexperiments on intra-dataset and cross-dataset validate the validity and\nsuperiority of TALL and SOTA TALL-Swin. TALL-Swin achieves 90.79$\\%$ AUC on the\nchallenging cross-dataset task, FaceForensics++ $\\to$ Celeb-DF. The code is\navailable at https://github.com/rainy-xu/TALL4Deepfake.",
        "translated": ""
    },
    {
        "title": "PseudoCal: A Source-Free Approach to Unsupervised Uncertainty\n  Calibration in Domain Adaptation",
        "url": "http://arxiv.org/abs/2307.07489v1",
        "pub_date": "2023-07-14",
        "summary": "Unsupervised domain adaptation (UDA) has witnessed remarkable advancements in\nimproving the accuracy of models for unlabeled target domains. However, the\ncalibration of predictive uncertainty in the target domain, a crucial aspect of\nthe safe deployment of UDA models, has received limited attention. The\nconventional in-domain calibration method, \\textit{temperature scaling}\n(TempScal), encounters challenges due to domain distribution shifts and the\nabsence of labeled target domain data. Recent approaches have employed\nimportance-weighting techniques to estimate the target-optimal temperature\nbased on re-weighted labeled source data. Nonetheless, these methods require\nsource data and suffer from unreliable density estimates under severe domain\nshifts, rendering them unsuitable for source-free UDA settings. To overcome\nthese limitations, we propose PseudoCal, a source-free calibration method that\nexclusively relies on unlabeled target data. Unlike previous approaches that\ntreat UDA calibration as a \\textit{covariate shift} problem, we consider it as\nan unsupervised calibration problem specific to the target domain. Motivated by\nthe factorization of the negative log-likelihood (NLL) objective in TempScal,\nwe generate a labeled pseudo-target set that captures the structure of the real\ntarget. By doing so, we transform the unsupervised calibration problem into a\nsupervised one, enabling us to effectively address it using widely-used\nin-domain methods like TempScal. Finally, we thoroughly evaluate the\ncalibration performance of PseudoCal by conducting extensive experiments on 10\nUDA methods, considering both traditional UDA settings and recent source-free\nUDA scenarios. The experimental results consistently demonstrate the superior\nperformance of PseudoCal, exhibiting significantly reduced calibration error\ncompared to existing calibration methods.",
        "translated": ""
    },
    {
        "title": "DreamTeacher: Pretraining Image Backbones with Deep Generative Models",
        "url": "http://arxiv.org/abs/2307.07487v1",
        "pub_date": "2023-07-14",
        "summary": "In this work, we introduce a self-supervised feature representation learning\nframework DreamTeacher that utilizes generative networks for pre-training\ndownstream image backbones. We propose to distill knowledge from a trained\ngenerative model into standard image backbones that have been well engineered\nfor specific perception tasks. We investigate two types of knowledge\ndistillation: 1) distilling learned generative features onto target image\nbackbones as an alternative to pretraining these backbones on large labeled\ndatasets such as ImageNet, and 2) distilling labels obtained from generative\nnetworks with task heads onto logits of target backbones. We perform extensive\nanalyses on multiple generative models, dense prediction benchmarks, and\nseveral pre-training regimes. We empirically find that our DreamTeacher\nsignificantly outperforms existing self-supervised representation learning\napproaches across the board. Unsupervised ImageNet pre-training with\nDreamTeacher leads to significant improvements over ImageNet classification\npre-training on downstream datasets, showcasing generative models, and\ndiffusion generative models specifically, as a promising approach to\nrepresentation learning on large, diverse datasets without requiring manual\nannotation.",
        "translated": ""
    },
    {
        "title": "Multimodal Distillation for Egocentric Action Recognition",
        "url": "http://arxiv.org/abs/2307.07483v1",
        "pub_date": "2023-07-14",
        "summary": "The focal point of egocentric video understanding is modelling hand-object\ninteractions. Standard models, e.g. CNNs or Vision Transformers, which receive\nRGB frames as input perform well. However, their performance improves further\nby employing additional input modalities that provide complementary cues, such\nas object detections, optical flow, audio, etc. The added complexity of the\nmodality-specific modules, on the other hand, makes these models impractical\nfor deployment. The goal of this work is to retain the performance of such a\nmultimodal approach, while using only the RGB frames as input at inference\ntime. We demonstrate that for egocentric action recognition on the\nEpic-Kitchens and the Something-Something datasets, students which are taught\nby multimodal teachers tend to be more accurate and better calibrated than\narchitecturally equivalent models trained on ground truth labels in a unimodal\nor multimodal fashion. We further adopt a principled multimodal knowledge\ndistillation framework, allowing us to deal with issues which occur when\napplying multimodal knowledge distillation in a naive manner. Lastly, we\ndemonstrate the achieved reduction in computational complexity, and show that\nour approach maintains higher performance with the reduction of the number of\ninput views.",
        "translated": ""
    },
    {
        "title": "Dual-Query Multiple Instance Learning for Dynamic Meta-Embedding based\n  Tumor Classification",
        "url": "http://arxiv.org/abs/2307.07482v1",
        "pub_date": "2023-07-14",
        "summary": "Whole slide image (WSI) assessment is a challenging and crucial step in\ncancer diagnosis and treatment planning. WSIs require high magnifications to\nfacilitate sub-cellular analysis. Precise annotations for patch- or even\npixel-level classifications in the context of gigapixel WSIs are tedious to\nacquire and require domain experts. Coarse-grained labels, on the other hand,\nare easily accessible, which makes WSI classification an ideal use case for\nmultiple instance learning (MIL). In our work, we propose a novel\nembedding-based Dual-Query MIL pipeline (DQ-MIL). We contribute to both the\nembedding and aggregation steps. Since all-purpose visual feature\nrepresentations are not yet available, embedding models are currently limited\nin terms of generalizability. With our work, we explore the potential of\ndynamic meta-embedding based on cutting-edge self-supervised pre-trained models\nin the context of MIL. Moreover, we propose a new MIL architecture capable of\ncombining MIL-attention with correlated self-attention. The Dual-Query\nPerceiver design of our approach allows us to leverage the concept of\nself-distillation and to combine the advantages of a small model in the context\nof a low data regime with the rich feature representation of a larger model. We\ndemonstrate the superior performance of our approach on three histopathological\ndatasets, where we show improvement of up to 10% over state-of-the-art\napproaches.",
        "translated": ""
    },
    {
        "title": "Interactive Spatiotemporal Token Attention Network for Skeleton-based\n  General Interactive Action Recognition",
        "url": "http://arxiv.org/abs/2307.07469v1",
        "pub_date": "2023-07-14",
        "summary": "Recognizing interactive action plays an important role in human-robot\ninteraction and collaboration. Previous methods use late fusion and\nco-attention mechanism to capture interactive relations, which have limited\nlearning capability or inefficiency to adapt to more interacting entities. With\nassumption that priors of each entity are already known, they also lack\nevaluations on a more general setting addressing the diversity of subjects. To\naddress these problems, we propose an Interactive Spatiotemporal Token\nAttention Network (ISTA-Net), which simultaneously model spatial, temporal, and\ninteractive relations. Specifically, our network contains a tokenizer to\npartition Interactive Spatiotemporal Tokens (ISTs), which is a unified way to\nrepresent motions of multiple diverse entities. By extending the entity\ndimension, ISTs provide better interactive representations. To jointly learn\nalong three dimensions in ISTs, multi-head self-attention blocks integrated\nwith 3D convolutions are designed to capture inter-token correlations. When\nmodeling correlations, a strict entity ordering is usually irrelevant for\nrecognizing interactive actions. To this end, Entity Rearrangement is proposed\nto eliminate the orderliness in ISTs for interchangeable entities. Extensive\nexperiments on four datasets verify the effectiveness of ISTA-Net by\noutperforming state-of-the-art methods. Our code is publicly available at\nhttps://github.com/Necolizer/ISTA-Net",
        "translated": ""
    },
    {
        "title": "Atlas-Based Interpretable Age Prediction",
        "url": "http://arxiv.org/abs/2307.07439v1",
        "pub_date": "2023-07-14",
        "summary": "Age prediction is an important part of medical assessments and research. It\ncan aid in detecting diseases as well as abnormal ageing by highlighting the\ndiscrepancy between chronological and biological age. To gain a comprehensive\nunderstanding of age-related changes observed in various body parts, we\ninvestigate them on a larger scale by using whole-body images. We utilise the\nGrad-CAM interpretability method to determine the body areas most predictive of\na person's age. We expand our analysis beyond individual subjects by employing\nregistration techniques to generate population-wide interpretability maps.\nFurthermore, we set state-of-the-art whole-body age prediction with a model\nthat achieves a mean absolute error of 2.76 years. Our findings reveal three\nprimary areas of interest: the spine, the autochthonous back muscles, and the\ncardiac region, which exhibits the highest importance.",
        "translated": ""
    },
    {
        "title": "Combining multitemporal optical and SAR data for LAI imputation with\n  BiLSTM network",
        "url": "http://arxiv.org/abs/2307.07434v1",
        "pub_date": "2023-07-14",
        "summary": "The Leaf Area Index (LAI) is vital for predicting winter wheat yield.\nAcquisition of crop conditions via Sentinel-2 remote sensing images can be\nhindered by persistent clouds, affecting yield predictions. Synthetic Aperture\nRadar (SAR) provides all-weather imagery, and the ratio between its cross- and\nco-polarized channels (C-band) shows a high correlation with time series LAI\nover winter wheat regions. This study evaluates the use of time series\nSentinel-1 VH/VV for LAI imputation, aiming to increase spatial-temporal\ndensity. We utilize a bidirectional LSTM (BiLSTM) network to impute time series\nLAI and use half mean squared error for each time step as the loss function. We\ntrained models on data from southern Germany and the North China Plain using\nonly LAI data generated by Sentinel-1 VH/VV and Sentinel-2. Experimental\nresults show BiLSTM outperforms traditional regression methods, capturing\nnonlinear dynamics between multiple time series. It proves robust in various\ngrowing conditions and is effective even with limited Sentinel-2 images.\nBiLSTM's performance surpasses that of LSTM, particularly over the senescence\nperiod. Therefore, BiLSTM can be used to impute LAI with time-series Sentinel-1\nVH/VV and Sentinel-2 data, and this method could be applied to other\ntime-series imputation issues.",
        "translated": ""
    },
    {
        "title": "Diffusion Models Beat GANs on Image Classification",
        "url": "http://arxiv.org/abs/2307.08702v1",
        "pub_date": "2023-07-17",
        "summary": "While many unsupervised learning models focus on one family of tasks, either\ngenerative or discriminative, we explore the possibility of a unified\nrepresentation learner: a model which uses a single pre-training stage to\naddress both families of tasks simultaneously. We identify diffusion models as\na prime candidate. Diffusion models have risen to prominence as a\nstate-of-the-art method for image generation, denoising, inpainting,\nsuper-resolution, manipulation, etc. Such models involve training a U-Net to\niteratively predict and remove noise, and the resulting model can synthesize\nhigh fidelity, diverse, novel images. The U-Net architecture, as a\nconvolution-based architecture, generates a diverse set of feature\nrepresentations in the form of intermediate feature maps. We present our\nfindings that these embeddings are useful beyond the noise prediction task, as\nthey contain discriminative information and can also be leveraged for\nclassification. We explore optimal methods for extracting and using these\nembeddings for classification tasks, demonstrating promising results on the\nImageNet classification task. We find that with careful feature selection and\npooling, diffusion models outperform comparable generative-discriminative\nmethods such as BigBiGAN for classification tasks. We investigate diffusion\nmodels in the transfer learning regime, examining their performance on several\nfine-grained visual classification datasets. We compare these embeddings to\nthose generated by competing architectures and pre-trainings for classification\ntasks.",
        "translated": ""
    },
    {
        "title": "Fast model inference and training on-board of Satellites",
        "url": "http://arxiv.org/abs/2307.08700v1",
        "pub_date": "2023-07-17",
        "summary": "Artificial intelligence onboard satellites has the potential to reduce data\ntransmission requirements, enable real-time decision-making and collaboration\nwithin constellations. This study deploys a lightweight foundational model\ncalled RaVAEn on D-Orbit's ION SCV004 satellite. RaVAEn is a variational\nauto-encoder (VAE) that generates compressed latent vectors from small image\ntiles, enabling several downstream tasks. In this work we demonstrate the\nreliable use of RaVAEn onboard a satellite, achieving an encoding time of\n0.110s for tiles of a 4.8x4.8 km$^2$ area. In addition, we showcase fast\nfew-shot training onboard a satellite using the latent representation of data.\nWe compare the deployment of the model on the on-board CPU and on the available\nMyriad vision processing unit (VPU) accelerator. To our knowledge, this work\nshows for the first time the deployment of a multi-task model on-board a\nCubeSat and the on-board training of a machine learning model.",
        "translated": ""
    },
    {
        "title": "Pair then Relation: Pair-Net for Panoptic Scene Graph Generation",
        "url": "http://arxiv.org/abs/2307.08699v1",
        "pub_date": "2023-07-17",
        "summary": "Panoptic Scene Graph (PSG) is a challenging task in Scene Graph Generation\n(SGG) that aims to create a more comprehensive scene graph representation using\npanoptic segmentation instead of boxes. However, current PSG methods have\nlimited performance, which can hinder downstream task development. To improve\nPSG methods, we conducted an in-depth analysis to identify the bottleneck of\nthe current PSG models, finding that inter-object pair-wise recall is a crucial\nfactor which was ignored by previous PSG methods. Based on this, we present a\nnovel framework: Pair then Relation (Pair-Net), which uses a Pair Proposal\nNetwork (PPN) to learn and filter sparse pair-wise relationships between\nsubjects and objects. We also observed the sparse nature of object pairs and\nused this insight to design a lightweight Matrix Learner within the PPN.\nThrough extensive ablation and analysis, our approach significantly improves\nupon leveraging the strong segmenter baseline. Notably, our approach achieves\nnew state-of-the-art results on the PSG benchmark, with over 10% absolute gains\ncompared to PSGFormer. The code of this paper is publicly available at\nhttps://github.com/king159/Pair-Net.",
        "translated": ""
    },
    {
        "title": "Flow Matching in Latent Space",
        "url": "http://arxiv.org/abs/2307.08698v1",
        "pub_date": "2023-07-17",
        "summary": "Flow matching is a recent framework to train generative models that exhibits\nimpressive empirical performance while being relatively easier to train\ncompared with diffusion-based models. Despite its advantageous properties,\nprior methods still face the challenges of expensive computing and a large\nnumber of function evaluations of off-the-shelf solvers in the pixel space.\nFurthermore, although latent-based generative methods have shown great success\nin recent years, this particular model type remains underexplored in this area.\nIn this work, we propose to apply flow matching in the latent spaces of\npretrained autoencoders, which offers improved computational efficiency and\nscalability for high-resolution image synthesis. This enables flow-matching\ntraining on constrained computational resources while maintaining their quality\nand flexibility. Additionally, our work stands as a pioneering contribution in\nthe integration of various conditions into flow matching for conditional\ngeneration tasks, including label-conditioned image generation, image\ninpainting, and semantic-to-image generation. Through extensive experiments,\nour approach demonstrates its effectiveness in both quantitative and\nqualitative results on various datasets, such as CelebA-HQ, FFHQ, LSUN Church &amp;\nBedroom, and ImageNet. We also provide a theoretical control of the\nWasserstein-2 distance between the reconstructed latent flow distribution and\ntrue data distribution, showing it is upper-bounded by the latent flow matching\nobjective. Our code will be available at\nhttps://github.com/VinAIResearch/LFM.git.",
        "translated": ""
    },
    {
        "title": "Neural Video Depth Stabilizer",
        "url": "http://arxiv.org/abs/2307.08695v1",
        "pub_date": "2023-07-17",
        "summary": "Video depth estimation aims to infer temporally consistent depth. Some\nmethods achieve temporal consistency by finetuning a single-image depth model\nduring test time using geometry and re-projection constraints, which is\ninefficient and not robust. An alternative approach is to learn how to enforce\ntemporal consistency from data, but this requires well-designed models and\nsufficient video depth data. To address these challenges, we propose a\nplug-and-play framework called Neural Video Depth Stabilizer (NVDS) that\nstabilizes inconsistent depth estimations and can be applied to different\nsingle-image depth models without extra effort. We also introduce a large-scale\ndataset, Video Depth in the Wild (VDW), which consists of 14,203 videos with\nover two million frames, making it the largest natural-scene video depth\ndataset to our knowledge. We evaluate our method on the VDW dataset as well as\ntwo public benchmarks and demonstrate significant improvements in consistency,\naccuracy, and efficiency compared to previous approaches. Our work serves as a\nsolid baseline and provides a data foundation for learning-based video depth\nmodels. We will release our dataset and code for future research.",
        "translated": ""
    },
    {
        "title": "SEMI-DiffusionInst: A Diffusion Model Based Approach for Semiconductor\n  Defect Classification and Segmentation",
        "url": "http://arxiv.org/abs/2307.08693v1",
        "pub_date": "2023-07-17",
        "summary": "With continuous progression of Moore's Law, integrated circuit (IC) device\ncomplexity is also increasing. Scanning Electron Microscope (SEM) image based\nextensive defect inspection and accurate metrology extraction are two main\nchallenges in advanced node (2 nm and beyond) technology. Deep learning (DL)\nalgorithm based computer vision approaches gained popularity in semiconductor\ndefect inspection over last few years. In this research work, a new\nsemiconductor defect inspection framework \"SEMI-DiffusionInst\" is investigated\nand compared to previous frameworks. To the best of the authors' knowledge,\nthis work is the first demonstration to accurately detect and precisely segment\nsemiconductor defect patterns by using a diffusion model. Different feature\nextractor networks as backbones and data sampling strategies are investigated\ntowards achieving a balanced trade-off between precision and computing\nefficiency. Our proposed approach outperforms previous work on overall mAP and\nperforms comparatively better or as per for almost all defect classes (per\nclass APs). The bounding box and segmentation mAPs achieved by the proposed\nSEMI-DiffusionInst model are improved by 3.83% and 2.10%,respectively. Among\nindividual defect types, precision on line collapse and thin bridge defects are\nimproved approximately 15% on detection task for both defect types. It has also\nbeen shown that by tuning inference hyperparameters, inference time can be\nimproved significantly without compromising model precision. Finally, certain\nlimitations and future work strategy to overcome them are discussed.",
        "translated": ""
    },
    {
        "title": "Implementation of a perception system for autonomous vehicles using a\n  detection-segmentation network in SoC FPGA",
        "url": "http://arxiv.org/abs/2307.08682v1",
        "pub_date": "2023-07-17",
        "summary": "Perception and control systems for autonomous vehicles are an active area of\nscientific and industrial research. These solutions should be characterised by\nhigh efficiency in recognising obstacles and other environmental elements in\ndifferent road conditions, real-time capability, and energy efficiency.\nAchieving such functionality requires an appropriate algorithm and a suitable\ncomputing platform. In this paper, we have used the MultiTaskV3\ndetection-segmentation network as the basis for a perception system that can\nperform both functionalities within a single architecture. It was appropriately\ntrained, quantised, and implemented on the AMD Xilinx Kria KV260 Vision AI\nembedded platform. By using this device, it was possible to parallelise and\naccelerate the computations. Furthermore, the whole system consumes relatively\nlittle power compared to a CPU-based implementation (an average of 5 watts,\ncompared to the minimum of 55 watts for weaker CPUs, and the small size (119mm\nx 140mm x 36mm) of the platform allows it to be used in devices where the\namount of space available is limited. It also achieves an accuracy higher than\n97% of the mAP (mean average precision) for object detection and above 90% of\nthe mIoU (mean intersection over union) for image segmentation. The article\nalso details the design of the Mecanum wheel vehicle, which was used to test\nthe proposed solution in a mock-up city.",
        "translated": ""
    },
    {
        "title": "CohortFinder: an open-source tool for data-driven partitioning of\n  biomedical image cohorts to yield robust machine learning models",
        "url": "http://arxiv.org/abs/2307.08673v1",
        "pub_date": "2023-07-17",
        "summary": "Batch effects (BEs) refer to systematic technical differences in data\ncollection unrelated to biological variations whose noise is shown to\nnegatively impact machine learning (ML) model generalizability. Here we release\nCohortFinder, an open-source tool aimed at mitigating BEs via data-driven\ncohort partitioning. We demonstrate CohortFinder improves ML model performance\nin downstream medical image processing tasks. CohortFinder is freely available\nfor download at cohortfinder.com.",
        "translated": ""
    },
    {
        "title": "Quaternion Convolutional Neural Networks: Current Advances and Future\n  Directions",
        "url": "http://arxiv.org/abs/2307.08663v1",
        "pub_date": "2023-07-17",
        "summary": "Since their first applications, Convolutional Neural Networks (CNNs) have\nsolved problems that have advanced the state-of-the-art in several domains.\nCNNs represent information using real numbers. Despite encouraging results,\ntheoretical analysis shows that representations such as hyper-complex numbers\ncan achieve richer representational capacities than real numbers, and that\nHamilton products can capture intrinsic interchannel relationships. Moreover,\nin the last few years, experimental research has shown that Quaternion-Valued\nCNNs (QCNNs) can achieve similar performance with fewer parameters than their\nreal-valued counterparts. This paper condenses research in the development of\nQCNNs from its very beginnings. We propose a conceptual organization of current\ntrends and analyze the main building blocks used in the design of QCNN models.\nBased on this conceptual organization, we propose future directions of\nresearch.",
        "translated": ""
    },
    {
        "title": "PolyGNN: Polyhedron-based Graph Neural Network for 3D Building\n  Reconstruction from Point Clouds",
        "url": "http://arxiv.org/abs/2307.08636v1",
        "pub_date": "2023-07-17",
        "summary": "We present PolyGNN, a polyhedron-based graph neural network for 3D building\nreconstruction from point clouds. PolyGNN learns to assemble primitives\nobtained by polyhedral decomposition via graph node classification, achieving a\nwatertight, compact, and weakly semantic reconstruction. To effectively\nrepresent arbitrary-shaped polyhedra in the neural network, we propose three\ndifferent sampling strategies to select representative points as\npolyhedron-wise queries, enabling efficient occupancy inference. Furthermore,\nwe incorporate the inter-polyhedron adjacency to enhance the classification of\nthe graph nodes. We also observe that existing city-building models are\nabstractions of the underlying instances. To address this abstraction gap and\nprovide a fair evaluation of the proposed method, we develop our method on a\nlarge-scale synthetic dataset covering 500k+ buildings with well-defined ground\ntruths of polyhedral class labels. We further conduct a transferability\nanalysis across cities and on real-world point clouds. Both qualitative and\nquantitative results demonstrate the effectiveness of our method, particularly\nits efficiency for large-scale reconstructions. The source code and data of our\nwork are available at https://github.com/chenzhaiyu/polygnn.",
        "translated": ""
    },
    {
        "title": "AnyDoor: Zero-shot Object-level Image Customization",
        "url": "http://arxiv.org/abs/2307.09481v1",
        "pub_date": "2023-07-18",
        "summary": "This work presents AnyDoor, a diffusion-based image generator with the power\nto teleport target objects to new scenes at user-specified locations in a\nharmonious way. Instead of tuning parameters for each object, our model is\ntrained only once and effortlessly generalizes to diverse object-scene\ncombinations at the inference stage. Such a challenging zero-shot setting\nrequires an adequate characterization of a certain object. To this end, we\ncomplement the commonly used identity feature with detail features, which are\ncarefully designed to maintain texture details yet allow versatile local\nvariations (e.g., lighting, orientation, posture, etc.), supporting the object\nin favorably blending with different surroundings. We further propose to borrow\nknowledge from video datasets, where we can observe various forms (i.e., along\nthe time axis) of a single object, leading to stronger model generalizability\nand robustness. Extensive experiments demonstrate the superiority of our\napproach over existing alternatives as well as its great potential in\nreal-world applications, such as virtual try-on and object moving. Project page\nis https://damo-vilab.github.io/AnyDoor-Page/.",
        "translated": ""
    },
    {
        "title": "FACTS: Facial Animation Creation using the Transfer of Styles",
        "url": "http://arxiv.org/abs/2307.09480v1",
        "pub_date": "2023-07-18",
        "summary": "The ability to accurately capture and express emotions is a critical aspect\nof creating believable characters in video games and other forms of\nentertainment. Traditionally, this animation has been achieved with artistic\neffort or performance capture, both requiring costs in time and labor. More\nrecently, audio-driven models have seen success, however, these often lack\nexpressiveness in areas not correlated to the audio signal. In this paper, we\npresent a novel approach to facial animation by taking existing animations and\nallowing for the modification of style characteristics. Specifically, we\nexplore the use of a StarGAN to enable the conversion of 3D facial animations\ninto different emotions and person-specific styles. We are able to maintain the\nlip-sync of the animations with this method thanks to the use of a novel\nviseme-preserving loss.",
        "translated": ""
    },
    {
        "title": "ChatSpot: Bootstrapping Multimodal LLMs via Precise Referring\n  Instruction Tuning",
        "url": "http://arxiv.org/abs/2307.09474v1",
        "pub_date": "2023-07-18",
        "summary": "Human-AI interactivity is a critical aspect that reflects the usability of\nmultimodal large language models (MLLMs). However, existing end-to-end MLLMs\nonly allow users to interact with them through language instructions, leading\nto the limitation of the interactive accuracy and efficiency. In this study, we\npresent precise referring instructions that utilize diverse reference\nrepresentations such as points and boxes as referring prompts to refer to the\nspecial region. This enables MLLMs to focus on the region of interest and\nachieve finer-grained interaction. Based on precise referring instruction, we\npropose ChatSpot, a unified end-to-end multimodal large language model that\nsupports diverse forms of interactivity including mouse clicks, drag-and-drop,\nand drawing boxes, which provides a more flexible and seamless interactive\nexperience. We also construct a multi-grained vision-language\ninstruction-following dataset based on existing datasets and GPT-4 generating.\nFurthermore, we design a series of evaluation tasks to assess the effectiveness\nof region recognition and interaction. Experimental results showcase ChatSpot's\npromising performance.",
        "translated": ""
    },
    {
        "title": "GroupLane: End-to-End 3D Lane Detection with Channel-wise Grouping",
        "url": "http://arxiv.org/abs/2307.09472v1",
        "pub_date": "2023-07-18",
        "summary": "Efficiency is quite important for 3D lane detection due to practical\ndeployment demand. In this work, we propose a simple, fast, and end-to-end\ndetector that still maintains high detection precision. Specifically, we devise\na set of fully convolutional heads based on row-wise classification. In\ncontrast to previous counterparts, ours supports recognizing both vertical and\nhorizontal lanes. Besides, our method is the first one to perform row-wise\nclassification in bird-eye-view. In the heads, we split feature into multiple\ngroups and every group of feature corresponds to a lane instance. During\ntraining, the predictions are associated with lane labels using the proposed\nsingle-win one-to-one matching to compute loss, and no post-processing\noperation is demanded for inference. In this way, our proposed fully\nconvolutional detector, GroupLane, realizes end-to-end detection like DETR.\nEvaluated on 3 real world 3D lane benchmarks, OpenLane, Once-3DLanes, and\nOpenLane-Huawei, GroupLane adopting ConvNext-Base as the backbone outperforms\nthe published state-of-the-art PersFormer by 13.6% F1 score in the OpenLane\nvalidation set. Besides, GroupLane with ResNet18 still surpasses PersFormer by\n4.9% F1 score, while the inference speed is nearly 7x faster and the FLOPs is\nonly 13.3% of it.",
        "translated": ""
    },
    {
        "title": "Occlusion Aware Student Emotion Recognition based on Facial Action Unit\n  Detection",
        "url": "http://arxiv.org/abs/2307.09465v1",
        "pub_date": "2023-07-18",
        "summary": "Given that approximately half of science, technology, engineering, and\nmathematics (STEM) undergraduate students in U.S. colleges and universities\nleave by the end of the first year [15], it is crucial to improve the quality\nof classroom environments. This study focuses on monitoring students' emotions\nin the classroom as an indicator of their engagement and proposes an approach\nto address this issue. The impact of different facial parts on the performance\nof an emotional recognition model is evaluated through experimentation. To test\nthe proposed model under partial occlusion, an artificially occluded dataset is\nintroduced. The novelty of this work lies in the proposal of an occlusion-aware\narchitecture for facial action units (AUs) extraction, which employs attention\nmechanism and adaptive feature learning. The AUs can be used later to classify\nfacial expressions in classroom settings.\n  This research paper's findings provide valuable insights into handling\nocclusion in analyzing facial images for emotional engagement analysis. The\nproposed experiments demonstrate the significance of considering occlusion and\nenhancing the reliability of facial analysis models in classroom environments.\nThese findings can also be extended to other settings where occlusions are\nprevalent.",
        "translated": ""
    },
    {
        "title": "A comparative analysis of SRGAN models",
        "url": "http://arxiv.org/abs/2307.09456v2",
        "pub_date": "2023-07-18",
        "summary": "In this study, we evaluate the performance of multiple state-of-the-art SRGAN\n(Super Resolution Generative Adversarial Network) models, ESRGAN, Real-ESRGAN\nand EDSR, on a benchmark dataset of real-world images which undergo degradation\nusing a pipeline. Our results show that some models seem to significantly\nincrease the resolution of the input images while preserving their visual\nquality, this is assessed using Tesseract OCR engine. We observe that EDSR-BASE\nmodel from huggingface outperforms the remaining candidate models in terms of\nboth quantitative metrics and subjective visual quality assessments with least\ncompute overhead. Specifically, EDSR generates images with higher peak\nsignal-to-noise ratio (PSNR) and structural similarity index (SSIM) values and\nare seen to return high quality OCR results with Tesseract OCR engine. These\nfindings suggest that EDSR is a robust and effective approach for single-image\nsuper-resolution and may be particularly well-suited for applications where\nhigh-quality visual fidelity is critical and optimized compute.",
        "translated": ""
    },
    {
        "title": "Unsupervised Conditional Slot Attention for Object Centric Learning",
        "url": "http://arxiv.org/abs/2307.09437v1",
        "pub_date": "2023-07-18",
        "summary": "Extracting object-level representations for downstream reasoning tasks is an\nemerging area in AI. Learning object-centric representations in an unsupervised\nsetting presents multiple challenges, a key one being binding an arbitrary\nnumber of object instances to a specialized object slot. Recent object-centric\nrepresentation methods like Slot Attention utilize iterative attention to learn\ncomposable representations with dynamic inference level binding but fail to\nachieve specialized slot level binding. To address this, in this paper we\npropose Unsupervised Conditional Slot Attention using a novel Probabilistic\nSlot Dictionary (PSD). We define PSD with (i) abstract object-level property\nvectors as key and (ii) parametric Gaussian distribution as its corresponding\nvalue. We demonstrate the benefits of the learnt specific object-level\nconditioning distributions in multiple downstream tasks, namely object\ndiscovery, compositional scene generation, and compositional visual reasoning.\nWe show that our method provides scene composition capabilities and a\nsignificant boost in a few shot adaptability tasks of compositional visual\nreasoning, while performing similarly or better than slot attention in object\ndiscovery tasks",
        "translated": ""
    },
    {
        "title": "Measuring Student Behavioral Engagement using Histogram of Actions",
        "url": "http://arxiv.org/abs/2307.09420v1",
        "pub_date": "2023-07-18",
        "summary": "In this paper, we propose a novel technique for measuring behavioral\nengagement through students' actions recognition. The proposed approach\nrecognizes student actions then predicts the student behavioral engagement\nlevel. For student action recognition, we use human skeletons to model student\npostures and upper body movements. To learn the dynamics of student upper body,\na 3D-CNN model is used. The trained 3D-CNN model is used to recognize actions\nwithin every 2minute video segment then these actions are used to build a\nhistogram of actions which encodes the student actions and their frequencies.\nThis histogram is utilized as an input to SVM classifier to classify whether\nthe student is engaged or disengaged. To evaluate the proposed framework, we\nbuild a dataset consisting of 1414 2-minute video segments annotated with 13\nactions and 112 video segments annotated with two engagement levels.\nExperimental results indicate that student actions can be recognized with top 1\naccuracy 83.63% and the proposed framework can capture the average engagement\nof the class.",
        "translated": ""
    },
    {
        "title": "Let's ViCE! Mimicking Human Cognitive Behavior in Image Generation\n  Evaluation",
        "url": "http://arxiv.org/abs/2307.09416v2",
        "pub_date": "2023-07-18",
        "summary": "Research in Image Generation has recently made significant progress,\nparticularly boosted by the introduction of Vision-Language models which are\nable to produce high-quality visual content based on textual inputs. Despite\nongoing advancements in terms of generation quality and realism, no methodical\nframeworks have been defined yet to quantitatively measure the quality of the\ngenerated content and the adherence with the prompted requests: so far, only\nhuman-based evaluations have been adopted for quality satisfaction and for\ncomparing different generative methods. We introduce a novel automated method\nfor Visual Concept Evaluation (ViCE), i.e. to assess consistency between a\ngenerated/edited image and the corresponding prompt/instructions, with a\nprocess inspired by the human cognitive behaviour. ViCE combines the strengths\nof Large Language Models (LLMs) and Visual Question Answering (VQA) into a\nunified pipeline, aiming to replicate the human cognitive process in quality\nassessment. This method outlines visual concepts, formulates image-specific\nverification questions, utilizes the Q&amp;A system to investigate the image, and\nscores the combined outcome. Although this brave new hypothesis of mimicking\nhumans in the image evaluation process is in its preliminary assessment stage,\nresults are promising and open the door to a new form of automatic evaluation\nwhich could have significant impact as the image generation or the image target\nediting tasks become more and more sophisticated.",
        "translated": ""
    },
    {
        "title": "Plug the Leaks: Advancing Audio-driven Talking Face Generation by\n  Preventing Unintended Information Flow",
        "url": "http://arxiv.org/abs/2307.09368v1",
        "pub_date": "2023-07-18",
        "summary": "Audio-driven talking face generation is the task of creating a\nlip-synchronized, realistic face video from given audio and reference frames.\nThis involves two major challenges: overall visual quality of generated images\non the one hand, and audio-visual synchronization of the mouth part on the\nother hand. In this paper, we start by identifying several problematic aspects\nof synchronization methods in recent audio-driven talking face generation\napproaches. Specifically, this involves unintended flow of lip and pose\ninformation from the reference to the generated image, as well as instabilities\nduring model training. Subsequently, we propose various techniques for\nobviating these issues: First, a silent-lip reference image generator prevents\nleaking of lips from the reference to the generated image. Second, an adaptive\ntriplet loss handles the pose leaking problem. Finally, we propose a stabilized\nformulation of synchronization loss, circumventing aforementioned training\ninstabilities while additionally further alleviating the lip leaking issue.\nCombining the individual improvements, we present state-of-the art performance\non LRS2 and LRW in both synchronization and visual quality. We further validate\nour design in various ablation experiments, confirming the individual\ncontributions as well as their complementary effects.",
        "translated": ""
    },
    {
        "title": "DNA-Rendering: A Diverse Neural Actor Repository for High-Fidelity\n  Human-centric Rendering",
        "url": "http://arxiv.org/abs/2307.10173v1",
        "pub_date": "2023-07-19",
        "summary": "Realistic human-centric rendering plays a key role in both computer vision\nand computer graphics. Rapid progress has been made in the algorithm aspect\nover the years, yet existing human-centric rendering datasets and benchmarks\nare rather impoverished in terms of diversity, which are crucial for rendering\neffect. Researchers are usually constrained to explore and evaluate a small set\nof rendering problems on current datasets, while real-world applications\nrequire methods to be robust across different scenarios. In this work, we\npresent DNA-Rendering, a large-scale, high-fidelity repository of human\nperformance data for neural actor rendering. DNA-Rendering presents several\nalluring attributes. First, our dataset contains over 1500 human subjects, 5000\nmotion sequences, and 67.5M frames' data volume. Second, we provide rich assets\nfor each subject -- 2D/3D human body keypoints, foreground masks, SMPLX models,\ncloth/accessory materials, multi-view images, and videos. These assets boost\nthe current method's accuracy on downstream rendering tasks. Third, we\nconstruct a professional multi-view system to capture data, which contains 60\nsynchronous cameras with max 4096 x 3000 resolution, 15 fps speed, and stern\ncamera calibration steps, ensuring high-quality resources for task training and\nevaluation. Along with the dataset, we provide a large-scale and quantitative\nbenchmark in full-scale, with multiple tasks to evaluate the existing progress\nof novel view synthesis, novel pose animation synthesis, and novel identity\nrendering methods. In this manuscript, we describe our DNA-Rendering effort as\na revealing of new observations, challenges, and future directions to\nhuman-centric rendering. The dataset, code, and benchmarks will be publicly\navailable at https://dna-rendering.github.io/",
        "translated": ""
    },
    {
        "title": "Adversarial Latent Autoencoder with Self-Attention for Structural Image\n  Synthesis",
        "url": "http://arxiv.org/abs/2307.10166v1",
        "pub_date": "2023-07-19",
        "summary": "Generative Engineering Design approaches driven by Deep Generative Models\n(DGM) have been proposed to facilitate industrial engineering processes. In\nsuch processes, designs often come in the form of images, such as blueprints,\nengineering drawings, and CAD models depending on the level of detail. DGMs\nhave been successfully employed for synthesis of natural images, e.g.,\ndisplaying animals, human faces and landscapes. However, industrial design\nimages are fundamentally different from natural scenes in that they contain\nrich structural patterns and long-range dependencies, which are challenging for\nconvolution-based DGMs to generate. Moreover, DGM-driven generation process is\ntypically triggered based on random noisy inputs, which outputs unpredictable\nsamples and thus cannot perform an efficient industrial design exploration. We\ntackle these challenges by proposing a novel model Self-Attention Adversarial\nLatent Autoencoder (SA-ALAE), which allows generating feasible design images of\ncomplex engineering parts. With SA-ALAE, users can not only explore novel\nvariants of an existing design, but also control the generation process by\noperating in latent space. The potential of SA-ALAE is shown by generating\nengineering blueprints in a real automotive design task.",
        "translated": ""
    },
    {
        "title": "Drone navigation and license place detection for vehicle location in\n  indoor spaces",
        "url": "http://arxiv.org/abs/2307.10165v2",
        "pub_date": "2023-07-19",
        "summary": "Millions of vehicles are transported every year, tightly parked in vessels or\nboats. To reduce the risks of associated safety issues like fires, knowing the\nlocation of vehicles is essential, since different vehicles may need different\nmitigation measures, e.g. electric cars. This work is aimed at creating a\nsolution based on a nano-drone that navigates across rows of parked vehicles\nand detects their license plates. We do so via a wall-following algorithm, and\na CNN trained to detect license plates. All computations are done in real-time\non the drone, which just sends position and detected images that allow the\ncreation of a 2D map with the position of the plates. Our solution is capable\nof reading all plates across eight test cases (with several rows of plates,\ndifferent drone speeds, or low light) by aggregation of measurements across\nseveral drone journeys.",
        "translated": ""
    },
    {
        "title": "Robust Driving Policy Learning with Guided Meta Reinforcement Learning",
        "url": "http://arxiv.org/abs/2307.10160v1",
        "pub_date": "2023-07-19",
        "summary": "Although deep reinforcement learning (DRL) has shown promising results for\nautonomous navigation in interactive traffic scenarios, existing work typically\nadopts a fixed behavior policy to control social vehicles in the training\nenvironment. This may cause the learned driving policy to overfit the\nenvironment, making it difficult to interact well with vehicles with different,\nunseen behaviors. In this work, we introduce an efficient method to train\ndiverse driving policies for social vehicles as a single meta-policy. By\nrandomizing the interaction-based reward functions of social vehicles, we can\ngenerate diverse objectives and efficiently train the meta-policy through\nguiding policies that achieve specific objectives. We further propose a\ntraining strategy to enhance the robustness of the ego vehicle's driving policy\nusing the environment where social vehicles are controlled by the learned\nmeta-policy. Our method successfully learns an ego driving policy that\ngeneralizes well to unseen situations with out-of-distribution (OOD) social\nagents' behaviors in a challenging uncontrolled T-intersection scenario.",
        "translated": ""
    },
    {
        "title": "FABRIC: Personalizing Diffusion Models with Iterative Feedback",
        "url": "http://arxiv.org/abs/2307.10159v1",
        "pub_date": "2023-07-19",
        "summary": "In an era where visual content generation is increasingly driven by machine\nlearning, the integration of human feedback into generative models presents\nsignificant opportunities for enhancing user experience and output quality.\nThis study explores strategies for incorporating iterative human feedback into\nthe generative process of diffusion-based text-to-image models. We propose\nFABRIC, a training-free approach applicable to a wide range of popular\ndiffusion models, which exploits the self-attention layer present in the most\nwidely used architectures to condition the diffusion process on a set of\nfeedback images. To ensure a rigorous assessment of our approach, we introduce\na comprehensive evaluation methodology, offering a robust mechanism to quantify\nthe performance of generative visual models that integrate human feedback. We\nshow that generation results improve over multiple rounds of iterative feedback\nthrough exhaustive analysis, implicitly optimizing arbitrary user preferences.\nThe potential applications of these findings extend to fields such as\npersonalized content creation and customization.",
        "translated": ""
    },
    {
        "title": "Leveraging Visemes for Better Visual Speech Representation and Lip\n  Reading",
        "url": "http://arxiv.org/abs/2307.10157v1",
        "pub_date": "2023-07-19",
        "summary": "Lip reading is a challenging task that has many potential applications in\nspeech recognition, human-computer interaction, and security systems. However,\nexisting lip reading systems often suffer from low accuracy due to the\nlimitations of video features. In this paper, we propose a novel approach that\nleverages visemes, which are groups of phonetically similar lip shapes, to\nextract more discriminative and robust video features for lip reading. We\nevaluate our approach on various tasks, including word-level and sentence-level\nlip reading, and audiovisual speech recognition using the Arman-AV dataset, a\nlargescale Persian corpus. Our experimental results show that our viseme based\napproach consistently outperforms the state-of-theart methods in all these\ntasks. The proposed method reduces the lip-reading word error rate (WER) by\n9.1% relative to the best previous method.",
        "translated": ""
    },
    {
        "title": "An Improved NeuMIP with Better Accuracy",
        "url": "http://arxiv.org/abs/2307.10135v1",
        "pub_date": "2023-07-19",
        "summary": "Neural reflectance models are capable of accurately reproducing the\nspatially-varying appearance of many real-world materials at different scales.\nHowever, existing methods have difficulties handling highly glossy materials.\nTo address this problem, we introduce a new neural reflectance model which,\ncompared with existing methods, better preserves not only specular highlights\nbut also fine-grained details. To this end, we enhance the neural network\nperformance by encoding input data to frequency space, inspired by NeRF, to\nbetter preserve the details. Furthermore, we introduce a gradient-based loss\nand employ it in multiple stages, adaptive to the progress of the learning\nphase. Lastly, we utilize an optional extension to the decoder network using\nthe Inception module for more accurate yet costly performance. We demonstrate\nthe effectiveness of our method using a variety of synthetic and real examples.",
        "translated": ""
    },
    {
        "title": "General vs. Long-Tailed Age Estimation: An Approach to Kill Two Birds\n  with One Stone",
        "url": "http://arxiv.org/abs/2307.10129v1",
        "pub_date": "2023-07-19",
        "summary": "Facial age estimation has received a lot of attention for its diverse\napplication scenarios. Most existing studies treat each sample equally and aim\nto reduce the average estimation error for the entire dataset, which can be\nsummarized as General Age Estimation. However, due to the long-tailed\ndistribution prevalent in the dataset, treating all samples equally will\ninevitably bias the model toward the head classes (usually the adult with a\nmajority of samples). Driven by this, some works suggest that each class should\nbe treated equally to improve performance in tail classes (with a minority of\nsamples), which can be summarized as Long-tailed Age Estimation. However,\nLong-tailed Age Estimation usually faces a performance trade-off, i.e.,\nachieving improvement in tail classes by sacrificing the head classes. In this\npaper, our goal is to design a unified framework to perform well on both tasks,\nkilling two birds with one stone. To this end, we propose a simple, effective,\nand flexible training paradigm named GLAE, which is two-fold. Our GLAE provides\na surprising improvement on Morph II, reaching the lowest MAE and CMAE of 1.14\nand 1.27 years, respectively. Compared to the previous best method, MAE dropped\nby up to 34%, which is an unprecedented improvement, and for the first time,\nMAE is close to 1 year old. Extensive experiments on other age benchmark\ndatasets, including CACD, MIVIA, and Chalearn LAP 2015, also indicate that GLAE\noutperforms the state-of-the-art approaches significantly.",
        "translated": ""
    },
    {
        "title": "Two Approaches to Supervised Image Segmentation",
        "url": "http://arxiv.org/abs/2307.10123v1",
        "pub_date": "2023-07-19",
        "summary": "Though performed almost effortlessly by humans, segmenting 2D gray-scale or\ncolor images in terms of their constituent regions of interest\n(e.g.~background, objects or portions of objects) constitutes one of the\ngreatest challenges in science and technology as a consequence of the involved\ndimensionality reduction(3D to 2D), noise, reflections, shades, and occlusions,\namong many other possible effects. While a large number of interesting\napproaches have been respectively suggested along the last decades, it was\nmainly with the more recent development of deep learning that more effective\nand general solutions have been obtained, currently constituting the basic\ncomparison reference for this type of operation. Also developed recently, a\nmultiset-based methodology has been described that is capable of encouraging\nperformance that combines spatial accuracy, stability, and robustness while\nrequiring minimal computational resources (hardware and/or training and\nrecognition time). The interesting features of the latter methodology mostly\nfollow from the enhanced selectivity and sensitivity, as well as good\nrobustness to data perturbations and outliers, allowed by the coincidence\nsimilarity index on which the multiset approach to supervised image\nsegmentation is based. After describing the deep learning and multiset\napproaches, the present work develops two comparison experiments between them\nwhich are primarily aimed at illustrating their respective main interesting\nfeatures when applied to the adopted specific type of data and parameter\nconfigurations. While the deep learning approach confirmed its potential for\nperforming image segmentation, the alternative multiset methodology allowed for\nencouraging accuracy while requiring little computational resources.",
        "translated": ""
    },
    {
        "title": "Boundary-Refined Prototype Generation: A General End-to-End Paradigm for\n  Semi-Supervised Semantic Segmentation",
        "url": "http://arxiv.org/abs/2307.10097v1",
        "pub_date": "2023-07-19",
        "summary": "Prototype-based classification is a classical method in machine learning, and\nrecently it has achieved remarkable success in semi-supervised semantic\nsegmentation. However, the current approach isolates the prototype\ninitialization process from the main training framework, which appears to be\nunnecessary. Furthermore, while the direct use of K-Means algorithm for\nprototype generation has considered rich intra-class variance, it may not be\nthe optimal solution for the classification task. To tackle these problems, we\npropose a novel boundary-refined prototype generation (BRPG) method, which is\nincorporated into the whole training framework. Specifically, our approach\nsamples and clusters high- and low-confidence features separately based on a\nconfidence threshold, aiming to generate prototypes closer to the class\nboundaries. Moreover, an adaptive prototype optimization strategy is introduced\nto make prototype augmentation for categories with scattered feature\ndistributions. Extensive experiments on the PASCAL VOC 2012 and Cityscapes\ndatasets demonstrate the superiority and scalability of the proposed method,\noutperforming the current state-of-the-art approaches. The code is available at\nxxxxxxxxxxxxxx.",
        "translated": ""
    },
    {
        "title": "PAPR: Proximity Attention Point Rendering",
        "url": "http://arxiv.org/abs/2307.11086v1",
        "pub_date": "2023-07-20",
        "summary": "Learning accurate and parsimonious point cloud representations of scene\nsurfaces from scratch remains a challenge in 3D representation learning.\nExisting point-based methods often suffer from the vanishing gradient problem\nor require a large number of points to accurately model scene geometry and\ntexture. To address these limitations, we propose Proximity Attention Point\nRendering (PAPR), a novel method that consists of a point-based scene\nrepresentation and a differentiable renderer. Our scene representation uses a\npoint cloud where each point is characterized by its spatial position,\nforeground score, and view-independent feature vector. The renderer selects the\nrelevant points for each ray and produces accurate colours using their\nassociated features. PAPR effectively learns point cloud positions to represent\nthe correct scene geometry, even when the initialization drastically differs\nfrom the target geometry. Notably, our method captures fine texture details\nwhile using only a parsimonious set of points. We also demonstrate four\npractical applications of our method: geometry editing, object manipulation,\ntexture transfer, and exposure control. More results and code are available on\nour project website at https://zvict.github.io/papr/.",
        "translated": ""
    },
    {
        "title": "Representation Learning in Anomaly Detection: Successes, Limits and a\n  Grand Challenge",
        "url": "http://arxiv.org/abs/2307.11085v1",
        "pub_date": "2023-07-20",
        "summary": "In this perspective paper, we argue that the dominant paradigm in anomaly\ndetection cannot scale indefinitely and will eventually hit fundamental limits.\nThis is due to the a no free lunch principle for anomaly detection. These\nlimitations can be overcome when there are strong tasks priors, as is the case\nfor many industrial tasks. When such priors do not exists, the task is much\nharder for anomaly detection. We pose two such tasks as grand challenges for\nanomaly detection: i) scientific discovery by anomaly detection ii) a\n\"mini-grand\" challenge of detecting the most anomalous image in the ImageNet\ndataset. We believe new anomaly detection tools and ideas would need to be\ndeveloped to overcome these challenges.",
        "translated": ""
    },
    {
        "title": "GLSFormer: Gated - Long, Short Sequence Transformer for Step Recognition\n  in Surgical Videos",
        "url": "http://arxiv.org/abs/2307.11081v1",
        "pub_date": "2023-07-20",
        "summary": "Automated surgical step recognition is an important task that can\nsignificantly improve patient safety and decision-making during surgeries.\nExisting state-of-the-art methods for surgical step recognition either rely on\nseparate, multi-stage modeling of spatial and temporal information or operate\non short-range temporal resolution when learned jointly. However, the benefits\nof joint modeling of spatio-temporal features and long-range information are\nnot taken in account. In this paper, we propose a vision transformer-based\napproach to jointly learn spatio-temporal features directly from sequence of\nframe-level patches. Our method incorporates a gated-temporal attention\nmechanism that intelligently combines short-term and long-term spatio-temporal\nfeature representations. We extensively evaluate our approach on two cataract\nsurgery video datasets, namely Cataract-101 and D99, and demonstrate superior\nperformance compared to various state-of-the-art methods. These results\nvalidate the suitability of our proposed approach for automated surgical step\nrecognition. Our code is released at:\nhttps://github.com/nisargshah1999/GLSFormer",
        "translated": ""
    },
    {
        "title": "AlignDet: Aligning Pre-training and Fine-tuning in Object Detection",
        "url": "http://arxiv.org/abs/2307.11077v1",
        "pub_date": "2023-07-20",
        "summary": "The paradigm of large-scale pre-training followed by downstream fine-tuning\nhas been widely employed in various object detection algorithms. In this paper,\nwe reveal discrepancies in data, model, and task between the pre-training and\nfine-tuning procedure in existing practices, which implicitly limit the\ndetector's performance, generalization ability, and convergence speed. To this\nend, we propose AlignDet, a unified pre-training framework that can be adapted\nto various existing detectors to alleviate the discrepancies. AlignDet\ndecouples the pre-training process into two stages, i.e., image-domain and\nbox-domain pre-training. The image-domain pre-training optimizes the detection\nbackbone to capture holistic visual abstraction, and box-domain pre-training\nlearns instance-level semantics and task-aware concepts to initialize the parts\nout of the backbone. By incorporating the self-supervised pre-trained\nbackbones, we can pre-train all modules for various detectors in an\nunsupervised paradigm. As depicted in Figure 1, extensive experiments\ndemonstrate that AlignDet can achieve significant improvements across diverse\nprotocols, such as detection algorithm, model backbone, data setting, and\ntraining schedule. For example, AlignDet improves FCOS by 5.3 mAP, RetinaNet by\n2.1 mAP, Faster R-CNN by 3.3 mAP, and DETR by 2.3 mAP under fewer epochs.",
        "translated": ""
    },
    {
        "title": "Learning Dense UV Completion for Human Mesh Recovery",
        "url": "http://arxiv.org/abs/2307.11074v1",
        "pub_date": "2023-07-20",
        "summary": "Human mesh reconstruction from a single image is challenging in the presence\nof occlusion, which can be caused by self, objects, or other humans. Existing\nmethods either fail to separate human features accurately or lack proper\nsupervision for feature completion. In this paper, we propose Dense Inpainting\nHuman Mesh Recovery (DIMR), a two-stage method that leverages dense\ncorrespondence maps to handle occlusion. Our method utilizes a dense\ncorrespondence map to separate visible human features and completes human\nfeatures on a structured UV map dense human with an attention-based feature\ncompletion module. We also design a feature inpainting training procedure that\nguides the network to learn from unoccluded features. We evaluate our method on\nseveral datasets and demonstrate its superior performance under heavily\noccluded scenarios compared to other methods. Extensive experiments show that\nour method obviously outperforms prior SOTA methods on heavily occluded images\nand achieves comparable results on the standard benchmarks (3DPW).",
        "translated": ""
    },
    {
        "title": "OBJECT 3DIT: Language-guided 3D-aware Image Editing",
        "url": "http://arxiv.org/abs/2307.11073v1",
        "pub_date": "2023-07-20",
        "summary": "Existing image editing tools, while powerful, typically disregard the\nunderlying 3D geometry from which the image is projected. As a result, edits\nmade using these tools may become detached from the geometry and lighting\nconditions that are at the foundation of the image formation process. In this\nwork, we formulate the newt ask of language-guided 3D-aware editing, where\nobjects in an image should be edited according to a language instruction in\ncontext of the underlying 3D scene. To promote progress towards this goal, we\nrelease OBJECT: a dataset consisting of 400K editing examples created from\nprocedurally generated 3D scenes. Each example consists of an input image,\nediting instruction in language, and the edited image. We also introduce 3DIT :\nsingle and multi-task models for four editing tasks. Our models show impressive\nabilities to understand the 3D composition of entire scenes, factoring in\nsurrounding objects, surfaces, lighting conditions, shadows, and\nphysically-plausible object configurations. Surprisingly, training on only\nsynthetic scenes from OBJECT, editing capabilities of 3DIT generalize to\nreal-world images.",
        "translated": ""
    },
    {
        "title": "CNOS: A Strong Baseline for CAD-based Novel Object Segmentation",
        "url": "http://arxiv.org/abs/2307.11067v1",
        "pub_date": "2023-07-20",
        "summary": "We propose a simple three-stage approach to segment unseen objects in RGB\nimages using their CAD models. Leveraging recent powerful foundation models,\nDINOv2 and Segment Anything, we create descriptors and generate proposals,\nincluding binary masks for a given input RGB image. By matching proposals with\nreference descriptors created from CAD models, we achieve precise object ID\nassignment along with modal masks. We experimentally demonstrate that our\nmethod achieves state-of-the-art results in CAD-based novel object\nsegmentation, surpassing existing approaches on the seven core datasets of the\nBOP challenge by 19.8\\% AP using the same BOP evaluation protocol. Our source\ncode is available at https://github.com/nv-nguyen/cnos.",
        "translated": ""
    },
    {
        "title": "Driving Policy Prediction based on Deep Learning Models",
        "url": "http://arxiv.org/abs/2307.11058v1",
        "pub_date": "2023-07-20",
        "summary": "In this project, we implemented an end-to-end system that takes in combined\nvisual features of video frames from a normal camera and depth information from\na cloud points scanner, and predicts driving policies (vehicle speed and\nsteering angle). We verified the safety of our system by comparing the\npredicted results with standard behaviors by real-world experienced drivers.\nOur test results show that the predictions can be considered as accurate in at\nlease half of the testing cases (50% 80%, depending on the model), and using\ncombined features improved the performance in most cases than using video\nframes only.",
        "translated": ""
    },
    {
        "title": "HRFNet: High-Resolution Forgery Network for Localizing Satellite Image\n  Manipulation",
        "url": "http://arxiv.org/abs/2307.11052v1",
        "pub_date": "2023-07-20",
        "summary": "Existing high-resolution satellite image forgery localization methods rely on\npatch-based or downsampling-based training. Both of these training methods have\nmajor drawbacks, such as inaccurate boundaries between pristine and forged\nregions, the generation of unwanted artifacts, etc. To tackle the\naforementioned challenges, inspired by the high-resolution image segmentation\nliterature, we propose a novel model called HRFNet to enable satellite image\nforgery localization effectively. Specifically, equipped with shallow and deep\nbranches, our model can successfully integrate RGB and resampling features in\nboth global and local manners to localize forgery more accurately. We perform\nvarious experiments to demonstrate that our method achieves the best\nperformance, while the memory requirement and processing speed are not\ncompromised compared to existing methods.",
        "translated": ""
    },
    {
        "title": "Cascade-DETR: Delving into High-Quality Universal Object Detection",
        "url": "http://arxiv.org/abs/2307.11035v1",
        "pub_date": "2023-07-20",
        "summary": "Object localization in general environments is a fundamental part of vision\nsystems. While dominating on the COCO benchmark, recent Transformer-based\ndetection methods are not competitive in diverse domains. Moreover, these\nmethods still struggle to very accurately estimate the object bounding boxes in\ncomplex environments.\n  We introduce Cascade-DETR for high-quality universal object detection. We\njointly tackle the generalization to diverse domains and localization accuracy\nby proposing the Cascade Attention layer, which explicitly integrates\nobject-centric information into the detection decoder by limiting the attention\nto the previous box prediction. To further enhance accuracy, we also revisit\nthe scoring of queries. Instead of relying on classification scores, we predict\nthe expected IoU of the query, leading to substantially more well-calibrated\nconfidences. Lastly, we introduce a universal object detection benchmark,\nUDB10, that contains 10 datasets from diverse domains. While also advancing the\nstate-of-the-art on COCO, Cascade-DETR substantially improves DETR-based\ndetectors on all datasets in UDB10, even by over 10 mAP in some cases. The\nimprovements under stringent quality requirements are even more pronounced. Our\ncode and models will be released at https://github.com/SysCV/cascade-detr.",
        "translated": ""
    },
    {
        "title": "BandRe: Rethinking Band-Pass Filters for Scale-Wise Object Detection\n  Evaluation",
        "url": "http://arxiv.org/abs/2307.11748v1",
        "pub_date": "2023-07-21",
        "summary": "Scale-wise evaluation of object detectors is important for real-world\napplications. However, existing metrics are either coarse or not sufficiently\nreliable. In this paper, we propose novel scale-wise metrics that strike a\nbalance between fineness and reliability, using a filter bank consisting of\ntriangular and trapezoidal band-pass filters. We conduct experiments with two\nmethods on two datasets and show that the proposed metrics can highlight the\ndifferences between the methods and between the datasets. Code is available at\nhttps://github.com/shinya7y/UniverseNet .",
        "translated": ""
    },
    {
        "title": "3D Skeletonization of Complex Grapevines for Robotic Pruning",
        "url": "http://arxiv.org/abs/2307.11706v1",
        "pub_date": "2023-07-21",
        "summary": "Robotic pruning of dormant grapevines is an area of active research in order\nto promote vine balance and grape quality, but so far robotic efforts have\nlargely focused on planar, simplified vines not representative of commercial\nvineyards. This paper aims to advance the robotic perception capabilities\nnecessary for pruning in denser and more complex vine structures by extending\nplant skeletonization techniques. The proposed pipeline generates skeletal\ngrapevine models that have lower reprojection error and higher connectivity\nthan baseline algorithms. We also show how 3D and skeletal information enables\nprediction accuracy of pruning weight for dense vines surpassing prior work,\nwhere pruning weight is an important vine metric influencing pruning site\nselection.",
        "translated": ""
    },
    {
        "title": "SACReg: Scene-Agnostic Coordinate Regression for Visual Localization",
        "url": "http://arxiv.org/abs/2307.11702v1",
        "pub_date": "2023-07-21",
        "summary": "Scene coordinates regression (SCR), i.e., predicting 3D coordinates for every\npixel of a given image, has recently shown promising potential. However,\nexisting methods remain mostly scene-specific or limited to small scenes and\nthus hardly scale to realistic datasets. In this paper, we propose a new\nparadigm where a single generic SCR model is trained once to be then deployed\nto new test scenes, regardless of their scale and without further finetuning.\nFor a given query image, it collects inputs from off-the-shelf image retrieval\ntechniques and Structure-from-Motion databases: a list of relevant database\nimages with sparse pointwise 2D-3D annotations. The model is based on the\ntransformer architecture and can take a variable number of images and sparse\n2D-3D annotations as input. It is trained on a few diverse datasets and\nsignificantly outperforms other scene regression approaches on several\nbenchmarks, including scene-specific models, for visual localization. In\nparticular, we set a new state of the art on the Cambridge localization\nbenchmark, even outperforming feature-matching-based approaches.",
        "translated": ""
    },
    {
        "title": "Enhancing CLIP with GPT-4: Harnessing Visual Descriptions as Prompts",
        "url": "http://arxiv.org/abs/2307.11661v1",
        "pub_date": "2023-07-21",
        "summary": "Contrastive pretrained large Vision-Language Models (VLMs) like CLIP have\nrevolutionized visual representation learning by providing good performance on\ndownstream datasets. VLMs are 0-shot adapted to a downstream dataset by\ndesigning prompts that are relevant to the dataset. Such prompt engineering\nmakes use of domain expertise and a validation dataset. Meanwhile, recent\ndevelopments in generative pretrained models like GPT-4 mean they can be used\nas advanced internet search tools. They can also be manipulated to provide\nvisual information in any structure. In this work, we show that GPT-4 can be\nused to generate text that is visually descriptive and how this can be used to\nadapt CLIP to downstream tasks. We show considerable improvements in 0-shot\ntransfer accuracy on specialized fine-grained datasets like EuroSAT (~7%), DTD\n(~7%), SUN397 (~4.6%), and CUB (~3.3%) when compared to CLIP's default prompt.\nWe also design a simple few-shot adapter that learns to choose the best\npossible sentences to construct generalizable classifiers that outperform the\nrecently proposed CoCoOP by ~2% on average and by over 4% on 4 specialized\nfine-grained datasets. We will release the code, prompts, and auxiliary text\ndataset upon acceptance.",
        "translated": ""
    },
    {
        "title": "FEDD -- Fair, Efficient, and Diverse Diffusion-based Lesion Segmentation\n  and Malignancy Classification",
        "url": "http://arxiv.org/abs/2307.11654v1",
        "pub_date": "2023-07-21",
        "summary": "Skin diseases affect millions of people worldwide, across all ethnicities.\nIncreasing diagnosis accessibility requires fair and accurate segmentation and\nclassification of dermatology images. However, the scarcity of annotated\nmedical images, especially for rare diseases and underrepresented skin tones,\nposes a challenge to the development of fair and accurate models. In this\nstudy, we introduce a Fair, Efficient, and Diverse Diffusion-based framework\nfor skin lesion segmentation and malignancy classification. FEDD leverages\nsemantically meaningful feature embeddings learned through a denoising\ndiffusion probabilistic backbone and processes them via linear probes to\nachieve state-of-the-art performance on Diverse Dermatology Images (DDI). We\nachieve an improvement in intersection over union of 0.18, 0.13, 0.06, and 0.07\nwhile using only 5%, 10%, 15%, and 20% labeled samples, respectively.\nAdditionally, FEDD trained on 10% of DDI demonstrates malignancy classification\naccuracy of 81%, 14% higher compared to the state-of-the-art. We showcase high\nefficiency in data-constrained scenarios while providing fair performance for\ndiverse skin tones and rare malignancy conditions. Our newly annotated DDI\nsegmentation masks and training code can be found on\nhttps://github.com/hectorcarrion/fedd.",
        "translated": ""
    },
    {
        "title": "Morphological Image Analysis and Feature Extraction for Reasoning with\n  AI-based Defect Detection and Classification Models",
        "url": "http://arxiv.org/abs/2307.11643v1",
        "pub_date": "2023-07-21",
        "summary": "As the use of artificial intelligent (AI) models becomes more prevalent in\nindustries such as engineering and manufacturing, it is essential that these\nmodels provide transparent reasoning behind their predictions. This paper\nproposes the AI-Reasoner, which extracts the morphological characteristics of\ndefects (DefChars) from images and utilises decision trees to reason with the\nDefChar values. Thereafter, the AI-Reasoner exports visualisations (i.e.\ncharts) and textual explanations to provide insights into outputs made by\nmasked-based defect detection and classification models. It also provides\neffective mitigation strategies to enhance data pre-processing and overall\nmodel performance. The AI-Reasoner was tested on explaining the outputs of an\nIE Mask R-CNN model using a set of 366 images containing defects. The results\ndemonstrated its effectiveness in explaining the IE Mask R-CNN model's\npredictions. Overall, the proposed AI-Reasoner provides a solution for\nimproving the performance of AI models in industrial applications that require\ndefect analysis.",
        "translated": ""
    },
    {
        "title": "Deep Reinforcement Learning Based System for Intraoperative\n  Hyperspectral Video Autofocusing",
        "url": "http://arxiv.org/abs/2307.11638v1",
        "pub_date": "2023-07-21",
        "summary": "Hyperspectral imaging (HSI) captures a greater level of spectral detail than\ntraditional optical imaging, making it a potentially valuable intraoperative\ntool when precise tissue differentiation is essential. Hardware limitations of\ncurrent optical systems used for handheld real-time video HSI result in a\nlimited focal depth, thereby posing usability issues for integration of the\ntechnology into the operating room. This work integrates a focus-tunable liquid\nlens into a video HSI exoscope, and proposes novel video autofocusing methods\nbased on deep reinforcement learning. A first-of-its-kind robotic focal-time\nscan was performed to create a realistic and reproducible testing dataset. We\nbenchmarked our proposed autofocus algorithm against traditional policies, and\nfound our novel approach to perform significantly ($p&lt;0.05$) better than\ntraditional techniques ($0.070\\pm.098$ mean absolute focal error compared to\n$0.146\\pm.148$). In addition, we performed a blinded usability trial by having\ntwo neurosurgeons compare the system with different autofocus policies, and\nfound our novel approach to be the most favourable, making our system a\ndesirable addition for intraoperative HSI.",
        "translated": ""
    },
    {
        "title": "OxfordTVG-HIC: Can Machine Make Humorous Captions from Images?",
        "url": "http://arxiv.org/abs/2307.11636v1",
        "pub_date": "2023-07-21",
        "summary": "This paper presents OxfordTVG-HIC (Humorous Image Captions), a large-scale\ndataset for humour generation and understanding. Humour is an abstract,\nsubjective, and context-dependent cognitive construct involving several\ncognitive factors, making it a challenging task to generate and interpret.\nHence, humour generation and understanding can serve as a new task for\nevaluating the ability of deep-learning methods to process abstract and\nsubjective information. Due to the scarcity of data, humour-related generation\ntasks such as captioning remain under-explored. To address this gap,\nOxfordTVG-HIC offers approximately 2.9M image-text pairs with humour scores to\ntrain a generalizable humour captioning model. Contrary to existing captioning\ndatasets, OxfordTVG-HIC features a wide range of emotional and semantic\ndiversity resulting in out-of-context examples that are particularly conducive\nto generating humour. Moreover, OxfordTVG-HIC is curated devoid of offensive\ncontent. We also show how OxfordTVG-HIC can be leveraged for evaluating the\nhumour of a generated text. Through explainability analysis of the trained\nmodels, we identify the visual and linguistic cues influential for evoking\nhumour prediction (and generation). We observe qualitatively that these cues\nare aligned with the benign violation theory of humour in cognitive psychology.",
        "translated": ""
    },
    {
        "title": "Divide and Adapt: Active Domain Adaptation via Customized Learning",
        "url": "http://arxiv.org/abs/2307.11618v1",
        "pub_date": "2023-07-21",
        "summary": "Active domain adaptation (ADA) aims to improve the model adaptation\nperformance by incorporating active learning (AL) techniques to label a\nmaximally-informative subset of target samples. Conventional AL methods do not\nconsider the existence of domain shift, and hence, fail to identify the truly\nvaluable samples in the context of domain adaptation. To accommodate active\nlearning and domain adaption, the two naturally different tasks, in a\ncollaborative framework, we advocate that a customized learning strategy for\nthe target data is the key to the success of ADA solutions. We present\nDivide-and-Adapt (DiaNA), a new ADA framework that partitions the target\ninstances into four categories with stratified transferable properties. With a\nnovel data subdivision protocol based on uncertainty and domainness, DiaNA can\naccurately recognize the most gainful samples. While sending the informative\ninstances for annotation, DiaNA employs tailored learning strategies for the\nremaining categories. Furthermore, we propose an informativeness score that\nunifies the data partitioning criteria. This enables the use of a Gaussian\nmixture model (GMM) to automatically sample unlabeled data into the proposed\nfour categories. Thanks to the \"divideand-adapt\" spirit, DiaNA can handle data\nwith large variations of domain gap. In addition, we show that DiaNA can\ngeneralize to different domain adaptation settings, such as unsupervised domain\nadaptation (UDA), semi-supervised domain adaptation (SSDA), source-free domain\nadaptation (SFDA), etc.",
        "translated": ""
    },
    {
        "title": "Consistency-guided Meta-Learning for Bootstrapping Semi-Supervised\n  Medical Image Segmentation",
        "url": "http://arxiv.org/abs/2307.11604v1",
        "pub_date": "2023-07-21",
        "summary": "Medical imaging has witnessed remarkable progress but usually requires a\nlarge amount of high-quality annotated data which is time-consuming and costly\nto obtain. To alleviate this burden, semi-supervised learning has garnered\nattention as a potential solution. In this paper, we present Meta-Learning for\nBootstrapping Medical Image Segmentation (MLB-Seg), a novel method for tackling\nthe challenge of semi-supervised medical image segmentation. Specifically, our\napproach first involves training a segmentation model on a small set of clean\nlabeled images to generate initial labels for unlabeled data. To further\noptimize this bootstrapping process, we introduce a per-pixel weight mapping\nsystem that dynamically assigns weights to both the initialized labels and the\nmodel's own predictions. These weights are determined using a meta-process that\nprioritizes pixels with loss gradient directions closer to those of clean data,\nwhich is based on a small set of precisely annotated images. To facilitate the\nmeta-learning process, we additionally introduce a consistency-based Pseudo\nLabel Enhancement (PLE) scheme that improves the quality of the model's own\npredictions by ensembling predictions from various augmented versions of the\nsame input. In order to improve the quality of the weight maps obtained through\nmultiple augmentations of a single input, we introduce a mean teacher into the\nPLE scheme. This method helps to reduce noise in the weight maps and stabilize\nits generation process. Our extensive experimental results on public atrial and\nprostate segmentation datasets demonstrate that our proposed method achieves\nstate-of-the-art results under semi-supervision. Our code is available at\nhttps://github.com/aijinrjinr/MLB-Seg.",
        "translated": ""
    },
    {
        "title": "3D-LLM: Injecting the 3D World into Large Language Models",
        "url": "http://arxiv.org/abs/2307.12981v1",
        "pub_date": "2023-07-24",
        "summary": "Large language models (LLMs) and Vision-Language Models (VLMs) have been\nproven to excel at multiple tasks, such as commonsense reasoning. Powerful as\nthese models can be, they are not grounded in the 3D physical world, which\ninvolves richer concepts such as spatial relationships, affordances, physics,\nlayout, and so on. In this work, we propose to inject the 3D world into large\nlanguage models and introduce a whole new family of 3D-LLMs. Specifically,\n3D-LLMs can take 3D point clouds and their features as input and perform a\ndiverse set of 3D-related tasks, including captioning, dense captioning, 3D\nquestion answering, task decomposition, 3D grounding, 3D-assisted dialog,\nnavigation, and so on. Using three types of prompting mechanisms that we\ndesign, we are able to collect over 300k 3D-language data covering these tasks.\nTo efficiently train 3D-LLMs, we first utilize a 3D feature extractor that\nobtains 3D features from rendered multi- view images. Then, we use 2D VLMs as\nour backbones to train our 3D-LLMs. By introducing a 3D localization mechanism,\n3D-LLMs can better capture 3D spatial information. Experiments on ScanQA show\nthat our model outperforms state-of-the-art baselines by a large margin (e.g.,\nthe BLEU-1 score surpasses state-of-the-art score by 9%). Furthermore,\nexperiments on our held-in datasets for 3D captioning, task composition, and\n3D-assisted dialogue show that our model outperforms 2D VLMs. Qualitative\nexamples also show that our model could perform more tasks beyond the scope of\nexisting LLMs and VLMs. Project Page: : https://vis-www.cs.umass.edu/3dllm/.",
        "translated": ""
    },
    {
        "title": "A Systematic Survey of Prompt Engineering on Vision-Language Foundation\n  Models",
        "url": "http://arxiv.org/abs/2307.12980v1",
        "pub_date": "2023-07-24",
        "summary": "Prompt engineering is a technique that involves augmenting a large\npre-trained model with task-specific hints, known as prompts, to adapt the\nmodel to new tasks. Prompts can be created manually as natural language\ninstructions or generated automatically as either natural language instructions\nor vector representations. Prompt engineering enables the ability to perform\npredictions based solely on prompts without updating model parameters, and the\neasier application of large pre-trained models in real-world tasks. In past\nyears, Prompt engineering has been well-studied in natural language processing.\nRecently, it has also been intensively studied in vision-language modeling.\nHowever, there is currently a lack of a systematic overview of prompt\nengineering on pre-trained vision-language models. This paper aims to provide a\ncomprehensive survey of cutting-edge research in prompt engineering on three\ntypes of vision-language models: multimodal-to-text generation models (e.g.\nFlamingo), image-text matching models (e.g. CLIP), and text-to-image generation\nmodels (e.g. Stable Diffusion). For each type of model, a brief model summary,\nprompting methods, prompting-based applications, and the corresponding\nresponsibility and integrity issues are summarized and discussed. Furthermore,\nthe commonalities and differences between prompting on vision-language models,\nlanguage models, and vision models are also discussed. The challenges, future\ndirections, and research opportunities are summarized to foster future research\non this topic.",
        "translated": ""
    },
    {
        "title": "DFA3D: 3D Deformable Attention For 2D-to-3D Feature Lifting",
        "url": "http://arxiv.org/abs/2307.12972v1",
        "pub_date": "2023-07-24",
        "summary": "In this paper, we propose a new operator, called 3D DeFormable Attention\n(DFA3D), for 2D-to-3D feature lifting, which transforms multi-view 2D image\nfeatures into a unified 3D space for 3D object detection. Existing feature\nlifting approaches, such as Lift-Splat-based and 2D attention-based, either use\nestimated depth to get pseudo LiDAR features and then splat them to a 3D space,\nwhich is a one-pass operation without feature refinement, or ignore depth and\nlift features by 2D attention mechanisms, which achieve finer semantics while\nsuffering from a depth ambiguity problem. In contrast, our DFA3D-based method\nfirst leverages the estimated depth to expand each view's 2D feature map to 3D\nand then utilizes DFA3D to aggregate features from the expanded 3D feature\nmaps. With the help of DFA3D, the depth ambiguity problem can be effectively\nalleviated from the root, and the lifted features can be progressively refined\nlayer by layer, thanks to the Transformer-like architecture. In addition, we\npropose a mathematically equivalent implementation of DFA3D which can\nsignificantly improve its memory efficiency and computational speed. We\nintegrate DFA3D into several methods that use 2D attention-based feature\nlifting with only a few modifications in code and evaluate on the nuScenes\ndataset. The experiment results show a consistent improvement of +1.41\\% mAP on\naverage, and up to +15.1\\% mAP improvement when high-quality depth information\nis available, demonstrating the superiority, applicability, and huge potential\nof DFA3D. The code is available at\nhttps://github.com/IDEA-Research/3D-deformable-attention.git.",
        "translated": ""
    },
    {
        "title": "Volcanic ash delimitation using Artificial Intelligence based on Pix2Pix",
        "url": "http://arxiv.org/abs/2307.12970v1",
        "pub_date": "2023-07-24",
        "summary": "Volcanic eruptions emit ash that can be harmful to human health and cause\ndamage to infrastructure, economic activities and the environment. The\ndelimitation of ash clouds allows to know their behavior and dispersion, which\nhelps in the prevention and mitigation of this phenomenon. Traditional methods\ntake advantage of specialized software programs to process the bands or\nchannels that compose the satellite images. However, their use is limited to\nexperts and demands a lot of time and significant computational resources. In\nrecent years, Artificial Intelligence has been a milestone in the computational\ntreatment of complex problems in different areas. In particular, Deep Learning\ntechniques allow automatic, fast and accurate processing of digital images. The\npresent work proposes the use of the Pix2Pix model, a type of generative\nadversarial network that, once trained, learns the mapping of input images to\noutput images. The architecture of such a network consisting of a generator and\na discriminator provides the versatility needed to produce black and white ash\ncloud images from multispectral satellite images. The evaluation of the model,\nbased on loss and accuracy plots, a confusion matrix, and visual inspection,\nindicates a satisfactory solution for accurate ash cloud delineation,\napplicable in any area of the world and becomes a useful tool in risk\nmanagement.",
        "translated": ""
    },
    {
        "title": "Learning Dense Correspondences between Photos and Sketches",
        "url": "http://arxiv.org/abs/2307.12967v1",
        "pub_date": "2023-07-24",
        "summary": "Humans effortlessly grasp the connection between sketches and real-world\nobjects, even when these sketches are far from realistic. Moreover, human\nsketch understanding goes beyond categorization -- critically, it also entails\nunderstanding how individual elements within a sketch correspond to parts of\nthe physical world it represents. What are the computational ingredients needed\nto support this ability? Towards answering this question, we make two\ncontributions: first, we introduce a new sketch-photo correspondence benchmark,\n$\\textit{PSC6k}$, containing 150K annotations of 6250 sketch-photo pairs across\n125 object categories, augmenting the existing Sketchy dataset with\nfine-grained correspondence metadata. Second, we propose a self-supervised\nmethod for learning dense correspondences between sketch-photo pairs, building\nupon recent advances in correspondence learning for pairs of photos. Our model\nuses a spatial transformer network to estimate the warp flow between latent\nrepresentations of a sketch and photo extracted by a contrastive learning-based\nConvNet backbone. We found that this approach outperformed several strong\nbaselines and produced predictions that were quantitatively consistent with\nother warp-based methods. However, our benchmark also revealed systematic\ndifferences between predictions of the suite of models we tested and those of\nhumans. Taken together, our work suggests a promising path towards developing\nartificial systems that achieve more human-like understanding of visual images\nat different levels of abstraction. Project page:\nhttps://photo-sketch-correspondence.github.io",
        "translated": ""
    },
    {
        "title": "Audio-Enhanced Text-to-Video Retrieval using Text-Conditioned Feature\n  Alignment",
        "url": "http://arxiv.org/abs/2307.12964v1",
        "pub_date": "2023-07-24",
        "summary": "Text-to-video retrieval systems have recently made significant progress by\nutilizing pre-trained models trained on large-scale image-text pairs. However,\nmost of the latest methods primarily focus on the video modality while\ndisregarding the audio signal for this task. Nevertheless, a recent advancement\nby ECLIPSE has improved long-range text-to-video retrieval by developing an\naudiovisual video representation. Nonetheless, the objective of the\ntext-to-video retrieval task is to capture the complementary audio and video\ninformation that is pertinent to the text query rather than simply achieving\nbetter audio and video alignment. To address this issue, we introduce TEFAL, a\nTExt-conditioned Feature ALignment method that produces both audio and video\nrepresentations conditioned on the text query. Instead of using only an\naudiovisual attention block, which could suppress the audio information\nrelevant to the text query, our approach employs two independent cross-modal\nattention blocks that enable the text to attend to the audio and video\nrepresentations separately. Our proposed method's efficacy is demonstrated on\nfour benchmark datasets that include audio: MSR-VTT, LSMDC, VATEX, and\nCharades, and achieves better than state-of-the-art performance consistently\nacross the four datasets. This is attributed to the additional\ntext-query-conditioned audio representation and the complementary information\nit adds to the text-query-conditioned video representation.",
        "translated": ""
    },
    {
        "title": "On Privileged and Convergent Bases in Neural Network Representations",
        "url": "http://arxiv.org/abs/2307.12941v1",
        "pub_date": "2023-07-24",
        "summary": "In this study, we investigate whether the representations learned by neural\nnetworks possess a privileged and convergent basis. Specifically, we examine\nthe significance of feature directions represented by individual neurons.\nFirst, we establish that arbitrary rotations of neural representations cannot\nbe inverted (unlike linear networks), indicating that they do not exhibit\ncomplete rotational invariance. Subsequently, we explore the possibility of\nmultiple bases achieving identical performance. To do this, we compare the\nbases of networks trained with the same parameters but with varying random\ninitializations. Our study reveals two findings: (1) Even in wide networks such\nas WideResNets, neural networks do not converge to a unique basis; (2) Basis\ncorrelation increases significantly when a few early layers of the network are\nfrozen identically.\n  Furthermore, we analyze Linear Mode Connectivity, which has been studied as a\nmeasure of basis correlation. Our findings give evidence that while Linear Mode\nConnectivity improves with increased network width, this improvement is not due\nto an increase in basis correlation.",
        "translated": ""
    },
    {
        "title": "Hierarchical Skeleton Meta-Prototype Contrastive Learning with Hard\n  Skeleton Mining for Unsupervised Person Re-Identification",
        "url": "http://arxiv.org/abs/2307.12917v1",
        "pub_date": "2023-07-24",
        "summary": "With rapid advancements in depth sensors and deep learning, skeleton-based\nperson re-identification (re-ID) models have recently achieved remarkable\nprogress with many advantages. Most existing solutions learn single-level\nskeleton features from body joints with the assumption of equal skeleton\nimportance, while they typically lack the ability to exploit more informative\nskeleton features from various levels such as limb level with more global body\npatterns. The label dependency of these methods also limits their flexibility\nin learning more general skeleton representations. This paper proposes a\ngeneric unsupervised Hierarchical skeleton Meta-Prototype Contrastive learning\n(Hi-MPC) approach with Hard Skeleton Mining (HSM) for person re-ID with\nunlabeled 3D skeletons. Firstly, we construct hierarchical representations of\nskeletons to model coarse-to-fine body and motion features from the levels of\nbody joints, components, and limbs. Then a hierarchical meta-prototype\ncontrastive learning model is proposed to cluster and contrast the most typical\nskeleton features (\"prototypes\") from different-level skeletons. By converting\noriginal prototypes into meta-prototypes with multiple homogeneous\ntransformations, we induce the model to learn the inherent consistency of\nprototypes to capture more effective skeleton features for person re-ID.\nFurthermore, we devise a hard skeleton mining mechanism to adaptively infer the\ninformative importance of each skeleton, so as to focus on harder skeletons to\nlearn more discriminative skeleton representations. Extensive evaluations on\nfive datasets demonstrate that our approach outperforms a wide variety of\nstate-of-the-art skeleton-based methods. We further show the general\napplicability of our method to cross-view person re-ID and RGB-based scenarios\nwith estimated skeletons.",
        "translated": ""
    },
    {
        "title": "Towards a Visual-Language Foundation Model for Computational Pathology",
        "url": "http://arxiv.org/abs/2307.12914v1",
        "pub_date": "2023-07-24",
        "summary": "The accelerated adoption of digital pathology and advances in deep learning\nhave enabled the development of powerful models for various pathology tasks\nacross a diverse array of diseases and patient cohorts. However, model training\nis often difficult due to label scarcity in the medical domain and the model's\nusage is limited by the specific task and disease for which it is trained.\nAdditionally, most models in histopathology leverage only image data, a stark\ncontrast to how humans teach each other and reason about histopathologic\nentities. We introduce CONtrastive learning from Captions for Histopathology\n(CONCH), a visual-language foundation model developed using diverse sources of\nhistopathology images, biomedical text, and notably over 1.17 million\nimage-caption pairs via task-agnostic pretraining. Evaluated on a suite of 13\ndiverse benchmarks, CONCH can be transferred to a wide range of downstream\ntasks involving either or both histopathology images and text, achieving\nstate-of-the-art performance on histology image classification, segmentation,\ncaptioning, text-to-image and image-to-text retrieval. CONCH represents a\nsubstantial leap over concurrent visual-language pretrained systems for\nhistopathology, with the potential to directly facilitate a wide array of\nmachine learning-based workflows requiring minimal or no further supervised\nfine-tuning.",
        "translated": ""
    },
    {
        "title": "Dyn-E: Local Appearance Editing of Dynamic Neural Radiance Fields",
        "url": "http://arxiv.org/abs/2307.12909v1",
        "pub_date": "2023-07-24",
        "summary": "Recently, the editing of neural radiance fields (NeRFs) has gained\nconsiderable attention, but most prior works focus on static scenes while\nresearch on the appearance editing of dynamic scenes is relatively lacking. In\nthis paper, we propose a novel framework to edit the local appearance of\ndynamic NeRFs by manipulating pixels in a single frame of training video.\nSpecifically, to locally edit the appearance of dynamic NeRFs while preserving\nunedited regions, we introduce a local surface representation of the edited\nregion, which can be inserted into and rendered along with the original NeRF\nand warped to arbitrary other frames through a learned invertible motion\nrepresentation network. By employing our method, users without professional\nexpertise can easily add desired content to the appearance of a dynamic scene.\nWe extensively evaluate our approach on various scenes and show that our\napproach achieves spatially and temporally consistent editing results. Notably,\nour approach is versatile and applicable to different variants of dynamic NeRF\nrepresentations.",
        "translated": ""
    },
    {
        "title": "Benchmarking and Analyzing Generative Data for Visual Recognition",
        "url": "http://arxiv.org/abs/2307.13697v1",
        "pub_date": "2023-07-25",
        "summary": "Advancements in large pre-trained generative models have expanded their\npotential as effective data generators in visual recognition. This work delves\ninto the impact of generative images, primarily comparing paradigms that\nharness external data (\\ie generative \\vs retrieval \\vs original).\n  Our key contributions are: \\textbf{1) GenBench Construction:} We devise\n\\textbf{GenBench}, a broad benchmark comprising 22 datasets with 2548\ncategories, to appraise generative data across various visual recognition\ntasks. \\textbf{2) CLER Score:} To address the insufficient correlation of\nexisting metrics (\\eg, FID, CLIP score) with downstream recognition\nperformance, we propose \\textbf{CLER}, a training-free metric indicating\ngenerative data's efficiency for recognition tasks prior to training.\n\\textbf{3) New Baselines:} Comparisons of generative data with retrieved data\nfrom the same external pool help to elucidate the unique traits of generative\ndata. \\textbf{4) External Knowledge Injection:} By fine-tuning special token\nembeddings for each category via Textual Inversion, performance improves across\n17 datasets, except when dealing with low-resolution reference images.\n  Our exhaustive benchmark and analysis spotlight generative data's promise in\nvisual recognition, while identifying key challenges for future investigation.",
        "translated": ""
    },
    {
        "title": "The Visual Language of Fabrics",
        "url": "http://arxiv.org/abs/2307.13681v1",
        "pub_date": "2023-07-25",
        "summary": "We introduce text2fabric, a novel dataset that links free-text descriptions\nto various fabric materials. The dataset comprises 15,000 natural language\ndescriptions associated to 3,000 corresponding images of fabric materials.\nTraditionally, material descriptions come in the form of tags/keywords, which\nlimits their expressivity, induces pre-existing knowledge of the appropriate\nvocabulary, and ultimately leads to a chopped description system. Therefore, we\nstudy the use of free-text as a more appropriate way to describe material\nappearance, taking the use case of fabrics as a common item that non-experts\nmay often deal with. Based on the analysis of the dataset, we identify a\ncompact lexicon, set of attributes and key structure that emerge from the\ndescriptions. This allows us to accurately understand how people describe\nfabrics and draw directions for generalization to other types of materials. We\nalso show that our dataset enables specializing large vision-language models\nsuch as CLIP, creating a meaningful latent space for fabric appearance, and\nsignificantly improving applications such as fine-grained material retrieval\nand automatic captioning.",
        "translated": ""
    },
    {
        "title": "Personal Protective Equipment Detection in Extreme Construction\n  Conditions",
        "url": "http://arxiv.org/abs/2307.13654v1",
        "pub_date": "2023-07-25",
        "summary": "Object detection has been widely applied for construction safety management,\nespecially personal protective equipment (PPE) detection. Though the existing\nPPE detection models trained on conventional datasets have achieved excellent\nresults, their performance dramatically declines in extreme construction\nconditions. A robust detection model NST-YOLOv5 is developed by combining the\nneural style transfer (NST) and YOLOv5 technologies. Five extreme conditions\nare considered and simulated via the NST module to endow the detection model\nwith excellent robustness, including low light, intense light, sand dust, fog,\nand rain. Experiments show that the NST has great potential as a tool for\nextreme data synthesis since it is better at simulating extreme conditions than\nother traditional image processing algorithms and helps the NST-YOLOv5 achieve\n0.141 and 0.083 mAP_(05:95) improvements in synthesized and real-world extreme\ndata. This study provides a new feasible way to obtain a more robust detection\nmodel for extreme construction conditions.",
        "translated": ""
    },
    {
        "title": "QuickQual: Lightweight, convenient retinal image quality scoring with\n  off-the-shelf pretrained models",
        "url": "http://arxiv.org/abs/2307.13646v1",
        "pub_date": "2023-07-25",
        "summary": "Image quality remains a key problem for both traditional and deep learning\n(DL)-based approaches to retinal image analysis, but identifying poor quality\nimages can be time consuming and subjective. Thus, automated methods for\nretinal image quality scoring (RIQS) are needed. The current state-of-the-art\nis MCFNet, composed of three Densenet121 backbones each operating in a\ndifferent colour space. MCFNet, and the EyeQ dataset released by the same\nauthors, was a huge step forward for RIQS. We present QuickQual, a simple\napproach to RIQS, consisting of a single off-the-shelf ImageNet-pretrained\nDensenet121 backbone plus a Support Vector Machine (SVM). QuickQual performs\nvery well, setting a new state-of-the-art for EyeQ (Accuracy: 88.50% vs 88.00%\nfor MCFNet; AUC: 0.9687 vs 0.9588). This suggests that RIQS can be solved with\ngeneric perceptual features learned on natural images, as opposed to requiring\nDL models trained on large amounts of fundus images. Additionally, we propose a\nFixed Prior linearisation scheme, that converts EyeQ from a 3-way\nclassification to a continuous logistic regression task. For this task, we\npresent a second model, QuickQual MEga Minified Estimator (QuickQual-MEME),\nthat consists of only 10 parameters on top of an off-the-shelf Densenet121 and\ncan distinguish between gradable and ungradable images with an accuracy of\n89.18% (AUC: 0.9537). Code and model are available on GitHub:\nhttps://github.com/justinengelmann/QuickQual . QuickQual is so lightweight,\nthat the entire inference code (and even the parameters for QuickQual-MEME) is\nalready contained in this paper.",
        "translated": ""
    },
    {
        "title": "Learning Transferable Object-Centric Diffeomorphic Transformations for\n  Data Augmentation in Medical Image Segmentation",
        "url": "http://arxiv.org/abs/2307.13645v1",
        "pub_date": "2023-07-25",
        "summary": "Obtaining labelled data in medical image segmentation is challenging due to\nthe need for pixel-level annotations by experts. Recent works have shown that\naugmenting the object of interest with deformable transformations can help\nmitigate this challenge. However, these transformations have been learned\nglobally for the image, limiting their transferability across datasets or\napplicability in problems where image alignment is difficult. While\nobject-centric augmentations provide a great opportunity to overcome these\nissues, existing works are only focused on position and random transformations\nwithout considering shape variations of the objects. To this end, we propose a\nnovel object-centric data augmentation model that is able to learn the shape\nvariations for the objects of interest and augment the object in place without\nmodifying the rest of the image. We demonstrated its effectiveness in improving\nkidney tumour segmentation when leveraging shape variations learned both from\nwithin the same dataset and transferred from external datasets.",
        "translated": ""
    },
    {
        "title": "Optical Flow boosts Unsupervised Localization and Segmentation",
        "url": "http://arxiv.org/abs/2307.13640v1",
        "pub_date": "2023-07-25",
        "summary": "Unsupervised localization and segmentation are long-standing robot vision\nchallenges that describe the critical ability for an autonomous robot to learn\nto decompose images into individual objects without labeled data. These tasks\nare important because of the limited availability of dense image manual\nannotation and the promising vision of adapting to an evolving set of object\ncategories in lifelong learning. Most recent methods focus on using visual\nappearance continuity as object cues by spatially clustering features obtained\nfrom self-supervised vision transformers (ViT). In this work, we leverage\nmotion cues, inspired by the common fate principle that pixels that share\nsimilar movements tend to belong to the same object. We propose a new loss term\nformulation that uses optical flow in unlabeled videos to encourage\nself-supervised ViT features to become closer to each other if their\ncorresponding spatial locations share similar movements, and vice versa. We use\nthe proposed loss function to finetune vision transformers that were originally\ntrained on static images. Our fine-tuning procedure outperforms\nstate-of-the-art techniques for unsupervised semantic segmentation through\nlinear probing, without the use of any labeled data. This procedure also\ndemonstrates increased performance over original ViT networks across\nunsupervised object localization and semantic segmentation benchmarks.",
        "translated": ""
    },
    {
        "title": "Fake It Without Making It: Conditioned Face Generation for Accurate 3D\n  Face Shape Estimation",
        "url": "http://arxiv.org/abs/2307.13639v1",
        "pub_date": "2023-07-25",
        "summary": "Accurate 3D face shape estimation is an enabling technology with applications\nin healthcare, security, and creative industries, yet current state-of-the-art\nmethods either rely on self-supervised training with 2D image data or\nsupervised training with very limited 3D data. To bridge this gap, we present a\nnovel approach which uses a conditioned stable diffusion model for face image\ngeneration, leveraging the abundance of 2D facial information to inform 3D\nspace. By conditioning stable diffusion on depth maps sampled from a 3D\nMorphable Model (3DMM) of the human face, we generate diverse and\nshape-consistent images, forming the basis of SynthFace. We introduce this\nlarge-scale synthesised dataset of 250K photorealistic images and corresponding\n3DMM parameters. We further propose ControlFace, a deep neural network, trained\non SynthFace, which achieves competitive performance on the NoW benchmark,\nwithout requiring 3D supervision or manual 3D asset creation.",
        "translated": ""
    },
    {
        "title": "RecursiveDet: End-to-End Region-based Recursive Object Detection",
        "url": "http://arxiv.org/abs/2307.13619v1",
        "pub_date": "2023-07-25",
        "summary": "End-to-end region-based object detectors like Sparse R-CNN usually have\nmultiple cascade bounding box decoding stages, which refine the current\npredictions according to their previous results. Model parameters within each\nstage are independent, evolving a huge cost. In this paper, we find the general\nsetting of decoding stages is actually redundant. By simply sharing parameters\nand making a recursive decoder, the detector already obtains a significant\nimprovement. The recursive decoder can be further enhanced by positional\nencoding (PE) of the proposal box, which makes it aware of the exact locations\nand sizes of input bounding boxes, thus becoming adaptive to proposals from\ndifferent stages during the recursion. Moreover, we also design\ncenterness-based PE to distinguish the RoI feature element and dynamic\nconvolution kernels at different positions within the bounding box. To validate\nthe effectiveness of the proposed method, we conduct intensive ablations and\nbuild the full model on three recent mainstream region-based detectors. The\nRecusiveDet is able to achieve obvious performance boosts with even fewer model\nparameters and slightly increased computation cost. Codes are available at\nhttps://github.com/bravezzzzzz/RecursiveDet.",
        "translated": ""
    },
    {
        "title": "Object-based Probabilistic Similarity Evidence of Sparse Latent Features\n  from Fully Convolutional Networks",
        "url": "http://arxiv.org/abs/2307.13606v1",
        "pub_date": "2023-07-25",
        "summary": "Similarity analysis using neural networks has emerged as a powerful technique\nfor understanding and categorizing complex patterns in various domains. By\nleveraging the latent representations learned by neural networks, data objects\nsuch as images can be compared effectively. This research explores the\nutilization of latent information generated by fully convolutional networks\n(FCNs) in similarity analysis, notably to estimate the visual resemblance of\nobjects segmented in 2D pictures. To do this, the analytical scheme comprises\ntwo steps: (1) extracting and transforming feature patterns per 2D object from\na trained FCN, and (2) identifying the most similar patterns through fuzzy\ninference. The step (2) can be further enhanced by incorporating a weighting\nscheme that considers the significance of latent variables in the analysis. The\nresults provide valuable insights into the benefits and challenges of employing\nneural network-based similarity analysis for discerning data patterns\neffectively.",
        "translated": ""
    },
    {
        "title": "Decisive Data using Multi-Modality Optical Sensors for Advanced\n  Vehicular Systems",
        "url": "http://arxiv.org/abs/2307.13600v1",
        "pub_date": "2023-07-25",
        "summary": "Optical sensors have played a pivotal role in acquiring real world data for\ncritical applications. This data, when integrated with advanced machine\nlearning algorithms provides meaningful information thus enhancing human\nvision. This paper focuses on various optical technologies for design and\ndevelopment of state-of-the-art out-cabin forward vision systems and in-cabin\ndriver monitoring systems. The focused optical sensors include Longwave Thermal\nImaging (LWIR) cameras, Near Infrared (NIR), Neuromorphic/ event cameras,\nVisible CMOS cameras and Depth cameras. Further the paper discusses different\npotential applications which can be employed using the unique strengths of each\nthese optical modalities in real time environment.",
        "translated": ""
    },
    {
        "title": "Virtual Mirrors: Non-Line-of-Sight Imaging Beyond the Third Bounce",
        "url": "http://arxiv.org/abs/2307.14341v1",
        "pub_date": "2023-07-26",
        "summary": "Non-line-of-sight (NLOS) imaging methods are capable of reconstructing\ncomplex scenes that are not visible to an observer using indirect illumination.\nHowever, they assume only third-bounce illumination, so they are currently\nlimited to single-corner configurations, and present limited visibility when\nimaging surfaces at certain orientations. To reason about and tackle these\nlimitations, we make the key observation that planar diffuse surfaces behave\nspecularly at wavelengths used in the computational wave-based NLOS imaging\ndomain. We call such surfaces virtual mirrors. We leverage this observation to\nexpand the capabilities of NLOS imaging using illumination beyond the third\nbounce, addressing two problems: imaging single-corner objects at limited\nvisibility angles, and imaging objects hidden behind two corners. To image\nobjects at limited visibility angles, we first analyze the reflections of the\nknown illuminated point on surfaces of the scene as an estimator of the\nposition and orientation of objects with limited visibility. We then image\nthose limited visibility objects by computationally building secondary\napertures at other surfaces that observe the target object from a direct\nvisibility perspective. Beyond single-corner NLOS imaging, we exploit the\nspecular behavior of virtual mirrors to image objects hidden behind a second\ncorner by imaging the space behind such virtual mirrors, where the mirror image\nof objects hidden around two corners is formed. No specular surfaces were\ninvolved in the making of this paper.",
        "translated": ""
    },
    {
        "title": "MAMo: Leveraging Memory and Attention for Monocular Video Depth\n  Estimation",
        "url": "http://arxiv.org/abs/2307.14336v1",
        "pub_date": "2023-07-26",
        "summary": "We propose MAMo, a novel memory and attention frame-work for monocular video\ndepth estimation. MAMo can augment and improve any single-image depth\nestimation networks into video depth estimation models, enabling them to take\nadvantage of the temporal information to predict more accurate depth. In MAMo,\nwe augment model with memory which aids the depth prediction as the model\nstreams through the video. Specifically, the memory stores learned visual and\ndisplacement tokens of the previous time instances. This allows the depth\nnetwork to cross-reference relevant features from the past when predicting\ndepth on the current frame. We introduce a novel scheme to continuously update\nthe memory, optimizing it to keep tokens that correspond with both the past and\nthe present visual information. We adopt attention-based approach to process\nmemory features where we first learn the spatio-temporal relation among the\nresultant visual and displacement memory tokens using self-attention module.\nFurther, the output features of self-attention are aggregated with the current\nvisual features through cross-attention. The cross-attended features are\nfinally given to a decoder to predict depth on the current frame. Through\nextensive experiments on several benchmarks, including KITTI, NYU-Depth V2, and\nDDAD, we show that MAMo consistently improves monocular depth estimation\nnetworks and sets new state-of-the-art (SOTA) accuracy. Notably, our MAMo video\ndepth estimation provides higher accuracy with lower latency, when omparing to\nSOTA cost-volume-based video depth models.",
        "translated": ""
    },
    {
        "title": "Towards Generalist Biomedical AI",
        "url": "http://arxiv.org/abs/2307.14334v1",
        "pub_date": "2023-07-26",
        "summary": "Medicine is inherently multimodal, with rich data modalities spanning text,\nimaging, genomics, and more. Generalist biomedical artificial intelligence (AI)\nsystems that flexibly encode, integrate, and interpret this data at scale can\npotentially enable impactful applications ranging from scientific discovery to\ncare delivery. To enable the development of these models, we first curate\nMultiMedBench, a new multimodal biomedical benchmark. MultiMedBench encompasses\n14 diverse tasks such as medical question answering, mammography and\ndermatology image interpretation, radiology report generation and\nsummarization, and genomic variant calling. We then introduce Med-PaLM\nMultimodal (Med-PaLM M), our proof of concept for a generalist biomedical AI\nsystem. Med-PaLM M is a large multimodal generative model that flexibly encodes\nand interprets biomedical data including clinical language, imaging, and\ngenomics with the same set of model weights. Med-PaLM M reaches performance\ncompetitive with or exceeding the state of the art on all MultiMedBench tasks,\noften surpassing specialist models by a wide margin. We also report examples of\nzero-shot generalization to novel medical concepts and tasks, positive transfer\nlearning across tasks, and emergent zero-shot medical reasoning. To further\nprobe the capabilities and limitations of Med-PaLM M, we conduct a radiologist\nevaluation of model-generated (and human) chest X-ray reports and observe\nencouraging performance across model scales. In a side-by-side ranking on 246\nretrospective chest X-rays, clinicians express a pairwise preference for\nMed-PaLM M reports over those produced by radiologists in up to 40.50% of\ncases, suggesting potential clinical utility. While considerable work is needed\nto validate these models in real-world use cases, our results represent a\nmilestone towards the development of generalist biomedical AI systems.",
        "translated": ""
    },
    {
        "title": "Event-based Vision for Early Prediction of Manipulation Actions",
        "url": "http://arxiv.org/abs/2307.14332v1",
        "pub_date": "2023-07-26",
        "summary": "Neuromorphic visual sensors are artificial retinas that output sequences of\nasynchronous events when brightness changes occur in the scene. These sensors\noffer many advantages including very high temporal resolution, no motion blur\nand smart data compression ideal for real-time processing. In this study, we\nintroduce an event-based dataset on fine-grained manipulation actions and\nperform an experimental study on the use of transformers for action prediction\nwith events. There is enormous interest in the fields of cognitive robotics and\nhuman-robot interaction on understanding and predicting human actions as early\nas possible. Early prediction allows anticipating complex stages for planning,\nenabling effective and real-time interaction. Our Transformer network uses\nevents to predict manipulation actions as they occur, using online inference.\nThe model succeeds at predicting actions early on, building up confidence over\ntime and achieving state-of-the-art classification. Moreover, the\nattention-based transformer architecture allows us to study the role of the\nspatio-temporal patterns selected by the model. Our experiments show that the\nTransformer network captures action dynamic features outperforming video-based\napproaches and succeeding with scenarios where the differences between actions\nlie in very subtle cues. Finally, we release the new event dataset, which is\nthe first in the literature for manipulation action recognition. Code will be\navailable at https://github.com/DaniDeniz/EventVisionTransformer.",
        "translated": ""
    },
    {
        "title": "Visual Instruction Inversion: Image Editing via Visual Prompting",
        "url": "http://arxiv.org/abs/2307.14331v1",
        "pub_date": "2023-07-26",
        "summary": "Text-conditioned image editing has emerged as a powerful tool for editing\nimages. However, in many situations, language can be ambiguous and ineffective\nin describing specific image edits. When faced with such challenges, visual\nprompts can be a more informative and intuitive way to convey ideas. We present\na method for image editing via visual prompting. Given pairs of example that\nrepresent the \"before\" and \"after\" images of an edit, our goal is to learn a\ntext-based editing direction that can be used to perform the same edit on new\nimages. We leverage the rich, pretrained editing capabilities of text-to-image\ndiffusion models by inverting visual prompts into editing instructions. Our\nresults show that with just one example pair, we can achieve competitive\nresults compared to state-of-the-art text-conditioned image editing frameworks.",
        "translated": ""
    },
    {
        "title": "Unraveling the Complexity of Splitting Sequential Data: Tackling\n  Challenges in Video and Time Series Analysis",
        "url": "http://arxiv.org/abs/2307.14294v1",
        "pub_date": "2023-07-26",
        "summary": "Splitting of sequential data, such as videos and time series, is an essential\nstep in various data analysis tasks, including object tracking and anomaly\ndetection. However, splitting sequential data presents a variety of challenges\nthat can impact the accuracy and reliability of subsequent analyses. This\nconcept article examines the challenges associated with splitting sequential\ndata, including data acquisition, data representation, split ratio selection,\nsetting up quality criteria, and choosing suitable selection strategies. We\nexplore these challenges through two real-world examples: motor test benches\nand particle tracking in liquids.",
        "translated": ""
    },
    {
        "title": "US &amp; MR Image-Fusion Based on Skin Co-Registration",
        "url": "http://arxiv.org/abs/2307.14288v1",
        "pub_date": "2023-07-26",
        "summary": "The study and development of innovative solutions for the advanced\nvisualisation, representation and analysis of medical images offer different\nresearch directions. Current practice in medical imaging consists in combining\nreal-time US with imaging modalities that allow internal anatomy acquisitions,\nsuch as CT, MRI, PET or similar. Application of image-fusion approaches can be\nfound in tracking surgical tools and/or needles, in real-time during\ninterventions. Thus, this work proposes a fusion imaging system for the\nregistration of CT and MRI images with real-time US acquisition leveraging a 3D\ncamera sensor. The main focus of the work is the portability of the system and\nits applicability to different anatomical districts.",
        "translated": ""
    },
    {
        "title": "Large-scale Fully-Unsupervised Re-Identification",
        "url": "http://arxiv.org/abs/2307.14278v1",
        "pub_date": "2023-07-26",
        "summary": "Fully-unsupervised Person and Vehicle Re-Identification have received\nincreasing attention due to their broad applicability in surveillance,\nforensics, event understanding, and smart cities, without requiring any manual\nannotation. However, most of the prior art has been evaluated in datasets that\nhave just a couple thousand samples. Such small-data setups often allow the use\nof costly techniques in time and memory footprints, such as Re-Ranking, to\nimprove clustering results. Moreover, some previous work even pre-selects the\nbest clustering hyper-parameters for each dataset, which is unrealistic in a\nlarge-scale fully-unsupervised scenario. In this context, this work tackles a\nmore realistic scenario and proposes two strategies to learn from large-scale\nunlabeled data. The first strategy performs a local neighborhood sampling to\nreduce the dataset size in each iteration without violating neighborhood\nrelationships. A second strategy leverages a novel Re-Ranking technique, which\nhas a lower time upper bound complexity and reduces the memory complexity from\nO(n^2) to O(kn) with k &lt;&lt; n. To avoid the pre-selection of specific\nhyper-parameter values for the clustering algorithm, we also present a novel\nscheduling algorithm that adjusts the density parameter during training, to\nleverage the diversity of samples and keep the learning robust to noisy\nlabeling. Finally, due to the complementary knowledge learned by different\nmodels, we also introduce a co-training strategy that relies upon the\npermutation of predicted pseudo-labels, among the backbones, with no need for\nany hyper-parameters or weighting optimization. The proposed methodology\noutperforms the state-of-the-art methods in well-known benchmarks and in the\nchallenging large-scale Veri-Wild dataset, with a faster and memory-efficient\nRe-Ranking strategy, and a large-scale, noisy-robust, and ensemble-based\nlearning approach.",
        "translated": ""
    },
    {
        "title": "G2L: Semantically Aligned and Uniform Video Grounding via Geodesic and\n  Game Theory",
        "url": "http://arxiv.org/abs/2307.14277v1",
        "pub_date": "2023-07-26",
        "summary": "The recent video grounding works attempt to introduce vanilla contrastive\nlearning into video grounding. However, we claim that this naive solution is\nsuboptimal. Contrastive learning requires two key properties: (1)\n\\emph{alignment} of features of similar samples, and (2) \\emph{uniformity} of\nthe induced distribution of the normalized features on the hypersphere. Due to\ntwo annoying issues in video grounding: (1) the co-existence of some visual\nentities in both ground truth and other moments, \\ie semantic overlapping; (2)\nonly a few moments in the video are annotated, \\ie sparse annotation dilemma,\nvanilla contrastive learning is unable to model the correlations between\ntemporally distant moments and learned inconsistent video representations. Both\ncharacteristics lead to vanilla contrastive learning being unsuitable for video\ngrounding. In this paper, we introduce Geodesic and Game Localization (G2L), a\nsemantically aligned and uniform video grounding framework via geodesic and\ngame theory. We quantify the correlations among moments leveraging the geodesic\ndistance that guides the model to learn the correct cross-modal\nrepresentations. Furthermore, from the novel perspective of game theory, we\npropose semantic Shapley interaction based on geodesic distance sampling to\nlearn fine-grained semantic alignment in similar moments. Experiments on three\nbenchmarks demonstrate the effectiveness of our method.",
        "translated": ""
    },
    {
        "title": "Deepfake Image Generation for Improved Brain Tumor Segmentation",
        "url": "http://arxiv.org/abs/2307.14273v1",
        "pub_date": "2023-07-26",
        "summary": "As the world progresses in technology and health, awareness of disease by\nrevealing asymptomatic signs improves. It is important to detect and treat\ntumors in early stage as it can be life-threatening. Computer-aided\ntechnologies are used to overcome lingering limitations facing disease\ndiagnosis, while brain tumor segmentation remains a difficult process,\nespecially when multi-modality data is involved. This is mainly attributed to\nineffective training due to lack of data and corresponding labelling. This work\ninvestigates the feasibility of employing deep-fake image generation for\neffective brain tumor segmentation. To this end, a Generative Adversarial\nNetwork was used for image-to-image translation for increasing dataset size,\nfollowed by image segmentation using a U-Net-based convolutional neural network\ntrained with deepfake images. Performance of the proposed approach is compared\nwith ground truth of four publicly available datasets. Results show improved\nperformance in terms of image segmentation quality metrics, and could\npotentially assist when training with limited data.",
        "translated": ""
    },
    {
        "title": "To Adapt or Not to Adapt? Real-Time Adaptation for Semantic Segmentation",
        "url": "http://arxiv.org/abs/2307.15063v1",
        "pub_date": "2023-07-27",
        "summary": "The goal of Online Domain Adaptation for semantic segmentation is to handle\nunforeseeable domain changes that occur during deployment, like sudden weather\nevents. However, the high computational costs associated with brute-force\nadaptation make this paradigm unfeasible for real-world applications. In this\npaper we propose HAMLET, a Hardware-Aware Modular Least Expensive Training\nframework for real-time domain adaptation. Our approach includes a\nhardware-aware back-propagation orchestration agent (HAMT) and a dedicated\ndomain-shift detector that enables active control over when and how the model\nis adapted (LT). Thanks to these advancements, our approach is capable of\nperforming semantic segmentation while simultaneously adapting at more than\n29FPS on a single consumer-grade GPU. Our framework's encouraging accuracy and\nspeed trade-off is demonstrated on OnDA and SHIFT benchmarks through\nexperimental results.",
        "translated": ""
    },
    {
        "title": "Self-Supervised Visual Acoustic Matching",
        "url": "http://arxiv.org/abs/2307.15064v1",
        "pub_date": "2023-07-27",
        "summary": "Acoustic matching aims to re-synthesize an audio clip to sound as if it were\nrecorded in a target acoustic environment. Existing methods assume access to\npaired training data, where the audio is observed in both source and target\nenvironments, but this limits the diversity of training data or requires the\nuse of simulated data or heuristics to create paired samples. We propose a\nself-supervised approach to visual acoustic matching where training samples\ninclude only the target scene image and audio -- without acoustically\nmismatched source audio for reference. Our approach jointly learns to\ndisentangle room acoustics and re-synthesize audio into the target environment,\nvia a conditional GAN framework and a novel metric that quantifies the level of\nresidual acoustic information in the de-biased audio. Training with either\nin-the-wild web data or simulated data, we demonstrate it outperforms the\nstate-of-the-art on multiple challenging datasets and a wide variety of\nreal-world audio and environments.",
        "translated": ""
    },
    {
        "title": "The RoboDepth Challenge: Methods and Advancements Towards Robust Depth\n  Estimation",
        "url": "http://arxiv.org/abs/2307.15061v1",
        "pub_date": "2023-07-27",
        "summary": "Accurate depth estimation under out-of-distribution (OoD) scenarios, such as\nadverse weather conditions, sensor failure, and noise contamination, is\ndesirable for safety-critical applications. Existing depth estimation systems,\nhowever, suffer inevitably from real-world corruptions and perturbations and\nare struggled to provide reliable depth predictions under such cases. In this\npaper, we summarize the winning solutions from the RoboDepth Challenge -- an\nacademic competition designed to facilitate and advance robust OoD depth\nestimation. This challenge was developed based on the newly established KITTI-C\nand NYUDepth2-C benchmarks. We hosted two stand-alone tracks, with an emphasis\non robust self-supervised and robust fully-supervised depth estimation,\nrespectively. Out of more than two hundred participants, nine unique and\ntop-performing solutions have appeared, with novel designs ranging from the\nfollowing aspects: spatial- and frequency-domain augmentations, masked image\nmodeling, image restoration and super-resolution, adversarial training,\ndiffusion-based noise suppression, vision-language pre-training, learned model\nensembling, and hierarchical feature enhancement. Extensive experimental\nanalyses along with insightful observations are drawn to better understand the\nrationale behind each design. We hope this challenge could lay a solid\nfoundation for future research on robust and reliable depth estimation and\nbeyond. The datasets, competition toolkit, workshop recordings, and source code\nfrom the winning teams are publicly available on the challenge website.",
        "translated": ""
    },
    {
        "title": "MARS: An Instance-aware, Modular and Realistic Simulator for Autonomous\n  Driving",
        "url": "http://arxiv.org/abs/2307.15058v1",
        "pub_date": "2023-07-27",
        "summary": "Nowadays, autonomous cars can drive smoothly in ordinary cases, and it is\nwidely recognized that realistic sensor simulation will play a critical role in\nsolving remaining corner cases by simulating them. To this end, we propose an\nautonomous driving simulator based upon neural radiance fields (NeRFs).\nCompared with existing works, ours has three notable features: (1)\nInstance-aware. Our simulator models the foreground instances and background\nenvironments separately with independent networks so that the static (e.g.,\nsize and appearance) and dynamic (e.g., trajectory) properties of instances can\nbe controlled separately. (2) Modular. Our simulator allows flexible switching\nbetween different modern NeRF-related backbones, sampling strategies, input\nmodalities, etc. We expect this modular design to boost academic progress and\nindustrial deployment of NeRF-based autonomous driving simulation. (3)\nRealistic. Our simulator set new state-of-the-art photo-realism results given\nthe best module selection. Our simulator will be open-sourced while most of our\ncounterparts are not. Project page: https://open-air-sun.github.io/mars/.",
        "translated": ""
    },
    {
        "title": "PointOdyssey: A Large-Scale Synthetic Dataset for Long-Term Point\n  Tracking",
        "url": "http://arxiv.org/abs/2307.15055v1",
        "pub_date": "2023-07-27",
        "summary": "We introduce PointOdyssey, a large-scale synthetic dataset, and data\ngeneration framework, for the training and evaluation of long-term fine-grained\ntracking algorithms. Our goal is to advance the state-of-the-art by placing\nemphasis on long videos with naturalistic motion. Toward the goal of\nnaturalism, we animate deformable characters using real-world motion capture\ndata, we build 3D scenes to match the motion capture environments, and we\nrender camera viewpoints using trajectories mined via structure-from-motion on\nreal videos. We create combinatorial diversity by randomizing character\nappearance, motion profiles, materials, lighting, 3D assets, and atmospheric\neffects. Our dataset currently includes 104 videos, averaging 2,000 frames\nlong, with orders of magnitude more correspondence annotations than prior work.\nWe show that existing methods can be trained from scratch in our dataset and\noutperform the published variants. Finally, we introduce modifications to the\nPIPs point tracking method, greatly widening its temporal receptive field,\nwhich improves its performance on PointOdyssey as well as on two real-world\nbenchmarks. Our data and code are publicly available at:\nhttps://pointodyssey.com",
        "translated": ""
    },
    {
        "title": "Learning Depth Estimation for Transparent and Mirror Surfaces",
        "url": "http://arxiv.org/abs/2307.15052v1",
        "pub_date": "2023-07-27",
        "summary": "Inferring the depth of transparent or mirror (ToM) surfaces represents a hard\nchallenge for either sensors, algorithms, or deep networks. We propose a simple\npipeline for learning to estimate depth properly for such surfaces with neural\nnetworks, without requiring any ground-truth annotation. We unveil how to\nobtain reliable pseudo labels by in-painting ToM objects in images and\nprocessing them with a monocular depth estimation model. These labels can be\nused to fine-tune existing monocular or stereo networks, to let them learn how\nto deal with ToM surfaces. Experimental results on the Booster dataset show the\ndramatic improvements enabled by our remarkably simple proposal.",
        "translated": ""
    },
    {
        "title": "Regularized Mask Tuning: Uncovering Hidden Knowledge in Pre-trained\n  Vision-Language Models",
        "url": "http://arxiv.org/abs/2307.15049v1",
        "pub_date": "2023-07-27",
        "summary": "Prompt tuning and adapter tuning have shown great potential in transferring\npre-trained vision-language models (VLMs) to various downstream tasks. In this\nwork, we design a new type of tuning method, termed as regularized mask tuning,\nwhich masks the network parameters through a learnable selection. Inspired by\nneural pathways, we argue that the knowledge required by a downstream task\nalready exists in the pre-trained weights but just gets concealed in the\nupstream pre-training stage. To bring the useful knowledge back into light, we\nfirst identify a set of parameters that are important to a given downstream\ntask, then attach a binary mask to each parameter, and finally optimize these\nmasks on the downstream data with the parameters frozen. When updating the\nmask, we introduce a novel gradient dropout strategy to regularize the\nparameter selection, in order to prevent the model from forgetting old\nknowledge and overfitting the downstream data. Experimental results on 11\ndatasets demonstrate the consistent superiority of our method over previous\nalternatives. It is noteworthy that we manage to deliver 18.73% performance\nimprovement compared to the zero-shot CLIP via masking an average of only 2.56%\nparameters. Furthermore, our method is synergistic with most existing\nparameter-efficient tuning methods and can boost the performance on top of\nthem. Project page can be found here (https://wuw2019.github.io/RMT/).",
        "translated": ""
    },
    {
        "title": "A Transformer-based Approach for Arabic Offline Handwritten Text\n  Recognition",
        "url": "http://arxiv.org/abs/2307.15045v1",
        "pub_date": "2023-07-27",
        "summary": "Handwriting recognition is a challenging and critical problem in the fields\nof pattern recognition and machine learning, with applications spanning a wide\nrange of domains. In this paper, we focus on the specific issue of recognizing\noffline Arabic handwritten text. Existing approaches typically utilize a\ncombination of convolutional neural networks for image feature extraction and\nrecurrent neural networks for temporal modeling, with connectionist temporal\nclassification used for text generation. However, these methods suffer from a\nlack of parallelization due to the sequential nature of recurrent neural\nnetworks. Furthermore, these models cannot account for linguistic rules,\nnecessitating the use of an external language model in the post-processing\nstage to boost accuracy. To overcome these issues, we introduce two alternative\narchitectures, namely the Transformer Transducer and the standard\nsequence-to-sequence Transformer, and compare their performance in terms of\naccuracy and speed. Our approach can model language dependencies and relies\nonly on the attention mechanism, thereby making it more parallelizable and less\ncomplex. We employ pre-trained Transformers for both image understanding and\nlanguage modeling. Our evaluation on the Arabic KHATT dataset demonstrates that\nour proposed method outperforms the current state-of-the-art approaches for\nrecognizing offline Arabic handwritten text.",
        "translated": ""
    },
    {
        "title": "TEDi: Temporally-Entangled Diffusion for Long-Term Motion Synthesis",
        "url": "http://arxiv.org/abs/2307.15042v1",
        "pub_date": "2023-07-27",
        "summary": "The gradual nature of a diffusion process that synthesizes samples in small\nincrements constitutes a key ingredient of Denoising Diffusion Probabilistic\nModels (DDPM), which have presented unprecedented quality in image synthesis\nand been recently explored in the motion domain. In this work, we propose to\nadapt the gradual diffusion concept (operating along a diffusion time-axis)\ninto the temporal-axis of the motion sequence. Our key idea is to extend the\nDDPM framework to support temporally varying denoising, thereby entangling the\ntwo axes. Using our special formulation, we iteratively denoise a motion buffer\nthat contains a set of increasingly-noised poses, which auto-regressively\nproduces an arbitrarily long stream of frames. With a stationary diffusion\ntime-axis, in each diffusion step we increment only the temporal-axis of the\nmotion such that the framework produces a new, clean frame which is removed\nfrom the beginning of the buffer, followed by a newly drawn noise vector that\nis appended to it. This new mechanism paves the way towards a new framework for\nlong-term motion synthesis with applications to character animation and other\ndomains.",
        "translated": ""
    },
    {
        "title": "Diverse Inpainting and Editing with GAN Inversion",
        "url": "http://arxiv.org/abs/2307.15033v1",
        "pub_date": "2023-07-27",
        "summary": "Recent inversion methods have shown that real images can be inverted into\nStyleGAN's latent space and numerous edits can be achieved on those images\nthanks to the semantically rich feature representations of well-trained GAN\nmodels. However, extensive research has also shown that image inversion is\nchallenging due to the trade-off between high-fidelity reconstruction and\neditability. In this paper, we tackle an even more difficult task, inverting\nerased images into GAN's latent space for realistic inpaintings and editings.\nFurthermore, by augmenting inverted latent codes with different latent samples,\nwe achieve diverse inpaintings. Specifically, we propose to learn an encoder\nand mixing network to combine encoded features from erased images with\nStyleGAN's mapped features from random samples. To encourage the mixing network\nto utilize both inputs, we train the networks with generated data via a novel\nset-up. We also utilize higher-rate features to prevent color inconsistencies\nbetween the inpainted and unerased parts. We run extensive experiments and\ncompare our method with state-of-the-art inversion and inpainting methods.\nQualitative metrics and visual comparisons show significant improvements.",
        "translated": ""
    },
    {
        "title": "Semi-Supervised Object Detection in the Open World",
        "url": "http://arxiv.org/abs/2307.15710v1",
        "pub_date": "2023-07-28",
        "summary": "Existing approaches for semi-supervised object detection assume a fixed set\nof classes present in training and unlabeled datasets, i.e., in-distribution\n(ID) data. The performance of these techniques significantly degrades when\nthese techniques are deployed in the open-world, due to the fact that the\nunlabeled and test data may contain objects that were not seen during training,\ni.e., out-of-distribution (OOD) data. The two key questions that we explore in\nthis paper are: can we detect these OOD samples and if so, can we learn from\nthem? With these considerations in mind, we propose the Open World\nSemi-supervised Detection framework (OWSSD) that effectively detects OOD data\nalong with a semi-supervised learning pipeline that learns from both ID and OOD\ndata. We introduce an ensemble based OOD detector consisting of lightweight\nauto-encoder networks trained only on ID data. Through extensive evalulation,\nwe demonstrate that our method performs competitively against state-of-the-art\nOOD detection algorithms and also significantly boosts the semi-supervised\nlearning performance in open-world scenarios.",
        "translated": ""
    },
    {
        "title": "MeMOTR: Long-Term Memory-Augmented Transformer for Multi-Object Tracking",
        "url": "http://arxiv.org/abs/2307.15700v2",
        "pub_date": "2023-07-28",
        "summary": "As a video task, Multiple Object Tracking (MOT) is expected to capture\ntemporal information of targets effectively. Unfortunately, most existing\nmethods only explicitly exploit the object features between adjacent frames,\nwhile lacking the capacity to model long-term temporal information. In this\npaper, we propose MeMOTR, a long-term memory-augmented Transformer for\nmulti-object tracking. Our method is able to make the same object's track\nembedding more stable and distinguishable by leveraging long-term memory\ninjection with a customized memory-attention layer. This significantly improves\nthe target association ability of our model. Experimental results on DanceTrack\nshow that MeMOTR impressively surpasses the state-of-the-art method by 7.9% and\n13.0% on HOTA and AssA metrics, respectively. Furthermore, our model also\noutperforms other Transformer-based methods on association performance on MOT17\nand generalizes well on BDD100K. Code is available at\nhttps://github.com/MCG-NJU/MeMOTR.",
        "translated": ""
    },
    {
        "title": "SimDETR: Simplifying self-supervised pretraining for DETR",
        "url": "http://arxiv.org/abs/2307.15697v1",
        "pub_date": "2023-07-28",
        "summary": "DETR-based object detectors have achieved remarkable performance but are\nsample-inefficient and exhibit slow convergence. Unsupervised pretraining has\nbeen found to be helpful to alleviate these impediments, allowing training with\nlarge amounts of unlabeled data to improve the detector's performance. However,\nexisting methods have their own limitations, like keeping the detector's\nbackbone frozen in order to avoid performance degradation and utilizing\npretraining objectives misaligned with the downstream task. To overcome these\nlimitations, we propose a simple pretraining framework for DETR-based detectors\nthat consists of three simple yet key ingredients: (i) richer, semantics-based\ninitial proposals derived from high-level feature maps, (ii) discriminative\ntraining using object pseudo-labels produced via clustering, (iii)\nself-training to take advantage of the improved object proposals learned by the\ndetector. We report two main findings: (1) Our pretraining outperforms prior\nDETR pretraining works on both the full and low data regimes by significant\nmargins. (2) We show we can pretrain DETR from scratch (including the backbone)\ndirectly on complex image datasets like COCO, paving the path for unsupervised\nrepresentation learning directly using DETR.",
        "translated": ""
    },
    {
        "title": "PatchMixer: Rethinking network design to boost generalization for 3D\n  point cloud understanding",
        "url": "http://arxiv.org/abs/2307.15692v1",
        "pub_date": "2023-07-28",
        "summary": "The recent trend in deep learning methods for 3D point cloud understanding is\nto propose increasingly sophisticated architectures either to better capture 3D\ngeometries or by introducing possibly undesired inductive biases. Moreover,\nprior works introducing novel architectures compared their performance on the\nsame domain, devoting less attention to their generalization to other domains.\nWe argue that the ability of a model to transfer the learnt knowledge to\ndifferent domains is an important feature that should be evaluated to\nexhaustively assess the quality of a deep network architecture. In this work we\npropose PatchMixer, a simple yet effective architecture that extends the ideas\nbehind the recent MLP-Mixer paper to 3D point clouds. The novelties of our\napproach are the processing of local patches instead of the whole shape to\npromote robustness to partial point clouds, and the aggregation of patch-wise\nfeatures using an MLP as a simpler alternative to the graph convolutions or the\nattention mechanisms that are used in prior works. We evaluated our method on\nthe shape classification and part segmentation tasks, achieving superior\ngeneralization performance compared to a selection of the most relevant deep\narchitectures.",
        "translated": ""
    },
    {
        "title": "TrackAgent: 6D Object Tracking via Reinforcement Learning",
        "url": "http://arxiv.org/abs/2307.15671v1",
        "pub_date": "2023-07-28",
        "summary": "Tracking an object's 6D pose, while either the object itself or the observing\ncamera is moving, is important for many robotics and augmented reality\napplications. While exploiting temporal priors eases this problem,\nobject-specific knowledge is required to recover when tracking is lost. Under\nthe tight time constraints of the tracking task, RGB(D)-based methods are often\nconceptionally complex or rely on heuristic motion models. In comparison, we\npropose to simplify object tracking to a reinforced point cloud (depth only)\nalignment task. This allows us to train a streamlined approach from scratch\nwith limited amounts of sparse 3D point clouds, compared to the large datasets\nof diverse RGBD sequences required in previous works. We incorporate temporal\nframe-to-frame registration with object-based recovery by frame-to-model\nrefinement using a reinforcement learning (RL) agent that jointly solves for\nboth objectives. We also show that the RL agent's uncertainty and a\nrendering-based mask propagation are effective reinitialization triggers.",
        "translated": ""
    },
    {
        "title": "Multi-layer Aggregation as a key to feature-based OOD detection",
        "url": "http://arxiv.org/abs/2307.15647v1",
        "pub_date": "2023-07-28",
        "summary": "Deep Learning models are easily disturbed by variations in the input images\nthat were not observed during the training stage, resulting in unpredictable\npredictions. Detecting such Out-of-Distribution (OOD) images is particularly\ncrucial in the context of medical image analysis, where the range of possible\nabnormalities is extremely wide. Recently, a new category of methods has\nemerged, based on the analysis of the intermediate features of a trained model.\nThese methods can be divided into 2 groups: single-layer methods that consider\nthe feature map obtained at a fixed, carefully chosen layer, and multi-layer\nmethods that consider the ensemble of the feature maps generated by the model.\nWhile promising, a proper comparison of these algorithms is still lacking. In\nthis work, we compared various feature-based OOD detection methods on a large\nspectra of OOD (20 types), representing approximately 7800 3D MRIs. Our\nexperiments shed the light on two phenomenons. First, multi-layer methods\nconsistently outperform single-layer approaches, which tend to have\ninconsistent behaviour depending on the type of anomaly. Second, the OOD\ndetection performance highly depends on the architecture of the underlying\nneural network.",
        "translated": ""
    },
    {
        "title": "Scale-aware Test-time Click Adaptation for Pulmonary Nodule and Mass\n  Segmentation",
        "url": "http://arxiv.org/abs/2307.15645v1",
        "pub_date": "2023-07-28",
        "summary": "Pulmonary nodules and masses are crucial imaging features in lung cancer\nscreening that require careful management in clinical diagnosis. Despite the\nsuccess of deep learning-based medical image segmentation, the robust\nperformance on various sizes of lesions of nodule and mass is still\nchallenging. In this paper, we propose a multi-scale neural network with\nscale-aware test-time adaptation to address this challenge. Specifically, we\nintroduce an adaptive Scale-aware Test-time Click Adaptation method based on\neffortlessly obtainable lesion clicks as test-time cues to enhance segmentation\nperformance, particularly for large lesions. The proposed method can be\nseamlessly integrated into existing networks. Extensive experiments on both\nopen-source and in-house datasets consistently demonstrate the effectiveness of\nthe proposed method over some CNN and Transformer-based segmentation methods.\nOur code is available at https://github.com/SplinterLi/SaTTCA",
        "translated": ""
    },
    {
        "title": "Scaling Data Generation in Vision-and-Language Navigation",
        "url": "http://arxiv.org/abs/2307.15644v1",
        "pub_date": "2023-07-28",
        "summary": "Recent research in language-guided visual navigation has demonstrated a\nsignificant demand for the diversity of traversable environments and the\nquantity of supervision for training generalizable agents. To tackle the common\ndata scarcity issue in existing vision-and-language navigation datasets, we\npropose an effective paradigm for generating large-scale data for learning,\nwhich applies 1200+ photo-realistic environments from HM3D and Gibson datasets\nand synthesizes 4.9 million instruction trajectory pairs using fully-accessible\nresources on the web. Importantly, we investigate the influence of each\ncomponent in this paradigm on the agent's performance and study how to\nadequately apply the augmented data to pre-train and fine-tune an agent. Thanks\nto our large-scale dataset, the performance of an existing agent can be pushed\nup (+11% absolute with regard to previous SoTA) to a significantly new best of\n80% single-run success rate on the R2R test split by simple imitation learning.\nThe long-lasting generalization gap between navigating in seen and unseen\nenvironments is also reduced to less than 1% (versus 8% in the previous best\nmethod). Moreover, our paradigm also facilitates different models to achieve\nnew state-of-the-art navigation results on CVDN, REVERIE, and R2R in continuous\nenvironments.",
        "translated": ""
    },
    {
        "title": "CLIP Brings Better Features to Visual Aesthetics Learners",
        "url": "http://arxiv.org/abs/2307.15640v1",
        "pub_date": "2023-07-28",
        "summary": "The success of pre-training approaches on a variety of downstream tasks has\nrevitalized the field of computer vision. Image aesthetics assessment (IAA) is\none of the ideal application scenarios for such methods due to subjective and\nexpensive labeling procedure. In this work, an unified and flexible two-phase\n\\textbf{C}LIP-based \\textbf{S}emi-supervised \\textbf{K}nowledge\n\\textbf{D}istillation paradigm is proposed, namely \\textbf{\\textit{CSKD}}.\nSpecifically, we first integrate and leverage a multi-source unlabeled dataset\nto align rich features between a given visual encoder and an off-the-shelf CLIP\nimage encoder via feature alignment loss. Notably, the given visual encoder is\nnot limited by size or structure and, once well-trained, it can seamlessly\nserve as a better visual aesthetic learner for both student and teacher. In the\nsecond phase, the unlabeled data is also utilized in semi-supervised IAA\nlearning to further boost student model performance when applied in\nlatency-sensitive production scenarios. By analyzing the attention distance and\nentropy before and after feature alignment, we notice an alleviation of feature\ncollapse issue, which in turn showcase the necessity of feature alignment\ninstead of training directly based on CLIP image encoder. Extensive experiments\nindicate the superiority of CSKD, which achieves state-of-the-art performance\non multiple widely used IAA benchmarks.",
        "translated": ""
    },
    {
        "title": "TriadNet: Sampling-free predictive intervals for lesional volume in 3D\n  brain MR images",
        "url": "http://arxiv.org/abs/2307.15638v1",
        "pub_date": "2023-07-28",
        "summary": "The volume of a brain lesion (e.g. infarct or tumor) is a powerful indicator\nof patient prognosis and can be used to guide the therapeutic strategy.\nLesional volume estimation is usually performed by segmentation with deep\nconvolutional neural networks (CNN), currently the state-of-the-art approach.\nHowever, to date, few work has been done to equip volume segmentation tools\nwith adequate quantitative predictive intervals, which can hinder their\nusefulness and acceptation in clinical practice. In this work, we propose\nTriadNet, a segmentation approach relying on a multi-head CNN architecture,\nwhich provides both the lesion volumes and the associated predictive intervals\nsimultaneously, in less than a second. We demonstrate its superiority over\nother solutions on BraTS 2021, a large-scale MRI glioblastoma image database.",
        "translated": ""
    },
    {
        "title": "DiVA-360: The Dynamic Visuo-Audio Dataset for Immersive Neural Fields",
        "url": "http://arxiv.org/abs/2307.16897v1",
        "pub_date": "2023-07-31",
        "summary": "Advances in neural fields are enabling high-fidelity capture of the shape and\nappearance of static and dynamic scenes. However, their capabilities lag behind\nthose offered by representations such as pixels or meshes due to algorithmic\nchallenges and the lack of large-scale real-world datasets. We address the\ndataset limitation with DiVA-360, a real-world 360 dynamic visual-audio dataset\nwith synchronized multimodal visual, audio, and textual information about\ntable-scale scenes. It contains 46 dynamic scenes, 30 static scenes, and 95\nstatic objects spanning 11 categories captured using a new hardware system\nusing 53 RGB cameras at 120 FPS and 6 microphones for a total of 8.6M image\nframes and 1360 s of dynamic data. We provide detailed text descriptions for\nall scenes, foreground-background segmentation masks, category-specific 3D pose\nalignment for static objects, as well as metrics for comparison. Our data,\nhardware and software, and code are available at https://diva360.github.io/.",
        "translated": ""
    },
    {
        "title": "Disruptive Autoencoders: Leveraging Low-level features for 3D Medical\n  Image Pre-training",
        "url": "http://arxiv.org/abs/2307.16896v1",
        "pub_date": "2023-07-31",
        "summary": "Harnessing the power of pre-training on large-scale datasets like ImageNet\nforms a fundamental building block for the progress of representation\nlearning-driven solutions in computer vision. Medical images are inherently\ndifferent from natural images as they are acquired in the form of many\nmodalities (CT, MR, PET, Ultrasound etc.) and contain granulated information\nlike tissue, lesion, organs etc. These characteristics of medical images\nrequire special attention towards learning features representative of local\ncontext. In this work, we focus on designing an effective pre-training\nframework for 3D radiology images. First, we propose a new masking strategy\ncalled local masking where the masking is performed across channel embeddings\ninstead of tokens to improve the learning of local feature representations. We\ncombine this with classical low-level perturbations like adding noise and\ndownsampling to further enable low-level representation learning. To this end,\nwe introduce Disruptive Autoencoders, a pre-training framework that attempts to\nreconstruct the original image from disruptions created by a combination of\nlocal masking and low-level perturbations. Additionally, we also devise a\ncross-modal contrastive loss (CMCL) to accommodate the pre-training of multiple\nmodalities in a single framework. We curate a large-scale dataset to enable\npre-training of 3D medical radiology images (MRI and CT). The proposed\npre-training framework is tested across multiple downstream tasks and achieves\nstate-of-the-art performance. Notably, our proposed method tops the public test\nleaderboard of BTCV multi-organ segmentation challenge.",
        "translated": ""
    },
    {
        "title": "Image Synthesis under Limited Data: A Survey and Taxonomy",
        "url": "http://arxiv.org/abs/2307.16879v1",
        "pub_date": "2023-07-31",
        "summary": "Deep generative models, which target reproducing the given data distribution\nto produce novel samples, have made unprecedented advancements in recent years.\nTheir technical breakthroughs have enabled unparalleled quality in the\nsynthesis of visual content. However, one critical prerequisite for their\ntremendous success is the availability of a sufficient number of training\nsamples, which requires massive computation resources. When trained on limited\ndata, generative models tend to suffer from severe performance deterioration\ndue to overfitting and memorization. Accordingly, researchers have devoted\nconsiderable attention to develop novel models that are capable of generating\nplausible and diverse images from limited training data recently. Despite\nnumerous efforts to enhance training stability and synthesis quality in the\nlimited data scenarios, there is a lack of a systematic survey that provides 1)\na clear problem definition, critical challenges, and taxonomy of various tasks;\n2) an in-depth analysis on the pros, cons, and remain limitations of existing\nliterature; as well as 3) a thorough discussion on the potential applications\nand future directions in the field of image synthesis under limited data. In\norder to fill this gap and provide a informative introduction to researchers\nwho are new to this topic, this survey offers a comprehensive review and a\nnovel taxonomy on the development of image synthesis under limited data. In\nparticular, it covers the problem definition, requirements, main solutions,\npopular benchmarks, and remain challenges in a comprehensive and all-around\nmanner.",
        "translated": ""
    },
    {
        "title": "Revisiting the Parameter Efficiency of Adapters from the Perspective of\n  Precision Redundancy",
        "url": "http://arxiv.org/abs/2307.16867v1",
        "pub_date": "2023-07-31",
        "summary": "Current state-of-the-art results in computer vision depend in part on\nfine-tuning large pre-trained vision models. However, with the exponential\ngrowth of model sizes, the conventional full fine-tuning, which needs to store\na individual network copy for each tasks, leads to increasingly huge storage\nand transmission overhead. Adapter-based Parameter-Efficient Tuning (PET)\nmethods address this challenge by tuning lightweight adapters inserted into the\nfrozen pre-trained models. In this paper, we investigate how to make adapters\neven more efficient, reaching a new minimum size required to store a\ntask-specific fine-tuned network. Inspired by the observation that the\nparameters of adapters converge at flat local minima, we find that adapters are\nresistant to noise in parameter space, which means they are also resistant to\nlow numerical precision. To train low-precision adapters, we propose a\ncomputational-efficient quantization method which minimizes the quantization\nerror. Through extensive experiments, we find that low-precision adapters\nexhibit minimal performance degradation, and even 1-bit precision is sufficient\nfor adapters. The experimental results demonstrate that 1-bit adapters\noutperform all other PET methods on both the VTAB-1K benchmark and few-shot\nFGVC tasks, while requiring the smallest storage size. Our findings show, for\nthe first time, the significant potential of quantization techniques in PET,\nproviding a general solution to enhance the parameter efficiency of\nadapter-based PET methods. Code: https://github.com/JieShibo/PETL-ViT",
        "translated": ""
    },
    {
        "title": "Universal Adversarial Defense in Remote Sensing Based on Pre-trained\n  Denoising Diffusion Models",
        "url": "http://arxiv.org/abs/2307.16865v1",
        "pub_date": "2023-07-31",
        "summary": "Deep neural networks (DNNs) have achieved tremendous success in many remote\nsensing (RS) applications. However, their vulnerability to the threat of\nadversarial perturbations should not be neglected. Unfortunately, current\nadversarial defense approaches in RS studies usually suffer from performance\nfluctuation and unnecessary re-training costs due to the need for prior\nknowledge of the adversarial perturbations among RS data. To circumvent these\nchallenges, we propose a universal adversarial defense approach in RS imagery\n(UAD-RS) using pre-trained diffusion models to defend the common DNNs against\nmultiple unknown adversarial attacks. Specifically, the generative diffusion\nmodels are first pre-trained on different RS datasets to learn generalized\nrepresentations in various data domains. After that, a universal adversarial\npurification framework is developed using the forward and reverse process of\nthe pre-trained diffusion models to purify the perturbations from adversarial\nsamples. Furthermore, an adaptive noise level selection (ANLS) mechanism is\nbuilt to capture the optimal noise level of the diffusion model that can\nachieve the best purification results closest to the clean samples according to\ntheir Frechet Inception Distance (FID) in deep feature space. As a result, only\na single pre-trained diffusion model is needed for the universal purification\nof adversarial samples on each dataset, which significantly alleviates the\nre-training efforts for each attack setting and maintains high performance\nwithout the prior knowledge of adversarial perturbations. Experiments on four\nheterogeneous RS datasets regarding scene classification and semantic\nsegmentation verify that UAD-RS outperforms state-of-the-art adversarial\npurification approaches with a universal defense against seven commonly\nexisting adversarial perturbations.",
        "translated": ""
    },
    {
        "title": "MetaCAM: Ensemble-Based Class Activation Map",
        "url": "http://arxiv.org/abs/2307.16863v1",
        "pub_date": "2023-07-31",
        "summary": "The need for clear, trustworthy explanations of deep learning model\npredictions is essential for high-criticality fields, such as medicine and\nbiometric identification. Class Activation Maps (CAMs) are an increasingly\npopular category of visual explanation methods for Convolutional Neural\nNetworks (CNNs). However, the performance of individual CAMs depends largely on\nexperimental parameters such as the selected image, target class, and model.\nHere, we propose MetaCAM, an ensemble-based method for combining multiple\nexisting CAM methods based on the consensus of the top-k% most highly activated\npixels across component CAMs. We perform experiments to quantifiably determine\nthe optimal combination of 11 CAMs for a given MetaCAM experiment. A new method\ndenoted Cumulative Residual Effect (CRE) is proposed to summarize large-scale\nensemble-based experiments. We also present adaptive thresholding and\ndemonstrate how it can be applied to individual CAMs to improve their\nperformance, measured using pixel perturbation method Remove and Debias (ROAD).\nLastly, we show that MetaCAM outperforms existing CAMs and refines the most\nsalient regions of images used for model predictions. In a specific example,\nMetaCAM improved ROAD performance to 0.393 compared to 11 individual CAMs with\nranges from -0.101-0.172, demonstrating the importance of combining CAMs\nthrough an ensembling method and adaptive thresholding.",
        "translated": ""
    },
    {
        "title": "Random Sub-Samples Generation for Self-Supervised Real Image Denoising",
        "url": "http://arxiv.org/abs/2307.16825v1",
        "pub_date": "2023-07-31",
        "summary": "With sufficient paired training samples, the supervised deep learning methods\nhave attracted much attention in image denoising because of their superior\nperformance. However, it is still very challenging to widely utilize the\nsupervised methods in real cases due to the lack of paired noisy-clean images.\nMeanwhile, most self-supervised denoising methods are ineffective as well when\napplied to the real-world denoising tasks because of their strict assumptions\nin applications. For example, as a typical method for self-supervised\ndenoising, the original blind spot network (BSN) assumes that the noise is\npixel-wise independent, which is much different from the real cases. To solve\nthis problem, we propose a novel self-supervised real image denoising framework\nnamed Sampling Difference As Perturbation (SDAP) based on Random Sub-samples\nGeneration (RSG) with a cyclic sample difference loss. Specifically, we dig\ndeeper into the properties of BSN to make it more suitable for real noise.\nSurprisingly, we find that adding an appropriate perturbation to the training\nimages can effectively improve the performance of BSN. Further, we propose that\nthe sampling difference can be considered as perturbation to achieve better\nresults. Finally we propose a new BSN framework in combination with our RSG\nstrategy. The results show that it significantly outperforms other\nstate-of-the-art self-supervised denoising methods on real-world datasets. The\ncode is available at https://github.com/p1y2z3/SDAP.",
        "translated": ""
    },
    {
        "title": "Capturing Co-existing Distortions in User-Generated Content for\n  No-reference Video Quality Assessment",
        "url": "http://arxiv.org/abs/2307.16813v1",
        "pub_date": "2023-07-31",
        "summary": "Video Quality Assessment (VQA), which aims to predict the perceptual quality\nof a video, has attracted raising attention with the rapid development of\nstreaming media technology, such as Facebook, TikTok, Kwai, and so on. Compared\nwith other sequence-based visual tasks (\\textit{e.g.,} action recognition), VQA\nfaces two under-estimated challenges unresolved in User Generated Content (UGC)\nvideos. \\textit{First}, it is not rare that several frames containing serious\ndistortions (\\textit{e.g.,}blocking, blurriness), can determine the perceptual\nquality of the whole video, while other sequence-based tasks require more\nframes of equal importance for representations. \\textit{Second}, the perceptual\nquality of a video exhibits a multi-distortion distribution, due to the\ndifferences in the duration and probability of occurrence for various\ndistortions. In order to solve the above challenges, we propose \\textit{Visual\nQuality Transformer (VQT)} to extract quality-related sparse features more\nefficiently. Methodologically, a Sparse Temporal Attention (STA) is proposed to\nsample keyframes by analyzing the temporal correlation between frames, which\nreduces the computational complexity from $O(T^2)$ to $O(T \\log T)$.\nStructurally, a Multi-Pathway Temporal Network (MPTN) utilizes multiple STA\nmodules with different degrees of sparsity in parallel, capturing co-existing\ndistortions in a video. Experimentally, VQT demonstrates superior performance\nthan many \\textit{state-of-the-art} methods in three public no-reference VQA\ndatasets. Furthermore, VQT shows better performance in four full-reference VQA\ndatasets against widely-adopted industrial algorithms (\\textit{i.e.,} VMAF and\nAVQT).",
        "translated": ""
    },
    {
        "title": "DPMix: Mixture of Depth and Point Cloud Video Experts for 4D Action\n  Segmentation",
        "url": "http://arxiv.org/abs/2307.16803v1",
        "pub_date": "2023-07-31",
        "summary": "In this technical report, we present our findings from the research conducted\non the Human-Object Interaction 4D (HOI4D) dataset for egocentric action\nsegmentation task. As a relatively novel research area, point cloud video\nmethods might not be good at temporal modeling, especially for long point cloud\nvideos (\\eg, 150 frames). In contrast, traditional video understanding methods\nhave been well developed. Their effectiveness on temporal modeling has been\nwidely verified on many large scale video datasets. Therefore, we convert point\ncloud videos into depth videos and employ traditional video modeling methods to\nimprove 4D action segmentation. By ensembling depth and point cloud video\nmethods, the accuracy is significantly improved. The proposed method, named\nMixture of Depth and Point cloud video experts (DPMix), achieved the first\nplace in the 4D Action Segmentation Track of the HOI4D Challenge 2023.",
        "translated": ""
    },
    {
        "title": "From Generation to Suppression: Towards Effective Irregular Glow Removal\n  for Nighttime Visibility Enhancement",
        "url": "http://arxiv.org/abs/2307.16783v1",
        "pub_date": "2023-07-31",
        "summary": "Most existing Low-Light Image Enhancement (LLIE) methods are primarily\ndesigned to improve brightness in dark regions, which suffer from severe\ndegradation in nighttime images. However, these methods have limited\nexploration in another major visibility damage, the glow effects in real night\nscenes. Glow effects are inevitable in the presence of artificial light sources\nand cause further diffused blurring when directly enhanced. To settle this\nissue, we innovatively consider the glow suppression task as learning physical\nglow generation via multiple scattering estimation according to the Atmospheric\nPoint Spread Function (APSF). In response to the challenges posed by uneven\nglow intensity and varying source shapes, an APSF-based Nighttime Imaging Model\nwith Near-field Light Sources (NIM-NLS) is specifically derived to design a\nscalable Light-aware Blind Deconvolution Network (LBDN). The glow-suppressed\nresult is then brightened via a Retinex-based Enhancement Module (REM).\nRemarkably, the proposed glow suppression method is based on zero-shot learning\nand does not rely on any paired or unpaired training data. Empirical\nevaluations demonstrate the effectiveness of the proposed method in both glow\nsuppression and low-light enhancement tasks.",
        "translated": ""
    },
    {
        "title": "LISA: Reasoning Segmentation via Large Language Model",
        "url": "http://arxiv.org/abs/2308.00692v1",
        "pub_date": "2023-08-01",
        "summary": "Although perception systems have made remarkable advancements in recent\nyears, they still rely on explicit human instruction to identify the target\nobjects or categories before executing visual recognition tasks. Such systems\nlack the ability to actively reason and comprehend implicit user intentions. In\nthis work, we propose a new segmentation task -- reasoning segmentation. The\ntask is designed to output a segmentation mask given a complex and implicit\nquery text. Furthermore, we establish a benchmark comprising over one thousand\nimage-instruction pairs, incorporating intricate reasoning and world knowledge\nfor evaluation purposes. Finally, we present LISA: large Language Instructed\nSegmentation Assistant, which inherits the language generation capabilities of\nthe multi-modal Large Language Model (LLM) while also possessing the ability to\nproduce segmentation masks. We expand the original vocabulary with a &lt;SEG&gt;\ntoken and propose the embedding-as-mask paradigm to unlock the segmentation\ncapability. Remarkably, LISA can handle cases involving: 1) complex reasoning;\n2) world knowledge; 3) explanatory answers; 4) multi-turn conversation. Also,\nit demonstrates robust zero-shot capability when trained exclusively on\nreasoning-free datasets. In addition, fine-tuning the model with merely 239\nreasoning segmentation image-instruction pairs results in further performance\nenhancement. Experiments show our method not only unlocks new reasoning\nsegmentation capabilities but also proves effective in both complex reasoning\nsegmentation and standard referring segmentation tasks. Code, models, and demo\nare at https://github.com/dvlab-research/LISA.",
        "translated": ""
    },
    {
        "title": "AnyLoc: Towards Universal Visual Place Recognition",
        "url": "http://arxiv.org/abs/2308.00688v1",
        "pub_date": "2023-08-01",
        "summary": "Visual Place Recognition (VPR) is vital for robot localization. To date, the\nmost performant VPR approaches are environment- and task-specific: while they\nexhibit strong performance in structured environments (predominantly urban\ndriving), their performance degrades severely in unstructured environments,\nrendering most approaches brittle to robust real-world deployment. In this\nwork, we develop a universal solution to VPR -- a technique that works across a\nbroad range of structured and unstructured environments (urban, outdoors,\nindoors, aerial, underwater, and subterranean environments) without any\nre-training or fine-tuning. We demonstrate that general-purpose feature\nrepresentations derived from off-the-shelf self-supervised models with no\nVPR-specific training are the right substrate upon which to build such a\nuniversal VPR solution. Combining these derived features with unsupervised\nfeature aggregation enables our suite of methods, AnyLoc, to achieve up to 4X\nsignificantly higher performance than existing approaches. We further obtain a\n6% improvement in performance by characterizing the semantic properties of\nthese features, uncovering unique domains which encapsulate datasets from\nsimilar environments. Our detailed experiments and analysis lay a foundation\nfor building VPR solutions that may be deployed anywhere, anytime, and across\nanyview. We encourage the readers to explore our project page and interactive\ndemos: https://anyloc.github.io/.",
        "translated": ""
    },
    {
        "title": "Applicability of scaling laws to vision encoding models",
        "url": "http://arxiv.org/abs/2308.00678v1",
        "pub_date": "2023-08-01",
        "summary": "In this paper, we investigated how to build a high-performance vision\nencoding model to predict brain activity as part of our participation in the\nAlgonauts Project 2023 Challenge. The challenge provided brain activity\nrecorded by functional MRI (fMRI) while participants viewed images. Several\nvision models with parameter sizes ranging from 86M to 4.3B were used to build\npredictive models. To build highly accurate models, we focused our analysis on\ntwo main aspects: (1) How does the sample size of the fMRI training set change\nthe prediction accuracy? (2) How does the prediction accuracy across the visual\ncortex vary with the parameter size of the vision models? The results show that\nas the sample size used during training increases, the prediction accuracy\nimproves according to the scaling law. Similarly, we found that as the\nparameter size of the vision models increases, the prediction accuracy improves\naccording to the scaling law. These results suggest that increasing the sample\nsize of the fMRI training set and the parameter size of visual models may\ncontribute to more accurate visual models of the brain and lead to a better\nunderstanding of visual neuroscience.",
        "translated": ""
    },
    {
        "title": "Tool Documentation Enables Zero-Shot Tool-Usage with Large Language\n  Models",
        "url": "http://arxiv.org/abs/2308.00675v1",
        "pub_date": "2023-08-01",
        "summary": "Today, large language models (LLMs) are taught to use new tools by providing\na few demonstrations of the tool's usage. Unfortunately, demonstrations are\nhard to acquire, and can result in undesirable biased usage if the wrong\ndemonstration is chosen. Even in the rare scenario that demonstrations are\nreadily available, there is no principled selection protocol to determine how\nmany and which ones to provide. As tasks grow more complex, the selection\nsearch grows combinatorially and invariably becomes intractable. Our work\nprovides an alternative to demonstrations: tool documentation. We advocate the\nuse of tool documentation, descriptions for the individual tool usage, over\ndemonstrations. We substantiate our claim through three main empirical findings\non 6 tasks across both vision and language modalities. First, on existing\nbenchmarks, zero-shot prompts with only tool documentation are sufficient for\neliciting proper tool usage, achieving performance on par with few-shot\nprompts. Second, on a newly collected realistic tool-use dataset with hundreds\nof available tool APIs, we show that tool documentation is significantly more\nvaluable than demonstrations, with zero-shot documentation significantly\noutperforming few-shot without documentation. Third, we highlight the benefits\nof tool documentations by tackling image generation and video tracking using\njust-released unseen state-of-the-art models as tools. Finally, we highlight\nthe possibility of using tool documentation to automatically enable new\napplications: by using nothing more than the documentation of GroundingDino,\nStable Diffusion, XMem, and SAM, LLMs can re-invent the functionalities of the\njust-released Grounded-SAM and Track Anything models.",
        "translated": ""
    },
    {
        "title": "Toward Zero-shot Character Recognition: A Gold Standard Dataset with\n  Radical-level Annotations",
        "url": "http://arxiv.org/abs/2308.00655v1",
        "pub_date": "2023-08-01",
        "summary": "Optical character recognition (OCR) methods have been applied to diverse\ntasks, e.g., street view text recognition and document analysis. Recently,\nzero-shot OCR has piqued the interest of the research community because it\nconsiders a practical OCR scenario with unbalanced data distribution. However,\nthere is a lack of benchmarks for evaluating such zero-shot methods that apply\na divide-and-conquer recognition strategy by decomposing characters into\nradicals. Meanwhile, radical recognition, as another important OCR task, also\nlacks radical-level annotation for model training. In this paper, we construct\nan ancient Chinese character image dataset that contains both radical-level and\ncharacter-level annotations to satisfy the requirements of the above-mentioned\nmethods, namely, ACCID, where radical-level annotations include radical\ncategories, radical locations, and structural relations. To increase the\nadaptability of ACCID, we propose a splicing-based synthetic character\nalgorithm to augment the training samples and apply an image denoising method\nto improve the image quality. By introducing character decomposition and\nrecombination, we propose a baseline method for zero-shot OCR. The experimental\nresults demonstrate the validity of ACCID and the baseline model quantitatively\nand qualitatively.",
        "translated": ""
    },
    {
        "title": "Human-M3: A Multi-view Multi-modal Dataset for 3D Human Pose Estimation\n  in Outdoor Scenes",
        "url": "http://arxiv.org/abs/2308.00628v1",
        "pub_date": "2023-08-01",
        "summary": "3D human pose estimation in outdoor environments has garnered increasing\nattention recently. However, prevalent 3D human pose datasets pertaining to\noutdoor scenes lack diversity, as they predominantly utilize only one type of\nmodality (RGB image or pointcloud), and often feature only one individual\nwithin each scene. This limited scope of dataset infrastructure considerably\nhinders the variability of available data. In this article, we propose\nHuman-M3, an outdoor multi-modal multi-view multi-person human pose database\nwhich includes not only multi-view RGB videos of outdoor scenes but also\ncorresponding pointclouds. In order to obtain accurate human poses, we propose\nan algorithm based on multi-modal data input to generate ground truth\nannotation. This benefits from robust pointcloud detection and tracking, which\nsolves the problem of inaccurate human localization and matching ambiguity that\nmay exist in previous multi-view RGB videos in outdoor multi-person scenes, and\ngenerates reliable ground truth annotations. Evaluation of multiple different\nmodalities algorithms has shown that this database is challenging and suitable\nfor future research. Furthermore, we propose a 3D human pose estimation\nalgorithm based on multi-modal data input, which demonstrates the advantages of\nmulti-modal data input for 3D human pose estimation. Code and data will be\nreleased on https://github.com/soullessrobot/Human-M3-Dataset.",
        "translated": ""
    },
    {
        "title": "NeRT: Implicit Neural Representations for General Unsupervised\n  Turbulence Mitigation",
        "url": "http://arxiv.org/abs/2308.00622v1",
        "pub_date": "2023-08-01",
        "summary": "The atmospheric and water turbulence mitigation problems have emerged as\nchallenging inverse problems in computer vision and optics communities over the\nyears. However, current methods either rely heavily on the quality of the\ntraining dataset or fail to generalize over various scenarios, such as static\nscenes, dynamic scenes, and text reconstructions. We propose a general implicit\nneural representation for unsupervised atmospheric and water turbulence\nmitigation (NeRT). NeRT leverages the implicit neural representations and the\nphysically correct tilt-then-blur turbulence model to reconstruct the clean,\nundistorted image, given only dozens of distorted input images. Moreover, we\nshow that NeRT outperforms the state-of-the-art through various qualitative and\nquantitative evaluations of atmospheric and water turbulence datasets.\nFurthermore, we demonstrate the ability of NeRT to eliminate uncontrolled\nturbulence from real-world environments. Lastly, we incorporate NeRT into\ncontinuously captured video sequences and demonstrate $48 \\times$ speedup.",
        "translated": ""
    },
    {
        "title": "Explainable Cost-Sensitive Deep Neural Networks for Brain Tumor\n  Detection from Brain MRI Images considering Data Imbalance",
        "url": "http://arxiv.org/abs/2308.00608v1",
        "pub_date": "2023-08-01",
        "summary": "This paper presents a research study on the use of Convolutional Neural\nNetwork (CNN), ResNet50, InceptionV3, EfficientNetB0 and NASNetMobile models to\nefficiently detect brain tumors in order to reduce the time required for manual\nreview of the report and create an automated system for classifying brain\ntumors. An automated pipeline is proposed, which encompasses five models: CNN,\nResNet50, InceptionV3, EfficientNetB0 and NASNetMobile. The performance of the\nproposed architecture is evaluated on a balanced dataset and found to yield an\naccuracy of 99.33% for fine-tuned InceptionV3 model. Furthermore, Explainable\nAI approaches are incorporated to visualize the model's latent behavior in\norder to understand its black box behavior. To further optimize the training\nprocess, a cost-sensitive neural network approach has been proposed in order to\nwork with imbalanced datasets which has achieved almost 4% more accuracy than\nthe conventional models used in our experiments. The cost-sensitive InceptionV3\n(CS-InceptionV3) and CNN (CS-CNN) show a promising accuracy of 92.31% and a\nrecall value of 1.00 respectively on an imbalanced dataset. The proposed models\nhave shown great potential in improving tumor detection accuracy and must be\nfurther developed for application in practical solutions. We have provided the\ndatasets and made our implementations publicly available at -\nhttps://github.com/shahariar-shibli/Explainable-Cost-Sensitive-Deep-Neural-Networks-for-Brain-Tumor-Detection-from-Brain-MRI-Images",
        "translated": ""
    },
    {
        "title": "Beyond One-Hot-Encoding: Injecting Semantics to Drive Image Classifiers",
        "url": "http://arxiv.org/abs/2308.00607v1",
        "pub_date": "2023-08-01",
        "summary": "Images are loaded with semantic information that pertains to real-world\nontologies: dog breeds share mammalian similarities, food pictures are often\ndepicted in domestic environments, and so on. However, when training machine\nlearning models for image classification, the relative similarities amongst\nobject classes are commonly paired with one-hot-encoded labels. According to\nthis logic, if an image is labelled as 'spoon', then 'tea-spoon' and 'shark'\nare equally wrong in terms of training loss. To overcome this limitation, we\nexplore the integration of additional goals that reflect ontological and\nsemantic knowledge, improving model interpretability and trustworthiness. We\nsuggest a generic approach that allows to derive an additional loss term\nstarting from any kind of semantic information about the classification label.\nFirst, we show how to apply our approach to ontologies and word embeddings, and\ndiscuss how the resulting information can drive a supervised learning process.\nSecond, we use our semantically enriched loss to train image classifiers, and\nanalyse the trade-offs between accuracy, mistake severity, and learned internal\nrepresentations. Finally, we discuss how this approach can be further exploited\nin terms of explainability and adversarial robustness. Code repository:\nhttps://github.com/S1M0N38/semantic-encodings",
        "translated": ""
    },
    {
        "title": "MonoNext: A 3D Monocular Object Detection with ConvNext",
        "url": "http://arxiv.org/abs/2308.00596v1",
        "pub_date": "2023-08-01",
        "summary": "Autonomous driving perception tasks rely heavily on cameras as the primary\nsensor for Object Detection, Semantic Segmentation, Instance Segmentation, and\nObject Tracking. However, RGB images captured by cameras lack depth\ninformation, which poses a significant challenge in 3D detection tasks. To\nsupplement this missing data, mapping sensors such as LIDAR and RADAR are used\nfor accurate 3D Object Detection. Despite their significant accuracy, the\nmulti-sensor models are expensive and require a high computational demand. In\ncontrast, Monocular 3D Object Detection models are becoming increasingly\npopular, offering a faster, cheaper, and easier-to-implement solution for 3D\ndetections. This paper introduces a different Multi-Tasking Learning approach\ncalled MonoNext that utilizes a spatial grid to map objects in the scene.\nMonoNext employs a straightforward approach based on the ConvNext network and\nrequires only 3D bounding box annotated data. In our experiments with the KITTI\ndataset, MonoNext achieved high precision and competitive performance\ncomparable with state-of-the-art approaches. Furthermore, by adding more\ntraining data, MonoNext surpassed itself and achieved higher accuracies.",
        "translated": ""
    },
    {
        "title": "ELIXR: Towards a general purpose X-ray artificial intelligence system\n  through alignment of large language models and radiology vision encoders",
        "url": "http://arxiv.org/abs/2308.01317v1",
        "pub_date": "2023-08-02",
        "summary": "Our approach, which we call Embeddings for Language/Image-aligned X-Rays, or\nELIXR, leverages a language-aligned image encoder combined or grafted onto a\nfixed LLM, PaLM 2, to perform a broad range of tasks. We train this lightweight\nadapter architecture using images paired with corresponding free-text radiology\nreports from the MIMIC-CXR dataset. ELIXR achieved state-of-the-art performance\non zero-shot chest X-ray (CXR) classification (mean AUC of 0.850 across 13\nfindings), data-efficient CXR classification (mean AUCs of 0.893 and 0.898\nacross five findings (atelectasis, cardiomegaly, consolidation, pleural\neffusion, and pulmonary edema) for 1% (~2,200 images) and 10% (~22,000 images)\ntraining data), and semantic search (0.76 normalized discounted cumulative gain\n(NDCG) across nineteen queries, including perfect retrieval on twelve of them).\nCompared to existing data-efficient methods including supervised contrastive\nlearning (SupCon), ELIXR required two orders of magnitude less data to reach\nsimilar performance. ELIXR also showed promise on CXR vision-language tasks,\ndemonstrating overall accuracies of 58.7% and 62.5% on visual question\nanswering and report quality assurance tasks, respectively. These results\nsuggest that ELIXR is a robust and versatile approach to CXR AI.",
        "translated": ""
    },
    {
        "title": "Patched Denoising Diffusion Models For High-Resolution Image Synthesis",
        "url": "http://arxiv.org/abs/2308.01316v1",
        "pub_date": "2023-08-02",
        "summary": "We propose an effective denoising diffusion model for generating\nhigh-resolution images (e.g., 1024$\\times$512), trained on small-size image\npatches (e.g., 64$\\times$64). We name our algorithm Patch-DM, in which a new\nfeature collage strategy is designed to avoid the boundary artifact when\nsynthesizing large-size images. Feature collage systematically crops and\ncombines partial features of the neighboring patches to predict the features of\na shifted image patch, allowing the seamless generation of the entire image due\nto the overlap in the patch feature space. Patch-DM produces high-quality image\nsynthesis results on our newly collected dataset of nature images\n(1024$\\times$512), as well as on standard benchmarks of smaller sizes\n(256$\\times$256), including LSUN-Bedroom, LSUN-Church, and FFHQ. We compare our\nmethod with previous patch-based generation methods and achieve\nstate-of-the-art FID scores on all four datasets. Further, Patch-DM also\nreduces memory complexity compared to the classic diffusion models.",
        "translated": ""
    },
    {
        "title": "More Context, Less Distraction: Visual Classification by Inferring and\n  Conditioning on Contextual Attributes",
        "url": "http://arxiv.org/abs/2308.01313v1",
        "pub_date": "2023-08-02",
        "summary": "CLIP, as a foundational vision language model, is widely used in zero-shot\nimage classification due to its ability to understand various visual concepts\nand natural language descriptions. However, how to fully leverage CLIP's\nunprecedented human-like understanding capabilities to achieve better zero-shot\nclassification is still an open question. This paper draws inspiration from the\nhuman visual perception process: a modern neuroscience view suggests that in\nclassifying an object, humans first infer its class-independent attributes\n(e.g., background and orientation) which help separate the foreground object\nfrom the background, and then make decisions based on this information.\nInspired by this, we observe that providing CLIP with contextual attributes\nimproves zero-shot classification and mitigates reliance on spurious features.\nWe also observe that CLIP itself can reasonably infer the attributes from an\nimage. With these observations, we propose a training-free, two-step zero-shot\nclassification method named PerceptionCLIP. Given an image, it first infers\ncontextual attributes (e.g., background) and then performs object\nclassification conditioning on them. Our experiments show that PerceptionCLIP\nachieves better generalization, group robustness, and better interpretability.\nFor example, PerceptionCLIP with ViT-L/14 improves the worst group accuracy by\n16.5% on the Waterbirds dataset and by 3.5% on CelebA.",
        "translated": ""
    },
    {
        "title": "Revisiting DETR Pre-training for Object Detection",
        "url": "http://arxiv.org/abs/2308.01300v1",
        "pub_date": "2023-08-02",
        "summary": "Motivated by that DETR-based approaches have established new records on COCO\ndetection and segmentation benchmarks, many recent endeavors show increasing\ninterest in how to further improve DETR-based approaches by pre-training the\nTransformer in a self-supervised manner while keeping the backbone frozen. Some\nstudies already claimed significant improvements in accuracy. In this paper, we\ntake a closer look at their experimental methodology and check if their\napproaches are still effective on the very recent state-of-the-art such as\n$\\mathcal{H}$-Deformable-DETR. We conduct thorough experiments on COCO object\ndetection tasks to study the influence of the choice of pre-training datasets,\nlocalization, and classification target generation schemes. Unfortunately, we\nfind the previous representative self-supervised approach such as DETReg, fails\nto boost the performance of the strong DETR-based approaches on full data\nregimes. We further analyze the reasons and find that simply combining a more\naccurate box predictor and Objects$365$ benchmark can significantly improve the\nresults in follow-up experiments. We demonstrate the effectiveness of our\napproach by achieving strong object detection results of AP=$59.3\\%$ on COCO\nval set, which surpasses $\\mathcal{H}$-Deformable-DETR + Swin-L by +$1.4\\%$.\nLast, we generate a series of synthetic pre-training datasets by combining the\nvery recent image-to-text captioning models (LLaVA) and text-to-image\ngenerative models (SDXL). Notably, pre-training on these synthetic datasets\nleads to notable improvements in object detection performance. Looking ahead,\nwe anticipate substantial advantages through the future expansion of the\nsynthetic pre-training dataset.",
        "translated": ""
    },
    {
        "title": "Incorporating Season and Solar Specificity into Renderings made by a\n  NeRF Architecture using Satellite Images",
        "url": "http://arxiv.org/abs/2308.01262v1",
        "pub_date": "2023-08-02",
        "summary": "As a result of Shadow NeRF and Sat-NeRF, it is possible to take the solar\nangle into account in a NeRF-based framework for rendering a scene from a novel\nviewpoint using satellite images for training. Our work extends those\ncontributions and shows how one can make the renderings season-specific. Our\nmain challenge was creating a Neural Radiance Field (NeRF) that could render\nseasonal features independently of viewing angle and solar angle while still\nbeing able to render shadows. We teach our network to render seasonal features\nby introducing one more input variable -- time of the year. However, the small\ntraining datasets typical of satellite imagery can introduce ambiguities in\ncases where shadows are present in the same location for every image of a\nparticular season. We add additional terms to the loss function to discourage\nthe network from using seasonal features for accounting for shadows. We show\nthe performance of our network on eight Areas of Interest containing images\ncaptured by the Maxar WorldView-3 satellite. This evaluation includes tests\nmeasuring the ability of our framework to accurately render novel views,\ngenerate height maps, predict shadows, and specify seasonal features\nindependently from shadows. Our ablation studies justify the choices made for\nnetwork design parameters.",
        "translated": ""
    },
    {
        "title": "Learning Spatial Distribution of Long-Term Trackers Scores",
        "url": "http://arxiv.org/abs/2308.01256v1",
        "pub_date": "2023-08-02",
        "summary": "Long-Term tracking is a hot topic in Computer Vision. In this context,\ncompetitive models are presented every year, showing a constant growth rate in\nperformances, mainly measured in standardized protocols as Visual Object\nTracking (VOT) and Object Tracking Benchmark (OTB). Fusion-trackers strategy\nhas been applied over last few years for overcoming the known re-detection\nproblem, turning out to be an important breakthrough. Following this approach,\nthis work aims to generalize the fusion concept to an arbitrary number of\ntrackers used as baseline trackers in the pipeline, leveraging a learning phase\nto better understand how outcomes correlate with each other, even when no\ntarget is present. A model and data independence conjecture will be evidenced\nin the manuscript, yielding a recall of 0.738 on LTB-50 dataset when learning\nfrom VOT-LT2022, and 0.619 by reversing the two datasets. In both cases,\nresults are strongly competitive with state-of-the-art and recall turns out to\nbe the first on the podium.",
        "translated": ""
    },
    {
        "title": "A Hyper-pixel-wise Contrastive Learning Augmented Segmentation Network\n  for Old Landslide Detection Using High-Resolution Remote Sensing Images and\n  Digital Elevation Model Data",
        "url": "http://arxiv.org/abs/2308.01251v1",
        "pub_date": "2023-08-02",
        "summary": "As a harzard disaster, landslide often brings tremendous losses to humanity,\nso it's necessary to achieve reliable detection of landslide. However, the\nproblems of visual blur and small-sized dataset cause great challenges for old\nlandslide detection task when using remote sensing data. To reliably extract\nsemantic features, a hyper-pixel-wise contrastive learning augmented\nsegmentation network (HPCL-Net) is proposed, which augments the local salient\nfeature extraction from the boundaries of landslides through HPCL and fuses the\nheterogeneous infromation in the semantic space from High-Resolution Remote\nSensing Images and Digital Elevation Model Data data. For full utilization of\nthe precious samples, a global hyper-pixel-wise sample pair queues-based\ncontrastive learning method, which includes the construction of global queues\nthat store hyper-pixel-wise samples and the updating scheme of a momentum\nencoder, is developed, reliably enhancing the extraction ability of semantic\nfeatures. The proposed HPCL-Net is evaluated on a Loess Plateau old landslide\ndataset and experiment results show that the model greatly improves the\nreliablity of old landslide detection compared to the previous old landslide\nsegmentation model, where mIoU metric is increased from 0.620 to 0.651,\nLandslide IoU metric is increased from 0.334 to 0.394 and F1-score metric is\nincreased from 0.501 to 0.565.",
        "translated": ""
    },
    {
        "title": "A Hybrid Approach To Real-Time Multi-Object Tracking",
        "url": "http://arxiv.org/abs/2308.01248v1",
        "pub_date": "2023-08-02",
        "summary": "Multi-Object Tracking, also known as Multi-Target Tracking, is a significant\narea of computer vision that has many uses in a variety of settings. The\ndevelopment of deep learning, which has encouraged researchers to propose more\nand more work in this direction, has significantly impacted the scientific\nadvancement around the study of tracking as well as many other domains related\nto computer vision. In fact, all of the solutions that are currently\nstate-of-the-art in the literature and in the tracking industry, are built on\ntop of deep learning methodologies that produce exceptionally good results.\nDeep learning is enabled thanks to the ever more powerful technology\nresearchers can use to handle the significant computational resources demanded\nby these models. However, when real-time is a main requirement, developing a\ntracking system without being constrained by expensive hardware support with\nenormous computational resources is necessary to widen tracking applications in\nreal-world contexts. To this end, a compromise is to combine powerful deep\nstrategies with more traditional approaches to favor considerably lower\nprocessing solutions at the cost of less accurate tracking results even though\nsuitable for real-time domains. Indeed, the present work goes in that\ndirection, proposing a hybrid strategy for real-time multi-target tracking that\ncombines effectively a classical optical flow algorithm with a deep learning\narchitecture, targeted to a human-crowd tracking system exhibiting a desirable\ntrade-off between performance in tracking precision and computational costs.\nThe developed architecture was experimented with different settings, and\nyielded a MOTA of 0.608 out of the compared state-of-the-art 0.549 results, and\nabout half the running time when introducing the optical flow phase, achieving\nalmost the same performance in terms of accuracy.",
        "translated": ""
    },
    {
        "title": "Tirtha -- An Automated Platform to Crowdsource Images and Create 3D\n  Models of Heritage Sites",
        "url": "http://arxiv.org/abs/2308.01246v1",
        "pub_date": "2023-08-02",
        "summary": "Digital preservation of Cultural Heritage (CH) sites is crucial to protect\nthem against damage from natural disasters or human activities. Creating 3D\nmodels of CH sites has become a popular method of digital preservation thanks\nto advancements in computer vision and photogrammetry. However, the process is\ntime-consuming, expensive, and typically requires specialized equipment and\nexpertise, posing challenges in resource-limited developing countries.\nAdditionally, the lack of an open repository for 3D models hinders research and\npublic engagement with their heritage. To address these issues, we propose\nTirtha, a web platform for crowdsourcing images of CH sites and creating their\n3D models. Tirtha utilizes state-of-the-art Structure from Motion (SfM) and\nMulti-View Stereo (MVS) techniques. It is modular, extensible and\ncost-effective, allowing for the incorporation of new techniques as\nphotogrammetry advances. Tirtha is accessible through a web interface at\nhttps://tirtha.niser.ac.in and can be deployed on-premise or in a cloud\nenvironment. In our case studies, we demonstrate the pipeline's effectiveness\nby creating 3D models of temples in Odisha, India, using crowdsourced images.\nThese models are available for viewing, interaction, and download on the Tirtha\nwebsite. Our work aims to provide a dataset of crowdsourced images and 3D\nreconstructions for research in computer vision, heritage conservation, and\nrelated domains. Overall, Tirtha is a step towards democratizing digital\npreservation, primarily in resource-limited developing countries.",
        "translated": ""
    },
    {
        "title": "CMUNeXt: An Efficient Medical Image Segmentation Network based on Large\n  Kernel and Skip Fusion",
        "url": "http://arxiv.org/abs/2308.01239v2",
        "pub_date": "2023-08-02",
        "summary": "The U-shaped architecture has emerged as a crucial paradigm in the design of\nmedical image segmentation networks. However, due to the inherent local\nlimitations of convolution, a fully convolutional segmentation network with\nU-shaped architecture struggles to effectively extract global context\ninformation, which is vital for the precise localization of lesions. While\nhybrid architectures combining CNNs and Transformers can address these issues,\ntheir application in real medical scenarios is limited due to the computational\nresource constraints imposed by the environment and edge devices. In addition,\nthe convolutional inductive bias in lightweight networks adeptly fits the\nscarce medical data, which is lacking in the Transformer based network. In\norder to extract global context information while taking advantage of the\ninductive bias, we propose CMUNeXt, an efficient fully convolutional\nlightweight medical image segmentation network, which enables fast and accurate\nauxiliary diagnosis in real scene scenarios. CMUNeXt leverages large kernel and\ninverted bottleneck design to thoroughly mix distant spatial and location\ninformation, efficiently extracting global context information. We also\nintroduce the Skip-Fusion block, designed to enable smooth skip-connections and\nensure ample feature fusion. Experimental results on multiple medical image\ndatasets demonstrate that CMUNeXt outperforms existing heavyweight and\nlightweight medical image segmentation networks in terms of segmentation\nperformance, while offering a faster inference speed, lighter weights, and a\nreduced computational cost. The code is available at\nhttps://github.com/FengheTan9/CMUNeXt.",
        "translated": ""
    },
    {
        "title": "The All-Seeing Project: Towards Panoptic Visual Recognition and\n  Understanding of the Open World",
        "url": "http://arxiv.org/abs/2308.01907v1",
        "pub_date": "2023-08-03",
        "summary": "We present the All-Seeing (AS) project: a large-scale data and model for\nrecognizing and understanding everything in the open world. Using a scalable\ndata engine that incorporates human feedback and efficient models in the loop,\nwe create a new dataset (AS-1B) with over 1 billion regions annotated with\nsemantic tags, question-answering pairs, and detailed captions. It covers a\nwide range of 3.5 million common and rare concepts in the real world, and has\n132.2 billion tokens that describe the concepts and their attributes.\nLeveraging this new dataset, we develop the All-Seeing model (ASM), a unified\nframework for panoptic visual recognition and understanding. The model is\ntrained with open-ended language prompts and locations, which allows it to\ngeneralize to various vision and language tasks with remarkable zero-shot\nperformance, including region-text retrieval, region recognition, captioning,\nand question-answering. We hope that this project can serve as a foundation for\nvision-language artificial general intelligence research. Models and the\ndataset shall be released at https://github.com/OpenGVLab/All-Seeing, and demo\ncan be seen at https://huggingface.co/spaces/OpenGVLab/all-seeing.",
        "translated": ""
    },
    {
        "title": "Revisiting Deformable Convolution for Depth Completion",
        "url": "http://arxiv.org/abs/2308.01905v1",
        "pub_date": "2023-08-03",
        "summary": "Depth completion, which aims to generate high-quality dense depth maps from\nsparse depth maps, has attracted increasing attention in recent years. Previous\nwork usually employs RGB images as guidance, and introduces iterative spatial\npropagation to refine estimated coarse depth maps. However, most of the\npropagation refinement methods require several iterations and suffer from a\nfixed receptive field, which may contain irrelevant and useless information\nwith very sparse input. In this paper, we address these two challenges\nsimultaneously by revisiting the idea of deformable convolution. We propose an\neffective architecture that leverages deformable kernel convolution as a\nsingle-pass refinement module, and empirically demonstrate its superiority. To\nbetter understand the function of deformable convolution and exploit it for\ndepth completion, we further systematically investigate a variety of\nrepresentative strategies. Our study reveals that, different from prior work,\ndeformable convolution needs to be applied on an estimated depth map with a\nrelatively high density for better performance. We evaluate our model on the\nlarge-scale KITTI dataset and achieve state-of-the-art level performance in\nboth accuracy and inference speed. Our code is available at\nhttps://github.com/AlexSunNik/ReDC.",
        "translated": ""
    },
    {
        "title": "DETR Doesn't Need Multi-Scale or Locality Design",
        "url": "http://arxiv.org/abs/2308.01904v1",
        "pub_date": "2023-08-03",
        "summary": "This paper presents an improved DETR detector that maintains a \"plain\"\nnature: using a single-scale feature map and global cross-attention\ncalculations without specific locality constraints, in contrast to previous\nleading DETR-based detectors that reintroduce architectural inductive biases of\nmulti-scale and locality into the decoder. We show that two simple technologies\nare surprisingly effective within a plain design to compensate for the lack of\nmulti-scale feature maps and locality constraints. The first is a box-to-pixel\nrelative position bias (BoxRPB) term added to the cross-attention formulation,\nwhich well guides each query to attend to the corresponding object region while\nalso providing encoding flexibility. The second is masked image modeling\n(MIM)-based backbone pre-training which helps learn representation with\nfine-grained localization ability and proves crucial for remedying dependencies\non the multi-scale feature maps. By incorporating these technologies and recent\nadvancements in training and problem formation, the improved \"plain\" DETR\nshowed exceptional improvements over the original DETR detector. By leveraging\nthe Object365 dataset for pre-training, it achieved 63.9 mAP accuracy using a\nSwin-L backbone, which is highly competitive with state-of-the-art detectors\nwhich all heavily rely on multi-scale feature maps and region-based feature\nextraction. Code is available at https://github.com/impiga/Plain-DETR .",
        "translated": ""
    },
    {
        "title": "UniSim: A Neural Closed-Loop Sensor Simulator",
        "url": "http://arxiv.org/abs/2308.01898v1",
        "pub_date": "2023-08-03",
        "summary": "Rigorously testing autonomy systems is essential for making safe self-driving\nvehicles (SDV) a reality. It requires one to generate safety critical scenarios\nbeyond what can be collected safely in the world, as many scenarios happen\nrarely on public roads. To accurately evaluate performance, we need to test the\nSDV on these scenarios in closed-loop, where the SDV and other actors interact\nwith each other at each timestep. Previously recorded driving logs provide a\nrich resource to build these new scenarios from, but for closed loop\nevaluation, we need to modify the sensor data based on the new scene\nconfiguration and the SDV's decisions, as actors might be added or removed and\nthe trajectories of existing actors and the SDV will differ from the original\nlog. In this paper, we present UniSim, a neural sensor simulator that takes a\nsingle recorded log captured by a sensor-equipped vehicle and converts it into\na realistic closed-loop multi-sensor simulation. UniSim builds neural feature\ngrids to reconstruct both the static background and dynamic actors in the\nscene, and composites them together to simulate LiDAR and camera data at new\nviewpoints, with actors added or removed and at new placements. To better\nhandle extrapolated views, we incorporate learnable priors for dynamic objects,\nand leverage a convolutional network to complete unseen regions. Our\nexperiments show UniSim can simulate realistic sensor data with small domain\ngap on downstream tasks. With UniSim, we demonstrate closed-loop evaluation of\nan autonomy system on safety-critical scenarios as if it were in the real\nworld.",
        "translated": ""
    },
    {
        "title": "DualCoOp++: Fast and Effective Adaptation to Multi-Label Recognition\n  with Limited Annotations",
        "url": "http://arxiv.org/abs/2308.01890v1",
        "pub_date": "2023-08-03",
        "summary": "Multi-label image recognition in the low-label regime is a task of great\nchallenge and practical significance. Previous works have focused on learning\nthe alignment between textual and visual spaces to compensate for limited image\nlabels, yet may suffer from reduced accuracy due to the scarcity of\nhigh-quality multi-label annotations. In this research, we leverage the\npowerful alignment between textual and visual features pretrained with millions\nof auxiliary image-text pairs. We introduce an efficient and effective\nframework called Evidence-guided Dual Context Optimization (DualCoOp++), which\nserves as a unified approach for addressing partial-label and zero-shot\nmulti-label recognition. In DualCoOp++ we separately encode evidential,\npositive, and negative contexts for target classes as parametric components of\nthe linguistic input (i.e., prompts). The evidential context aims to discover\nall the related visual content for the target class, and serves as guidance to\naggregate positive and negative contexts from the spatial domain of the image,\nenabling better distinguishment between similar categories. Additionally, we\nintroduce a Winner-Take-All module that promotes inter-class interaction during\ntraining, while avoiding the need for extra parameters and costs. As DualCoOp++\nimposes minimal additional learnable overhead on the pretrained vision-language\nframework, it enables rapid adaptation to multi-label recognition tasks with\nlimited annotations and even unseen classes. Experiments on standard\nmulti-label recognition benchmarks across two challenging low-label settings\ndemonstrate the superior performance of our approach compared to\nstate-of-the-art methods.",
        "translated": ""
    },
    {
        "title": "FROD: Robust Object Detection for Free",
        "url": "http://arxiv.org/abs/2308.01888v1",
        "pub_date": "2023-08-03",
        "summary": "Object detection is a vital task in computer vision and has become an\nintegral component of numerous critical systems. However, state-of-the-art\nobject detectors, similar to their classification counterparts, are susceptible\nto small adversarial perturbations that can significantly alter their normal\nbehavior. Unlike classification, the robustness of object detectors has not\nbeen thoroughly explored. In this work, we take the initial step towards\nbridging the gap between the robustness of classification and object detection\nby leveraging adversarially trained classification models. Merely utilizing\nadversarially trained models as backbones for object detection does not result\nin robustness. We propose effective modifications to the classification-based\nbackbone to instill robustness in object detection without incurring any\ncomputational overhead. To further enhance the robustness achieved by the\nproposed modified backbone, we introduce two lightweight components: imitation\nloss and delayed adversarial training. Extensive experiments on the MS-COCO and\nPascal VOC datasets are conducted to demonstrate the effectiveness of our\nproposed approach.",
        "translated": ""
    },
    {
        "title": "Reconstructing Three-Dimensional Models of Interacting Humans",
        "url": "http://arxiv.org/abs/2308.01854v1",
        "pub_date": "2023-08-03",
        "summary": "Understanding 3d human interactions is fundamental for fine-grained scene\nanalysis and behavioural modeling. However, most of the existing models predict\nincorrect, lifeless 3d estimates, that miss the subtle human contact\naspects--the essence of the event--and are of little use for detailed\nbehavioral understanding. This paper addresses such issues with several\ncontributions: (1) we introduce models for interaction signature estimation\n(ISP) encompassing contact detection, segmentation, and 3d contact signature\nprediction; (2) we show how such components can be leveraged to ensure contact\nconsistency during 3d reconstruction; (3) we construct several large datasets\nfor learning and evaluating 3d contact prediction and reconstruction methods;\nspecifically, we introduce CHI3D, a lab-based accurate 3d motion capture\ndataset with 631 sequences containing $2,525$ contact events, $728,664$ ground\ntruth 3d poses, as well as FlickrCI3D, a dataset of $11,216$ images, with\n$14,081$ processed pairs of people, and $81,233$ facet-level surface\ncorrespondences. Finally, (4) we propose methodology for recovering the\nground-truth pose and shape of interacting people in a controlled setup and (5)\nannotate all 3d interaction motions in CHI3D with textual descriptions. Motion\ndata in multiple formats (GHUM and SMPLX parameters, Human3.6m 3d joints) is\nmade available for research purposes at \\url{https://ci3d.imar.ro}, together\nwith an evaluation server and a public benchmark.",
        "translated": ""
    },
    {
        "title": "Synthesizing Long-Term Human Motions with Diffusion Models via Coherent\n  Sampling",
        "url": "http://arxiv.org/abs/2308.01850v1",
        "pub_date": "2023-08-03",
        "summary": "Text-to-motion generation has gained increasing attention, but most existing\nmethods are limited to generating short-term motions that correspond to a\nsingle sentence describing a single action. However, when a text stream\ndescribes a sequence of continuous motions, the generated motions corresponding\nto each sentence may not be coherently linked. Existing long-term motion\ngeneration methods face two main issues. Firstly, they cannot directly generate\ncoherent motions and require additional operations such as interpolation to\nprocess the generated actions. Secondly, they generate subsequent actions in an\nautoregressive manner without considering the influence of future actions on\nprevious ones. To address these issues, we propose a novel approach that\nutilizes a past-conditioned diffusion model with two optional coherent sampling\nmethods: Past Inpainting Sampling and Compositional Transition Sampling. Past\nInpainting Sampling completes subsequent motions by treating previous motions\nas conditions, while Compositional Transition Sampling models the distribution\nof the transition as the composition of two adjacent motions guided by\ndifferent text prompts. Our experimental results demonstrate that our proposed\nmethod is capable of generating compositional and coherent long-term 3D human\nmotions controlled by a user-instructed long text stream. The code is available\nat\n\\href{https://github.com/yangzhao1230/PCMDM}{https://github.com/yangzhao1230/PCMDM}.",
        "translated": ""
    },
    {
        "title": "Is your data alignable? Principled and interpretable alignability\n  testing and integration of single-cell data",
        "url": "http://arxiv.org/abs/2308.01839v1",
        "pub_date": "2023-08-03",
        "summary": "Single-cell data integration can provide a comprehensive molecular view of\ncells, and many algorithms have been developed to remove unwanted technical or\nbiological variations and integrate heterogeneous single-cell datasets. Despite\ntheir wide usage, existing methods suffer from several fundamental limitations.\nIn particular, we lack a rigorous statistical test for whether two\nhigh-dimensional single-cell datasets are alignable (and therefore should even\nbe aligned). Moreover, popular methods can substantially distort the data\nduring alignment, making the aligned data and downstream analysis difficult to\ninterpret. To overcome these limitations, we present a spectral manifold\nalignment and inference (SMAI) framework, which enables principled and\ninterpretable alignability testing and structure-preserving integration of\nsingle-cell data. SMAI provides a statistical test to robustly determine the\nalignability between datasets to avoid misleading inference, and is justified\nby high-dimensional statistical theory. On a diverse range of real and\nsimulated benchmark datasets, it outperforms commonly used alignment methods.\nMoreover, we show that SMAI improves various downstream analyses such as\nidentification of differentially expressed genes and imputation of single-cell\nspatial transcriptomics, providing further biological insights. SMAI's\ninterpretability also enables quantification and a deeper understanding of the\nsources of technical confounders in single-cell data.",
        "translated": ""
    },
    {
        "title": "Deep Neural Networks Fused with Textures for Image Classification",
        "url": "http://arxiv.org/abs/2308.01813v1",
        "pub_date": "2023-08-03",
        "summary": "Fine-grained image classification (FGIC) is a challenging task in computer\nvision for due to small visual differences among inter-subcategories, but,\nlarge intra-class variations. Deep learning methods have achieved remarkable\nsuccess in solving FGIC. In this paper, we propose a fusion approach to address\nFGIC by combining global texture with local patch-based information. The first\npipeline extracts deep features from various fixed-size non-overlapping patches\nand encodes features by sequential modelling using the long short-term memory\n(LSTM). Another path computes image-level textures at multiple scales using the\nlocal binary patterns (LBP). The advantages of both streams are integrated to\nrepresent an efficient feature vector for image classification. The method is\ntested on eight datasets representing the human faces, skin lesions, food\ndishes, marine lives, etc. using four standard backbone CNNs. Our method has\nattained better classification accuracy over existing methods with notable\nmargins.",
        "translated": ""
    },
    {
        "title": "MM-Vet: Evaluating Large Multimodal Models for Integrated Capabilities",
        "url": "http://arxiv.org/abs/2308.02490v1",
        "pub_date": "2023-08-04",
        "summary": "We propose MM-Vet, an evaluation benchmark that examines large multimodal\nmodels (LMMs) on complicated multimodal tasks. Recent LMMs have shown various\nintriguing abilities, such as solving math problems written on the blackboard,\nreasoning about events and celebrities in news images, and explaining visual\njokes. Rapid model advancements pose challenges to evaluation benchmark\ndevelopment. Problems include: (1) How to systematically structure and evaluate\nthe complicated multimodal tasks; (2) How to design evaluation metrics that\nwork well across question and answer types; and (3) How to give model insights\nbeyond a simple performance ranking. To this end, we present MM-Vet, designed\nbased on the insight that the intriguing ability to solve complicated tasks is\noften achieved by a generalist model being able to integrate different core\nvision-language (VL) capabilities. MM-Vet defines 6 core VL capabilities and\nexamines the 16 integrations of interest derived from the capability\ncombination. For evaluation metrics, we propose an LLM-based evaluator for\nopen-ended outputs. The evaluator enables the evaluation across different\nquestion types and answer styles, resulting in a unified scoring metric. We\nevaluate representative LMMs on MM-Vet, providing insights into the\ncapabilities of different LMM system paradigms and models. Code and data are\navailable at https://github.com/yuweihao/MM-Vet.",
        "translated": ""
    },
    {
        "title": "Convolutions Die Hard: Open-Vocabulary Segmentation with Single Frozen\n  Convolutional CLIP",
        "url": "http://arxiv.org/abs/2308.02487v1",
        "pub_date": "2023-08-04",
        "summary": "Open-vocabulary segmentation is a challenging task requiring segmenting and\nrecognizing objects from an open set of categories. One way to address this\nchallenge is to leverage multi-modal models, such as CLIP, to provide image and\ntext features in a shared embedding space, which bridges the gap between\nclosed-vocabulary and open-vocabulary recognition. Hence, existing methods\noften adopt a two-stage framework to tackle the problem, where the inputs first\ngo through a mask generator and then through the CLIP model along with the\npredicted masks. This process involves extracting features from images multiple\ntimes, which can be ineffective and inefficient. By contrast, we propose to\nbuild everything into a single-stage framework using a shared Frozen\nConvolutional CLIP backbone, which not only significantly simplifies the\ncurrent two-stage pipeline, but also remarkably yields a better accuracy-cost\ntrade-off. The proposed FC-CLIP, benefits from the following observations: the\nfrozen CLIP backbone maintains the ability of open-vocabulary classification\nand can also serve as a strong mask generator, and the convolutional CLIP\ngeneralizes well to a larger input resolution than the one used during\ncontrastive image-text pretraining. When training on COCO panoptic data only\nand testing in a zero-shot manner, FC-CLIP achieve 26.8 PQ, 16.8 AP, and 34.1\nmIoU on ADE20K, 18.2 PQ, 27.9 mIoU on Mapillary Vistas, 44.0 PQ, 26.8 AP, 56.2\nmIoU on Cityscapes, outperforming the prior art by +4.2 PQ, +2.4 AP, +4.2 mIoU\non ADE20K, +4.0 PQ on Mapillary Vistas and +20.1 PQ on Cityscapes,\nrespectively. Additionally, the training and testing time of FC-CLIP is 7.5x\nand 6.6x significantly faster than the same prior art, while using 5.9x fewer\nparameters. FC-CLIP also sets a new state-of-the-art performance across various\nopen-vocabulary semantic segmentation datasets. Code at\nhttps://github.com/bytedance/fc-clip",
        "translated": ""
    },
    {
        "title": "Towards Generalist Foundation Model for Radiology",
        "url": "http://arxiv.org/abs/2308.02463v1",
        "pub_date": "2023-08-04",
        "summary": "In this study, we aim to initiate the development of Radiology Foundation\nModel, termed as RadFM.We consider the construction of foundational models from\nthe perspectives of data, model design, and evaluation thoroughly. Our\ncontribution can be concluded as follows: (i), we construct a large-scale\nMedical Multi-modal Dataset, MedMD, consisting of 16M 2D and 3D medical scans.\nTo the best of our knowledge, this is the first multi-modal dataset containing\n3D medical scans. (ii), We propose an architecture that enables visually\nconditioned generative pre-training, allowing for the integration of text input\ninterleaved with 2D or 3D medical scans to generate response for diverse\nradiologic tasks. The model was initially pre-trained on MedMD and subsequently\ndomain-specific fine-tuned on RadMD, a radiologic cleaned version of MedMD,\ncontaining 3M radiologic visual-language pairs. (iii), we propose a new\nevaluation benchmark that comprises five tasks, aiming to comprehensively\nassess the capability of foundation models in handling practical clinical\nproblems. Our experimental results confirm that RadFM significantly outperforms\nexisting multi-modal foundation models. The codes, data, and model checkpoint\nwill all be made publicly available to promote further research and development\nin the field.",
        "translated": ""
    },
    {
        "title": "A Bi-variant Variational Model for Diffeomorphic Image Registration with\n  Relaxed Jacobian Determinant Constraints",
        "url": "http://arxiv.org/abs/2308.02393v1",
        "pub_date": "2023-08-04",
        "summary": "Diffeomorphic registration has become a powerful approach for seeking a\nsmooth and invertible spatial transformation between two coordinate systems\nwhich have been measured via the template and reference images. While the\npointwise volume-preserving constraint is effective for some problems, it is\ntoo stringent for many other problems especially when the local deformations\nare relatively large, because it may lead to a poor large-deformation for\nenforcing local matching.In this paper, we propose a novel bi-variant\ndiffeomorphic image registration model with the soft constraint of Jacobian\nequation, which allows local deformations to shrink and grow in a flexible\nrange.The Jacobian determinant of the transformation is explicitly controlled\nby optimizing the relaxation function. To prevent deformation folding and\nenhance the smoothness of deformation, we not only impose a positivity\nconstraint in optimizing the relaxation function, but also employ a regularizer\nto ensure the smoothness of the relaxation function.Furthermore, the positivity\nconstraint ensures that is as close to one as possible, which helps to obtain a\nvolume-preserving transformation on average.We further analyze the existence of\nthe minimizer for the variational model and propose a penalty splitting method\nwith a multilevel strategy to solve this model. Numerical experiments show that\nthe proposed algorithm is convergent, and the positivity constraint can control\nthe range of relative volume and not compromise registration accuracy.\nMoreover, the proposed model produces diffeomorphic maps for large deformation,\nand achieves better performance compared to the several existing registration\nmodels.",
        "translated": ""
    },
    {
        "title": "Universal Defensive Underpainting Patch: Making Your Text Invisible to\n  Optical Character Recognition",
        "url": "http://arxiv.org/abs/2308.02369v1",
        "pub_date": "2023-08-04",
        "summary": "Optical Character Recognition (OCR) enables automatic text extraction from\nscanned or digitized text images, but it also makes it easy to pirate valuable\nor sensitive text from these images. Previous methods to prevent OCR piracy by\ndistorting characters in text images are impractical in real-world scenarios,\nas pirates can capture arbitrary portions of the text images, rendering the\ndefenses ineffective. In this work, we propose a novel and effective defense\nmechanism termed the Universal Defensive Underpainting Patch (UDUP) that\nmodifies the underpainting of text images instead of the characters. UDUP is\ncreated through an iterative optimization process to craft a small, fixed-size\ndefensive patch that can generate non-overlapping underpainting for text images\nof any size. Experimental results show that UDUP effectively defends against\nunauthorized OCR under the setting of any screenshot range or complex image\nbackground. It is agnostic to the content, size, colors, and languages of\ncharacters, and is robust to typical image operations such as scaling and\ncompressing. In addition, the transferability of UDUP is demonstrated by\nevading several off-the-shelf OCRs. The code is available at\nhttps://github.com/QRICKDD/UDUP.",
        "translated": ""
    },
    {
        "title": "Brain MRI Segmentation using Template-Based Training and Visual\n  Perception Augmentation",
        "url": "http://arxiv.org/abs/2308.02363v1",
        "pub_date": "2023-08-04",
        "summary": "Deep learning models usually require sufficient training data to achieve high\naccuracy, but obtaining labeled data can be time-consuming and labor-intensive.\nHere we introduce a template-based training method to train a 3D U-Net model\nfrom scratch using only one population-averaged brain MRI template and its\nassociated segmentation label. The process incorporated visual perception\naugmentation to enhance the model's robustness in handling diverse image inputs\nand mitigating overfitting. Leveraging this approach, we trained 3D U-Net\nmodels for mouse, rat, marmoset, rhesus, and human brain MRI to achieve\nsegmentation tasks such as skull-stripping, brain segmentation, and tissue\nprobability mapping. This tool effectively addresses the limited availability\nof training data and holds significant potential for expanding deep learning\napplications in image analysis, providing researchers with a unified solution\nto train deep neural networks with only one image sample.",
        "translated": ""
    },
    {
        "title": "T-UNet: Triplet UNet for Change Detection in High-Resolution Remote\n  Sensing Images",
        "url": "http://arxiv.org/abs/2308.02356v1",
        "pub_date": "2023-08-04",
        "summary": "Remote sensing image change detection aims to identify the differences\nbetween images acquired at different times in the same area. It is widely used\nin land management, environmental monitoring, disaster assessment and other\nfields. Currently, most change detection methods are based on Siamese network\nstructure or early fusion structure. Siamese structure focuses on extracting\nobject features at different times but lacks attention to change information,\nwhich leads to false alarms and missed detections. Early fusion (EF) structure\nfocuses on extracting features after the fusion of images of different phases\nbut ignores the significance of object features at different times for\ndetecting change details, making it difficult to accurately discern the edges\nof changed objects. To address these issues and obtain more accurate results,\nwe propose a novel network, Triplet UNet(T-UNet), based on a three-branch\nencoder, which is capable to simultaneously extract the object features and the\nchange features between the pre- and post-time-phase images through triplet\nencoder. To effectively interact and fuse the features extracted from the three\nbranches of triplet encoder, we propose a multi-branch spatial-spectral\ncross-attention module (MBSSCA). In the decoder stage, we introduce the channel\nattention mechanism (CAM) and spatial attention mechanism (SAM) to fully mine\nand integrate detailed textures information at the shallow layer and semantic\nlocalization information at the deep layer.",
        "translated": ""
    },
    {
        "title": "A Parameter-efficient Multi-subject Model for Predicting fMRI Activity",
        "url": "http://arxiv.org/abs/2308.02351v1",
        "pub_date": "2023-08-04",
        "summary": "This is the Algonauts 2023 submission report for team \"BlobGPT\". Our model\nconsists of a multi-subject linear encoding head attached to a pretrained trunk\nmodel. The multi-subject head consists of three components: (1) a shared\nmulti-layer feature projection, (2) shared plus subject-specific low-dimension\nlinear transformations, and (3) a shared PCA fMRI embedding. In this report, we\nexplain these components in more detail and present some experimental results.\nOur code is available at https://github.com/cmi-dair/algonauts23.",
        "translated": ""
    },
    {
        "title": "RobustMQ: Benchmarking Robustness of Quantized Models",
        "url": "http://arxiv.org/abs/2308.02350v1",
        "pub_date": "2023-08-04",
        "summary": "Quantization has emerged as an essential technique for deploying deep neural\nnetworks (DNNs) on devices with limited resources. However, quantized models\nexhibit vulnerabilities when exposed to various noises in real-world\napplications. Despite the importance of evaluating the impact of quantization\non robustness, existing research on this topic is limited and often disregards\nestablished principles of robustness evaluation, resulting in incomplete and\ninconclusive findings. To address this gap, we thoroughly evaluated the\nrobustness of quantized models against various noises (adversarial attacks,\nnatural corruptions, and systematic noises) on ImageNet. The comprehensive\nevaluation results empirically provide valuable insights into the robustness of\nquantized models in various scenarios, for example: (1) quantized models\nexhibit higher adversarial robustness than their floating-point counterparts,\nbut are more vulnerable to natural corruptions and systematic noises; (2) in\ngeneral, increasing the quantization bit-width results in a decrease in\nadversarial robustness, an increase in natural robustness, and an increase in\nsystematic robustness; (3) among corruption methods, \\textit{impulse noise} and\n\\textit{glass blur} are the most harmful to quantized models, while\n\\textit{brightness} has the least impact; (4) among systematic noises, the\n\\textit{nearest neighbor interpolation} has the highest impact, while bilinear\ninterpolation, cubic interpolation, and area interpolation are the three least\nharmful. Our research contributes to advancing the robust quantization of\nmodels and their deployment in real-world scenarios.",
        "translated": ""
    },
    {
        "title": "Class Incremental Learning with Self-Supervised Pre-Training and\n  Prototype Learning",
        "url": "http://arxiv.org/abs/2308.02346v1",
        "pub_date": "2023-08-04",
        "summary": "Deep Neural Network (DNN) has achieved great success on datasets of closed\nclass set. However, new classes, like new categories of social media topics,\nare continuously added to the real world, making it necessary to incrementally\nlearn. This is hard for DNN because it tends to focus on fitting to new classes\nwhile ignoring old classes, a phenomenon known as catastrophic forgetting.\nState-of-the-art methods rely on knowledge distillation and data replay\ntechniques but still have limitations. In this work, we analyze the causes of\ncatastrophic forgetting in class incremental learning, which owes to three\nfactors: representation drift, representation confusion, and classifier\ndistortion. Based on this view, we propose a two-stage learning framework with\na fixed encoder and an incrementally updated prototype classifier. The encoder\nis trained with self-supervised learning to generate a feature space with high\nintrinsic dimensionality, thus improving its transferability and generality.\nThe classifier incrementally learns new prototypes while retaining the\nprototypes of previously learned data, which is crucial in preserving the\ndecision boundary.Our method does not rely on preserved samples of old classes,\nis thus a non-exemplar based CIL method. Experiments on public datasets show\nthat our method can significantly outperform state-of-the-art exemplar-based\nmethods when they reserved 5 examplers per class, under the incremental setting\nof 10 phases, by 18.24% on CIFAR-100 and 9.37% on ImageNet100.",
        "translated": ""
    },
    {
        "title": "3D Motion Magnification: Visualizing Subtle Motions with Time Varying\n  Radiance Fields",
        "url": "http://arxiv.org/abs/2308.03757v1",
        "pub_date": "2023-08-07",
        "summary": "Motion magnification helps us visualize subtle, imperceptible motion.\nHowever, prior methods only work for 2D videos captured with a fixed camera. We\npresent a 3D motion magnification method that can magnify subtle motions from\nscenes captured by a moving camera, while supporting novel view rendering. We\nrepresent the scene with time-varying radiance fields and leverage the Eulerian\nprinciple for motion magnification to extract and amplify the variation of the\nembedding of a fixed point over time. We study and validate our proposed\nprinciple for 3D motion magnification using both implicit and tri-plane-based\nradiance fields as our underlying 3D scene representation. We evaluate the\neffectiveness of our method on both synthetic and real-world scenes captured\nunder various camera setups.",
        "translated": ""
    },
    {
        "title": "FSD V2: Improving Fully Sparse 3D Object Detection with Virtual Voxels",
        "url": "http://arxiv.org/abs/2308.03755v1",
        "pub_date": "2023-08-07",
        "summary": "LiDAR-based fully sparse architecture has garnered increasing attention.\nFSDv1 stands out as a representative work, achieving impressive efficacy and\nefficiency, albeit with intricate structures and handcrafted designs. In this\npaper, we present FSDv2, an evolution that aims to simplify the previous FSDv1\nwhile eliminating the inductive bias introduced by its handcrafted\ninstance-level representation, thus promoting better general applicability. To\nthis end, we introduce the concept of \\textbf{virtual voxels}, which takes over\nthe clustering-based instance segmentation in FSDv1. Virtual voxels not only\naddress the notorious issue of the Center Feature Missing problem in fully\nsparse detectors but also endow the framework with a more elegant and\nstreamlined approach. Consequently, we develop a suite of components to\ncomplement the virtual voxel concept, including a virtual voxel encoder, a\nvirtual voxel mixer, and a virtual voxel assignment strategy. Through empirical\nvalidation, we demonstrate that the virtual voxel mechanism is functionally\nsimilar to the handcrafted clustering in FSDv1 while being more general. We\nconduct experiments on three large-scale datasets: Waymo Open Dataset,\nArgoverse 2 dataset, and nuScenes dataset. Our results showcase\nstate-of-the-art performance on all three datasets, highlighting the\nsuperiority of FSDv2 in long-range scenarios and its general applicability to\nachieve competitive performance across diverse scenarios. Moreover, we provide\ncomprehensive experimental analysis to elucidate the workings of FSDv2. To\nfoster reproducibility and further research, we have open-sourced FSDv2 at\nhttps://github.com/tusen-ai/SST.",
        "translated": ""
    },
    {
        "title": "Mask Frozen-DETR: High Quality Instance Segmentation with One GPU",
        "url": "http://arxiv.org/abs/2308.03747v1",
        "pub_date": "2023-08-07",
        "summary": "In this paper, we aim to study how to build a strong instance segmenter with\nminimal training time and GPUs, as opposed to the majority of current\napproaches that pursue more accurate instance segmenter by building more\nadvanced frameworks at the cost of longer training time and higher GPU\nrequirements. To achieve this, we introduce a simple and general framework,\ntermed Mask Frozen-DETR, which can convert any existing DETR-based object\ndetection model into a powerful instance segmentation model. Our method only\nrequires training an additional lightweight mask network that predicts instance\nmasks within the bounding boxes given by a frozen DETR-based object detector.\nRemarkably, our method outperforms the state-of-the-art instance segmentation\nmethod Mask DINO in terms of performance on the COCO test-dev split (55.3% vs.\n54.7%) while being over 10X times faster to train. Furthermore, all of our\nexperiments can be trained using only one Tesla V100 GPU with 16 GB of memory,\ndemonstrating the significant efficiency of our proposed framework.",
        "translated": ""
    },
    {
        "title": "Tiny LVLM-eHub: Early Multimodal Experiments with Bard",
        "url": "http://arxiv.org/abs/2308.03729v1",
        "pub_date": "2023-08-07",
        "summary": "Recent advancements in Large Vision-Language Models (LVLMs) have demonstrated\nsignificant progress in tackling complex multimodal tasks. Among these\ncutting-edge developments, Google's Bard stands out for its remarkable\nmultimodal capabilities, promoting comprehensive comprehension and reasoning\nacross various domains. This work presents an early and holistic evaluation of\nLVLMs' multimodal abilities, with a particular focus on Bard, by proposing a\nlightweight variant of LVLM-eHub, named Tiny LVLM-eHub. In comparison to the\nvanilla version, Tiny LVLM-eHub possesses several appealing properties.\nFirstly, it provides a systematic assessment of six categories of multimodal\ncapabilities, including visual perception, visual knowledge acquisition, visual\nreasoning, visual commonsense, object hallucination, and embodied intelligence,\nthrough quantitative evaluation of $42$ standard text-related visual\nbenchmarks. Secondly, it conducts an in-depth analysis of LVLMs' predictions\nusing the ChatGPT Ensemble Evaluation (CEE), which leads to a robust and\naccurate evaluation and exhibits improved alignment with human evaluation\ncompared to the word matching approach. Thirdly, it comprises a mere $2.1$K\nimage-text pairs, facilitating ease of use for practitioners to evaluate their\nown offline LVLMs. Through extensive experimental analysis, this study\ndemonstrates that Bard outperforms previous LVLMs in most multimodal\ncapabilities except object hallucination, to which Bard is still susceptible.\nTiny LVLM-eHub serves as a baseline evaluation for various LVLMs and encourages\ninnovative strategies aimed at advancing multimodal techniques. Our project is\npublicly available at \\url{https://github.com/OpenGVLab/Multi-Modality-Arena}.",
        "translated": ""
    },
    {
        "title": "AdaptiveSAM: Towards Efficient Tuning of SAM for Surgical Scene\n  Segmentation",
        "url": "http://arxiv.org/abs/2308.03726v1",
        "pub_date": "2023-08-07",
        "summary": "Segmentation is a fundamental problem in surgical scene analysis using\nartificial intelligence. However, the inherent data scarcity in this domain\nmakes it challenging to adapt traditional segmentation techniques for this\ntask. To tackle this issue, current research employs pretrained models and\nfinetunes them on the given data. Even so, these require training deep networks\nwith millions of parameters every time new data becomes available. A recently\npublished foundation model, Segment-Anything (SAM), generalizes well to a large\nvariety of natural images, hence tackling this challenge to a reasonable\nextent. However, SAM does not generalize well to the medical domain as is\nwithout utilizing a large amount of compute resources for fine-tuning and using\ntask-specific prompts. Moreover, these prompts are in the form of\nbounding-boxes or foreground/background points that need to be annotated\nexplicitly for every image, making this solution increasingly tedious with\nhigher data size. In this work, we propose AdaptiveSAM - an adaptive\nmodification of SAM that can adjust to new datasets quickly and efficiently,\nwhile enabling text-prompted segmentation. For finetuning AdaptiveSAM, we\npropose an approach called bias-tuning that requires a significantly smaller\nnumber of trainable parameters than SAM (less than 2\\%). At the same time,\nAdaptiveSAM requires negligible expert intervention since it uses free-form\ntext as prompt and can segment the object of interest with just the label name\nas prompt. Our experiments show that AdaptiveSAM outperforms current\nstate-of-the-art methods on various medical imaging datasets including surgery,\nultrasound and X-ray. Code is available at\nhttps://github.com/JayParanjape/biastuning",
        "translated": ""
    },
    {
        "title": "Efficient Temporal Sentence Grounding in Videos with Multi-Teacher\n  Knowledge Distillation",
        "url": "http://arxiv.org/abs/2308.03725v1",
        "pub_date": "2023-08-07",
        "summary": "Temporal Sentence Grounding in Videos (TSGV) aims to detect the event\ntimestamps described by the natural language query from untrimmed videos. This\npaper discusses the challenge of achieving efficient computation in TSGV models\nwhile maintaining high performance. Most existing approaches exquisitely design\ncomplex architectures to improve accuracy with extra layers and loss, suffering\nfrom inefficiency and heaviness. Although some works have noticed that, they\nonly make an issue of feature fusion layers, which can hardly enjoy the\nhighspeed merit in the whole clunky network. To tackle this problem, we propose\na novel efficient multi-teacher model (EMTM) based on knowledge distillation to\ntransfer diverse knowledge from both heterogeneous and isomorphic networks.\nSpecifically, We first unify different outputs of the heterogeneous models into\none single form. Next, a Knowledge Aggregation Unit (KAU) is built to acquire\nhigh-quality integrated soft labels from multiple teachers. After that, the KAU\nmodule leverages the multi-scale video and global query information to\nadaptively determine the weights of different teachers. A Shared Encoder\nstrategy is then proposed to solve the problem that the student shallow layers\nhardly benefit from teachers, in which an isomorphic teacher is collaboratively\ntrained with the student to align their hidden states. Extensive experimental\nresults on three popular TSGV benchmarks demonstrate that our method is both\neffective and efficient without bells and whistles.",
        "translated": ""
    },
    {
        "title": "Dimensionality Reduction for Improving Out-of-Distribution Detection in\n  Medical Image Segmentation",
        "url": "http://arxiv.org/abs/2308.03723v1",
        "pub_date": "2023-08-07",
        "summary": "Clinically deployed segmentation models are known to fail on data outside of\ntheir training distribution. As these models perform well on most cases, it is\nimperative to detect out-of-distribution (OOD) images at inference to protect\nagainst automation bias. This work applies the Mahalanobis distance post hoc to\nthe bottleneck features of a Swin UNETR model that segments the liver on\nT1-weighted magnetic resonance imaging. By reducing the dimensions of the\nbottleneck features with principal component analysis, OOD images were detected\nwith high performance and minimal computational load.",
        "translated": ""
    },
    {
        "title": "SEM-GAT: Explainable Semantic Pose Estimation using Learned Graph\n  Attention",
        "url": "http://arxiv.org/abs/2308.03718v1",
        "pub_date": "2023-08-07",
        "summary": "This paper proposes a GNN-based method for exploiting semantics and local\ngeometry to guide the identification of reliable pointcloud registration\ncandidates. Semantic and morphological features of the environment serve as key\nreference points for registration, enabling accurate lidar-based pose\nestimation. Our novel lightweight static graph structure informs our\nattention-based keypoint node aggregation GNN network by identifying semantic\ninstance-based relationships, acting as inductive bias to significantly reduce\nthe computational burden of pointcloud registration. By connecting candidate\nnodes and exploiting cross-graph attention, we identify confidence scores for\nall potential registration correspondences, estimating the displacement between\npointcloud scans. Our pipeline enables introspective analysis of the model's\nperformance by correlating it with the individual contributions of local\nstructures in the environment, providing valuable insights into the system's\nbehaviour. We test our method on the KITTI odometry dataset, achieving\ncompetitive accuracy compared to benchmark methods and a higher track\nsmoothness while relying on significantly fewer network parameters.",
        "translated": ""
    },
    {
        "title": "Automated Real Time Delineation of Supraclavicular Brachial Plexus in\n  Neck Ultrasonography Videos: A Deep Learning Approach",
        "url": "http://arxiv.org/abs/2308.03717v1",
        "pub_date": "2023-08-07",
        "summary": "Peripheral nerve blocks are crucial to treatment of post-surgical pain and\nare associated with reduction in perioperative opioid use and hospital stay.\nAccurate interpretation of sono-anatomy is critical for the success of\nultrasound (US) guided peripheral nerve blocks and can be challenging to the\nnew operators. This prospective study enrolled 227 subjects who were\nsystematically scanned for supraclavicular and interscalene brachial plexus in\nvarious settings using three different US machines to create a dataset of 227\nunique videos. In total, 41,000 video frames were annotated by experienced\nanaesthesiologists using partial automation with object tracking and active\ncontour algorithms. Four baseline neural network models were trained on the\ndataset and their performance was evaluated for object detection and\nsegmentation tasks. Generalizability of the best suited model was then tested\non the datasets constructed from separate US scanners with and without\nfine-tuning. The results demonstrate that deep learning models can be leveraged\nfor real time segmentation of supraclavicular brachial plexus in neck\nultrasonography videos with high accuracy and reliability. Model was also\ntested for its ability to differentiate between supraclavicular and adjoining\ninterscalene brachial plexus. The entire dataset has been released publicly for\nfurther study by the research community.",
        "translated": ""
    },
    {
        "title": "Scaling may be all you need for achieving human-level object recognition\n  capacity with human-like visual experience",
        "url": "http://arxiv.org/abs/2308.03712v1",
        "pub_date": "2023-08-07",
        "summary": "This paper asks whether current self-supervised learning methods, if\nsufficiently scaled up, would be able to reach human-level visual object\nrecognition capabilities with the same type and amount of visual experience\nhumans learn from. Previous work on this question only considered the scaling\nof data size. Here, we consider the simultaneous scaling of data size, model\nsize, and image resolution. We perform a scaling experiment with vision\ntransformers up to 633M parameters in size (ViT-H/14) trained with up to 5K\nhours of human-like video data (long, continuous, mostly egocentric videos)\nwith image resolutions of up to 476x476 pixels. The efficiency of masked\nautoencoders (MAEs) as a self-supervised learning algorithm makes it possible\nto run this scaling experiment on an unassuming academic budget. We find that\nit is feasible to reach human-level object recognition capacity at sub-human\nscales of model size, data size, and image size, if these factors are scaled up\nsimultaneously. To give a concrete example, we estimate that a 2.5B parameter\nViT model trained with 20K hours (2.3 years) of human-like video data with a\nspatial resolution of 952x952 pixels should be able to reach human-level\naccuracy on ImageNet. Human-level competence is thus achievable for a\nfundamental perceptual capability from human-like perceptual experience\n(human-like in both amount and type) with extremely generic learning algorithms\nand architectures and without any substantive inductive biases.",
        "translated": ""
    },
    {
        "title": "When More is Less: Incorporating Additional Datasets Can Hurt\n  Performance By Introducing Spurious Correlations",
        "url": "http://arxiv.org/abs/2308.04431v1",
        "pub_date": "2023-08-08",
        "summary": "In machine learning, incorporating more data is often seen as a reliable\nstrategy for improving model performance; this work challenges that notion by\ndemonstrating that the addition of external datasets in many cases can hurt the\nresulting model's performance. In a large-scale empirical study across\ncombinations of four different open-source chest x-ray datasets and 9 different\nlabels, we demonstrate that in 43% of settings, a model trained on data from\ntwo hospitals has poorer worst group accuracy over both hospitals than a model\ntrained on just a single hospital's data. This surprising result occurs even\nthough the added hospital makes the training distribution more similar to the\ntest distribution. We explain that this phenomenon arises from the spurious\ncorrelation that emerges between the disease and hospital, due to\nhospital-specific image artifacts. We highlight the trade-off one encounters\nwhen training on multiple datasets, between the obvious benefit of additional\ndata and insidious cost of the introduced spurious correlation. In some cases,\nbalancing the dataset can remove the spurious correlation and improve\nperformance, but it is not always an effective strategy. We contextualize our\nresults within the literature on spurious correlations to help explain these\noutcomes. Our experiments underscore the importance of exercising caution when\nselecting training data for machine learning models, especially in settings\nwhere there is a risk of spurious correlations such as with medical imaging.\nThe risks outlined highlight the need for careful data selection and model\nevaluation in future research and practice.",
        "translated": ""
    },
    {
        "title": "A Deep-Learning Method Using Auto-encoder and Generative Adversarial\n  Network for Anomaly Detection on Ancient Stone Stele Surfaces",
        "url": "http://arxiv.org/abs/2308.04426v1",
        "pub_date": "2023-08-08",
        "summary": "Accurate detection of natural deterioration and man-made damage on the\nsurfaces of ancient stele in the first instance is essential for their\npreventive conservation. Existing methods for cultural heritage preservation\nare not able to achieve this goal perfectly due to the difficulty of balancing\naccuracy, efficiency, timeliness, and cost. This paper presents a deep-learning\nmethod to automatically detect above mentioned emergencies on ancient stone\nstele in real time, employing autoencoder (AE) and generative adversarial\nnetwork (GAN). The proposed method overcomes the limitations of existing\nmethods by requiring no extensive anomaly samples while enabling comprehensive\ndetection of unpredictable anomalies. the method includes stages of monitoring,\ndata acquisition, pre-processing, model structuring, and post-processing.\nTaking the Longmen Grottoes' stone steles as a case study, an unsupervised\nlearning model based on AE and GAN architectures is proposed and validated with\na reconstruction accuracy of 99.74\\%. The method's evaluation revealed the\nproficient detection of seven artificially designed anomalies and demonstrated\nprecision and reliability without false alarms. This research provides novel\nideas and possibilities for the application of deep learning in the field of\ncultural heritage.",
        "translated": ""
    },
    {
        "title": "DiffCR: A Fast Conditional Diffusion Framework for Cloud Removal from\n  Optical Satellite Images",
        "url": "http://arxiv.org/abs/2308.04417v1",
        "pub_date": "2023-08-08",
        "summary": "Optical satellite images are a critical data source; however, cloud cover\noften compromises their quality, hindering image applications and analysis.\nConsequently, effectively removing clouds from optical satellite images has\nemerged as a prominent research direction. While recent advancements in cloud\nremoval primarily rely on generative adversarial networks, which may yield\nsuboptimal image quality, diffusion models have demonstrated remarkable success\nin diverse image-generation tasks, showcasing their potential in addressing\nthis challenge. This paper presents a novel framework called DiffCR, which\nleverages conditional guided diffusion with deep convolutional networks for\nhigh-performance cloud removal for optical satellite imagery. Specifically, we\nintroduce a decoupled encoder for conditional image feature extraction,\nproviding a robust color representation to ensure the close similarity of\nappearance information between the conditional input and the synthesized\noutput. Moreover, we propose a novel and efficient time and condition fusion\nblock within the cloud removal model to accurately simulate the correspondence\nbetween the appearance in the conditional image and the target image at a low\ncomputational cost. Extensive experimental evaluations on two commonly used\nbenchmark datasets demonstrate that DiffCR consistently achieves\nstate-of-the-art performance on all metrics, with parameter and computational\ncomplexities amounting to only 5.1% and 5.4%, respectively, of those previous\nbest methods. The source code, pre-trained models, and all the experimental\nresults will be publicly available at https://github.com/XavierJiezou/DiffCR\nupon the paper's acceptance of this work.",
        "translated": ""
    },
    {
        "title": "Digging into Depth Priors for Outdoor Neural Radiance Fields",
        "url": "http://arxiv.org/abs/2308.04413v1",
        "pub_date": "2023-08-08",
        "summary": "Neural Radiance Fields (NeRF) have demonstrated impressive performance in\nvision and graphics tasks, such as novel view synthesis and immersive reality.\nHowever, the shape-radiance ambiguity of radiance fields remains a challenge,\nespecially in the sparse viewpoints setting. Recent work resorts to integrating\ndepth priors into outdoor NeRF training to alleviate the issue. However, the\ncriteria for selecting depth priors and the relative merits of different priors\nhave not been thoroughly investigated. Moreover, the relative merits of\nselecting different approaches to use the depth priors is also an unexplored\nproblem. In this paper, we provide a comprehensive study and evaluation of\nemploying depth priors to outdoor neural radiance fields, covering common depth\nsensing technologies and most application ways. Specifically, we conduct\nextensive experiments with two representative NeRF methods equipped with four\ncommonly-used depth priors and different depth usages on two widely used\noutdoor datasets. Our experimental results reveal several interesting findings\nthat can potentially benefit practitioners and researchers in training their\nNeRF models with depth priors. Project Page:\nhttps://cwchenwang.github.io/outdoor-nerf-depth",
        "translated": ""
    },
    {
        "title": "V-DETR: DETR with Vertex Relative Position Encoding for 3D Object\n  Detection",
        "url": "http://arxiv.org/abs/2308.04409v1",
        "pub_date": "2023-08-08",
        "summary": "We introduce a highly performant 3D object detector for point clouds using\nthe DETR framework. The prior attempts all end up with suboptimal results\nbecause they fail to learn accurate inductive biases from the limited scale of\ntraining data. In particular, the queries often attend to points that are far\naway from the target objects, violating the locality principle in object\ndetection. To address the limitation, we introduce a novel 3D Vertex Relative\nPosition Encoding (3DV-RPE) method which computes position encoding for each\npoint based on its relative position to the 3D boxes predicted by the queries\nin each decoder layer, thus providing clear information to guide the model to\nfocus on points near the objects, in accordance with the principle of locality.\nIn addition, we systematically improve the pipeline from various aspects such\nas data normalization based on our understanding of the task. We show\nexceptional results on the challenging ScanNetV2 benchmark, achieving\nsignificant improvements over the previous 3DETR in\n$\\rm{AP}_{25}$/$\\rm{AP}_{50}$ from 65.0\\%/47.0\\% to 77.8\\%/66.0\\%,\nrespectively. In addition, our method sets a new record on ScanNetV2 and SUN\nRGB-D datasets.Code will be released at http://github.com/yichaoshen-MS/V-DETR.",
        "translated": ""
    },
    {
        "title": "Person Re-Identification without Identification via Event Anonymization",
        "url": "http://arxiv.org/abs/2308.04402v2",
        "pub_date": "2023-08-08",
        "summary": "Wide-scale use of visual surveillance in public spaces puts individual\nprivacy at stake while increasing resource consumption (energy, bandwidth, and\ncomputation). Neuromorphic vision sensors (event-cameras) have been recently\nconsidered a valid solution to the privacy issue because they do not capture\ndetailed RGB visual information of the subjects in the scene. However, recent\ndeep learning architectures have been able to reconstruct images from event\ncameras with high fidelity, reintroducing a potential threat to privacy for\nevent-based vision applications. In this paper, we aim to anonymize\nevent-streams to protect the identity of human subjects against such image\nreconstruction attacks. To achieve this, we propose an end-to-end network\narchitecture jointly optimized for the twofold objective of preserving privacy\nand performing a downstream task such as person ReId. Our network learns to\nscramble events, enforcing the degradation of images recovered from the privacy\nattacker. In this work, we also bring to the community the first ever\nevent-based person ReId dataset gathered to evaluate the performance of our\napproach. We validate our approach with extensive experiments and report\nresults on the synthetic event data simulated from the publicly available\nSoftBio dataset and our proposed Event-ReId dataset.",
        "translated": ""
    },
    {
        "title": "LEFormer: A Hybrid CNN-Transformer Architecture for Accurate Lake\n  Extraction from Remote Sensing Imagery",
        "url": "http://arxiv.org/abs/2308.04397v1",
        "pub_date": "2023-08-08",
        "summary": "Lake extraction from remote sensing imagery is challenging due to the complex\nshapes of lakes and the presence of noise. Existing methods suffer from blurred\nsegmentation boundaries and poor foreground modeling. In this paper, we propose\na hybrid CNN-Transformer architecture, called LEFormer, for accurate lake\nextraction. LEFormer contains four main modules: CNN encoder, Transformer\nencoder, cross-encoder fusion, and lightweight decoder. The CNN encoder\nrecovers local spatial information and improves fine-scale details.\nSimultaneously, the Transformer encoder captures long-range dependencies\nbetween sequences of any length, allowing them to obtain global features and\ncontext information better. Finally, a lightweight decoder is employed for mask\nprediction. We evaluate the performance and efficiency of LEFormer on two\ndatasets, the Surface Water (SW) and the Qinghai-Tibet Plateau Lake (QTPL).\nExperimental results show that LEFormer consistently achieves state-of-the-art\n(SOTA) performance and efficiency on these two datasets, outperforming existing\nmethods. Specifically, LEFormer achieves 90.86% and 97.42% mIoU on the SW and\nQTPL datasets with a parameter count of 3.61M, respectively, while being 20x\nminor than the previous SOTA method.",
        "translated": ""
    },
    {
        "title": "Data Augmentation-Based Unsupervised Domain Adaptation In Medical\n  Imaging",
        "url": "http://arxiv.org/abs/2308.04395v1",
        "pub_date": "2023-08-08",
        "summary": "Deep learning-based models in medical imaging often struggle to generalize\neffectively to new scans due to data heterogeneity arising from differences in\nhardware, acquisition parameters, population, and artifacts. This limitation\npresents a significant challenge in adopting machine learning models for\nclinical practice. We propose an unsupervised method for robust domain\nadaptation in brain MRI segmentation by leveraging MRI-specific augmentation\ntechniques. To evaluate the effectiveness of our method, we conduct extensive\nexperiments across diverse datasets, modalities, and segmentation tasks,\ncomparing against the state-of-the-art methods. The results show that our\nproposed approach achieves high accuracy, exhibits broad applicability, and\nshowcases remarkable robustness against domain shift in various tasks,\nsurpassing the state-of-the-art performance in the majority of cases.",
        "translated": ""
    },
    {
        "title": "DELFlow: Dense Efficient Learning of Scene Flow for Large-Scale Point\n  Clouds",
        "url": "http://arxiv.org/abs/2308.04383v2",
        "pub_date": "2023-08-08",
        "summary": "Point clouds are naturally sparse, while image pixels are dense. The\ninconsistency limits feature fusion from both modalities for point-wise scene\nflow estimation. Previous methods rarely predict scene flow from the entire\npoint clouds of the scene with one-time inference due to the memory\ninefficiency and heavy overhead from distance calculation and sorting involved\nin commonly used farthest point sampling, KNN, and ball query algorithms for\nlocal feature aggregation. To mitigate these issues in scene flow learning, we\nregularize raw points to a dense format by storing 3D coordinates in 2D grids.\nUnlike the sampling operation commonly used in existing works, the dense 2D\nrepresentation 1) preserves most points in the given scene, 2) brings in a\nsignificant boost of efficiency, and 3) eliminates the density gap between\npoints and pixels, allowing us to perform effective feature fusion. We also\npresent a novel warping projection technique to alleviate the information loss\nproblem resulting from the fact that multiple points could be mapped into one\ngrid during projection when computing cost volume. Sufficient experiments\ndemonstrate the efficiency and effectiveness of our method, outperforming the\nprior-arts on the FlyingThings3D and KITTI dataset.",
        "translated": ""
    },
    {
        "title": "Your Negative May not Be True Negative: Boosting Image-Text Matching\n  with False Negative Elimination",
        "url": "http://arxiv.org/abs/2308.04380v1",
        "pub_date": "2023-08-08",
        "summary": "Most existing image-text matching methods adopt triplet loss as the\noptimization objective, and choosing a proper negative sample for the triplet\nof &lt;anchor, positive, negative&gt; is important for effectively training the\nmodel, e.g., hard negatives make the model learn efficiently and effectively.\nHowever, we observe that existing methods mainly employ the most similar\nsamples as hard negatives, which may not be true negatives. In other words, the\nsamples with high similarity but not paired with the anchor may reserve\npositive semantic associations, and we call them false negatives. Repelling\nthese false negatives in triplet loss would mislead the semantic representation\nlearning and result in inferior retrieval performance. In this paper, we\npropose a novel False Negative Elimination (FNE) strategy to select negatives\nvia sampling, which could alleviate the problem introduced by false negatives.\nSpecifically, we first construct the distributions of positive and negative\nsamples separately via their similarities with the anchor, based on the\nfeatures extracted from image and text encoders. Then we calculate the false\nnegative probability of a given sample based on its similarity with the anchor\nand the above distributions via the Bayes' rule, which is employed as the\nsampling weight during negative sampling process. Since there may not exist any\nfalse negative in a small batch size, we design a memory module with momentum\nto retain a large negative buffer and implement our negative sampling strategy\nspanning over the buffer. In addition, to make the model focus on hard\nnegatives, we reassign the sampling weights for the simple negatives with a\ncut-down strategy. The extensive experiments are conducted on Flickr30K and\nMS-COCO, and the results demonstrate the superiority of our proposed false\nnegative elimination strategy. The code is available at\nhttps://github.com/LuminosityX/FNE.",
        "translated": ""
    },
    {
        "title": "Scene-Generalizable Interactive Segmentation of Radiance Fields",
        "url": "http://arxiv.org/abs/2308.05104v1",
        "pub_date": "2023-08-09",
        "summary": "Existing methods for interactive segmentation in radiance fields entail\nscene-specific optimization and thus cannot generalize across different scenes,\nwhich greatly limits their applicability. In this work we make the first\nattempt at Scene-Generalizable Interactive Segmentation in Radiance Fields\n(SGISRF) and propose a novel SGISRF method, which can perform 3D object\nsegmentation for novel (unseen) scenes represented by radiance fields, guided\nby only a few interactive user clicks in a given set of multi-view 2D images.\nIn particular, the proposed SGISRF focuses on addressing three crucial\nchallenges with three specially designed techniques. First, we devise the\nCross-Dimension Guidance Propagation to encode the scarce 2D user clicks into\ninformative 3D guidance representations. Second, the Uncertainty-Eliminated 3D\nSegmentation module is designed to achieve efficient yet effective 3D\nsegmentation. Third, Concealment-Revealed Supervised Learning scheme is\nproposed to reveal and correct the concealed 3D segmentation errors resulted\nfrom the supervision in 2D space with only 2D mask annotations. Extensive\nexperiments on two real-world challenging benchmarks covering diverse scenes\ndemonstrate 1) effectiveness and scene-generalizability of the proposed method,\n2) favorable performance compared to classical method requiring scene-specific\noptimization.",
        "translated": ""
    },
    {
        "title": "LayoutLLM-T2I: Eliciting Layout Guidance from LLM for Text-to-Image\n  Generation",
        "url": "http://arxiv.org/abs/2308.05095v1",
        "pub_date": "2023-08-09",
        "summary": "In the text-to-image generation field, recent remarkable progress in Stable\nDiffusion makes it possible to generate rich kinds of novel photorealistic\nimages. However, current models still face misalignment issues (e.g.,\nproblematic spatial relation understanding and numeration failure) in complex\nnatural scenes, which impedes the high-faithfulness text-to-image generation.\nAlthough recent efforts have been made to improve controllability by giving\nfine-grained guidance (e.g., sketch and scribbles), this issue has not been\nfundamentally tackled since users have to provide such guidance information\nmanually. In this work, we strive to synthesize high-fidelity images that are\nsemantically aligned with a given textual prompt without any guidance. Toward\nthis end, we propose a coarse-to-fine paradigm to achieve layout planning and\nimage generation. Concretely, we first generate the coarse-grained layout\nconditioned on a given textual prompt via in-context learning based on Large\nLanguage Models. Afterward, we propose a fine-grained object-interaction\ndiffusion method to synthesize high-faithfulness images conditioned on the\nprompt and the automatically generated layout. Extensive experiments\ndemonstrate that our proposed method outperforms the state-of-the-art models in\nterms of layout and image generation. Our code and settings are available at\n\\url{https://layoutllm-t2i.github.io}.",
        "translated": ""
    },
    {
        "title": "A degree of image identification at sub-human scales could be possible\n  with more advanced clusters",
        "url": "http://arxiv.org/abs/2308.05092v1",
        "pub_date": "2023-08-09",
        "summary": "The purpose of the research is to determine if currently available\nself-supervised learning techniques can accomplish human level comprehension of\nvisual images using the same degree and amount of sensory input that people\nacquire from. Initial research on this topic solely considered data volume\nscaling. Here, we scale both the volume of data and the quality of the image.\nThis scaling experiment is a self-supervised learning method that may be done\nwithout any outside financing. We find that scaling up data volume and picture\nresolution at the same time enables human-level item detection performance at\nsub-human sizes.We run a scaling experiment with vision transformers trained on\nup to 200000 images up to 256 ppi.",
        "translated": ""
    },
    {
        "title": "Constructing Holistic Spatio-Temporal Scene Graph for Video Semantic\n  Role Labeling",
        "url": "http://arxiv.org/abs/2308.05081v1",
        "pub_date": "2023-08-09",
        "summary": "Video Semantic Role Labeling (VidSRL) aims to detect the salient events from\ngiven videos, by recognizing the predict-argument event structures and the\ninterrelationships between events. While recent endeavors have put forth\nmethods for VidSRL, they can be mostly subject to two key drawbacks, including\nthe lack of fine-grained spatial scene perception and the insufficiently\nmodeling of video temporality. Towards this end, this work explores a novel\nholistic spatio-temporal scene graph (namely HostSG) representation based on\nthe existing dynamic scene graph structures, which well model both the\nfine-grained spatial semantics and temporal dynamics of videos for VidSRL.\nBuilt upon the HostSG, we present a nichetargeting VidSRL framework. A\nscene-event mapping mechanism is first designed to bridge the gap between the\nunderlying scene structure and the high-level event semantic structure,\nresulting in an overall hierarchical scene-event (termed ICE) graph structure.\nWe further perform iterative structure refinement to optimize the ICE graph,\nsuch that the overall structure representation can best coincide with end task\ndemand. Finally, three subtask predictions of VidSRL are jointly decoded, where\nthe end-to-end paradigm effectively avoids error propagation. On the benchmark\ndataset, our framework boosts significantly over the current best-performing\nmodel. Further analyses are shown for a better understanding of the advances of\nour methods.",
        "translated": ""
    },
    {
        "title": "Drones4Good: Supporting Disaster Relief Through Remote Sensing and AI",
        "url": "http://arxiv.org/abs/2308.05074v1",
        "pub_date": "2023-08-09",
        "summary": "In order to respond effectively in the aftermath of a disaster, emergency\nservices and relief organizations rely on timely and accurate information about\nthe affected areas. Remote sensing has the potential to significantly reduce\nthe time and effort required to collect such information by enabling a rapid\nsurvey of large areas. To achieve this, the main challenge is the automatic\nextraction of relevant information from remotely sensed data. In this work, we\nshow how the combination of drone-based data with deep learning methods enables\nautomated and large-scale situation assessment. In addition, we demonstrate the\nintegration of onboard image processing techniques for the deployment of\nautonomous drone-based aid delivery. The results show the feasibility of a\nrapid and large-scale image analysis in the field, and that onboard image\nprocessing can increase the safety of drone-based aid deliveries.",
        "translated": ""
    },
    {
        "title": "Volumetric Fast Fourier Convolution for Detecting Ink on the Carbonized\n  Herculaneum Papyri",
        "url": "http://arxiv.org/abs/2308.05070v1",
        "pub_date": "2023-08-09",
        "summary": "Recent advancements in Digital Document Restoration (DDR) have led to\nsignificant breakthroughs in analyzing highly damaged written artifacts. Among\nthose, there has been an increasing interest in applying Artificial\nIntelligence techniques for virtually unwrapping and automatically detecting\nink on the Herculaneum papyri collection. This collection consists of\ncarbonized scrolls and fragments of documents, which have been digitized via\nX-ray tomography to allow the development of ad-hoc deep learning-based DDR\nsolutions. In this work, we propose a modification of the Fast Fourier\nConvolution operator for volumetric data and apply it in a segmentation\narchitecture for ink detection on the challenging Herculaneum papyri,\ndemonstrating its suitability via deep experimental analysis. To encourage the\nresearch on this task and the application of the proposed operator to other\ntasks involving volumetric data, we will release our implementation\n(https://github.com/aimagelab/vffc)",
        "translated": ""
    },
    {
        "title": "Geometric Learning-Based Transformer Network for Estimation of\n  Segmentation Errors",
        "url": "http://arxiv.org/abs/2308.05068v2",
        "pub_date": "2023-08-09",
        "summary": "Many segmentation networks have been proposed for 3D volumetric segmentation\nof tumors and organs at risk. Hospitals and clinical institutions seek to\naccelerate and minimize the efforts of specialists in image segmentation.\nStill, in case of errors generated by these networks, clinicians would have to\nmanually edit the generated segmentation maps. Given a 3D volume and its\nputative segmentation map, we propose an approach to identify and measure\nerroneous regions in the segmentation map. Our method can estimate error at any\npoint or node in a 3D mesh generated from a possibly erroneous volumetric\nsegmentation map, serving as a Quality Assurance tool. We propose a graph\nneural network-based transformer based on the Nodeformer architecture to\nmeasure and classify the segmentation errors at any point. We have evaluated\nour network on a high-resolution micro-CT dataset of the human inner-ear bony\nlabyrinth structure by simulating erroneous 3D segmentation maps. Our network\nincorporates a convolutional encoder to compute node-centric features from the\ninput micro-CT data, the Nodeformer to learn the latent graph embeddings, and a\nMulti-Layer Perceptron (MLP) to compute and classify the node-wise errors. Our\nnetwork achieves a mean absolute error of ~0.042 over other Graph Neural\nNetworks (GNN) and an accuracy of 79.53% over other GNNs in estimating and\nclassifying the node-wise errors, respectively. We also put forth vertex-normal\nprediction as a custom pretext task for pre-training the CNN encoder to improve\nthe network's overall performance. Qualitative analysis shows the efficiency of\nour network in correctly classifying errors and reducing misclassifications.",
        "translated": ""
    },
    {
        "title": "A Novel Method for improving accuracy in neural network by reinstating\n  traditional back propagation technique",
        "url": "http://arxiv.org/abs/2308.05059v1",
        "pub_date": "2023-08-09",
        "summary": "Deep learning has revolutionized industries like computer vision, natural\nlanguage processing, and speech recognition. However, back propagation, the\nmain method for training deep neural networks, faces challenges like\ncomputational overhead and vanishing gradients. In this paper, we propose a\nnovel instant parameter update methodology that eliminates the need for\ncomputing gradients at each layer. Our approach accelerates learning, avoids\nthe vanishing gradient problem, and outperforms state-of-the-art methods on\nbenchmark data sets. This research presents a promising direction for efficient\nand effective deep neural network training.",
        "translated": ""
    },
    {
        "title": "PAT: Position-Aware Transformer for Dense Multi-Label Action Detection",
        "url": "http://arxiv.org/abs/2308.05051v1",
        "pub_date": "2023-08-09",
        "summary": "We present PAT, a transformer-based network that learns complex temporal\nco-occurrence action dependencies in a video by exploiting multi-scale temporal\nfeatures. In existing methods, the self-attention mechanism in transformers\nloses the temporal positional information, which is essential for robust action\ndetection. To address this issue, we (i) embed relative positional encoding in\nthe self-attention mechanism and (ii) exploit multi-scale temporal\nrelationships by designing a novel non hierarchical network, in contrast to the\nrecent transformer-based approaches that use a hierarchical structure. We argue\nthat joining the self-attention mechanism with multiple sub-sampling processes\nin the hierarchical approaches results in increased loss of positional\ninformation. We evaluate the performance of our proposed approach on two\nchallenging dense multi-label benchmark datasets, and show that PAT improves\nthe current state-of-the-art result by 1.1% and 0.6% mAP on the Charades and\nMultiTHUMOS datasets, respectively, thereby achieving the new state-of-the-art\nmAP at 26.5% and 44.6%, respectively. We also perform extensive ablation\nstudies to examine the impact of the different components of our proposed\nnetwork.",
        "translated": ""
    },
    {
        "title": "Density Crop-guided Semi-supervised Object Detection in Aerial Images",
        "url": "http://arxiv.org/abs/2308.05032v1",
        "pub_date": "2023-08-09",
        "summary": "One of the important bottlenecks in training modern object detectors is the\nneed for labeled images where bounding box annotations have to be produced for\neach object present in the image. This bottleneck is further exacerbated in\naerial images where the annotators have to label small objects often\ndistributed in clusters on high-resolution images. In recent days, the\nmean-teacher approach trained with pseudo-labels and weak-strong augmentation\nconsistency is gaining popularity for semi-supervised object detection.\nHowever, a direct adaptation of such semi-supervised detectors for aerial\nimages where small clustered objects are often present, might not lead to\noptimal results. In this paper, we propose a density crop-guided\nsemi-supervised detector that identifies the cluster of small objects during\ntraining and also exploits them to improve performance at inference. During\ntraining, image crops of clusters identified from labeled and unlabeled images\nare used to augment the training set, which in turn increases the chance of\ndetecting small objects and creating good pseudo-labels for small objects on\nthe unlabeled images. During inference, the detector is not only able to detect\nthe objects of interest but also regions with a high density of small objects\n(density crops) so that detections from the input image and detections from\nimage crops are combined, resulting in an overall more accurate object\nprediction, especially for small objects. Empirical studies on the popular\nbenchmarks of VisDrone and DOTA datasets show the effectiveness of our density\ncrop-guided semi-supervised detector with an average improvement of more than\n2\\% over the basic mean-teacher method in COCO style AP. Our code is available\nat: https://github.com/akhilpm/DroneSSOD.",
        "translated": ""
    },
    {
        "title": "Iterative Reweighted Least Squares Networks With Convergence Guarantees\n  for Solving Inverse Imaging Problems",
        "url": "http://arxiv.org/abs/2308.05745v1",
        "pub_date": "2023-08-10",
        "summary": "In this work we present a novel optimization strategy for image\nreconstruction tasks under analysis-based image regularization, which promotes\nsparse and/or low-rank solutions in some learned transform domain. We\nparameterize such regularizers using potential functions that correspond to\nweighted extensions of the $\\ell_p^p$-vector and $\\mathcal{S}_p^p$\nSchatten-matrix quasi-norms with $0 &lt; p \\le 1$. Our proposed minimization\nstrategy extends the Iteratively Reweighted Least Squares (IRLS) method,\ntypically used for synthesis-based $\\ell_p$ and $\\mathcal{S}_p$ norm and\nanalysis-based $\\ell_1$ and nuclear norm regularization. We prove that under\nmild conditions our minimization algorithm converges linearly to a stationary\npoint, and we provide an upper bound for its convergence rate. Further, to\nselect the parameters of the regularizers that deliver the best results for the\nproblem at hand, we propose to learn them from training data by formulating the\nsupervised learning process as a stochastic bilevel optimization problem. We\nshow that thanks to the convergence guarantees of our proposed minimization\nstrategy, such optimization can be successfully performed with a\nmemory-efficient implicit back-propagation scheme. We implement our learned\nIRLS variants as recurrent networks and assess their performance on the\nchallenging image reconstruction tasks of non-blind deblurring,\nsuper-resolution and demosaicking. The comparisons against other existing\nlearned reconstruction approaches demonstrate that our overall method is very\ncompetitive and in many cases outperforms existing unrolled networks, whose\nnumber of parameters is orders of magnitude higher than in our case.",
        "translated": ""
    },
    {
        "title": "PlankAssembly: Robust 3D Reconstruction from Three Orthographic Views\n  with Learnt Shape Programs",
        "url": "http://arxiv.org/abs/2308.05744v1",
        "pub_date": "2023-08-10",
        "summary": "In this paper, we develop a new method to automatically convert 2D line\ndrawings from three orthographic views into 3D CAD models. Existing methods for\nthis problem reconstruct 3D models by back-projecting the 2D observations into\n3D space while maintaining explicit correspondence between the input and\noutput. Such methods are sensitive to errors and noises in the input, thus\noften fail in practice where the input drawings created by human designers are\nimperfect. To overcome this difficulty, we leverage the attention mechanism in\na Transformer-based sequence generation model to learn flexible mappings\nbetween the input and output. Further, we design shape programs which are\nsuitable for generating the objects of interest to boost the reconstruction\naccuracy and facilitate CAD modeling applications. Experiments on a new\nbenchmark dataset show that our method significantly outperforms existing ones\nwhen the inputs are noisy or incomplete.",
        "translated": ""
    },
    {
        "title": "Neural Progressive Meshes",
        "url": "http://arxiv.org/abs/2308.05741v1",
        "pub_date": "2023-08-10",
        "summary": "The recent proliferation of 3D content that can be consumed on hand-held\ndevices necessitates efficient tools for transmitting large geometric data,\ne.g., 3D meshes, over the Internet. Detailed high-resolution assets can pose a\nchallenge to storage as well as transmission bandwidth, and level-of-detail\ntechniques are often used to transmit an asset using an appropriate bandwidth\nbudget. It is especially desirable for these methods to transmit data\nprogressively, improving the quality of the geometry with more data. Our key\ninsight is that the geometric details of 3D meshes often exhibit similar local\npatterns even across different shapes, and thus can be effectively represented\nwith a shared learned generative space. We learn this space using a\nsubdivision-based encoder-decoder architecture trained in advance on a large\ncollection of surfaces. We further observe that additional residual features\ncan be transmitted progressively between intermediate levels of subdivision\nthat enable the client to control the tradeoff between bandwidth cost and\nquality of reconstruction, providing a neural progressive mesh representation.\nWe evaluate our method on a diverse set of complex 3D shapes and demonstrate\nthat it outperforms baselines in terms of compression ratio and reconstruction\nquality.",
        "translated": ""
    },
    {
        "title": "Zero Grads Ever Given: Learning Local Surrogate Losses for\n  Non-Differentiable Graphics",
        "url": "http://arxiv.org/abs/2308.05739v1",
        "pub_date": "2023-08-10",
        "summary": "Gradient-based optimization is now ubiquitous across graphics, but\nunfortunately can not be applied to problems with undefined or zero gradients.\nTo circumvent this issue, the loss function can be manually replaced by a\n\"surrogate\" that has similar minima but is differentiable. Our proposed\nframework, ZeroGrads, automates this process by learning a neural approximation\nof the objective function, the surrogate, which in turn can be used to\ndifferentiate through arbitrary black-box graphics pipelines. We train the\nsurrogate on an actively smoothed version of the objective and encourage\nlocality, focusing the surrogate's capacity on what matters at the current\ntraining episode. The fitting is performed online, alongside the parameter\noptimization, and self-supervised, without pre-computed data or pre-trained\nmodels. As sampling the objective is expensive (it requires a full rendering or\nsimulator run), we devise an efficient sampling scheme that allows for\ntractable run-times and competitive performance at little overhead. We\ndemonstrate optimizing diverse non-convex, non-differentiable black-box\nproblems in graphics, such as visibility in rendering, discrete parameter\nspaces in procedural modelling or optimal control in physics-driven animation.\nIn contrast to more traditional algorithms, our approach scales well to higher\ndimensions, which we demonstrate on problems with up to 35k interlinked\nvariables.",
        "translated": ""
    },
    {
        "title": "Follow Anything: Open-set detection, tracking, and following in\n  real-time",
        "url": "http://arxiv.org/abs/2308.05737v1",
        "pub_date": "2023-08-10",
        "summary": "Tracking and following objects of interest is critical to several robotics\nuse cases, ranging from industrial automation to logistics and warehousing, to\nhealthcare and security. In this paper, we present a robotic system to detect,\ntrack, and follow any object in real-time. Our approach, dubbed ``follow\nanything'' (FAn), is an open-vocabulary and multimodal model -- it is not\nrestricted to concepts seen at training time and can be applied to novel\nclasses at inference time using text, images, or click queries. Leveraging rich\nvisual descriptors from large-scale pre-trained models (foundation models), FAn\ncan detect and segment objects by matching multimodal queries (text, images,\nclicks) against an input image sequence. These detected and segmented objects\nare tracked across image frames, all while accounting for occlusion and object\nre-emergence. We demonstrate FAn on a real-world robotic system (a micro aerial\nvehicle) and report its ability to seamlessly follow the objects of interest in\na real-time control loop. FAn can be deployed on a laptop with a lightweight\n(6-8 GB) graphics card, achieving a throughput of 6-20 frames per second. To\nenable rapid adoption, deployment, and extensibility, we open-source all our\ncode on our project webpage at https://github.com/alaamaalouf/FollowAnything .\nWe also encourage the reader the watch our 5-minutes explainer video in this\nhttps://www.youtube.com/watch?v=6Mgt3EPytrw .",
        "translated": ""
    },
    {
        "title": "MapTRv2: An End-to-End Framework for Online Vectorized HD Map\n  Construction",
        "url": "http://arxiv.org/abs/2308.05736v1",
        "pub_date": "2023-08-10",
        "summary": "High-definition (HD) map provides abundant and precise static environmental\ninformation of the driving scene, serving as a fundamental and indispensable\ncomponent for planning in autonomous driving system. In this paper, we present\n\\textbf{Map} \\textbf{TR}ansformer, an end-to-end framework for online\nvectorized HD map construction. We propose a unified permutation-equivalent\nmodeling approach, \\ie, modeling map element as a point set with a group of\nequivalent permutations, which accurately describes the shape of map element\nand stabilizes the learning process. We design a hierarchical query embedding\nscheme to flexibly encode structured map information and perform hierarchical\nbipartite matching for map element learning. To speed up convergence, we\nfurther introduce auxiliary one-to-many matching and dense supervision. The\nproposed method well copes with various map elements with arbitrary shapes. It\nruns at real-time inference speed and achieves state-of-the-art performance on\nboth nuScenes and Argoverse2 datasets. Abundant qualitative results show stable\nand robust map construction quality in complex and various driving scenes. Code\nand more demos are available at \\url{https://github.com/hustvl/MapTR} for\nfacilitating further studies and applications.",
        "translated": ""
    },
    {
        "title": "FrozenRecon: Pose-free 3D Scene Reconstruction with Frozen Depth Models",
        "url": "http://arxiv.org/abs/2308.05733v1",
        "pub_date": "2023-08-10",
        "summary": "3D scene reconstruction is a long-standing vision task. Existing approaches\ncan be categorized into geometry-based and learning-based methods. The former\nleverages multi-view geometry but can face catastrophic failures due to the\nreliance on accurate pixel correspondence across views. The latter was\nproffered to mitigate these issues by learning 2D or 3D representation\ndirectly. However, without a large-scale video or 3D training data, it can\nhardly generalize to diverse real-world scenarios due to the presence of tens\nof millions or even billions of optimization parameters in the deep network.\nRecently, robust monocular depth estimation models trained with large-scale\ndatasets have been proven to possess weak 3D geometry prior, but they are\ninsufficient for reconstruction due to the unknown camera parameters, the\naffine-invariant property, and inter-frame inconsistency. Here, we propose a\nnovel test-time optimization approach that can transfer the robustness of\naffine-invariant depth models such as LeReS to challenging diverse scenes while\nensuring inter-frame consistency, with only dozens of parameters to optimize\nper video frame. Specifically, our approach involves freezing the pre-trained\naffine-invariant depth model's depth predictions, rectifying them by optimizing\nthe unknown scale-shift values with a geometric consistency alignment module,\nand employing the resulting scale-consistent depth maps to robustly obtain\ncamera poses and achieve dense scene reconstruction, even in low-texture\nregions. Experiments show that our method achieves state-of-the-art\ncross-dataset reconstruction on five zero-shot testing datasets.",
        "translated": ""
    },
    {
        "title": "Rethinking Integration of Prediction and Planning in Deep Learning-Based\n  Automated Driving Systems: A Review",
        "url": "http://arxiv.org/abs/2308.05731v1",
        "pub_date": "2023-08-10",
        "summary": "Automated driving has the potential to revolutionize personal, public, and\nfreight mobility. Besides the enormous challenge of perception, i.e. accurately\nperceiving the environment using available sensor data, automated driving\ncomprises planning a safe, comfortable, and efficient motion trajectory. To\npromote safety and progress, many works rely on modules that predict the future\nmotion of surrounding traffic. Modular automated driving systems commonly\nhandle prediction and planning as sequential separate tasks. While this\naccounts for the influence of surrounding traffic on the ego-vehicle, it fails\nto anticipate the reactions of traffic participants to the ego-vehicle's\nbehavior. Recent works suggest that integrating prediction and planning in an\ninterdependent joint step is necessary to achieve safe, efficient, and\ncomfortable driving. While various models implement such integrated systems, a\ncomprehensive overview and theoretical understanding of different principles\nare lacking. We systematically review state-of-the-art deep learning-based\nprediction, planning, and integrated prediction and planning models. Different\nfacets of the integration ranging from model architecture and model design to\nbehavioral aspects are considered and related to each other. Moreover, we\ndiscuss the implications, strengths, and limitations of different integration\nmethods. By pointing out research gaps, describing relevant future challenges,\nand highlighting trends in the research field, we identify promising directions\nfor future research.",
        "translated": ""
    },
    {
        "title": "Deformable Mixer Transformer with Gating for Multi-Task Learning of\n  Dense Prediction",
        "url": "http://arxiv.org/abs/2308.05721v1",
        "pub_date": "2023-08-10",
        "summary": "CNNs and Transformers have their own advantages and both have been widely\nused for dense prediction in multi-task learning (MTL). Most of the current\nstudies on MTL solely rely on CNN or Transformer. In this work, we present a\nnovel MTL model by combining both merits of deformable CNN and query-based\nTransformer with shared gating for multi-task learning of dense prediction.\nThis combination may offer a simple and efficient solution owing to its\npowerful and flexible task-specific learning and advantages of lower cost, less\ncomplexity and smaller parameters than the traditional MTL methods. We\nintroduce deformable mixer Transformer with gating (DeMTG), a simple and\neffective encoder-decoder architecture up-to-date that incorporates the\nconvolution and attention mechanism in a unified network for MTL. It is\nexquisitely designed to use advantages of each block, and provide deformable\nand comprehensive features for all tasks from local and global perspective.\nFirst, the deformable mixer encoder contains two types of operators: the\nchannel-aware mixing operator leveraged to allow communication among different\nchannels, and the spatial-aware deformable operator with deformable convolution\napplied to efficiently sample more informative spatial locations. Second, the\ntask-aware gating transformer decoder is used to perform the task-specific\npredictions, in which task interaction block integrated with self-attention is\napplied to capture task interaction features, and the task query block\nintegrated with gating attention is leveraged to select corresponding\ntask-specific features. Further, the experiment results demonstrate that the\nproposed DeMTG uses fewer GFLOPs and significantly outperforms current\nTransformer-based and CNN-based competitive models on a variety of metrics on\nthree dense prediction datasets. Our code and models are available at\nhttps://github.com/yangyangxu0/DeMTG.",
        "translated": ""
    },
    {
        "title": "Shadow Datasets, New challenging datasets for Causal Representation\n  Learning",
        "url": "http://arxiv.org/abs/2308.05707v1",
        "pub_date": "2023-08-10",
        "summary": "Discovering causal relations among semantic factors is an emergent topic in\nrepresentation learning. Most causal representation learning (CRL) methods are\nfully supervised, which is impractical due to costly labeling. To resolve this\nrestriction, weakly supervised CRL methods were introduced. To evaluate CRL\nperformance, four existing datasets, Pendulum, Flow, CelebA(BEARD) and\nCelebA(SMILE), are utilized. However, existing CRL datasets are limited to\nsimple graphs with few generative factors. Thus we propose two new datasets\nwith a larger number of diverse generative factors and more sophisticated\ncausal graphs. In addition, current real datasets, CelebA(BEARD) and\nCelebA(SMILE), the originally proposed causal graphs are not aligned with the\ndataset distributions. Thus, we propose modifications to them.",
        "translated": ""
    },
    {
        "title": "FunnyBirds: A Synthetic Vision Dataset for a Part-Based Analysis of\n  Explainable AI Methods",
        "url": "http://arxiv.org/abs/2308.06248v1",
        "pub_date": "2023-08-11",
        "summary": "The field of explainable artificial intelligence (XAI) aims to uncover the\ninner workings of complex deep neural models. While being crucial for\nsafety-critical domains, XAI inherently lacks ground-truth explanations, making\nits automatic evaluation an unsolved problem. We address this challenge by\nproposing a novel synthetic vision dataset, named FunnyBirds, and accompanying\nautomatic evaluation protocols. Our dataset allows performing semantically\nmeaningful image interventions, e.g., removing individual object parts, which\nhas three important implications. First, it enables analyzing explanations on a\npart level, which is closer to human comprehension than existing methods that\nevaluate on a pixel level. Second, by comparing the model output for inputs\nwith removed parts, we can estimate ground-truth part importances that should\nbe reflected in the explanations. Third, by mapping individual explanations\ninto a common space of part importances, we can analyze a variety of different\nexplanation types in a single common framework. Using our tools, we report\nresults for 24 different combinations of neural models and XAI methods,\ndemonstrating the strengths and weaknesses of the assessed methods in a fully\nautomatic and systematic manner.",
        "translated": ""
    },
    {
        "title": "Continual Face Forgery Detection via Historical Distribution Preserving",
        "url": "http://arxiv.org/abs/2308.06217v1",
        "pub_date": "2023-08-11",
        "summary": "Face forgery techniques have advanced rapidly and pose serious security\nthreats. Existing face forgery detection methods try to learn generalizable\nfeatures, but they still fall short of practical application. Additionally,\nfinetuning these methods on historical training data is resource-intensive in\nterms of time and storage. In this paper, we focus on a novel and challenging\nproblem: Continual Face Forgery Detection (CFFD), which aims to efficiently\nlearn from new forgery attacks without forgetting previous ones. Specifically,\nwe propose a Historical Distribution Preserving (HDP) framework that reserves\nand preserves the distributions of historical faces. To achieve this, we use\nuniversal adversarial perturbation (UAP) to simulate historical forgery\ndistribution, and knowledge distillation to maintain the distribution variation\nof real faces across different models. We also construct a new benchmark for\nCFFD with three evaluation protocols. Our extensive experiments on the\nbenchmarks show that our method outperforms the state-of-the-art competitors.",
        "translated": ""
    },
    {
        "title": "Exploring Predicate Visual Context in Detecting of Human-Object\n  Interactions",
        "url": "http://arxiv.org/abs/2308.06202v1",
        "pub_date": "2023-08-11",
        "summary": "Recently, the DETR framework has emerged as the dominant approach for\nhuman--object interaction (HOI) research. In particular, two-stage\ntransformer-based HOI detectors are amongst the most performant and\ntraining-efficient approaches. However, these often condition HOI\nclassification on object features that lack fine-grained contextual\ninformation, eschewing pose and orientation information in favour of visual\ncues about object identity and box extremities. This naturally hinders the\nrecognition of complex or ambiguous interactions. In this work, we study these\nissues through visualisations and carefully designed experiments. Accordingly,\nwe investigate how best to re-introduce image features via cross-attention.\nWith an improved query design, extensive exploration of keys and values, and\nbox pair positional embeddings as spatial guidance, our model with enhanced\npredicate visual context (PViC) outperforms state-of-the-art methods on the\nHICO-DET and V-COCO benchmarks, while maintaining low training cost.",
        "translated": ""
    },
    {
        "title": "DIG In: Evaluating Disparities in Image Generations with Indicators for\n  Geographic Diversity",
        "url": "http://arxiv.org/abs/2308.06198v1",
        "pub_date": "2023-08-11",
        "summary": "The unprecedented photorealistic results achieved by recent text-to-image\ngenerative systems and their increasing use as plug-and-play content creation\nsolutions make it crucial to understand their potential biases. In this work,\nwe introduce three indicators to evaluate the realism, diversity and\nprompt-generation consistency of text-to-image generative systems when prompted\nto generate objects from across the world. Our indicators complement\nqualitative analysis of the broader impact of such systems by enabling\nautomatic and efficient benchmarking of geographic disparities, an important\nstep towards building responsible visual content creation systems. We use our\nproposed indicators to analyze potential geographic biases in state-of-the-art\nvisual content creation systems and find that: (1) models have less realism and\ndiversity of generations when prompting for Africa and West Asia than Europe,\n(2) prompting with geographic information comes at a cost to prompt-consistency\nand diversity of generated images, and (3) models exhibit more region-level\ndisparities for some objects than others. Perhaps most interestingly, our\nindicators suggest that progress in image generation quality has come at the\ncost of real-world geographic representation. Our comprehensive evaluation\nconstitutes a crucial step towards ensuring a positive experience of visual\ncontent creation for everyone.",
        "translated": ""
    },
    {
        "title": "Complex Facial Expression Recognition Using Deep Knowledge Distillation\n  of Basic Features",
        "url": "http://arxiv.org/abs/2308.06197v1",
        "pub_date": "2023-08-11",
        "summary": "Complex emotion recognition is a cognitive task that has so far eluded the\nsame excellent performance of other tasks that are at or above the level of\nhuman cognition. Emotion recognition through facial expressions is particularly\ndifficult due to the complexity of emotions expressed by the human face. For a\nmachine to approach the same level of performance in this domain as a human, it\nmay need to synthesise knowledge and understand new concepts in real-time as\nhumans do. Humans are able to learn new concepts using only few examples, by\ndistilling the important information from memories and discarding the rest.\nSimilarly, continual learning methods learn new classes whilst retaining the\nknowledge of known classes, whilst few-shot learning methods are able to learn\nnew classes using very few training examples. We propose a novel continual\nlearning method inspired by human cognition and learning that can accurately\nrecognise new compound expression classes using few training samples, by\nbuilding on and retaining its knowledge of basic expression classes. Using\nGradCAM visualisations, we demonstrate the relationship between basic and\ncompound facial expressions, which our method leverages through knowledge\ndistillation and a novel Predictive Sorting Memory Replay. Our method achieves\nthe current state-of-the-art in continual learning for complex facial\nexpression recognition with 74.28% Overall Accuracy on new classes. We also\ndemonstrate that using continual learning for complex facial expression\nrecognition achieves far better performance than non-continual learning\nmethods, improving on state-of-the-art non-continual learning methods by\n13.95%. To the best of our knowledge, our work is also the first to apply\nfew-shot learning to complex facial expression recognition, achieving the\nstate-of-the-art with 100% accuracy using a single training sample for each\nexpression class.",
        "translated": ""
    },
    {
        "title": "Physical Adversarial Attacks For Camera-based Smart Systems: Current\n  Trends, Categorization, Applications, Research Challenges, and Future Outlook",
        "url": "http://arxiv.org/abs/2308.06173v1",
        "pub_date": "2023-08-11",
        "summary": "In this paper, we present a comprehensive survey of the current trends\nfocusing specifically on physical adversarial attacks. We aim to provide a\nthorough understanding of the concept of physical adversarial attacks,\nanalyzing their key characteristics and distinguishing features. Furthermore,\nwe explore the specific requirements and challenges associated with executing\nattacks in the physical world. Our article delves into various physical\nadversarial attack methods, categorized according to their target tasks in\ndifferent applications, including classification, detection, face recognition,\nsemantic segmentation and depth estimation. We assess the performance of these\nattack methods in terms of their effectiveness, stealthiness, and robustness.\nWe examine how each technique strives to ensure the successful manipulation of\nDNNs while mitigating the risk of detection and withstanding real-world\ndistortions. Lastly, we discuss the current challenges and outline potential\nfuture research directions in the field of physical adversarial attacks. We\nhighlight the need for enhanced defense mechanisms, the exploration of novel\nattack strategies, the evaluation of attacks in different application domains,\nand the establishment of standardized benchmarks and evaluation criteria for\nphysical adversarial attacks. Through this comprehensive survey, we aim to\nprovide a valuable resource for researchers, practitioners, and policymakers to\ngain a holistic understanding of physical adversarial attacks in computer\nvision and facilitate the development of robust and secure DNN-based systems.",
        "translated": ""
    },
    {
        "title": "Rethinking the Localization in Weakly Supervised Object Localization",
        "url": "http://arxiv.org/abs/2308.06161v1",
        "pub_date": "2023-08-11",
        "summary": "Weakly supervised object localization (WSOL) is one of the most popular and\nchallenging tasks in computer vision. This task is to localize the objects in\nthe images given only the image-level supervision. Recently, dividing WSOL into\ntwo parts (class-agnostic object localization and object classification) has\nbecome the state-of-the-art pipeline for this task. However, existing solutions\nunder this pipeline usually suffer from the following drawbacks: 1) they are\nnot flexible since they can only localize one object for each image due to the\nadopted single-class regression (SCR) for localization; 2) the generated pseudo\nbounding boxes may be noisy, but the negative impact of such noise is not well\naddressed. To remedy these drawbacks, we first propose to replace SCR with a\nbinary-class detector (BCD) for localizing multiple objects, where the detector\nis trained by discriminating the foreground and background. Then we design a\nweighted entropy (WE) loss using the unlabeled data to reduce the negative\nimpact of noisy bounding boxes. Extensive experiments on the popular\nCUB-200-2011 and ImageNet-1K datasets demonstrate the effectiveness of our\nmethod.",
        "translated": ""
    },
    {
        "title": "DatasetDM: Synthesizing Data with Perception Annotations Using Diffusion\n  Models",
        "url": "http://arxiv.org/abs/2308.06160v1",
        "pub_date": "2023-08-11",
        "summary": "Current deep networks are very data-hungry and benefit from training on\nlargescale datasets, which are often time-consuming to collect and annotate. By\ncontrast, synthetic data can be generated infinitely using generative models\nsuch as DALL-E and diffusion models, with minimal effort and cost. In this\npaper, we present DatasetDM, a generic dataset generation model that can\nproduce diverse synthetic images and the corresponding high-quality perception\nannotations (e.g., segmentation masks, and depth). Our method builds upon the\npre-trained diffusion model and extends text-guided image synthesis to\nperception data generation. We show that the rich latent code of the diffusion\nmodel can be effectively decoded as accurate perception annotations using a\ndecoder module. Training the decoder only needs less than 1% (around 100\nimages) manually labeled images, enabling the generation of an infinitely large\nannotated dataset. Then these synthetic data can be used for training various\nperception models for downstream tasks. To showcase the power of the proposed\napproach, we generate datasets with rich dense pixel-wise labels for a wide\nrange of downstream tasks, including semantic segmentation, instance\nsegmentation, and depth estimation. Notably, it achieves 1) state-of-the-art\nresults on semantic segmentation and instance segmentation; 2) significantly\nmore robust on domain generalization than using the real data alone; and\nstate-of-the-art results in zero-shot segmentation setting; and 3) flexibility\nfor efficient application and novel task composition (e.g., image editing). The\nproject website and code can be found at\nhttps://weijiawu.github.io/DatasetDM_page/ and\nhttps://github.com/showlab/DatasetDM, respectively",
        "translated": ""
    },
    {
        "title": "Efficient Large-scale AUV-based Visual Seafloor Mapping",
        "url": "http://arxiv.org/abs/2308.06147v1",
        "pub_date": "2023-08-11",
        "summary": "Driven by the increasing number of marine data science applications, there is\na growing interest in surveying and exploring the vast, uncharted terrain of\nthe deep sea with robotic platforms. Despite impressive results achieved by\nmany on-land visual mapping algorithms in the past decades, transferring these\nmethods from land to the deep sea remains a challenge due to harsh\nenvironmental conditions. Typically, deep-sea exploration involves the use of\nautonomous underwater vehicles (AUVs) equipped with high-resolution cameras and\nartificial illumination systems. However, images obtained in this manner often\nsuffer from heterogeneous illumination and quality degradation due to\nattenuation and scattering, on top of refraction of light rays. All of this\ntogether often lets on-land SLAM approaches fail underwater or makes\nStructure-from-Motion approaches drift or omit difficult images, resulting in\ngaps, jumps or weakly registered areas. In this work, we present a system that\nincorporates recent developments in underwater imaging and visual mapping to\nfacilitate automated robotic 3D reconstruction of hectares of seafloor. Our\napproach is efficient in that it detects and reconsiders difficult, weakly\nregistered areas, to avoid omitting images and to make better use of limited\ndive time; on the other hand it is computationally efficient; leveraging a\nhybrid approach combining benefits from SLAM and Structure-from-Motion that\nruns much faster than incremental reconstructions while achieving at least\non-par performance. The proposed system has been extensively tested and\nevaluated during several research cruises, demonstrating its robustness and\npracticality in real-world conditions.",
        "translated": ""
    },
    {
        "title": "CompTLL-UNet: Compressed Domain Text-Line Localization in Challenging\n  Handwritten Documents using Deep Feature Learning from JPEG Coefficients",
        "url": "http://arxiv.org/abs/2308.06142v1",
        "pub_date": "2023-08-11",
        "summary": "Automatic localization of text-lines in handwritten documents is still an\nopen and challenging research problem. Various writing issues such as uneven\nspacing between the lines, oscillating and touching text, and the presence of\nskew become much more challenging when the case of complex handwritten document\nimages are considered for segmentation directly in their respective compressed\nrepresentation. This is because, the conventional way of processing compressed\ndocuments is through decompression, but here in this paper, we propose an idea\nthat employs deep feature learning directly from the JPEG compressed\ncoefficients without full decompression to accomplish text-line localization in\nthe JPEG compressed domain. A modified U-Net architecture known as Compressed\nText-Line Localization Network (CompTLL-UNet) is designed to accomplish it. The\nmodel is trained and tested with JPEG compressed version of benchmark datasets\nincluding ICDAR2017 (cBAD) and ICDAR2019 (cBAD), reporting the state-of-the-art\nperformance with reduced storage and computational costs in the JPEG compressed\ndomain.",
        "translated": ""
    },
    {
        "title": "Jurassic World Remake: Bringing Ancient Fossils Back to Life via\n  Zero-Shot Long Image-to-Image Translation",
        "url": "http://arxiv.org/abs/2308.07316v1",
        "pub_date": "2023-08-14",
        "summary": "With a strong understanding of the target domain from natural language, we\nproduce promising results in translating across large domain gaps and bringing\nskeletons back to life. In this work, we use text-guided latent diffusion\nmodels for zero-shot image-to-image translation (I2I) across large domain gaps\n(longI2I), where large amounts of new visual features and new geometry need to\nbe generated to enter the target domain. Being able to perform translations\nacross large domain gaps has a wide variety of real-world applications in\ncriminology, astrology, environmental conservation, and paleontology. In this\nwork, we introduce a new task Skull2Animal for translating between skulls and\nliving animals. On this task, we find that unguided Generative Adversarial\nNetworks (GANs) are not capable of translating across large domain gaps.\nInstead of these traditional I2I methods, we explore the use of guided\ndiffusion and image editing models and provide a new benchmark model,\nRevive-2I, capable of performing zero-shot I2I via text-prompting latent\ndiffusion models. We find that guidance is necessary for longI2I because, to\nbridge the large domain gap, prior knowledge about the target domain is needed.\nIn addition, we find that prompting provides the best and most scalable\ninformation about the target domain as classifier-guided diffusion models\nrequire retraining for specific use cases and lack stronger constraints on the\ntarget domain because of the wide variety of images they are trained on.",
        "translated": ""
    },
    {
        "title": "Dual Associated Encoder for Face Restoration",
        "url": "http://arxiv.org/abs/2308.07314v1",
        "pub_date": "2023-08-14",
        "summary": "Restoring facial details from low-quality (LQ) images has remained a\nchallenging problem due to its ill-posedness induced by various degradations in\nthe wild. The existing codebook prior mitigates the ill-posedness by leveraging\nan autoencoder and learned codebook of high-quality (HQ) features, achieving\nremarkable quality. However, existing approaches in this paradigm frequently\ndepend on a single encoder pre-trained on HQ data for restoring HQ images,\ndisregarding the domain gap between LQ and HQ images. As a result, the encoding\nof LQ inputs may be insufficient, resulting in suboptimal performance. To\ntackle this problem, we propose a novel dual-branch framework named DAEFR. Our\nmethod introduces an auxiliary LQ branch that extracts crucial information from\nthe LQ inputs. Additionally, we incorporate association training to promote\neffective synergy between the two branches, enhancing code prediction and\noutput quality. We evaluate the effectiveness of DAEFR on both synthetic and\nreal-world datasets, demonstrating its superior performance in restoring facial\ndetails.",
        "translated": ""
    },
    {
        "title": "Group Pose: A Simple Baseline for End-to-End Multi-person Pose\n  Estimation",
        "url": "http://arxiv.org/abs/2308.07313v1",
        "pub_date": "2023-08-14",
        "summary": "In this paper, we study the problem of end-to-end multi-person pose\nestimation. State-of-the-art solutions adopt the DETR-like framework, and\nmainly develop the complex decoder, e.g., regarding pose estimation as keypoint\nbox detection and combining with human detection in ED-Pose, hierarchically\npredicting with pose decoder and joint (keypoint) decoder in PETR. We present a\nsimple yet effective transformer approach, named Group Pose. We simply regard\n$K$-keypoint pose estimation as predicting a set of $N\\times K$ keypoint\npositions, each from a keypoint query, as well as representing each pose with\nan instance query for scoring $N$ pose predictions. Motivated by the intuition\nthat the interaction, among across-instance queries of different types, is not\ndirectly helpful, we make a simple modification to decoder self-attention. We\nreplace single self-attention over all the $N\\times(K+1)$ queries with two\nsubsequent group self-attentions: (i) $N$ within-instance self-attention, with\neach over $K$ keypoint queries and one instance query, and (ii) $(K+1)$\nsame-type across-instance self-attention, each over $N$ queries of the same\ntype. The resulting decoder removes the interaction among across-instance\ntype-different queries, easing the optimization and thus improving the\nperformance. Experimental results on MS COCO and CrowdPose show that our\napproach without human box supervision is superior to previous methods with\ncomplex decoders, and even is slightly better than ED-Pose that uses human box\nsupervision. $\\href{https://github.com/Michel-liu/GroupPose-Paddle}{\\rm\nPaddle}$ and $\\href{https://github.com/Michel-liu/GroupPose}{\\rm PyTorch}$ code\nare available.",
        "translated": ""
    },
    {
        "title": "A Unified Masked Autoencoder with Patchified Skeletons for Motion\n  Synthesis",
        "url": "http://arxiv.org/abs/2308.07301v1",
        "pub_date": "2023-08-14",
        "summary": "The synthesis of human motion has traditionally been addressed through\ntask-dependent models that focus on specific challenges, such as predicting\nfuture motions or filling in intermediate poses conditioned on known key-poses.\nIn this paper, we present a novel task-independent model called UNIMASK-M,\nwhich can effectively address these challenges using a unified architecture.\nOur model obtains comparable or better performance than the state-of-the-art in\neach field. Inspired by Vision Transformers (ViTs), our UNIMASK-M model\ndecomposes a human pose into body parts to leverage the spatio-temporal\nrelationships existing in human motion. Moreover, we reformulate various\npose-conditioned motion synthesis tasks as a reconstruction problem with\ndifferent masking patterns given as input. By explicitly informing our model\nabout the masked joints, our UNIMASK-M becomes more robust to occlusions.\nExperimental results show that our model successfully forecasts human motion on\nthe Human3.6M dataset. Moreover, it achieves state-of-the-art results in motion\ninbetweening on the LaFAN1 dataset, particularly in long transition periods.\nMore information can be found on the project website\nhttps://sites.google.com/view/estevevallsmascaro/publications/unimask-m.",
        "translated": ""
    },
    {
        "title": "Accurate Eye Tracking from Dense 3D Surface Reconstructions using\n  Single-Shot Deflectometry",
        "url": "http://arxiv.org/abs/2308.07298v1",
        "pub_date": "2023-08-14",
        "summary": "Eye-tracking plays a crucial role in the development of virtual reality\ndevices, neuroscience research, and psychology. Despite its significance in\nnumerous applications, achieving an accurate, robust, and fast eye-tracking\nsolution remains a considerable challenge for current state-of-the-art methods.\nWhile existing reflection-based techniques (e.g., \"glint tracking\") are\nconsidered the most accurate, their performance is limited by their reliance on\nsparse 3D surface data acquired solely from the cornea surface. In this paper,\nwe rethink the way how specular reflections can be used for eye tracking: We\npropose a novel method for accurate and fast evaluation of the gaze direction\nthat exploits teachings from single-shot phase-measuring-deflectometry (PMD).\nIn contrast to state-of-the-art reflection-based methods, our method acquires\ndense 3D surface information of both cornea and sclera within only one single\ncamera frame (single-shot). Improvements in acquired reflection surface\npoints(\"glints\") of factors $&gt;3300 \\times$ are easily achievable. We show the\nfeasibility of our approach with experimentally evaluated gaze errors of only\n$\\leq 0.25^\\circ$ demonstrating a significant improvement over the current\nstate-of-the-art.",
        "translated": ""
    },
    {
        "title": "A Robust Approach Towards Distinguishing Natural and Computer Generated\n  Images using Multi-Colorspace fused and Enriched Vision Transformer",
        "url": "http://arxiv.org/abs/2308.07279v1",
        "pub_date": "2023-08-14",
        "summary": "The works in literature classifying natural and computer generated images are\nmostly designed as binary tasks either considering natural images versus\ncomputer graphics images only or natural images versus GAN generated images\nonly, but not natural images versus both classes of the generated images. Also,\neven though this forensic classification task of distinguishing natural and\ncomputer generated images gets the support of the new convolutional neural\nnetworks and transformer based architectures that can give remarkable\nclassification accuracies, they are seen to fail over the images that have\nundergone some post-processing operations usually performed to deceive the\nforensic algorithms, such as JPEG compression, gaussian noise, etc. This work\nproposes a robust approach towards distinguishing natural and computer\ngenerated images including both, computer graphics and GAN generated images\nusing a fusion of two vision transformers where each of the transformer\nnetworks operates in different color spaces, one in RGB and the other in YCbCr\ncolor space. The proposed approach achieves high performance gain when compared\nto a set of baselines, and also achieves higher robustness and generalizability\nthan the baselines. The features of the proposed model when visualized are seen\nto obtain higher separability for the classes than the input image features and\nthe baseline features. This work also studies the attention map visualizations\nof the networks of the fused model and observes that the proposed methodology\ncan capture more image information relevant to the forensic task of classifying\nnatural and generated images.",
        "translated": ""
    },
    {
        "title": "EasyEdit: An Easy-to-use Knowledge Editing Framework for Large Language\n  Models",
        "url": "http://arxiv.org/abs/2308.07269v1",
        "pub_date": "2023-08-14",
        "summary": "Large Language Models (LLMs) usually suffer from knowledge cutoff or fallacy\nissues, which means they are unaware of unseen events or generate text with\nincorrect facts owing to the outdated/noisy data. To this end, many knowledge\nediting approaches for LLMs have emerged -- aiming to subtly inject/edit\nupdated knowledge or adjust undesired behavior while minimizing the impact on\nunrelated inputs. Nevertheless, due to significant differences among various\nknowledge editing methods and the variations in task setups, there is no\nstandard implementation framework available for the community, which hinders\npractitioners to apply knowledge editing to applications. To address these\nissues, we propose EasyEdit, an easy-to-use knowledge editing framework for\nLLMs. It supports various cutting-edge knowledge editing approaches and can be\nreadily apply to many well-known LLMs such as T5, GPT-J, LlaMA, etc.\nEmpirically, we report the knowledge editing results on LlaMA-2 with EasyEdit,\ndemonstrating that knowledge editing surpasses traditional fine-tuning in terms\nof reliability and generalization. We have released the source code on GitHub\nat https://github.com/zjunlp/EasyEdit, along with Google Colab tutorials and\ncomprehensive documentation for beginners to get started. Besides, we present\nan online system for real-time knowledge editing, and a demo video at\nhttp://knowlm.zjukg.cn/easyedit.mp4.",
        "translated": ""
    },
    {
        "title": "Diving with Penguins: Detecting Penguins and their Prey in Animal-borne\n  Underwater Videos via Deep Learning",
        "url": "http://arxiv.org/abs/2308.07267v1",
        "pub_date": "2023-08-14",
        "summary": "African penguins (Spheniscus demersus) are an endangered species. Little is\nknown regarding their underwater hunting strategies and associated predation\nsuccess rates, yet this is essential for guiding conservation. Modern\nbio-logging technology has the potential to provide valuable insights, but\nmanually analysing large amounts of data from animal-borne video recorders\n(AVRs) is time-consuming. In this paper, we publish an animal-borne underwater\nvideo dataset of penguins and introduce a ready-to-deploy deep learning system\ncapable of robustly detecting penguins (mAP50@98.0%) and also instances of fish\n(mAP50@73.3%). We note that the detectors benefit explicitly from air-bubble\nlearning to improve accuracy. Extending this detector towards a dual-stream\nbehaviour recognition network, we also provide the first results for\nidentifying predation behaviour in penguin underwater videos. Whilst results\nare promising, further work is required for useful applicability of predation\nbehaviour detection in field scenarios. In summary, we provide a highly\nreliable underwater penguin detector, a fish detector, and a valuable first\nattempt towards an automated visual detection of complex behaviours in a marine\npredator. We publish the networks, the DivingWithPenguins video dataset,\nannotations, splits, and weights for full reproducibility and immediate\nusability by practitioners.",
        "translated": ""
    },
    {
        "title": "Efficient Real-time Smoke Filtration with 3D LiDAR for Search and Rescue\n  with Autonomous Heterogeneous Robotic Systems",
        "url": "http://arxiv.org/abs/2308.07264v1",
        "pub_date": "2023-08-14",
        "summary": "Search and Rescue (SAR) missions in harsh and unstructured Sub-Terranean\n(Sub-T) environments in the presence of aerosol particles have recently become\nthe main focus in the field of robotics. Aerosol particles such as smoke and\ndust directly affect the performance of any mobile robotic platform due to\ntheir reliance on their onboard perception systems for autonomous navigation\nand localization in Global Navigation Satellite System (GNSS)-denied\nenvironments. Although obstacle avoidance and object detection algorithms are\nrobust to the presence of noise to some degree, their performance directly\nrelies on the quality of captured data by onboard sensors such as Light\nDetection And Ranging (LiDAR) and camera. Thus, this paper proposes a novel\nmodular agnostic filtration pipeline based on intensity and spatial information\nsuch as local point density for removal of detected smoke particles from Point\nCloud (PCL) prior to its utilization for collision detection. Furthermore, the\nefficacy of the proposed framework in the presence of smoke during multiple\nfrontier exploration missions is investigated while the experimental results\nare presented to facilitate comparison with other methodologies and their\ncomputational impact. This provides valuable insight to the research community\nfor better utilization of filtration schemes based on available computation\nresources while considering the safe autonomous navigation of mobile robots.",
        "translated": ""
    },
    {
        "title": "Large-kernel Attention for Efficient and Robust Brain Lesion\n  Segmentation",
        "url": "http://arxiv.org/abs/2308.07251v1",
        "pub_date": "2023-08-14",
        "summary": "Vision transformers are effective deep learning models for vision tasks,\nincluding medical image segmentation. However, they lack efficiency and\ntranslational invariance, unlike convolutional neural networks (CNNs). To model\nlong-range interactions in 3D brain lesion segmentation, we propose an\nall-convolutional transformer block variant of the U-Net architecture. We\ndemonstrate that our model provides the greatest compromise in three factors:\nperformance competitive with the state-of-the-art; parameter efficiency of a\nCNN; and the favourable inductive biases of a transformer. Our public\nimplementation is available at https://github.com/liamchalcroft/MDUNet .",
        "translated": ""
    },
    {
        "title": "CoDeF: Content Deformation Fields for Temporally Consistent Video\n  Processing",
        "url": "http://arxiv.org/abs/2308.07926v1",
        "pub_date": "2023-08-15",
        "summary": "We present the content deformation field CoDeF as a new type of video\nrepresentation, which consists of a canonical content field aggregating the\nstatic contents in the entire video and a temporal deformation field recording\nthe transformations from the canonical image (i.e., rendered from the canonical\ncontent field) to each individual frame along the time axis.Given a target\nvideo, these two fields are jointly optimized to reconstruct it through a\ncarefully tailored rendering pipeline.We advisedly introduce some\nregularizations into the optimization process, urging the canonical content\nfield to inherit semantics (e.g., the object shape) from the video.With such a\ndesign, CoDeF naturally supports lifting image algorithms for video processing,\nin the sense that one can apply an image algorithm to the canonical image and\neffortlessly propagate the outcomes to the entire video with the aid of the\ntemporal deformation field.We experimentally show that CoDeF is able to lift\nimage-to-image translation to video-to-video translation and lift keypoint\ndetection to keypoint tracking without any training.More importantly, thanks to\nour lifting strategy that deploys the algorithms on only one image, we achieve\nsuperior cross-frame consistency in processed videos compared to existing\nvideo-to-video translation approaches, and even manage to track non-rigid\nobjects like water and smog.Project page can be found at\nhttps://qiuyu96.github.io/CoDeF/.",
        "translated": ""
    },
    {
        "title": "Solving Challenging Math Word Problems Using GPT-4 Code Interpreter with\n  Code-based Self-Verification",
        "url": "http://arxiv.org/abs/2308.07921v1",
        "pub_date": "2023-08-15",
        "summary": "Recent progress in large language models (LLMs) like GPT-4 and PaLM-2 has\nbrought significant advancements in addressing math reasoning problems. In\nparticular, OpenAI's latest version of GPT-4, known as GPT-4 Code Interpreter,\nshows remarkable performance on challenging math datasets. In this paper, we\nexplore the effect of code on enhancing LLMs' reasoning capability by\nintroducing different constraints on the \\textit{Code Usage Frequency} of GPT-4\nCode Interpreter. We found that its success can be largely attributed to its\npowerful skills in generating and executing code, evaluating the output of code\nexecution, and rectifying its solution when receiving unreasonable outputs.\nBased on this insight, we propose a novel and effective prompting method,\nexplicit \\uline{c}ode-based \\uline{s}elf-\\uline{v}erification~(CSV), to further\nboost the mathematical reasoning potential of GPT-4 Code Interpreter. This\nmethod employs a zero-shot prompt on GPT-4 Code Interpreter to encourage it to\nuse code to self-verify its answers. In instances where the verification state\nregisters as ``False'', the model shall automatically amend its solution,\nanalogous to our approach of rectifying errors during a mathematics\nexamination. Furthermore, we recognize that the states of the verification\nresult indicate the confidence of a solution, which can improve the\neffectiveness of majority voting. With GPT-4 Code Interpreter and CSV, we\nachieve an impressive zero-shot accuracy on MATH dataset \\textbf{(53.9\\% $\\to$\n84.3\\%)}.",
        "translated": ""
    },
    {
        "title": "Helping Hands: An Object-Aware Ego-Centric Video Recognition Model",
        "url": "http://arxiv.org/abs/2308.07918v1",
        "pub_date": "2023-08-15",
        "summary": "We introduce an object-aware decoder for improving the performance of\nspatio-temporal representations on ego-centric videos. The key idea is to\nenhance object-awareness during training by tasking the model to predict hand\npositions, object positions, and the semantic label of the objects using paired\ncaptions when available. At inference time the model only requires RGB frames\nas inputs, and is able to track and ground objects (although it has not been\ntrained explicitly for this). We demonstrate the performance of the\nobject-aware representations learnt by our model, by: (i) evaluating it for\nstrong transfer, i.e. through zero-shot testing, on a number of downstream\nvideo-text retrieval and classification benchmarks; and (ii) by using the\nrepresentations learned as input for long-term video understanding tasks (e.g.\nEpisodic Memory in Ego4D). In all cases the performance improves over the state\nof the art -- even compared to networks trained with far larger batch sizes. We\nalso show that by using noisy image-level detection as pseudo-labels in\ntraining, the model learns to provide better bounding boxes using video\nconsistency, as well as grounding the words in the associated text\ndescriptions. Overall, we show that the model can act as a drop-in replacement\nfor an ego-centric video model to improve performance through visual-text\ngrounding.",
        "translated": ""
    },
    {
        "title": "Relightable and Animatable Neural Avatar from Sparse-View Video",
        "url": "http://arxiv.org/abs/2308.07903v1",
        "pub_date": "2023-08-15",
        "summary": "This paper tackles the challenge of creating relightable and animatable\nneural avatars from sparse-view (or even monocular) videos of dynamic humans\nunder unknown illumination. Compared to studio environments, this setting is\nmore practical and accessible but poses an extremely challenging ill-posed\nproblem. Previous neural human reconstruction methods are able to reconstruct\nanimatable avatars from sparse views using deformed Signed Distance Fields\n(SDF) but cannot recover material parameters for relighting. While\ndifferentiable inverse rendering-based methods have succeeded in material\nrecovery of static objects, it is not straightforward to extend them to dynamic\nhumans as it is computationally intensive to compute pixel-surface intersection\nand light visibility on deformed SDFs for inverse rendering. To solve this\nchallenge, we propose a Hierarchical Distance Query (HDQ) algorithm to\napproximate the world space distances under arbitrary human poses.\nSpecifically, we estimate coarse distances based on a parametric human model\nand compute fine distances by exploiting the local deformation invariance of\nSDF. Based on the HDQ algorithm, we leverage sphere tracing to efficiently\nestimate the surface intersection and light visibility. This allows us to\ndevelop the first system to recover animatable and relightable neural avatars\nfrom sparse view (or monocular) inputs. Experiments demonstrate that our\napproach is able to produce superior results compared to state-of-the-art\nmethods. Our code will be released for reproducibility.",
        "translated": ""
    },
    {
        "title": "A Foundation LAnguage-Image model of the Retina (FLAIR): Encoding expert\n  knowledge in text supervision",
        "url": "http://arxiv.org/abs/2308.07898v1",
        "pub_date": "2023-08-15",
        "summary": "Foundation vision-language models are currently transforming computer vision,\nand are on the rise in medical imaging fueled by their very promising\ngeneralization capabilities. However, the initial attempts to transfer this new\nparadigm to medical imaging have shown less impressive performances than those\nobserved in other domains, due to the significant domain shift and the complex,\nexpert domain knowledge inherent to medical-imaging tasks. Motivated by the\nneed for domain-expert foundation models, we present FLAIR, a pre-trained\nvision-language model for universal retinal fundus image understanding. To this\nend, we compiled 37 open-access, mostly categorical fundus imaging datasets\nfrom various sources, with up to 97 different target conditions and 284,660\nimages. We integrate the expert's domain knowledge in the form of descriptive\ntextual prompts, during both pre-training and zero-shot inference, enhancing\nthe less-informative categorical supervision of the data. Such a textual\nexpert's knowledge, which we compiled from the relevant clinical literature and\ncommunity standards, describes the fine-grained features of the pathologies as\nwell as the hierarchies and dependencies between them. We report comprehensive\nevaluations, which illustrate the benefit of integrating expert knowledge and\nthe strong generalization capabilities of FLAIR under difficult scenarios with\ndomain shifts or unseen categories. When adapted with a lightweight linear\nprobe, FLAIR outperforms fully-trained, dataset-focused models, more so in the\nfew-shot regimes. Interestingly, FLAIR outperforms by a large margin more\ngeneralist, larger-scale image-language models, which emphasizes the potential\nof embedding experts' domain knowledge and the limitations of generalist models\nin medical imaging.",
        "translated": ""
    },
    {
        "title": "Memory-and-Anticipation Transformer for Online Action Understanding",
        "url": "http://arxiv.org/abs/2308.07893v1",
        "pub_date": "2023-08-15",
        "summary": "Most existing forecasting systems are memory-based methods, which attempt to\nmimic human forecasting ability by employing various memory mechanisms and have\nprogressed in temporal modeling for memory dependency. Nevertheless, an obvious\nweakness of this paradigm is that it can only model limited historical\ndependence and can not transcend the past. In this paper, we rethink the\ntemporal dependence of event evolution and propose a novel\nmemory-anticipation-based paradigm to model an entire temporal structure,\nincluding the past, present, and future. Based on this idea, we present\nMemory-and-Anticipation Transformer (MAT), a memory-anticipation-based\napproach, to address the online action detection and anticipation tasks. In\naddition, owing to the inherent superiority of MAT, it can process online\naction detection and anticipation tasks in a unified manner. The proposed MAT\nmodel is tested on four challenging benchmarks TVSeries, THUMOS'14, HDD, and\nEPIC-Kitchens-100, for online action detection and anticipation tasks, and it\nsignificantly outperforms all existing methods. Code is available at\nhttps://github.com/Echo0125/Memory-and-Anticipation-Transformer.",
        "translated": ""
    },
    {
        "title": "Link-Context Learning for Multimodal LLMs",
        "url": "http://arxiv.org/abs/2308.07891v1",
        "pub_date": "2023-08-15",
        "summary": "The ability to learn from context with novel concepts, and deliver\nappropriate responses are essential in human conversations. Despite current\nMultimodal Large Language Models (MLLMs) and Large Language Models (LLMs) being\ntrained on mega-scale datasets, recognizing unseen images or understanding\nnovel concepts in a training-free manner remains a challenge. In-Context\nLearning (ICL) explores training-free few-shot learning, where models are\nencouraged to ``learn to learn\" from limited tasks and generalize to unseen\ntasks. In this work, we propose link-context learning (LCL), which emphasizes\n\"reasoning from cause and effect\" to augment the learning capabilities of\nMLLMs. LCL goes beyond traditional ICL by explicitly strengthening the causal\nrelationship between the support set and the query set. By providing\ndemonstrations with causal links, LCL guides the model to discern not only the\nanalogy but also the underlying causal associations between data points, which\nempowers MLLMs to recognize unseen images and understand novel concepts more\neffectively. To facilitate the evaluation of this novel approach, we introduce\nthe ISEKAI dataset, comprising exclusively of unseen generated image-label\npairs designed for link-context learning. Extensive experiments show that our\nLCL-MLLM exhibits strong link-context learning capabilities to novel concepts\nover vanilla MLLMs. Code and data will be released at\nhttps://github.com/isekai-portal/Link-Context-Learning.",
        "translated": ""
    },
    {
        "title": "The Challenge of Fetal Cardiac MRI Reconstruction Using Deep Learning",
        "url": "http://arxiv.org/abs/2308.07885v1",
        "pub_date": "2023-08-15",
        "summary": "Dynamic free-breathing fetal cardiac MRI is one of the most challenging\nmodalities, which requires high temporal and spatial resolution to depict rapid\nchanges in a small fetal heart. The ability of deep learning methods to recover\nundersampled data could help to optimise the kt-SENSE acquisition strategy and\nimprove non-gated kt-SENSE reconstruction quality. In this work, we explore\nsupervised deep learning networks for reconstruction of kt-SENSE style acquired\ndata using an extensive in vivo dataset. Having access to fully-sampled\nlow-resolution multi-coil fetal cardiac MRI, we study the performance of the\nnetworks to recover fully-sampled data from undersampled data. We consider\nmodel architectures together with training strategies taking into account their\napplication in the real clinical setup used to collect the dataset to enable\nnetworks to recover prospectively undersampled data. We explore a set of\nmodifications to form a baseline performance evaluation for dynamic fetal\ncardiac MRI on real data. We systematically evaluate the models on\ncoil-combined data to reveal the effect of the suggested changes to the\narchitecture in the context of fetal heart properties. We show that the\nbest-performers recover a detailed depiction of the maternal anatomy on a large\nscale, but the dynamic properties of the fetal heart are under-represented.\nTraining directly on multi-coil data improves the performance of the models,\nallows their prospective application to undersampled data and makes them\noutperform CTFNet introduced for adult cardiac cine MRI. However, these models\ndeliver similar qualitative performances recovering the maternal body very well\nbut underestimating the dynamic properties of fetal heart. This dynamic feature\nof fast change of fetal heart that is highly localised suggests both more\ntargeted training and evaluation methods might be needed for fetal heart\napplication.",
        "translated": ""
    },
    {
        "title": "SEDA: Self-Ensembling ViT with Defensive Distillation and Adversarial\n  Training for robust Chest X-rays Classification",
        "url": "http://arxiv.org/abs/2308.07874v1",
        "pub_date": "2023-08-15",
        "summary": "Deep Learning methods have recently seen increased adoption in medical\nimaging applications. However, elevated vulnerabilities have been explored in\nrecent Deep Learning solutions, which can hinder future adoption. Particularly,\nthe vulnerability of Vision Transformer (ViT) to adversarial, privacy, and\nconfidentiality attacks raise serious concerns about their reliability in\nmedical settings. This work aims to enhance the robustness of self-ensembling\nViTs for the tuberculosis chest x-ray classification task. We propose\nSelf-Ensembling ViT with defensive Distillation and Adversarial training\n(SEDA). SEDA utilizes efficient CNN blocks to learn spatial features with\nvarious levels of abstraction from feature representations extracted from\nintermediate ViT blocks, that are largely unaffected by adversarial\nperturbations. Furthermore, SEDA leverages adversarial training in combination\nwith defensive distillation for improved robustness against adversaries.\nTraining using adversarial examples leads to better model generalizability and\nimproves its ability to handle perturbations. Distillation using soft\nprobabilities introduces uncertainty and variation into the output\nprobabilities, making it more difficult for adversarial and privacy attacks.\nExtensive experiments performed with the proposed architecture and training\nparadigm on publicly available Tuberculosis x-ray dataset shows SOTA efficacy\nof SEDA compared to SEViT in terms of computational efficiency with 70x times\nlighter framework and enhanced robustness of +9%.",
        "translated": ""
    },
    {
        "title": "Emotion Embeddings $\\unicode{x2014}$ Learning Stable and Homogeneous\n  Abstractions from Heterogeneous Affective Datasets",
        "url": "http://arxiv.org/abs/2308.07871v1",
        "pub_date": "2023-08-15",
        "summary": "Human emotion is expressed in many communication modalities and media formats\nand so their computational study is equally diversified into natural language\nprocessing, audio signal analysis, computer vision, etc. Similarly, the large\nvariety of representation formats used in previous research to describe\nemotions (polarity scales, basic emotion categories, dimensional approaches,\nappraisal theory, etc.) have led to an ever proliferating diversity of\ndatasets, predictive models, and software tools for emotion analysis. Because\nof these two distinct types of heterogeneity, at the expressional and\nrepresentational level, there is a dire need to unify previous work on\nincreasingly diverging data and label types. This article presents such a\nunifying computational model. We propose a training procedure that learns a\nshared latent representation for emotions, so-called emotion embeddings,\nindependent of different natural languages, communication modalities, media or\nrepresentation label formats, and even disparate model architectures.\nExperiments on a wide range of heterogeneous affective datasets indicate that\nthis approach yields the desired interoperability for the sake of reusability,\ninterpretability and flexibility, without penalizing prediction quality. Code\nand data are archived under https://doi.org/10.5281/zenodo.7405327 .",
        "translated": ""
    },
    {
        "title": "TeCH: Text-guided Reconstruction of Lifelike Clothed Humans",
        "url": "http://arxiv.org/abs/2308.08545v1",
        "pub_date": "2023-08-16",
        "summary": "Despite recent research advancements in reconstructing clothed humans from a\nsingle image, accurately restoring the \"unseen regions\" with high-level details\nremains an unsolved challenge that lacks attention. Existing methods often\ngenerate overly smooth back-side surfaces with a blurry texture. But how to\neffectively capture all visual attributes of an individual from a single image,\nwhich are sufficient to reconstruct unseen areas (e.g., the back view)?\nMotivated by the power of foundation models, TeCH reconstructs the 3D human by\nleveraging 1) descriptive text prompts (e.g., garments, colors, hairstyles)\nwhich are automatically generated via a garment parsing model and Visual\nQuestion Answering (VQA), 2) a personalized fine-tuned Text-to-Image diffusion\nmodel (T2I) which learns the \"indescribable\" appearance. To represent\nhigh-resolution 3D clothed humans at an affordable cost, we propose a hybrid 3D\nrepresentation based on DMTet, which consists of an explicit body shape grid\nand an implicit distance field. Guided by the descriptive prompts +\npersonalized T2I diffusion model, the geometry and texture of the 3D humans are\noptimized through multi-view Score Distillation Sampling (SDS) and\nreconstruction losses based on the original observation. TeCH produces\nhigh-fidelity 3D clothed humans with consistent &amp; delicate texture, and\ndetailed full-body geometry. Quantitative and qualitative experiments\ndemonstrate that TeCH outperforms the state-of-the-art methods in terms of\nreconstruction accuracy and rendering quality. The code will be publicly\navailable for research purposes at https://huangyangyi.github.io/tech",
        "translated": ""
    },
    {
        "title": "MeViS: A Large-scale Benchmark for Video Segmentation with Motion\n  Expressions",
        "url": "http://arxiv.org/abs/2308.08544v1",
        "pub_date": "2023-08-16",
        "summary": "This paper strives for motion expressions guided video segmentation, which\nfocuses on segmenting objects in video content based on a sentence describing\nthe motion of the objects. Existing referring video object datasets typically\nfocus on salient objects and use language expressions that contain excessive\nstatic attributes that could potentially enable the target object to be\nidentified in a single frame. These datasets downplay the importance of motion\nin video content for language-guided video object segmentation. To investigate\nthe feasibility of using motion expressions to ground and segment objects in\nvideos, we propose a large-scale dataset called MeViS, which contains numerous\nmotion expressions to indicate target objects in complex environments. We\nbenchmarked 5 existing referring video object segmentation (RVOS) methods and\nconducted a comprehensive comparison on the MeViS dataset. The results show\nthat current RVOS methods cannot effectively address motion expression-guided\nvideo segmentation. We further analyze the challenges and propose a baseline\napproach for the proposed MeViS dataset. The goal of our benchmark is to\nprovide a platform that enables the development of effective language-guided\nvideo segmentation algorithms that leverage motion expressions as a primary cue\nfor object segmentation in complex video scenes. The proposed MeViS dataset has\nbeen released at https://henghuiding.github.io/MeViS.",
        "translated": ""
    },
    {
        "title": "InsightMapper: A Closer Look at Inner-instance Information for\n  Vectorized High-Definition Mapping",
        "url": "http://arxiv.org/abs/2308.08543v1",
        "pub_date": "2023-08-16",
        "summary": "Vectorized high-definition (HD) maps contain detailed information about\nsurrounding road elements, which are crucial for various downstream tasks in\nmodern autonomous driving vehicles, such as vehicle planning and control.\nRecent works have attempted to directly detect the vectorized HD map as a point\nset prediction task, resulting in significant improvements in detection\nperformance. However, these approaches fail to analyze and exploit the\ninner-instance correlations between predicted points, impeding further\nadvancements. To address these challenges, we investigate the utilization of\ninner-$\\textbf{INS}$tance information for vectorized h$\\textbf{IGH}$-definition\nmapping through $\\textbf{T}$ransformers and introduce InsightMapper. This paper\npresents three novel designs within InsightMapper that leverage inner-instance\ninformation in distinct ways, including hybrid query generation, inner-instance\nquery fusion, and inner-instance feature aggregation. Comparative experiments\nare conducted on the NuScenes dataset, showcasing the superiority of our\nproposed method. InsightMapper surpasses previous state-of-the-art (SOTA)\nmethods by 5.78 mAP and 5.12 TOPO, which assess topology correctness.\nSimultaneously, InsightMapper maintains high efficiency during both training\nand inference phases, resulting in remarkable comprehensive performance. The\nproject page for this work is available at\nhttps://tonyxuqaq.github.io/projects/InsightMapper .",
        "translated": ""
    },
    {
        "title": "Ref-DVGO: Reflection-Aware Direct Voxel Grid Optimization for an\n  Improved Quality-Efficiency Trade-Off in Reflective Scene Reconstructio",
        "url": "http://arxiv.org/abs/2308.08530v1",
        "pub_date": "2023-08-16",
        "summary": "Neural Radiance Fields (NeRFs) have revolutionized the field of novel view\nsynthesis, demonstrating remarkable performance. However, the modeling and\nrendering of reflective objects remain challenging problems. Recent methods\nhave shown significant improvements over the baselines in handling reflective\nscenes, albeit at the expense of efficiency. In this work, we aim to strike a\nbalance between efficiency and quality. To this end, we investigate an\nimplicit-explicit approach based on conventional volume rendering to enhance\nthe reconstruction quality and accelerate the training and rendering processes.\nWe adopt an efficient density-based grid representation and reparameterize the\nreflected radiance in our pipeline. Our proposed reflection-aware approach\nachieves a competitive quality efficiency trade-off compared to competing\nmethods. Based on our experimental results, we propose and discuss hypotheses\nregarding the factors influencing the results of density-based methods for\nreconstructing reflective objects. The source code is available at:\nhttps://github.com/gkouros/ref-dvgo",
        "translated": ""
    },
    {
        "title": "Diagnosing Human-object Interaction Detectors",
        "url": "http://arxiv.org/abs/2308.08529v1",
        "pub_date": "2023-08-16",
        "summary": "Although we have witnessed significant progress in human-object interaction\n(HOI) detection with increasingly high mAP (mean Average Precision), a single\nmAP score is too concise to obtain an informative summary of a model's\nperformance and to understand why one approach is better than another. In this\npaper, we introduce a diagnosis toolbox for analyzing the error sources of the\nexisting HOI detection models. We first conduct holistic investigations in the\npipeline of HOI detection, consisting of human-object pair detection and then\ninteraction classification. We define a set of errors and the oracles to fix\neach of them. By measuring the mAP improvement obtained from fixing an error\nusing its oracle, we can have a detailed analysis of the significance of\ndifferent errors. We then delve into the human-object detection and interaction\nclassification, respectively, and check the model's behavior. For the first\ndetection task, we investigate both recall and precision, measuring the\ncoverage of ground-truth human-object pairs as well as the noisiness level in\nthe detections. For the second classification task, we compute mAP for\ninteraction classification only, without considering the detection scores. We\nalso measure the performance of the models in differentiating human-object\npairs with and without actual interactions using the AP (Average Precision)\nscore. Our toolbox is applicable for different methods across different\ndatasets and available at https://github.com/neu-vi/Diag-HOI.",
        "translated": ""
    },
    {
        "title": "Likelihood-Based Text-to-Image Evaluation with Patch-Level Perceptual\n  and Semantic Credit Assignment",
        "url": "http://arxiv.org/abs/2308.08525v1",
        "pub_date": "2023-08-16",
        "summary": "Text-to-image synthesis has made encouraging progress and attracted lots of\npublic attention recently. However, popular evaluation metrics in this area,\nlike the Inception Score and Fr'echet Inception Distance, incur several issues.\nFirst of all, they cannot explicitly assess the perceptual quality of generated\nimages and poorly reflect the semantic alignment of each text-image pair. Also,\nthey are inefficient and need to sample thousands of images to stabilise their\nevaluation results. In this paper, we propose to evaluate text-to-image\ngeneration performance by directly estimating the likelihood of the generated\nimages using a pre-trained likelihood-based text-to-image generative model,\ni.e., a higher likelihood indicates better perceptual quality and better\ntext-image alignment. To prevent the likelihood of being dominated by the\nnon-crucial part of the generated image, we propose several new designs to\ndevelop a credit assignment strategy based on the semantic and perceptual\nsignificance of the image patches. In the experiments, we evaluate the proposed\nmetric on multiple popular text-to-image generation models and datasets in\naccessing both the perceptual quality and the text-image alignment. Moreover,\nit can successfully assess the generation ability of these models with as few\nas a hundred samples, making it very efficient in practice.",
        "translated": ""
    },
    {
        "title": "Painter: Teaching Auto-regressive Language Models to Draw Sketches",
        "url": "http://arxiv.org/abs/2308.08520v1",
        "pub_date": "2023-08-16",
        "summary": "Large language models (LLMs) have made tremendous progress in natural\nlanguage understanding and they have also been successfully adopted in other\ndomains such as computer vision, robotics, reinforcement learning, etc. In this\nwork, we apply LLMs to image generation tasks by directly generating the\nvirtual brush strokes to paint an image. We present Painter, an LLM that can\nconvert user prompts in text description format to sketches by generating the\ncorresponding brush strokes in an auto-regressive way. We construct Painter\nbased on off-the-shelf LLM that is pre-trained on a large text corpus, by\nfine-tuning it on the new task while preserving language understanding\ncapabilities. We create a dataset of diverse multi-object sketches paired with\ntextual prompts that covers several object types and tasks. Painter can\ngenerate sketches from text descriptions, remove objects from canvas, and\ndetect and classify objects in sketches. Although this is an unprecedented\npioneering work in using LLMs for auto-regressive image generation, the results\nare very encouraging.",
        "translated": ""
    },
    {
        "title": "Exploiting Point-Wise Attention in 6D Object Pose Estimation Based on\n  Bidirectional Prediction",
        "url": "http://arxiv.org/abs/2308.08518v1",
        "pub_date": "2023-08-16",
        "summary": "Traditional geometric registration based estimation methods only exploit the\nCAD model implicitly, which leads to their dependence on observation quality\nand deficiency to occlusion.To address the problem,the paper proposes a\nbidirectional correspondence prediction network with a point-wise\nattention-aware mechanism. This network not only requires the model points to\npredict the correspondence but also explicitly models the geometric\nsimilarities between observations and the model prior.} Our key insight is that\nthe correlations between each model point and scene point provide essential\ninformation for learning point-pair matches. To further tackle the correlation\nnoises brought by feature distribution divergence, we design a simple but\neffective pseudo-siamese network to improve feature homogeneity.Experimental\nresults on the public datasets of LineMOD, YCB-Video, and Occ-LineMOD show that\nthe proposed method achieves better performance than other state-of-the-art\nmethods under the same evaluation criteria. Its robustness in estimating poses\nis greatly improved, especially in an environment with severe occlusions.",
        "translated": ""
    },
    {
        "title": "Two-and-a-half Order Score-based Model for Solving 3D Ill-posed Inverse\n  Problems",
        "url": "http://arxiv.org/abs/2308.08511v1",
        "pub_date": "2023-08-16",
        "summary": "Computed Tomography (CT) and Magnetic Resonance Imaging (MRI) are crucial\ntechnologies in the field of medical imaging. Score-based models have proven to\nbe effective in addressing different inverse problems encountered in CT and\nMRI, such as sparse-view CT and fast MRI reconstruction. However, these models\nface challenges in achieving accurate three dimensional (3D) volumetric\nreconstruction. The existing score-based models primarily focus on\nreconstructing two dimensional (2D) data distribution, leading to\ninconsistencies between adjacent slices in the reconstructed 3D volumetric\nimages. To overcome this limitation, we propose a novel two-and-a-half order\nscore-based model (TOSM). During the training phase, our TOSM learns data\ndistributions in 2D space, which reduces the complexity of training compared to\ndirectly working on 3D volumes. However, in the reconstruction phase, the TOSM\nupdates the data distribution in 3D space, utilizing complementary scores along\nthree directions (sagittal, coronal, and transaxial) to achieve a more precise\nreconstruction. The development of TOSM is built on robust theoretical\nprinciples, ensuring its reliability and efficacy. Through extensive\nexperimentation on large-scale sparse-view CT and fast MRI datasets, our method\ndemonstrates remarkable advancements and attains state-of-the-art results in\nsolving 3D ill-posed inverse problems. Notably, the proposed TOSM effectively\naddresses the inter-slice inconsistency issue, resulting in high-quality 3D\nvolumetric reconstruction.",
        "translated": ""
    },
    {
        "title": "ResBuilder: Automated Learning of Depth with Residual Structures",
        "url": "http://arxiv.org/abs/2308.08504v1",
        "pub_date": "2023-08-16",
        "summary": "In this work, we develop a neural architecture search algorithm, termed\nResbuilder, that develops ResNet architectures from scratch that achieve high\naccuracy at moderate computational cost. It can also be used to modify existing\narchitectures and has the capability to remove and insert ResNet blocks, in\nthis way searching for suitable architectures in the space of ResNet\narchitectures. In our experiments on different image classification datasets,\nResbuilder achieves close to state-of-the-art performance while saving\ncomputational cost compared to off-the-shelf ResNets. Noteworthy, we once tune\nthe parameters on CIFAR10 which yields a suitable default choice for all other\ndatasets. We demonstrate that this property generalizes even to industrial\napplications by applying our method with default parameters on a proprietary\nfraud detection dataset.",
        "translated": ""
    },
    {
        "title": "Towards Large-scale 3D Representation Learning with Multi-dataset Point\n  Prompt Training",
        "url": "http://arxiv.org/abs/2308.09718v1",
        "pub_date": "2023-08-18",
        "summary": "The rapid advancement of deep learning models often attributes to their\nability to leverage massive training data. In contrast, such privilege has not\nyet fully benefited 3D deep learning, mainly due to the limited availability of\nlarge-scale 3D datasets. Merging multiple available data sources and letting\nthem collaboratively train a single model is a potential solution. However, due\nto the large domain gap between 3D point cloud datasets, such mixed supervision\ncould adversely affect the model's performance and lead to degenerated\nperformance (i.e., negative transfer) compared to single-dataset training. In\nview of this challenge, we introduce Point Prompt Training (PPT), a novel\nframework for multi-dataset synergistic learning in the context of 3D\nrepresentation learning that supports multiple pre-training paradigms. Based on\nthis framework, we propose Prompt-driven Normalization, which adapts the model\nto different datasets with domain-specific prompts and Language-guided\nCategorical Alignment that decently unifies the multiple-dataset label spaces\nby leveraging the relationship between label text. Extensive experiments verify\nthat PPT can overcome the negative transfer associated with synergistic\nlearning and produce generalizable representations. Notably, it achieves\nstate-of-the-art performance on each dataset using a single weight-shared model\nwith supervised multi-dataset training. Moreover, when served as a pre-training\nframework, it outperforms other pre-training approaches regarding\nrepresentation quality and attains remarkable state-of-the-art performance\nacross over ten diverse downstream tasks spanning both indoor and outdoor 3D\nscenarios.",
        "translated": ""
    },
    {
        "title": "Smoothness Similarity Regularization for Few-Shot GAN Adaptation",
        "url": "http://arxiv.org/abs/2308.09717v1",
        "pub_date": "2023-08-18",
        "summary": "The task of few-shot GAN adaptation aims to adapt a pre-trained GAN model to\na small dataset with very few training images. While existing methods perform\nwell when the dataset for pre-training is structurally similar to the target\ndataset, the approaches suffer from training instabilities or memorization\nissues when the objects in the two domains have a very different structure. To\nmitigate this limitation, we propose a new smoothness similarity regularization\nthat transfers the inherently learned smoothness of the pre-trained GAN to the\nfew-shot target domain even if the two domains are very different. We evaluate\nour approach by adapting an unconditional and a class-conditional GAN to\ndiverse few-shot target domains. Our proposed method significantly outperforms\nprior few-shot GAN adaptation methods in the challenging case of structurally\ndissimilar source-target domains, while performing on par with the state of the\nart for similar source-target domains.",
        "translated": ""
    },
    {
        "title": "Diff2Lip: Audio Conditioned Diffusion Models for Lip-Synchronization",
        "url": "http://arxiv.org/abs/2308.09716v1",
        "pub_date": "2023-08-18",
        "summary": "The task of lip synchronization (lip-sync) seeks to match the lips of human\nfaces with different audio. It has various applications in the film industry as\nwell as for creating virtual avatars and for video conferencing. This is a\nchallenging problem as one needs to simultaneously introduce detailed,\nrealistic lip movements while preserving the identity, pose, emotions, and\nimage quality. Many of the previous methods trying to solve this problem suffer\nfrom image quality degradation due to a lack of complete contextual\ninformation. In this paper, we present Diff2Lip, an audio-conditioned\ndiffusion-based model which is able to do lip synchronization in-the-wild while\npreserving these qualities. We train our model on Voxceleb2, a video dataset\ncontaining in-the-wild talking face videos. Extensive studies show that our\nmethod outperforms popular methods like Wav2Lip and PC-AVS in Fr\\'echet\ninception distance (FID) metric and Mean Opinion Scores (MOS) of the users. We\nshow results on both reconstruction (same audio-video inputs) as well as cross\n(different audio-video inputs) settings on Voxceleb2 and LRW datasets. Video\nresults and code can be accessed from our project page (\nhttps://soumik-kanad.github.io/diff2lip ).",
        "translated": ""
    },
    {
        "title": "Dynamic 3D Gaussians: Tracking by Persistent Dynamic View Synthesis",
        "url": "http://arxiv.org/abs/2308.09713v1",
        "pub_date": "2023-08-18",
        "summary": "We present a method that simultaneously addresses the tasks of dynamic scene\nnovel-view synthesis and six degree-of-freedom (6-DOF) tracking of all dense\nscene elements. We follow an analysis-by-synthesis framework, inspired by\nrecent work that models scenes as a collection of 3D Gaussians which are\noptimized to reconstruct input images via differentiable rendering. To model\ndynamic scenes, we allow Gaussians to move and rotate over time while enforcing\nthat they have persistent color, opacity, and size. By regularizing Gaussians'\nmotion and rotation with local-rigidity constraints, we show that our Dynamic\n3D Gaussians correctly model the same area of physical space over time,\nincluding the rotation of that space. Dense 6-DOF tracking and dynamic\nreconstruction emerges naturally from persistent dynamic view synthesis,\nwithout requiring any correspondence or flow as input. We demonstrate a large\nnumber of downstream applications enabled by our representation, including\nfirst-person view synthesis, dynamic compositional scene synthesis, and 4D\nvideo editing.",
        "translated": ""
    },
    {
        "title": "HumanLiff: Layer-wise 3D Human Generation with Diffusion Model",
        "url": "http://arxiv.org/abs/2308.09712v1",
        "pub_date": "2023-08-18",
        "summary": "3D human generation from 2D images has achieved remarkable progress through\nthe synergistic utilization of neural rendering and generative models. Existing\n3D human generative models mainly generate a clothed 3D human as an\nundetectable 3D model in a single pass, while rarely considering the layer-wise\nnature of a clothed human body, which often consists of the human body and\nvarious clothes such as underwear, outerwear, trousers, shoes, etc. In this\nwork, we propose HumanLiff, the first layer-wise 3D human generative model with\na unified diffusion process. Specifically, HumanLiff firstly generates\nminimal-clothed humans, represented by tri-plane features, in a canonical\nspace, and then progressively generates clothes in a layer-wise manner. In this\nway, the 3D human generation is thus formulated as a sequence of\ndiffusion-based 3D conditional generation. To reconstruct more fine-grained 3D\nhumans with tri-plane representation, we propose a tri-plane shift operation\nthat splits each tri-plane into three sub-planes and shifts these sub-planes to\nenable feature grid subdivision. To further enhance the controllability of 3D\ngeneration with 3D layered conditions, HumanLiff hierarchically fuses tri-plane\nfeatures and 3D layered conditions to facilitate the 3D diffusion model\nlearning. Extensive experiments on two layer-wise 3D human datasets, SynBody\n(synthetic) and TightCap (real-world), validate that HumanLiff significantly\noutperforms state-of-the-art methods in layer-wise 3D human generation. Our\ncode will be available at https://skhu101.github.io/HumanLiff.",
        "translated": ""
    },
    {
        "title": "Robust Monocular Depth Estimation under Challenging Conditions",
        "url": "http://arxiv.org/abs/2308.09711v1",
        "pub_date": "2023-08-18",
        "summary": "While state-of-the-art monocular depth estimation approaches achieve\nimpressive results in ideal settings, they are highly unreliable under\nchallenging illumination and weather conditions, such as at nighttime or in the\npresence of rain. In this paper, we uncover these safety-critical issues and\ntackle them with md4all: a simple and effective solution that works reliably\nunder both adverse and ideal conditions, as well as for different types of\nlearning supervision. We achieve this by exploiting the efficacy of existing\nmethods under perfect settings. Therefore, we provide valid training signals\nindependently of what is in the input. First, we generate a set of complex\nsamples corresponding to the normal training ones. Then, we train the model by\nguiding its self- or full-supervision by feeding the generated samples and\ncomputing the standard losses on the corresponding original images. Doing so\nenables a single model to recover information across diverse conditions without\nmodifications at inference time. Extensive experiments on two challenging\npublic datasets, namely nuScenes and Oxford RobotCar, demonstrate the\neffectiveness of our techniques, outperforming prior works by a large margin in\nboth standard and challenging conditions. Source code and data are available\nat: https://md4all.github.io.",
        "translated": ""
    },
    {
        "title": "SimDA: Simple Diffusion Adapter for Efficient Video Generation",
        "url": "http://arxiv.org/abs/2308.09710v1",
        "pub_date": "2023-08-18",
        "summary": "The recent wave of AI-generated content has witnessed the great development\nand success of Text-to-Image (T2I) technologies. By contrast, Text-to-Video\n(T2V) still falls short of expectations though attracting increasing interests.\nExisting works either train from scratch or adapt large T2I model to videos,\nboth of which are computation and resource expensive. In this work, we propose\na Simple Diffusion Adapter (SimDA) that fine-tunes only 24M out of 1.1B\nparameters of a strong T2I model, adapting it to video generation in a\nparameter-efficient way. In particular, we turn the T2I model for T2V by\ndesigning light-weight spatial and temporal adapters for transfer learning.\nBesides, we change the original spatial attention to the proposed Latent-Shift\nAttention (LSA) for temporal consistency. With similar model architecture, we\nfurther train a video super-resolution model to generate high-definition\n(1024x1024) videos. In addition to T2V generation in the wild, SimDA could also\nbe utilized in one-shot video editing with only 2 minutes tuning. Doing so, our\nmethod could minimize the training effort with extremely few tunable parameters\nfor model adaptation.",
        "translated": ""
    },
    {
        "title": "Training with Product Digital Twins for AutoRetail Checkout",
        "url": "http://arxiv.org/abs/2308.09708v1",
        "pub_date": "2023-08-18",
        "summary": "Automating the checkout process is important in smart retail, where users\neffortlessly pass products by hand through a camera, triggering automatic\nproduct detection, tracking, and counting. In this emerging area, due to the\nlack of annotated training data, we introduce a dataset comprised of product 3D\nmodels, which allows for fast, flexible, and large-scale training data\ngeneration through graphic engine rendering. Within this context, we discern an\nintriguing facet, because of the user \"hands-on\" approach, bias in user\nbehavior leads to distinct patterns in the real checkout process. The existence\nof such patterns would compromise training effectiveness if training data fail\nto reflect the same. To address this user bias problem, we propose a training\ndata optimization framework, i.e., training with digital twins (DtTrain).\nSpecifically, we leverage the product 3D models and optimize their rendering\nviewpoint and illumination to generate \"digital twins\" that visually resemble\nrepresentative user images. These digital twins, inherit product labels and,\nwhen augmented, form the Digital Twin training set (DT set). Because the\ndigital twins individually mimic user bias, the resulting DT training set\nbetter reflects the characteristics of the target scenario and allows us to\ntrain more effective product detection and tracking models. In our experiment,\nwe show that DT set outperforms training sets created by existing dataset\nsynthesis methods in terms of counting accuracy. Moreover, by combining DT set\nwith pseudo-labeled real checkout data, further improvement is observed. The\ncode is available at https://github.com/yorkeyao/Automated-Retail-Checkout.",
        "translated": ""
    },
    {
        "title": "Guide3D: Create 3D Avatars from Text and Image Guidance",
        "url": "http://arxiv.org/abs/2308.09705v1",
        "pub_date": "2023-08-18",
        "summary": "Recently, text-to-image generation has exhibited remarkable advancements,\nwith the ability to produce visually impressive results. In contrast,\ntext-to-3D generation has not yet reached a comparable level of quality.\nExisting methods primarily rely on text-guided score distillation sampling\n(SDS), and they encounter difficulties in transferring 2D attributes of the\ngenerated images to 3D content. In this work, we aim to develop an effective 3D\ngenerative model capable of synthesizing high-resolution textured meshes by\nleveraging both textual and image information. To this end, we introduce\nGuide3D, a zero-shot text-and-image-guided generative model for 3D avatar\ngeneration based on diffusion models. Our model involves (1) generating\nsparse-view images of a text-consistent character using diffusion models, and\n(2) jointly optimizing multi-resolution differentiable marching tetrahedral\ngrids with pixel-aligned image features. We further propose a similarity-aware\nfeature fusion strategy for efficiently integrating features from different\nviews. Moreover, we introduce two novel training objectives as an alternative\nto calculating SDS, significantly enhancing the optimization process. We\nthoroughly evaluate the performance and components of our framework, which\noutperforms the current state-of-the-art in producing topologically and\nstructurally correct geometry and high-resolution textures. Guide3D enables the\ndirect transfer of 2D-generated images to the 3D space. Our code will be made\npublicly available.",
        "translated": ""
    },
    {
        "title": "Invariant Training 2D-3D Joint Hard Samples for Few-Shot Point Cloud\n  Recognition",
        "url": "http://arxiv.org/abs/2308.09694v1",
        "pub_date": "2023-08-18",
        "summary": "We tackle the data scarcity challenge in few-shot point cloud recognition of\n3D objects by using a joint prediction from a conventional 3D model and a\nwell-trained 2D model. Surprisingly, such an ensemble, though seems trivial,\nhas hardly been shown effective in recent 2D-3D models. We find out the crux is\nthe less effective training for the ''joint hard samples'', which have high\nconfidence prediction on different wrong labels, implying that the 2D and 3D\nmodels do not collaborate well. To this end, our proposed invariant training\nstrategy, called InvJoint, does not only emphasize the training more on the\nhard samples, but also seeks the invariance between the conflicting 2D and 3D\nambiguous predictions. InvJoint can learn more collaborative 2D and 3D\nrepresentations for better ensemble. Extensive experiments on 3D shape\nclassification with widely adopted ModelNet10/40, ScanObjectNN and Toys4K, and\nshape retrieval with ShapeNet-Core validate the superiority of our InvJoint.",
        "translated": ""
    },
    {
        "title": "CamP: Camera Preconditioning for Neural Radiance Fields",
        "url": "http://arxiv.org/abs/2308.10902v1",
        "pub_date": "2023-08-21",
        "summary": "Neural Radiance Fields (NeRF) can be optimized to obtain high-fidelity 3D\nscene reconstructions of objects and large-scale scenes. However, NeRFs require\naccurate camera parameters as input -- inaccurate camera parameters result in\nblurry renderings. Extrinsic and intrinsic camera parameters are usually\nestimated using Structure-from-Motion (SfM) methods as a pre-processing step to\nNeRF, but these techniques rarely yield perfect estimates. Thus, prior works\nhave proposed jointly optimizing camera parameters alongside a NeRF, but these\nmethods are prone to local minima in challenging settings. In this work, we\nanalyze how different camera parameterizations affect this joint optimization\nproblem, and observe that standard parameterizations exhibit large differences\nin magnitude with respect to small perturbations, which can lead to an\nill-conditioned optimization problem. We propose using a proxy problem to\ncompute a whitening transform that eliminates the correlation between camera\nparameters and normalizes their effects, and we propose to use this transform\nas a preconditioner for the camera parameters during joint optimization. Our\npreconditioned camera optimization significantly improves reconstruction\nquality on scenes from the Mip-NeRF 360 dataset: we reduce error rates (RMSE)\nby 67% compared to state-of-the-art NeRF approaches that do not optimize for\ncameras like Zip-NeRF, and by 29% relative to state-of-the-art joint\noptimization approaches using the camera parameterization of SCNeRF. Our\napproach is easy to implement, does not significantly increase runtime, can be\napplied to a wide variety of camera parameterizations, and can\nstraightforwardly be incorporated into other NeRF-like models.",
        "translated": ""
    },
    {
        "title": "Structured World Models from Human Videos",
        "url": "http://arxiv.org/abs/2308.10901v1",
        "pub_date": "2023-08-21",
        "summary": "We tackle the problem of learning complex, general behaviors directly in the\nreal world. We propose an approach for robots to efficiently learn manipulation\nskills using only a handful of real-world interaction trajectories from many\ndifferent settings. Inspired by the success of learning from large-scale\ndatasets in the fields of computer vision and natural language, our belief is\nthat in order to efficiently learn, a robot must be able to leverage\ninternet-scale, human video data. Humans interact with the world in many\ninteresting ways, which can allow a robot to not only build an understanding of\nuseful actions and affordances but also how these actions affect the world for\nmanipulation. Our approach builds a structured, human-centric action space\ngrounded in visual affordances learned from human videos. Further, we train a\nworld model on human videos and fine-tune on a small amount of robot\ninteraction data without any task supervision. We show that this approach of\naffordance-space world models enables different robots to learn various\nmanipulation skills in complex settings, in under 30 minutes of interaction.\nVideos can be found at https://human-world-model.github.io",
        "translated": ""
    },
    {
        "title": "Few-Shot Physically-Aware Articulated Mesh Generation via Hierarchical\n  Deformation",
        "url": "http://arxiv.org/abs/2308.10898v1",
        "pub_date": "2023-08-21",
        "summary": "We study the problem of few-shot physically-aware articulated mesh\ngeneration. By observing an articulated object dataset containing only a few\nexamples, we wish to learn a model that can generate diverse meshes with high\nvisual fidelity and physical validity. Previous mesh generative models either\nhave difficulties in depicting a diverse data space from only a few examples or\nfail to ensure physical validity of their samples. Regarding the above\nchallenges, we propose two key innovations, including 1) a hierarchical mesh\ndeformation-based generative model based upon the divide-and-conquer philosophy\nto alleviate the few-shot challenge by borrowing transferrable deformation\npatterns from large scale rigid meshes and 2) a physics-aware deformation\ncorrection scheme to encourage physically plausible generations. We conduct\nextensive experiments on 6 articulated categories to demonstrate the\nsuperiority of our method in generating articulated meshes with better\ndiversity, higher visual fidelity, and better physical validity over previous\nmethods in the few-shot setting. Further, we validate solid contributions of\nour two innovations in the ablation study. Project page with code is available\nat https://meowuu7.github.io/few-arti-obj-gen.",
        "translated": ""
    },
    {
        "title": "Can Language Models Learn to Listen?",
        "url": "http://arxiv.org/abs/2308.10897v1",
        "pub_date": "2023-08-21",
        "summary": "We present a framework for generating appropriate facial responses from a\nlistener in dyadic social interactions based on the speaker's words. Given an\ninput transcription of the speaker's words with their timestamps, our approach\nautoregressively predicts a response of a listener: a sequence of listener\nfacial gestures, quantized using a VQ-VAE. Since gesture is a language\ncomponent, we propose treating the quantized atomic motion elements as\nadditional language token inputs to a transformer-based large language model.\nInitializing our transformer with the weights of a language model pre-trained\nonly on text results in significantly higher quality listener responses than\ntraining a transformer from scratch. We show that our generated listener motion\nis fluent and reflective of language semantics through quantitative metrics and\na qualitative user study. In our evaluation, we analyze the model's ability to\nutilize temporal and semantic aspects of spoken text. Project page:\nhttps://people.eecs.berkeley.edu/~evonne_ng/projects/text2listen/",
        "translated": ""
    },
    {
        "title": "Differentiable Shadow Mapping for Efficient Inverse Graphics",
        "url": "http://arxiv.org/abs/2308.10896v1",
        "pub_date": "2023-08-21",
        "summary": "We show how shadows can be efficiently generated in differentiable rendering\nof triangle meshes. Our central observation is that pre-filtered shadow\nmapping, a technique for approximating shadows based on rendering from the\nperspective of a light, can be combined with existing differentiable\nrasterizers to yield differentiable visibility information. We demonstrate at\nseveral inverse graphics problems that differentiable shadow maps are orders of\nmagnitude faster than differentiable light transport simulation with similar\naccuracy -- while differentiable rasterization without shadows often fails to\nconverge.",
        "translated": ""
    },
    {
        "title": "Unlocking Accuracy and Fairness in Differentially Private Image\n  Classification",
        "url": "http://arxiv.org/abs/2308.10888v1",
        "pub_date": "2023-08-21",
        "summary": "Privacy-preserving machine learning aims to train models on private data\nwithout leaking sensitive information. Differential privacy (DP) is considered\nthe gold standard framework for privacy-preserving training, as it provides\nformal privacy guarantees. However, compared to their non-private counterparts,\nmodels trained with DP often have significantly reduced accuracy. Private\nclassifiers are also believed to exhibit larger performance disparities across\nsubpopulations, raising fairness concerns. The poor performance of classifiers\ntrained with DP has prevented the widespread adoption of privacy preserving\nmachine learning in industry. Here we show that pre-trained foundation models\nfine-tuned with DP can achieve similar accuracy to non-private classifiers,\neven in the presence of significant distribution shifts between pre-training\ndata and downstream tasks. We achieve private accuracies within a few percent\nof the non-private state of the art across four datasets, including two medical\nimaging benchmarks. Furthermore, our private medical classifiers do not exhibit\nlarger performance disparities across demographic groups than non-private\nmodels. This milestone to make DP training a practical and reliable technology\nhas the potential to widely enable machine learning practitioners to train\nsafely on sensitive datasets while protecting individuals' privacy.",
        "translated": ""
    },
    {
        "title": "Vision Transformer Pruning Via Matrix Decomposition",
        "url": "http://arxiv.org/abs/2308.10839v1",
        "pub_date": "2023-08-21",
        "summary": "This is a further development of Vision Transformer Pruning via matrix\ndecomposition. The purpose of the Vision Transformer Pruning is to prune the\ndimension of the linear projection of the dataset by learning their associated\nimportance score in order to reduce the storage, run-time memory, and\ncomputational demands. In this paper we further reduce dimension and complexity\nof the linear projection by implementing and comparing several matrix\ndecomposition methods while preserving the generated important features. We end\nup selected the Singular Value Decomposition as the method to achieve our goal\nby comparing the original accuracy scores in the original Github repository and\nthe accuracy scores of using those matrix decomposition methods, including\nSingular Value Decomposition, four versions of QR Decomposition, and LU\nfactorization.",
        "translated": ""
    },
    {
        "title": "EigenPlaces: Training Viewpoint Robust Models for Visual Place\n  Recognition",
        "url": "http://arxiv.org/abs/2308.10832v1",
        "pub_date": "2023-08-21",
        "summary": "Visual Place Recognition is a task that aims to predict the place of an image\n(called query) based solely on its visual features. This is typically done\nthrough image retrieval, where the query is matched to the most similar images\nfrom a large database of geotagged photos, using learned global descriptors. A\nmajor challenge in this task is recognizing places seen from different\nviewpoints. To overcome this limitation, we propose a new method, called\nEigenPlaces, to train our neural network on images from different point of\nviews, which embeds viewpoint robustness into the learned global descriptors.\nThe underlying idea is to cluster the training data so as to explicitly present\nthe model with different views of the same points of interest. The selection of\nthis points of interest is done without the need for extra supervision. We then\npresent experiments on the most comprehensive set of datasets in literature,\nfinding that EigenPlaces is able to outperform previous state of the art on the\nmajority of datasets, while requiring 60\\% less GPU memory for training and\nusing 50\\% smaller descriptors. The code and trained models for EigenPlaces are\navailable at {\\small{\\url{https://github.com/gmberton/EigenPlaces}}}, while\nresults with any other baseline can be computed with the codebase at\n{\\small{\\url{https://github.com/gmberton/auto_VPR}}}.",
        "translated": ""
    },
    {
        "title": "Pixel Adaptive Deep Unfolding Transformer for Hyperspectral Image\n  Reconstruction",
        "url": "http://arxiv.org/abs/2308.10820v1",
        "pub_date": "2023-08-21",
        "summary": "Hyperspectral Image (HSI) reconstruction has made gratifying progress with\nthe deep unfolding framework by formulating the problem into a data module and\na prior module. Nevertheless, existing methods still face the problem of\ninsufficient matching with HSI data. The issues lie in three aspects: 1) fixed\ngradient descent step in the data module while the degradation of HSI is\nagnostic in the pixel-level. 2) inadequate prior module for 3D HSI cube. 3)\nstage interaction ignoring the differences in features at different stages. To\naddress these issues, in this work, we propose a Pixel Adaptive Deep Unfolding\nTransformer (PADUT) for HSI reconstruction. In the data module, a pixel\nadaptive descent step is employed to focus on pixel-level agnostic degradation.\nIn the prior module, we introduce the Non-local Spectral Transformer (NST) to\nemphasize the 3D characteristics of HSI for recovering. Moreover, inspired by\nthe diverse expression of features in different stages and depths, the stage\ninteraction is improved by the Fast Fourier Transform (FFT). Experimental\nresults on both simulated and real scenes exhibit the superior performance of\nour method compared to state-of-the-art HSI reconstruction methods. The code is\nreleased at: https://github.com/MyuLi/PADUT.",
        "translated": ""
    },
    {
        "title": "Jumping through Local Minima: Quantization in the Loss Landscape of\n  Vision Transformers",
        "url": "http://arxiv.org/abs/2308.10814v1",
        "pub_date": "2023-08-21",
        "summary": "Quantization scale and bit-width are the most important parameters when\nconsidering how to quantize a neural network. Prior work focuses on optimizing\nquantization scales in a global manner through gradient methods (gradient\ndescent \\&amp; Hessian analysis). Yet, when applying perturbations to quantization\nscales, we observe a very jagged, highly non-smooth test loss landscape. In\nfact, small perturbations in quantization scale can greatly affect accuracy,\nyielding a $0.5-0.8\\%$ accuracy boost in 4-bit quantized vision transformers\n(ViTs). In this regime, gradient methods break down, since they cannot reliably\nreach local minima. In our work, dubbed Evol-Q, we use evolutionary search to\neffectively traverse the non-smooth landscape. Additionally, we propose using\nan infoNCE loss, which not only helps combat overfitting on the small\ncalibration dataset ($1,000$ images) but also makes traversing such a highly\nnon-smooth surface easier. Evol-Q improves the top-1 accuracy of a fully\nquantized ViT-Base by $10.30\\%$, $0.78\\%$, and $0.15\\%$ for $3$-bit, $4$-bit,\nand $8$-bit weight quantization levels. Extensive experiments on a variety of\nCNN and ViT architectures further demonstrate its robustness in extreme\nquantization scenarios. Our code is available at\nhttps://github.com/enyac-group/evol-q",
        "translated": ""
    },
    {
        "title": "GRIP: Generating Interaction Poses Using Latent Consistency and Spatial\n  Cues",
        "url": "http://arxiv.org/abs/2308.11617v1",
        "pub_date": "2023-08-22",
        "summary": "Hands are dexterous and highly versatile manipulators that are central to how\nhumans interact with objects and their environment. Consequently, modeling\nrealistic hand-object interactions, including the subtle motion of individual\nfingers, is critical for applications in computer graphics, computer vision,\nand mixed reality. Prior work on capturing and modeling humans interacting with\nobjects in 3D focuses on the body and object motion, often ignoring hand pose.\nIn contrast, we introduce GRIP, a learning-based method that takes, as input,\nthe 3D motion of the body and the object, and synthesizes realistic motion for\nboth hands before, during, and after object interaction. As a preliminary step\nbefore synthesizing the hand motion, we first use a network, ANet, to denoise\nthe arm motion. Then, we leverage the spatio-temporal relationship between the\nbody and the object to extract two types of novel temporal interaction cues,\nand use them in a two-stage inference pipeline to generate the hand motion. In\nthe first stage, we introduce a new approach to enforce motion temporal\nconsistency in the latent space (LTC), and generate consistent interaction\nmotions. In the second stage, GRIP generates refined hand poses to avoid\nhand-object penetrations. Given sequences of noisy body and object motion, GRIP\nupgrades them to include hand-object interaction. Quantitative experiments and\nperceptual studies demonstrate that GRIP outperforms baseline methods and\ngeneralizes to unseen objects and motions from different motion-capture\ndatasets.",
        "translated": ""
    },
    {
        "title": "Delving into Motion-Aware Matching for Monocular 3D Object Tracking",
        "url": "http://arxiv.org/abs/2308.11607v1",
        "pub_date": "2023-08-22",
        "summary": "Recent advances of monocular 3D object detection facilitate the 3D\nmulti-object tracking task based on low-cost camera sensors. In this paper, we\nfind that the motion cue of objects along different time frames is critical in\n3D multi-object tracking, which is less explored in existing monocular-based\napproaches. In this paper, we propose a motion-aware framework for monocular 3D\nMOT. To this end, we propose MoMA-M3T, a framework that mainly consists of\nthree motion-aware components. First, we represent the possible movement of an\nobject related to all object tracklets in the feature space as its motion\nfeatures. Then, we further model the historical object tracklet along the time\nframe in a spatial-temporal perspective via a motion transformer. Finally, we\npropose a motion-aware matching module to associate historical object tracklets\nand current observations as final tracking results. We conduct extensive\nexperiments on the nuScenes and KITTI datasets to demonstrate that our MoMA-M3T\nachieves competitive performance against state-of-the-art methods. Moreover,\nthe proposed tracker is flexible and can be easily plugged into existing\nimage-based 3D object detectors without re-training. Code and models are\navailable at https://github.com/kuanchihhuang/MoMA-M3T.",
        "translated": ""
    },
    {
        "title": "StoryBench: A Multifaceted Benchmark for Continuous Story Visualization",
        "url": "http://arxiv.org/abs/2308.11606v1",
        "pub_date": "2023-08-22",
        "summary": "Generating video stories from text prompts is a complex task. In addition to\nhaving high visual quality, videos need to realistically adhere to a sequence\nof text prompts whilst being consistent throughout the frames. Creating a\nbenchmark for video generation requires data annotated over time, which\ncontrasts with the single caption used often in video datasets. To fill this\ngap, we collect comprehensive human annotations on three existing datasets, and\nintroduce StoryBench: a new, challenging multi-task benchmark to reliably\nevaluate forthcoming text-to-video models. Our benchmark includes three video\ngeneration tasks of increasing difficulty: action execution, where the next\naction must be generated starting from a conditioning video; story\ncontinuation, where a sequence of actions must be executed starting from a\nconditioning video; and story generation, where a video must be generated from\nonly text prompts. We evaluate small yet strong text-to-video baselines, and\nshow the benefits of training on story-like data algorithmically generated from\nexisting video captions. Finally, we establish guidelines for human evaluation\nof video stories, and reaffirm the need of better automatic metrics for video\ngeneration. StoryBench aims at encouraging future research efforts in this\nexciting new area.",
        "translated": ""
    },
    {
        "title": "GOPro: Generate and Optimize Prompts in CLIP using Self-Supervised\n  Learning",
        "url": "http://arxiv.org/abs/2308.11605v1",
        "pub_date": "2023-08-22",
        "summary": "Large-scale foundation models, such as CLIP, have demonstrated remarkable\nsuccess in visual recognition tasks by embedding images in a semantically rich\nspace. Self-supervised learning (SSL) has also shown promise in improving\nvisual recognition by learning invariant features. However, the combination of\nCLIP with SSL is found to face challenges due to the multi-task framework that\nblends CLIP's contrastive loss and SSL's loss, including difficulties with loss\nweighting and inconsistency among different views of images in CLIP's output\nspace. To overcome these challenges, we propose a prompt learning-based model\ncalled GOPro, which is a unified framework that ensures similarity between\nvarious augmented views of input images in a shared image-text embedding space,\nusing a pair of learnable image and text projectors atop CLIP, to promote\ninvariance and generalizability. To automatically learn such prompts, we\nleverage the visual content and style primitives extracted from pre-trained\nCLIP and adapt them to the target task. In addition to CLIP's cross-domain\ncontrastive loss, we introduce a visual contrastive loss and a novel prompt\nconsistency loss, considering the different views of the images. GOPro is\ntrained end-to-end on all three loss objectives, combining the strengths of\nCLIP and SSL in a principled manner. Empirical evaluations demonstrate that\nGOPro outperforms the state-of-the-art prompting techniques on three\nchallenging domain generalization tasks across multiple benchmarks by a\nsignificant margin. Our code is available at\nhttps://github.com/mainaksingha01/GOPro.",
        "translated": ""
    },
    {
        "title": "G3Reg: Pyramid Graph-based Global Registration using Gaussian Ellipsoid\n  Model",
        "url": "http://arxiv.org/abs/2308.11573v1",
        "pub_date": "2023-08-22",
        "summary": "This study introduces a novel framework, G3Reg, for fast and robust global\nregistration of LiDAR point clouds. In contrast to conventional complex\nkeypoints and descriptors, we extract fundamental geometric primitives\nincluding planes, clusters, and lines (PCL) from the raw point cloud to obtain\nlow-level semantic segments. Each segment is formulated as a unified Gaussian\nEllipsoid Model (GEM) by employing a probability ellipsoid to ensure the ground\ntruth centers are encompassed with a certain degree of probability. Utilizing\nthese GEMs, we then present a distrust-and-verify scheme based on a Pyramid\nCompatibility Graph for Global Registration (PAGOR). Specifically, we establish\nan upper bound, which can be traversed based on the confidence level for\ncompatibility testing to construct the pyramid graph. Gradually, we solve\nmultiple maximum cliques (MAC) for each level of the graph, generating numerous\ntransformation candidates. In the verification phase, we adopt a precise and\nefficient metric for point cloud alignment quality, founded on geometric\nprimitives, to identify the optimal candidate. The performance of the algorithm\nis extensively validated on three publicly available datasets and a\nself-collected multi-session dataset, without changing any parameter settings\nin the experimental evaluation. The results exhibit superior robustness and\nreal-time performance of the G3Reg framework compared to state-of-the-art\nmethods. Furthermore, we demonstrate the potential for integrating individual\nGEM and PAGOR components into other algorithmic frameworks to enhance their\nefficacy. To advance further research and promote community understanding, we\nhave publicly shared the source code.",
        "translated": ""
    },
    {
        "title": "SPANet: Frequency-balancing Token Mixer using Spectral Pooling\n  Aggregation Modulation",
        "url": "http://arxiv.org/abs/2308.11568v1",
        "pub_date": "2023-08-22",
        "summary": "Recent studies show that self-attentions behave like low-pass filters (as\nopposed to convolutions) and enhancing their high-pass filtering capability\nimproves model performance. Contrary to this idea, we investigate existing\nconvolution-based models with spectral analysis and observe that improving the\nlow-pass filtering in convolution operations also leads to performance\nimprovement. To account for this observation, we hypothesize that utilizing\noptimal token mixers that capture balanced representations of both high- and\nlow-frequency components can enhance the performance of models. We verify this\nby decomposing visual features into the frequency domain and combining them in\na balanced manner. To handle this, we replace the balancing problem with a mask\nfiltering problem in the frequency domain. Then, we introduce a novel\ntoken-mixer named SPAM and leverage it to derive a MetaFormer model termed as\nSPANet. Experimental results show that the proposed method provides a way to\nachieve this balance, and the balanced representations of both high- and\nlow-frequency components can improve the performance of models on multiple\ncomputer vision tasks. Our code is available at\n$\\href{https://doranlyong.github.io/projects/spanet/}{\\text{https://doranlyong.github.io/projects/spanet/}}$.",
        "translated": ""
    },
    {
        "title": "EndoNet: model for automatic calculation of H-score on histological\n  slides",
        "url": "http://arxiv.org/abs/2308.11562v1",
        "pub_date": "2023-08-22",
        "summary": "H-score is a semi-quantitative method used to assess the presence and\ndistribution of proteins in tissue samples by combining the intensity of\nstaining and percentage of stained nuclei. It is widely used but time-consuming\nand can be limited in accuracy and precision. Computer-aided methods may help\novercome these limitations and improve the efficiency of pathologists'\nworkflows. In this work, we developed a model EndoNet for automatic calculation\nof H-score on histological slides. Our proposed method uses neural networks and\nconsists of two main parts. The first is a detection model which predicts\nkeypoints of centers of nuclei. The second is a H-score module which calculates\nthe value of the H-score using mean pixel values of predicted keypoints. Our\nmodel was trained and validated on 1780 annotated tiles with a shape of 100x100\n$\\mu m$ and performed 0.77 mAP on a test dataset. Moreover, the model can be\nadjusted to a specific specialist or whole laboratory to reproduce the manner\nof calculating the H-score. Thus, EndoNet is effective and robust in the\nanalysis of histology slides, which can improve and significantly accelerate\nthe work of pathologists.",
        "translated": ""
    },
    {
        "title": "Target-Grounded Graph-Aware Transformer for Aerial Vision-and-Dialog\n  Navigation",
        "url": "http://arxiv.org/abs/2308.11561v2",
        "pub_date": "2023-08-22",
        "summary": "This report details the method of the winning entry of the AVDN Challenge in\nICCV 2023. The competition addresses the Aerial Navigation from Dialog History\n(ANDH) task, which requires a drone agent to associate dialog history with\naerial observations to reach the destination. For better cross-modal grounding\nabilities of the drone agent, we propose a Target-Grounded Graph-Aware\nTransformer (TG-GAT) framework. Concretely, TG-GAT first leverages a\ngraph-aware transformer to capture spatiotemporal dependency, which benefits\nnavigation state tracking and robust action planning. In addition, an auxiliary\nvisual grounding task is devised to boost the agent's awareness of referred\nlandmarks. Moreover, a hybrid augmentation strategy based on large language\nmodels is utilized to mitigate data scarcity limitations. Our TG-GAT framework\nwon the AVDN Challenge 2023, with 2.2% and 3.0% absolute improvements over the\nbaseline on SPL and SR metrics, respectively. The code is available at\nhttps://github.com/yifeisu/avdn-challenge.",
        "translated": ""
    },
    {
        "title": "Open Set Synthetic Image Source Attribution",
        "url": "http://arxiv.org/abs/2308.11557v1",
        "pub_date": "2023-08-22",
        "summary": "AI-generated images have become increasingly realistic and have garnered\nsignificant public attention. While synthetic images are intriguing due to\ntheir realism, they also pose an important misinformation threat. To address\nthis new threat, researchers have developed multiple algorithms to detect\nsynthetic images and identify their source generators. However, most existing\nsource attribution techniques are designed to operate in a closed-set scenario,\ni.e. they can only be used to discriminate between known image generators. By\ncontrast, new image-generation techniques are rapidly emerging. To contend with\nthis, there is a great need for open-set source attribution techniques that can\nidentify when synthetic images have originated from new, unseen generators. To\naddress this problem, we propose a new metric learning-based approach. Our\ntechnique works by learning transferrable embeddings capable of discriminating\nbetween generators, even when they are not seen during training. An image is\nfirst assigned to a candidate generator, then is accepted or rejected based on\nits distance in the embedding space from known generators' learned reference\npoints. Importantly, we identify that initializing our source attribution\nembedding network by pretraining it on image camera identification can improve\nour embeddings' transferability. Through a series of experiments, we\ndemonstrate our approach's ability to attribute the source of synthetic images\nin open-set scenarios.",
        "translated": ""
    },
    {
        "title": "Multi-event Video-Text Retrieval",
        "url": "http://arxiv.org/abs/2308.11551v1",
        "pub_date": "2023-08-22",
        "summary": "Video-Text Retrieval (VTR) is a crucial multi-modal task in an era of massive\nvideo-text data on the Internet. A plethora of work characterized by using a\ntwo-stream Vision-Language model architecture that learns a joint\nrepresentation of video-text pairs has become a prominent approach for the VTR\ntask. However, these models operate under the assumption of bijective\nvideo-text correspondences and neglect a more practical scenario where video\ncontent usually encompasses multiple events, while texts like user queries or\nwebpage metadata tend to be specific and correspond to single events. This\nestablishes a gap between the previous training objective and real-world\napplications, leading to the potential performance degradation of earlier\nmodels during inference. In this study, we introduce the Multi-event Video-Text\nRetrieval (MeVTR) task, addressing scenarios in which each video contains\nmultiple different events, as a niche scenario of the conventional Video-Text\nRetrieval Task. We present a simple model, Me-Retriever, which incorporates key\nevent video representation and a new MeVTR loss for the MeVTR task.\nComprehensive experiments show that this straightforward framework outperforms\nother models in the Video-to-Text and Text-to-Video tasks, effectively\nestablishing a robust baseline for the MeVTR task. We believe this work serves\nas a strong foundation for future studies. Code is available at\nhttps://github.com/gengyuanmax/MeVTR.",
        "translated": ""
    },
    {
        "title": "CHORUS: Learning Canonicalized 3D Human-Object Spatial Relations from\n  Unbounded Synthesized Images",
        "url": "http://arxiv.org/abs/2308.12288v1",
        "pub_date": "2023-08-23",
        "summary": "We present a method for teaching machines to understand and model the\nunderlying spatial common sense of diverse human-object interactions in 3D in a\nself-supervised way. This is a challenging task, as there exist specific\nmanifolds of the interactions that can be considered human-like and natural,\nbut the human pose and the geometry of objects can vary even for similar\ninteractions. Such diversity makes the annotating task of 3D interactions\ndifficult and hard to scale, which limits the potential to reason about that in\na supervised way. One way of learning the 3D spatial relationship between\nhumans and objects during interaction is by showing multiple 2D images captured\nfrom different viewpoints when humans interact with the same type of objects.\nThe core idea of our method is to leverage a generative model that produces\nhigh-quality 2D images from an arbitrary text prompt input as an \"unbounded\"\ndata generator with effective controllability and view diversity. Despite its\nimperfection of the image quality over real images, we demonstrate that the\nsynthesized images are sufficient to learn the 3D human-object spatial\nrelations. We present multiple strategies to leverage the synthesized images,\nincluding (1) the first method to leverage a generative image model for 3D\nhuman-object spatial relation learning; (2) a framework to reason about the 3D\nspatial relations from inconsistent 2D cues in a self-supervised manner via 3D\noccupancy reasoning with pose canonicalization; (3) semantic clustering to\ndisambiguate different types of interactions with the same object types; and\n(4) a novel metric to assess the quality of 3D spatial learning of interaction.\nProject Page: https://jellyheadandrew.github.io/projects/chorus",
        "translated": ""
    },
    {
        "title": "A Generative Approach for Image Registration of Visible-Thermal (VT)\n  Cancer Faces",
        "url": "http://arxiv.org/abs/2308.12271v1",
        "pub_date": "2023-08-23",
        "summary": "Since thermal imagery offers a unique modality to investigate pain, the U.S.\nNational Institutes of Health (NIH) has collected a large and diverse set of\ncancer patient facial thermograms for AI-based pain research. However,\ndiffering angles from camera capture between thermal and visible sensors has\nled to misalignment between Visible-Thermal (VT) images. We modernize the\nclassic computer vision task of image registration by applying and modifying a\ngenerative alignment algorithm to register VT cancer faces, without the need\nfor a reference or alignment parameters. By registering VT faces, we\ndemonstrate that the quality of thermal images produced in the generative AI\ndownstream task of Visible-to-Thermal (V2T) image translation significantly\nimproves up to 52.5\\%, than without registration. Images in this paper have\nbeen approved by the NIH NCI for public dissemination.",
        "translated": ""
    },
    {
        "title": "MolGrapher: Graph-based Visual Recognition of Chemical Structures",
        "url": "http://arxiv.org/abs/2308.12234v1",
        "pub_date": "2023-08-23",
        "summary": "The automatic analysis of chemical literature has immense potential to\naccelerate the discovery of new materials and drugs. Much of the critical\ninformation in patent documents and scientific articles is contained in\nfigures, depicting the molecule structures. However, automatically parsing the\nexact chemical structure is a formidable challenge, due to the amount of\ndetailed information, the diversity of drawing styles, and the need for\ntraining data. In this work, we introduce MolGrapher to recognize chemical\nstructures visually. First, a deep keypoint detector detects the atoms. Second,\nwe treat all candidate atoms and bonds as nodes and put them in a graph. This\nconstruct allows a natural graph representation of the molecule. Last, we\nclassify atom and bond nodes in the graph with a Graph Neural Network. To\naddress the lack of real training data, we propose a synthetic data generation\npipeline producing diverse and realistic results. In addition, we introduce a\nlarge-scale benchmark of annotated real molecule images, USPTO-30K, to spur\nresearch on this critical topic. Extensive experiments on five datasets show\nthat our approach significantly outperforms classical and learning-based\nmethods in most settings. Code, models, and datasets are available.",
        "translated": ""
    },
    {
        "title": "SPPNet: A Single-Point Prompt Network for Nuclei Image Segmentation",
        "url": "http://arxiv.org/abs/2308.12231v1",
        "pub_date": "2023-08-23",
        "summary": "Image segmentation plays an essential role in nuclei image analysis.\nRecently, the segment anything model has made a significant breakthrough in\nsuch tasks. However, the current model exists two major issues for cell\nsegmentation: (1) the image encoder of the segment anything model involves a\nlarge number of parameters. Retraining or even fine-tuning the model still\nrequires expensive computational resources. (2) in point prompt mode, points\nare sampled from the center of the ground truth and more than one set of points\nis expected to achieve reliable performance, which is not efficient for\npractical applications. In this paper, a single-point prompt network is\nproposed for nuclei image segmentation, called SPPNet. We replace the original\nimage encoder with a lightweight vision transformer. Also, an effective\nconvolutional block is added in parallel to extract the low-level semantic\ninformation from the image and compensate for the performance degradation due\nto the small image encoder. We propose a new point-sampling method based on the\nGaussian kernel. The proposed model is evaluated on the MoNuSeg-2018 dataset.\nThe result demonstrated that SPPNet outperforms existing U-shape architectures\nand shows faster convergence in training. Compared to the segment anything\nmodel, SPPNet shows roughly 20 times faster inference, with 1/70 parameters and\ncomputational cost. Particularly, only one set of points is required in both\nthe training and inference phases, which is more reasonable for clinical\napplications. The code for our work and more technical details can be found at\nhttps://github.com/xq141839/SPPNet.",
        "translated": ""
    },
    {
        "title": "CIParsing: Unifying Causality Properties into Multiple Human Parsing",
        "url": "http://arxiv.org/abs/2308.12218v1",
        "pub_date": "2023-08-23",
        "summary": "Existing methods of multiple human parsing (MHP) apply statistical models to\nacquire underlying associations between images and labeled body parts. However,\nacquired associations often contain many spurious correlations that degrade\nmodel generalization, leading statistical models to be vulnerable to visually\ncontextual variations in images (e.g., unseen image styles/external\ninterventions). To tackle this, we present a causality inspired parsing\nparadigm termed CIParsing, which follows fundamental causal principles\ninvolving two causal properties for human parsing (i.e., the causal diversity\nand the causal invariance). Specifically, we assume that an input image is\nconstructed by a mix of causal factors (the characteristics of body parts) and\nnon-causal factors (external contexts), where only the former ones cause the\ngeneration process of human parsing.Since causal/non-causal factors are\nunobservable, a human parser in proposed CIParsing is required to construct\nlatent representations of causal factors and learns to enforce representations\nto satisfy the causal properties. In this way, the human parser is able to rely\non causal factors w.r.t relevant evidence rather than non-causal factors w.r.t\nspurious correlations, thus alleviating model degradation and yielding improved\nparsing ability. Notably, the CIParsing is designed in a plug-and-play fashion\nand can be integrated into any existing MHP models. Extensive experiments\nconducted on two widely used benchmarks demonstrate the effectiveness and\ngeneralizability of our method.",
        "translated": ""
    },
    {
        "title": "SG-Former: Self-guided Transformer with Evolving Token Reallocation",
        "url": "http://arxiv.org/abs/2308.12216v1",
        "pub_date": "2023-08-23",
        "summary": "Vision Transformer has demonstrated impressive success across various vision\ntasks. However, its heavy computation cost, which grows quadratically with\nrespect to the token sequence length, largely limits its power in handling\nlarge feature maps. To alleviate the computation cost, previous works rely on\neither fine-grained self-attentions restricted to local small regions, or\nglobal self-attentions but to shorten the sequence length resulting in coarse\ngranularity. In this paper, we propose a novel model, termed as Self-guided\nTransformer~(SG-Former), towards effective global self-attention with adaptive\nfine granularity. At the heart of our approach is to utilize a significance\nmap, which is estimated through hybrid-scale self-attention and evolves itself\nduring training, to reallocate tokens based on the significance of each region.\nIntuitively, we assign more tokens to the salient regions for achieving\nfine-grained attention, while allocating fewer tokens to the minor regions in\nexchange for efficiency and global receptive fields. The proposed SG-Former\nachieves performance superior to state of the art: our base size model achieves\n\\textbf{84.7\\%} Top-1 accuracy on ImageNet-1K, \\textbf{51.2mAP} bbAP on CoCo,\n\\textbf{52.7mIoU} on ADE20K surpassing the Swin Transformer by \\textbf{+1.3\\% /\n+2.7 mAP/ +3 mIoU}, with lower computation costs and fewer parameters. The code\nis available at\n\\href{https://github.com/OliverRensu/SG-Former}{https://github.com/OliverRensu/SG-Former}",
        "translated": ""
    },
    {
        "title": "CLIPN for Zero-Shot OOD Detection: Teaching CLIP to Say No",
        "url": "http://arxiv.org/abs/2308.12213v2",
        "pub_date": "2023-08-23",
        "summary": "Out-of-distribution (OOD) detection refers to training the model on an\nin-distribution (ID) dataset to classify whether the input images come from\nunknown classes. Considerable effort has been invested in designing various OOD\ndetection methods based on either convolutional neural networks or\ntransformers. However, zero-shot OOD detection methods driven by CLIP, which\nonly require class names for ID, have received less attention. This paper\npresents a novel method, namely CLIP saying no (CLIPN), which empowers the\nlogic of saying no within CLIP. Our key motivation is to equip CLIP with the\ncapability of distinguishing OOD and ID samples using positive-semantic prompts\nand negation-semantic prompts. Specifically, we design a novel learnable no\nprompt and a no text encoder to capture negation semantics within images.\nSubsequently, we introduce two loss functions: the image-text binary-opposite\nloss and the text semantic-opposite loss, which we use to teach CLIPN to\nassociate images with no prompts, thereby enabling it to identify unknown\nsamples. Furthermore, we propose two threshold-free inference algorithms to\nperform OOD detection by utilizing negation semantics from no prompts and the\ntext encoder. Experimental results on 9 benchmark datasets (3 ID datasets and 6\nOOD datasets) for the OOD detection task demonstrate that CLIPN, based on\nViT-B-16, outperforms 7 well-used algorithms by at least 2.34% and 11.64% in\nterms of AUROC and FPR95 for zero-shot OOD detection on ImageNet-1K. Our CLIPN\ncan serve as a solid foundation for effectively leveraging CLIP in downstream\nOOD tasks. The code is available on https://github.com/xmed-lab/CLIPN.",
        "translated": ""
    },
    {
        "title": "Towards Real-Time Analysis of Broadcast Badminton Videos",
        "url": "http://arxiv.org/abs/2308.12199v1",
        "pub_date": "2023-08-23",
        "summary": "Analysis of player movements is a crucial subset of sports analysis. Existing\nplayer movement analysis methods use recorded videos after the match is over.\nIn this work, we propose an end-to-end framework for player movement analysis\nfor badminton matches on live broadcast match videos. We only use the visual\ninputs from the match and, unlike other approaches which use multi-modal sensor\ndata, our approach uses only visual cues. We propose a method to calculate the\non-court distance covered by both the players from the video feed of a live\nbroadcast badminton match. To perform this analysis, we focus on the gameplay\nby removing replays and other redundant parts of the broadcast match. We then\nperform player tracking to identify and track the movements of both players in\neach frame. Finally, we calculate the distance covered by each player and the\naverage speed with which they move on the court. We further show a heatmap of\nthe areas covered by the player on the court which is useful for analyzing the\ngameplay of the player. Our proposed framework was successfully used to analyze\nlive broadcast matches in real-time during the Premier Badminton League 2019\n(PBL 2019), with commentators and broadcasters appreciating the utility.",
        "translated": ""
    },
    {
        "title": "Sign Language Translation with Iterative Prototype",
        "url": "http://arxiv.org/abs/2308.12191v1",
        "pub_date": "2023-08-23",
        "summary": "This paper presents IP-SLT, a simple yet effective framework for sign\nlanguage translation (SLT). Our IP-SLT adopts a recurrent structure and\nenhances the semantic representation (prototype) of the input sign language\nvideo via an iterative refinement manner. Our idea mimics the behavior of human\nreading, where a sentence can be digested repeatedly, till reaching accurate\nunderstanding. Technically, IP-SLT consists of feature extraction, prototype\ninitialization, and iterative prototype refinement. The initialization module\ngenerates the initial prototype based on the visual feature extracted by the\nfeature extraction module. Then, the iterative refinement module leverages the\ncross-attention mechanism to polish the previous prototype by aggregating it\nwith the original video feature. Through repeated refinement, the prototype\nfinally converges to a more stable and accurate state, leading to a fluent and\nappropriate translation. In addition, to leverage the sequential dependence of\nprototypes, we further propose an iterative distillation loss to compress the\nknowledge of the final iteration into previous ones. As the autoregressive\ndecoding process is executed only once in inference, our IP-SLT is ready to\nimprove various SLT systems with acceptable overhead. Extensive experiments are\nconducted on public benchmarks to demonstrate the effectiveness of the IP-SLT.",
        "translated": ""
    },
    {
        "title": "Tumor-Centered Patching for Enhanced Medical Image Segmentation",
        "url": "http://arxiv.org/abs/2308.12168v1",
        "pub_date": "2023-08-23",
        "summary": "The realm of medical image diagnosis has advanced significantly with the\nintegration of computer-aided diagnosis and surgical systems. However,\nchallenges persist, particularly in achieving precise image segmentation. While\ndeep learning techniques show potential, obstacles like limited resources, slow\nconvergence, and class imbalance impede their effectiveness. Traditional\npatch-based methods, though common, struggle to capture intricate tumor\nboundaries and often lead to redundant samples, compromising computational\nefficiency and feature quality. To tackle these issues, this research\nintroduces an innovative approach centered on the tumor itself for patch-based\nimage analysis. This novel tumor-centered patching method aims to address the\nclass imbalance and boundary deficiencies, enabling focused and accurate tumor\nsegmentation. By aligning patches with the tumor's anatomical context, this\ntechnique enhances feature extraction accuracy and reduces computational load.\nExperimental results demonstrate improved class imbalance, with segmentation\nscores of 0.78, 0.76, and 0.71 for whole, core, and enhancing tumors,\nrespectively using a lightweight simple U-Net. This approach shows potential\nfor enhancing medical image segmentation and improving computer-aided diagnosis\nsystems.",
        "translated": ""
    },
    {
        "title": "ROAM: Robust and Object-aware Motion Generation using Neural Pose\n  Descriptors",
        "url": "http://arxiv.org/abs/2308.12969v1",
        "pub_date": "2023-08-24",
        "summary": "Existing automatic approaches for 3D virtual character motion synthesis\nsupporting scene interactions do not generalise well to new objects outside\ntraining distributions, even when trained on extensive motion capture datasets\nwith diverse objects and annotated interactions. This paper addresses this\nlimitation and shows that robustness and generalisation to novel scene objects\nin 3D object-aware character synthesis can be achieved by training a motion\nmodel with as few as one reference object. We leverage an implicit feature\nrepresentation trained on object-only datasets, which encodes an\nSE(3)-equivariant descriptor field around the object. Given an unseen object\nand a reference pose-object pair, we optimise for the object-aware pose that is\nclosest in the feature space to the reference pose. Finally, we use l-NSM,\ni.e., our motion generation model that is trained to seamlessly transition from\nlocomotion to object interaction with the proposed bidirectional pose blending\nscheme. Through comprehensive numerical comparisons to state-of-the-art methods\nand in a user study, we demonstrate substantial improvements in 3D virtual\ncharacter motion and interaction quality and robustness to scenarios with\nunseen objects. Our project page is available at\nhttps://vcai.mpi-inf.mpg.de/projects/ROAM/.",
        "translated": ""
    },
    {
        "title": "NeO 360: Neural Fields for Sparse View Synthesis of Outdoor Scenes",
        "url": "http://arxiv.org/abs/2308.12967v1",
        "pub_date": "2023-08-24",
        "summary": "Recent implicit neural representations have shown great results for novel\nview synthesis. However, existing methods require expensive per-scene\noptimization from many views hence limiting their application to real-world\nunbounded urban settings where the objects of interest or backgrounds are\nobserved from very few views. To mitigate this challenge, we introduce a new\napproach called NeO 360, Neural fields for sparse view synthesis of outdoor\nscenes. NeO 360 is a generalizable method that reconstructs 360{\\deg} scenes\nfrom a single or a few posed RGB images. The essence of our approach is in\ncapturing the distribution of complex real-world outdoor 3D scenes and using a\nhybrid image-conditional triplanar representation that can be queried from any\nworld point. Our representation combines the best of both voxel-based and\nbird's-eye-view (BEV) representations and is more effective and expressive than\neach. NeO 360's representation allows us to learn from a large collection of\nunbounded 3D scenes while offering generalizability to new views and novel\nscenes from as few as a single image during inference. We demonstrate our\napproach on the proposed challenging 360{\\deg} unbounded dataset, called NeRDS\n360, and show that NeO 360 outperforms state-of-the-art generalizable methods\nfor novel view synthesis while also offering editing and composition\ncapabilities. Project page:\nhttps://zubair-irshad.github.io/projects/neo360.html",
        "translated": ""
    },
    {
        "title": "Scenimefy: Learning to Craft Anime Scene via Semi-Supervised\n  Image-to-Image Translation",
        "url": "http://arxiv.org/abs/2308.12968v1",
        "pub_date": "2023-08-24",
        "summary": "Automatic high-quality rendering of anime scenes from complex real-world\nimages is of significant practical value. The challenges of this task lie in\nthe complexity of the scenes, the unique features of anime style, and the lack\nof high-quality datasets to bridge the domain gap. Despite promising attempts,\nprevious efforts are still incompetent in achieving satisfactory results with\nconsistent semantic preservation, evident stylization, and fine details. In\nthis study, we propose Scenimefy, a novel semi-supervised image-to-image\ntranslation framework that addresses these challenges. Our approach guides the\nlearning with structure-consistent pseudo paired data, simplifying the pure\nunsupervised setting. The pseudo data are derived uniquely from a\nsemantic-constrained StyleGAN leveraging rich model priors like CLIP. We\nfurther apply segmentation-guided data selection to obtain high-quality pseudo\nsupervision. A patch-wise contrastive style loss is introduced to improve\nstylization and fine details. Besides, we contribute a high-resolution anime\nscene dataset to facilitate future research. Our extensive experiments\ndemonstrate the superiority of our method over state-of-the-art baselines in\nterms of both perceptual quality and quantitative performance.",
        "translated": ""
    },
    {
        "title": "Qwen-VL: A Frontier Large Vision-Language Model with Versatile Abilities",
        "url": "http://arxiv.org/abs/2308.12966v1",
        "pub_date": "2023-08-24",
        "summary": "We introduce the Qwen-VL series, a set of large-scale vision-language models\ndesigned to perceive and understand both text and images. Comprising Qwen-VL\nand Qwen-VL-Chat, these models exhibit remarkable performance in tasks like\nimage captioning, question answering, visual localization, and flexible\ninteraction. The evaluation covers a wide range of tasks including zero-shot\ncaptioning, visual or document visual question answering, and grounding. We\ndemonstrate the Qwen-VL outperforms existing Large Vision Language Models\n(LVLMs). We present their architecture, training, capabilities, and\nperformance, highlighting their contributions to advancing multimodal\nartificial intelligence. Code, demo and models are available at\nhttps://github.com/QwenLM/Qwen-VL.",
        "translated": ""
    },
    {
        "title": "POCO: 3D Pose and Shape Estimation with Confidence",
        "url": "http://arxiv.org/abs/2308.12965v1",
        "pub_date": "2023-08-24",
        "summary": "The regression of 3D Human Pose and Shape (HPS) from an image is becoming\nincreasingly accurate. This makes the results useful for downstream tasks like\nhuman action recognition or 3D graphics. Yet, no regressor is perfect, and\naccuracy can be affected by ambiguous image evidence or by poses and appearance\nthat are unseen during training. Most current HPS regressors, however, do not\nreport the confidence of their outputs, meaning that downstream tasks cannot\ndifferentiate accurate estimates from inaccurate ones. To address this, we\ndevelop POCO, a novel framework for training HPS regressors to estimate not\nonly a 3D human body, but also their confidence, in a single feed-forward pass.\nSpecifically, POCO estimates both the 3D body pose and a per-sample variance.\nThe key idea is to introduce a Dual Conditioning Strategy (DCS) for regressing\nuncertainty that is highly correlated to pose reconstruction quality. The POCO\nframework can be applied to any HPS regressor and here we evaluate it by\nmodifying HMR, PARE, and CLIFF. In all cases, training the network to reason\nabout uncertainty helps it learn to more accurately estimate 3D pose. While\nthis was not our goal, the improvement is modest but consistent. Our main\nmotivation is to provide uncertainty estimates for downstream tasks; we\ndemonstrate this in two ways: (1) We use the confidence estimates to bootstrap\nHPS training. Given unlabelled image data, we take the confident estimates of a\nPOCO-trained regressor as pseudo ground truth. Retraining with this\nautomatically-curated data improves accuracy. (2) We exploit uncertainty in\nvideo pose estimation by automatically identifying uncertain frames (e.g. due\nto occlusion) and inpainting these from confident frames. Code and models will\nbe available for research at https://poco.is.tue.mpg.de.",
        "translated": ""
    },
    {
        "title": "Dense Text-to-Image Generation with Attention Modulation",
        "url": "http://arxiv.org/abs/2308.12964v1",
        "pub_date": "2023-08-24",
        "summary": "Existing text-to-image diffusion models struggle to synthesize realistic\nimages given dense captions, where each text prompt provides a detailed\ndescription for a specific image region. To address this, we propose\nDenseDiffusion, a training-free method that adapts a pre-trained text-to-image\nmodel to handle such dense captions while offering control over the scene\nlayout. We first analyze the relationship between generated images' layouts and\nthe pre-trained model's intermediate attention maps. Next, we develop an\nattention modulation method that guides objects to appear in specific regions\naccording to layout guidance. Without requiring additional fine-tuning or\ndatasets, we improve image generation performance given dense captions\nregarding both automatic and human evaluation scores. In addition, we achieve\nsimilar-quality visual results with models specifically trained with layout\nconditions.",
        "translated": ""
    },
    {
        "title": "MapPrior: Bird's-Eye View Map Layout Estimation with Generative Models",
        "url": "http://arxiv.org/abs/2308.12963v1",
        "pub_date": "2023-08-24",
        "summary": "Despite tremendous advancements in bird's-eye view (BEV) perception, existing\nmodels fall short in generating realistic and coherent semantic map layouts,\nand they fail to account for uncertainties arising from partial sensor\ninformation (such as occlusion or limited coverage). In this work, we introduce\nMapPrior, a novel BEV perception framework that combines a traditional\ndiscriminative BEV perception model with a learned generative model for\nsemantic map layouts. Our MapPrior delivers predictions with better accuracy,\nrealism, and uncertainty awareness. We evaluate our model on the large-scale\nnuScenes benchmark. At the time of submission, MapPrior outperforms the\nstrongest competing method, with significantly improved MMD and ECE scores in\ncamera- and LiDAR-based BEV perception.",
        "translated": ""
    },
    {
        "title": "Motion-Guided Masking for Spatiotemporal Representation Learning",
        "url": "http://arxiv.org/abs/2308.12962v1",
        "pub_date": "2023-08-24",
        "summary": "Several recent works have directly extended the image masked autoencoder\n(MAE) with random masking into video domain, achieving promising results.\nHowever, unlike images, both spatial and temporal information are important for\nvideo understanding. This suggests that the random masking strategy that is\ninherited from the image MAE is less effective for video MAE. This motivates\nthe design of a novel masking algorithm that can more efficiently make use of\nvideo saliency. Specifically, we propose a motion-guided masking algorithm\n(MGM) which leverages motion vectors to guide the position of each mask over\ntime. Crucially, these motion-based correspondences can be directly obtained\nfrom information stored in the compressed format of the video, which makes our\nmethod efficient and scalable. On two challenging large-scale video benchmarks\n(Kinetics-400 and Something-Something V2), we equip video MAE with our MGM and\nachieve up to +$1.3\\%$ improvement compared to previous state-of-the-art\nmethods. Additionally, our MGM achieves equivalent performance to previous\nvideo MAE using up to $66\\%$ fewer training epochs. Lastly, we show that MGM\ngeneralizes better to downstream transfer learning and domain adaptation tasks\non the UCF101, HMDB51, and Diving48 datasets, achieving up to +$4.9\\%$\nimprovement compared to baseline methods.",
        "translated": ""
    },
    {
        "title": "Less is More: Towards Efficient Few-shot 3D Semantic Segmentation via\n  Training-free Networks",
        "url": "http://arxiv.org/abs/2308.12961v1",
        "pub_date": "2023-08-24",
        "summary": "To reduce the reliance on large-scale datasets, recent works in 3D\nsegmentation resort to few-shot learning. Current 3D few-shot semantic\nsegmentation methods first pre-train the models on `seen' classes, and then\nevaluate their generalization performance on `unseen' classes. However, the\nprior pre-training stage not only introduces excessive time overhead, but also\nincurs a significant domain gap on `unseen' classes. To tackle these issues, we\npropose an efficient Training-free Few-shot 3D Segmentation netwrok, TFS3D, and\na further training-based variant, TFS3D-T. Without any learnable parameters,\nTFS3D extracts dense representations by trigonometric positional encodings, and\nachieves comparable performance to previous training-based methods. Due to the\nelimination of pre-training, TFS3D can alleviate the domain gap issue and save\na substantial amount of time. Building upon TFS3D, TFS3D-T only requires to\ntrain a lightweight query-support transferring attention (QUEST), which\nenhances the interaction between the few-shot query and support data.\nExperiments demonstrate TFS3D-T improves previous state-of-the-art methods by\n+6.93% and +17.96% mIoU respectively on S3DIS and ScanNet, while reducing the\ntraining time by -90%, indicating superior effectiveness and efficiency.",
        "translated": ""
    },
    {
        "title": "Towards Realistic Zero-Shot Classification via Self Structural Semantic\n  Alignment",
        "url": "http://arxiv.org/abs/2308.12960v1",
        "pub_date": "2023-08-24",
        "summary": "Large-scale pre-trained Vision Language Models (VLMs) have proven effective\nfor zero-shot classification. Despite the success, most traditional VLMs-based\nmethods are restricted by the assumption of partial source supervision or ideal\nvocabularies, which rarely satisfy the open-world scenario. In this paper, we\naim at a more challenging setting, Realistic Zero-Shot Classification, which\nassumes no annotation but instead a broad vocabulary. To address this\nchallenge, we propose the Self Structural Semantic Alignment (S^3A) framework,\nwhich extracts the structural semantic information from unlabeled data while\nsimultaneously self-learning. Our S^3A framework adopts a unique\nCluster-Vote-Prompt-Realign (CVPR) algorithm, which iteratively groups\nunlabeled data to derive structural semantics for pseudo-supervision. Our CVPR\nprocess includes iterative clustering on images, voting within each cluster to\nidentify initial class candidates from the vocabulary, generating\ndiscriminative prompts with large language models to discern confusing\ncandidates, and realigning images and the vocabulary as structural semantic\nalignment. Finally, we propose to self-learn the CLIP image encoder with both\nindividual and structural semantic alignment through a teacher-student learning\nstrategy. Our comprehensive experiments across various generic and fine-grained\nbenchmarks demonstrate that the S^3A method offers substantial improvements\nover existing VLMs-based approaches, achieving a more than 15% accuracy\nimprovement over CLIP on average. Our codes, models, and prompts are publicly\nreleased at https://github.com/sheng-eatamath/S3A.",
        "translated": ""
    },
    {
        "title": "Joint Modeling of Feature, Correspondence, and a Compressed Memory for\n  Video Object Segmentation",
        "url": "http://arxiv.org/abs/2308.13505v1",
        "pub_date": "2023-08-25",
        "summary": "Current prevailing Video Object Segmentation (VOS) methods usually perform\ndense matching between the current and reference frames after extracting their\nfeatures. One on hand, the decoupled modeling restricts the targets information\npropagation only at high-level feature space. On the other hand, the pixel-wise\nmatching leads to a lack of holistic understanding of the targets. To overcome\nthese issues, we propose a unified VOS framework, coined as JointFormer, for\njoint modeling the three elements of feature, correspondence, and a compressed\nmemory. The core design is the Joint Block, utilizing the flexibility of\nattention to simultaneously extract feature and propagate the targets\ninformation to the current tokens and the compressed memory token. This scheme\nallows to perform extensive information propagation and discriminative feature\nlearning. To incorporate the long-term temporal targets information, we also\ndevise a customized online updating mechanism for the compressed memory token,\nwhich can prompt the information flow along the temporal dimension and thus\nimprove the global modeling capability. Under the design, our method achieves a\nnew state-of-art performance on DAVIS 2017 val/test-dev (89.7% and 87.6%) and\nYouTube-VOS 2018/2019 val (87.0% and 87.0%) benchmarks, outperforming existing\nworks by a large margin.",
        "translated": ""
    },
    {
        "title": "A2Q: Accumulator-Aware Quantization with Guaranteed Overflow Avoidance",
        "url": "http://arxiv.org/abs/2308.13504v1",
        "pub_date": "2023-08-25",
        "summary": "We present accumulator-aware quantization (A2Q), a novel weight quantization\nmethod designed to train quantized neural networks (QNNs) to avoid overflow\nwhen using low-precision accumulators during inference. A2Q introduces a unique\nformulation inspired by weight normalization that constrains the L1-norm of\nmodel weights according to accumulator bit width bounds that we derive. Thus,\nin training QNNs for low-precision accumulation, A2Q also inherently promotes\nunstructured weight sparsity to guarantee overflow avoidance. We apply our\nmethod to deep learning-based computer vision tasks to show that A2Q can train\nQNNs for low-precision accumulators while maintaining model accuracy\ncompetitive with a floating-point baseline. In our evaluations, we consider the\nimpact of A2Q on both general-purpose platforms and programmable hardware.\nHowever, we primarily target model deployment on FPGAs because they can be\nprogrammed to fully exploit custom accumulator bit widths. Our experimentation\nshows accumulator bit width significantly impacts the resource efficiency of\nFPGA-based accelerators. On average across our benchmarks, A2Q offers up to a\n2.3x reduction in resource utilization over 32-bit accumulator counterparts\nwith 99.2% of the floating-point model accuracy.",
        "translated": ""
    },
    {
        "title": "Attending Generalizability in Course of Deep Fake Detection by Exploring\n  Multi-task Learning",
        "url": "http://arxiv.org/abs/2308.13503v1",
        "pub_date": "2023-08-25",
        "summary": "This work explores various ways of exploring multi-task learning (MTL)\ntechniques aimed at classifying videos as original or manipulated in\ncross-manipulation scenario to attend generalizability in deep fake scenario.\nThe dataset used in our evaluation is FaceForensics++, which features 1000\noriginal videos manipulated by four different techniques, with a total of 5000\nvideos. We conduct extensive experiments on multi-task learning and contrastive\ntechniques, which are well studied in literature for their generalization\nbenefits. It can be concluded that the proposed detection model is quite\ngeneralized, i.e., accurately detects manipulation methods not encountered\nduring training as compared to the state-of-the-art.",
        "translated": ""
    },
    {
        "title": "Open Gaze: An Open-Source Implementation Replicating Google's Eye\n  Tracking Paper",
        "url": "http://arxiv.org/abs/2308.13495v1",
        "pub_date": "2023-08-25",
        "summary": "Eye tracking has been a pivotal tool in diverse fields such as vision\nresearch, language analysis, and usability assessment. The majority of prior\ninvestigations, however, have concentrated on expansive desktop displays\nemploying specialized, costly eye tracking hardware that lacks scalability.\nRemarkably little insight exists into ocular movement patterns on smartphones,\ndespite their widespread adoption and significant usage. In this manuscript, we\npresent an open-source implementation of a smartphone-based gaze tracker that\nemulates the methodology proposed by a GooglePaper (whose source code remains\nproprietary). Our focus is on attaining accuracy comparable to that attained\nthrough the GooglePaper's methodology, without the necessity for supplementary\nhardware. Through the integration of machine learning techniques, we unveil an\naccurate eye tracking solution that is native to smartphones. Our approach\ndemonstrates precision akin to the state-of-the-art mobile eye trackers, which\nare characterized by a cost that is two orders of magnitude higher. Leveraging\nthe vast MIT GazeCapture dataset, which is available through registration on\nthe dataset's website, we successfully replicate crucial findings from previous\nstudies concerning ocular motion behavior in oculomotor tasks and saliency\nanalyses during natural image observation. Furthermore, we emphasize the\napplicability of smartphone-based gaze tracking in discerning reading\ncomprehension challenges. Our findings exhibit the inherent potential to\namplify eye movement research by significant proportions, accommodating\nparticipation from thousands of subjects with explicit consent. This\nscalability not only fosters advancements in vision research, but also extends\nits benefits to domains such as accessibility enhancement and healthcare\napplications.",
        "translated": ""
    },
    {
        "title": "Eventful Transformers: Leveraging Temporal Redundancy in Vision\n  Transformers",
        "url": "http://arxiv.org/abs/2308.13494v1",
        "pub_date": "2023-08-25",
        "summary": "Vision Transformers achieve impressive accuracy across a range of visual\nrecognition tasks. Unfortunately, their accuracy frequently comes with high\ncomputational costs. This is a particular issue in video recognition, where\nmodels are often applied repeatedly across frames or temporal chunks. In this\nwork, we exploit temporal redundancy between subsequent inputs to reduce the\ncost of Transformers for video processing. We describe a method for identifying\nand re-processing only those tokens that have changed significantly over time.\nOur proposed family of models, Eventful Transformers, can be converted from\nexisting Transformers (often without any re-training) and give adaptive control\nover the compute cost at runtime. We evaluate our method on large-scale\ndatasets for video object detection (ImageNet VID) and action recognition\n(EPIC-Kitchens 100). Our approach leads to significant computational savings\n(on the order of 2-4x) with only minor reductions in accuracy.",
        "translated": ""
    },
    {
        "title": "Ultrafast-and-Ultralight ConvNet-Based Intelligent Monitoring System for\n  Diagnosing Early-Stage Mpox Anytime and Anywhere",
        "url": "http://arxiv.org/abs/2308.13492v1",
        "pub_date": "2023-08-25",
        "summary": "Due to the lack of more efficient diagnostic tools for monkeypox, its spread\nremains unchecked, presenting a formidable challenge to global health. While\nthe high efficacy of deep learning models for monkeypox diagnosis has been\ndemonstrated in related studies, the overlook of inference speed, the parameter\nsize and diagnosis performance for early-stage monkeypox renders the models\ninapplicable in real-world settings. To address these challenges, we proposed\nan ultrafast and ultralight network named Fast-MpoxNet. Fast-MpoxNet possesses\nonly 0.27M parameters and can process input images at 68 frames per second\n(FPS) on the CPU. To counteract the diagnostic performance limitation brought\nabout by the small model capacity, it integrates the attention-based feature\nfusion module and the multiple auxiliary losses enhancement strategy for better\ndetecting subtle image changes and optimizing weights. Using transfer learning\nand five-fold cross-validation, Fast-MpoxNet achieves 94.26% Accuracy on the\nMpox dataset. Notably, its recall for early-stage monkeypox achieves 93.65%. By\nadopting data augmentation, our model's Accuracy rises to 98.40% and attains a\nPracticality Score (A new metric for measuring model practicality in real-time\ndiagnosis application) of 0.80. We also developed an application system named\nMpox-AISM V2 for both personal computers and mobile phones. Mpox-AISM V2\nfeatures ultrafast responses, offline functionality, and easy deployment,\nenabling accurate and real-time diagnosis for both the public and individuals\nin various real-world settings, especially in populous settings during the\noutbreak. Our work could potentially mitigate future monkeypox outbreak and\nilluminate a fresh paradigm for developing real-time diagnostic tools in the\nhealthcare field.",
        "translated": ""
    },
    {
        "title": "Temporal Uncertainty Localization to Enable Human-in-the-loop Analysis\n  of Dynamic Contrast-enhanced Cardiac MRI Datasets",
        "url": "http://arxiv.org/abs/2308.13488v1",
        "pub_date": "2023-08-25",
        "summary": "Dynamic contrast-enhanced (DCE) cardiac magnetic resonance imaging (CMRI) is\na widely used modality for diagnosing myocardial blood flow (perfusion)\nabnormalities. During a typical free-breathing DCE-CMRI scan, close to 300\ntime-resolved images of myocardial perfusion are acquired at various contrast\n\"wash in/out\" phases. Manual segmentation of myocardial contours in each\ntime-frame of a DCE image series can be tedious and time-consuming,\nparticularly when non-rigid motion correction has failed or is unavailable.\nWhile deep neural networks (DNNs) have shown promise for analyzing DCE-CMRI\ndatasets, a \"dynamic quality control\" (dQC) technique for reliably detecting\nfailed segmentations is lacking. Here we propose a new space-time uncertainty\nmetric as a dQC tool for DNN-based segmentation of free-breathing DCE-CMRI\ndatasets by validating the proposed metric on an external dataset and\nestablishing a human-in-the-loop framework to improve the segmentation results.\nIn the proposed approach, we referred the top 10% most uncertain segmentations\nas detected by our dQC tool to the human expert for refinement. This approach\nresulted in a significant increase in the Dice score (p&lt;0.001) and a notable\ndecrease in the number of images with failed segmentation (16.2% to 11.3%)\nwhereas the alternative approach of randomly selecting the same number of\nsegmentations for human referral did not achieve any significant improvement.\nOur results suggest that the proposed dQC framework has the potential to\naccurately identify poor-quality segmentations and may enable efficient\nDNN-based analysis of DCE-CMRI in a human-in-the-loop pipeline for clinical\ninterpretation and reporting of dynamic CMRI datasets.",
        "translated": ""
    },
    {
        "title": "Unlocking the Performance of Proximity Sensors by Utilizing Transient\n  Histograms",
        "url": "http://arxiv.org/abs/2308.13473v1",
        "pub_date": "2023-08-25",
        "summary": "We provide methods which recover planar scene geometry by utilizing the\ntransient histograms captured by a class of close-range time-of-flight (ToF)\ndistance sensor. A transient histogram is a one dimensional temporal waveform\nwhich encodes the arrival time of photons incident on the ToF sensor.\nTypically, a sensor processes the transient histogram using a proprietary\nalgorithm to produce distance estimates, which are commonly used in several\nrobotics applications. Our methods utilize the transient histogram directly to\nenable recovery of planar geometry more accurately than is possible using only\nproprietary distance estimates, and consistent recovery of the albedo of the\nplanar surface, which is not possible with proprietary distance estimates\nalone. This is accomplished via a differentiable rendering pipeline, which\nsimulates the transient imaging process, allowing direct optimization of scene\ngeometry to match observations. To validate our methods, we capture 3,800\nmeasurements of eight planar surfaces from a wide range of viewpoints, and show\nthat our method outperforms the proprietary-distance-estimate baseline by an\norder of magnitude in most scenarios. We demonstrate a simple robotics\napplication which uses our method to sense the distance to and slope of a\nplanar surface from a sensor mounted on the end effector of a robot arm.",
        "translated": ""
    },
    {
        "title": "A Fast Minimization Algorithm for the Euler Elastica Model Based on a\n  Bilinear Decomposition",
        "url": "http://arxiv.org/abs/2308.13471v1",
        "pub_date": "2023-08-25",
        "summary": "The Euler Elastica (EE) model with surface curvature can generate\nartifact-free results compared with the traditional total variation\nregularization model in image processing. However, strong nonlinearity and\nsingularity due to the curvature term in the EE model pose a great challenge\nfor one to design fast and stable algorithms for the EE model. In this paper,\nwe propose a new, fast, hybrid alternating minimization (HALM) algorithm for\nthe EE model based on a bilinear decomposition of the gradient of the\nunderlying image and prove the global convergence of the minimizing sequence\ngenerated by the algorithm under mild conditions. The HALM algorithm comprises\nthree sub-minimization problems and each is either solved in the closed form or\napproximated by fast solvers making the new algorithm highly accurate and\nefficient. We also discuss the extension of the HALM strategy to deal with\ngeneral curvature-based variational models, especially with a Lipschitz smooth\nfunctional of the curvature. A host of numerical experiments are conducted to\nshow that the new algorithm produces good results with much-improved efficiency\ncompared to other state-of-the-art algorithms for the EE model. As one of the\nbenchmarks, we show that the average running time of the HALM algorithm is at\nmost one-quarter of that of the fast operator-splitting-based\nDeng-Glowinski-Tai algorithm.",
        "translated": ""
    },
    {
        "title": "RestNet: Boosting Cross-Domain Few-Shot Segmentation with Residual\n  Transformation Network",
        "url": "http://arxiv.org/abs/2308.13469v1",
        "pub_date": "2023-08-25",
        "summary": "Cross-domain few-shot segmentation (CD-FSS) aims to achieve semantic\nsegmentation in previously unseen domains with a limited number of annotated\nsamples. Although existing CD-FSS models focus on cross-domain feature\ntransformation, relying exclusively on inter-domain knowledge transfer may lead\nto the loss of critical intra-domain information. To this end, we propose a\nnovel residual transformation network (RestNet) that facilitates knowledge\ntransfer while retaining the intra-domain support-query feature information.\nSpecifically, we propose a Semantic Enhanced Anchor Transform (SEAT) module\nthat maps features to a stable domain-agnostic space using advanced semantics.\nAdditionally, an Intra-domain Residual Enhancement (IRE) module is designed to\nmaintain the intra-domain representation of the original discriminant space in\nthe new space. We also propose a mask prediction strategy based on prototype\nfusion to help the model gradually learn how to segment. Our RestNet can\ntransfer cross-domain knowledge from both inter-domain and intra-domain without\nrequiring additional fine-tuning. Extensive experiments on ISIC, Chest X-ray,\nand FSS-1000 show that our RestNet achieves state-of-the-art performance. Our\ncode will be available soon.",
        "translated": ""
    },
    {
        "title": "Efficient Discovery and Effective Evaluation of Visual Perceptual\n  Similarity: A Benchmark and Beyond",
        "url": "http://arxiv.org/abs/2308.14753v1",
        "pub_date": "2023-08-28",
        "summary": "Visual similarities discovery (VSD) is an important task with broad\ne-commerce applications. Given an image of a certain object, the goal of VSD is\nto retrieve images of different objects with high perceptual visual similarity.\nAlthough being a highly addressed problem, the evaluation of proposed methods\nfor VSD is often based on a proxy of an identification-retrieval task,\nevaluating the ability of a model to retrieve different images of the same\nobject. We posit that evaluating VSD methods based on identification tasks is\nlimited, and faithful evaluation must rely on expert annotations. In this\npaper, we introduce the first large-scale fashion visual similarity benchmark\ndataset, consisting of more than 110K expert-annotated image pairs. Besides\nthis major contribution, we share insight from the challenges we faced while\ncurating this dataset. Based on these insights, we propose a novel and\nefficient labeling procedure that can be applied to any dataset. Our analysis\nexamines its limitations and inductive biases, and based on these findings, we\npropose metrics to mitigate those limitations. Though our primary focus lies on\nvisual similarity, the methodologies we present have broader applications for\ndiscovering and evaluating perceptual similarity across various domains.",
        "translated": ""
    },
    {
        "title": "MagicEdit: High-Fidelity and Temporally Coherent Video Editing",
        "url": "http://arxiv.org/abs/2308.14749v1",
        "pub_date": "2023-08-28",
        "summary": "In this report, we present MagicEdit, a surprisingly simple yet effective\nsolution to the text-guided video editing task. We found that high-fidelity and\ntemporally coherent video-to-video translation can be achieved by explicitly\ndisentangling the learning of content, structure and motion signals during\ntraining. This is in contradict to most existing methods which attempt to\njointly model both the appearance and temporal representation within a single\nframework, which we argue, would lead to degradation in per-frame quality.\nDespite its simplicity, we show that MagicEdit supports various downstream\nvideo editing tasks, including video stylization, local editing, video-MagicMix\nand video outpainting.",
        "translated": ""
    },
    {
        "title": "MagicAvatar: Multimodal Avatar Generation and Animation",
        "url": "http://arxiv.org/abs/2308.14748v1",
        "pub_date": "2023-08-28",
        "summary": "This report presents MagicAvatar, a framework for multimodal video generation\nand animation of human avatars. Unlike most existing methods that generate\navatar-centric videos directly from multimodal inputs (e.g., text prompts),\nMagicAvatar explicitly disentangles avatar video generation into two stages:\n(1) multimodal-to-motion and (2) motion-to-video generation. The first stage\ntranslates the multimodal inputs into motion/ control signals (e.g., human\npose, depth, DensePose); while the second stage generates avatar-centric video\nguided by these motion signals. Additionally, MagicAvatar supports avatar\nanimation by simply providing a few images of the target person. This\ncapability enables the animation of the provided human identity according to\nthe specific motion derived from the first stage. We demonstrate the\nflexibility of MagicAvatar through various applications, including text-guided\nand video-guided avatar generation, as well as multimodal avatar animation.",
        "translated": ""
    },
    {
        "title": "CoVR: Learning Composed Video Retrieval from Web Video Captions",
        "url": "http://arxiv.org/abs/2308.14746v1",
        "pub_date": "2023-08-28",
        "summary": "Composed Image Retrieval (CoIR) has recently gained popularity as a task that\nconsiders both text and image queries together, to search for relevant images\nin a database. Most CoIR approaches require manually annotated datasets,\ncomprising image-text-image triplets, where the text describes a modification\nfrom the query image to the target image. However, manual curation of CoIR\ntriplets is expensive and prevents scalability. In this work, we instead\npropose a scalable automatic dataset creation methodology that generates\ntriplets given video-caption pairs, while also expanding the scope of the task\nto include composed video retrieval (CoVR). To this end, we mine paired videos\nwith a similar caption from a large database, and leverage a large language\nmodel to generate the corresponding modification text. Applying this\nmethodology to the extensive WebVid2M collection, we automatically construct\nour WebVid-CoVR dataset, resulting in 1.6 million triplets. Moreover, we\nintroduce a new benchmark for CoVR with a manually annotated evaluation set,\nalong with baseline results. Our experiments further demonstrate that training\na CoVR model on our dataset effectively transfers to CoIR, leading to improved\nstate-of-the-art performance in the zero-shot setup on both the CIRR and\nFashionIQ benchmarks. Our code, datasets, and models are publicly available at\nhttps://imagine.enpc.fr/~ventural/covr.",
        "translated": ""
    },
    {
        "title": "Total Selfie: Generating Full-Body Selfies",
        "url": "http://arxiv.org/abs/2308.14740v1",
        "pub_date": "2023-08-28",
        "summary": "We present a method to generate full-body selfies -- photos that you take of\nyourself, but capturing your whole body as if someone else took the photo of\nyou from a few feet away. Our approach takes as input a pre-captured video of\nyour body, a target pose photo, and a selfie + background pair for each\nlocation. We introduce a novel diffusion-based approach to combine all of this\ninformation into high quality, well-composed photos of you with the desired\npose and background.",
        "translated": ""
    },
    {
        "title": "Flexible Techniques for Differentiable Rendering with 3D Gaussians",
        "url": "http://arxiv.org/abs/2308.14737v1",
        "pub_date": "2023-08-28",
        "summary": "Fast, reliable shape reconstruction is an essential ingredient in many\ncomputer vision applications. Neural Radiance Fields demonstrated that\nphotorealistic novel view synthesis is within reach, but was gated by\nperformance requirements for fast reconstruction of real scenes and objects.\nSeveral recent approaches have built on alternative shape representations, in\nparticular, 3D Gaussians. We develop extensions to these renderers, such as\nintegrating differentiable optical flow, exporting watertight meshes and\nrendering per-ray normals. Additionally, we show how two of the recent methods\nare interoperable with each other. These reconstructions are quick, robust, and\neasily performed on GPU or CPU. For code and visual examples, see\nhttps://leonidk.github.io/fmb-plus",
        "translated": ""
    },
    {
        "title": "PanoSwin: a Pano-style Swin Transformer for Panorama Understanding",
        "url": "http://arxiv.org/abs/2308.14726v1",
        "pub_date": "2023-08-28",
        "summary": "In panorama understanding, the widely used equirectangular projection (ERP)\nentails boundary discontinuity and spatial distortion. It severely deteriorates\nthe conventional CNNs and vision Transformers on panoramas. In this paper, we\npropose a simple yet effective architecture named PanoSwin to learn panorama\nrepresentations with ERP. To deal with the challenges brought by\nequirectangular projection, we explore a pano-style shift windowing scheme and\nnovel pitch attention to address the boundary discontinuity and the spatial\ndistortion, respectively. Besides, based on spherical distance and Cartesian\ncoordinates, we adapt absolute positional embeddings and relative positional\nbiases for panoramas to enhance panoramic geometry information. Realizing that\nplanar image understanding might share some common knowledge with panorama\nunderstanding, we devise a novel two-stage learning framework to facilitate\nknowledge transfer from the planar images to panoramas. We conduct experiments\nagainst the state-of-the-art on various panoramic tasks, i.e., panoramic object\ndetection, panoramic classification, and panoramic layout estimation. The\nexperimental results demonstrate the effectiveness of PanoSwin in panorama\nunderstanding.",
        "translated": ""
    },
    {
        "title": "R3D3: Dense 3D Reconstruction of Dynamic Scenes from Multiple Cameras",
        "url": "http://arxiv.org/abs/2308.14713v1",
        "pub_date": "2023-08-28",
        "summary": "Dense 3D reconstruction and ego-motion estimation are key challenges in\nautonomous driving and robotics. Compared to the complex, multi-modal systems\ndeployed today, multi-camera systems provide a simpler, low-cost alternative.\nHowever, camera-based 3D reconstruction of complex dynamic scenes has proven\nextremely difficult, as existing solutions often produce incomplete or\nincoherent results. We propose R3D3, a multi-camera system for dense 3D\nreconstruction and ego-motion estimation. Our approach iterates between\ngeometric estimation that exploits spatial-temporal information from multiple\ncameras, and monocular depth refinement. We integrate multi-camera feature\ncorrelation and dense bundle adjustment operators that yield robust geometric\ndepth and pose estimates. To improve reconstruction where geometric depth is\nunreliable, e.g. for moving objects or low-textured regions, we introduce\nlearnable scene priors via a depth refinement network. We show that this design\nenables a dense, consistent 3D reconstruction of challenging, dynamic outdoor\nenvironments. Consequently, we achieve state-of-the-art dense depth prediction\non the DDAD and NuScenes benchmarks.",
        "translated": ""
    },
    {
        "title": "VideoCutLER: Surprisingly Simple Unsupervised Video Instance\n  Segmentation",
        "url": "http://arxiv.org/abs/2308.14710v1",
        "pub_date": "2023-08-28",
        "summary": "Existing approaches to unsupervised video instance segmentation typically\nrely on motion estimates and experience difficulties tracking small or\ndivergent motions. We present VideoCutLER, a simple method for unsupervised\nmulti-instance video segmentation without using motion-based learning signals\nlike optical flow or training on natural videos. Our key insight is that using\nhigh-quality pseudo masks and a simple video synthesis method for model\ntraining is surprisingly sufficient to enable the resulting video model to\neffectively segment and track multiple instances across video frames. We show\nthe first competitive unsupervised learning results on the challenging\nYouTubeVIS-2019 benchmark, achieving 50.7% APvideo^50 , surpassing the previous\nstate-of-the-art by a large margin. VideoCutLER can also serve as a strong\npretrained model for supervised video instance segmentation tasks, exceeding\nDINO by 15.9% on YouTubeVIS-2019 in terms of APvideo.",
        "translated": ""
    },
    {
        "title": "360-Degree Panorama Generation from Few Unregistered NFoV Images",
        "url": "http://arxiv.org/abs/2308.14686v1",
        "pub_date": "2023-08-28",
        "summary": "360$^\\circ$ panoramas are extensively utilized as environmental light sources\nin computer graphics. However, capturing a 360$^\\circ$ $\\times$ 180$^\\circ$\npanorama poses challenges due to the necessity of specialized and costly\nequipment, and additional human resources. Prior studies develop various\nlearning-based generative methods to synthesize panoramas from a single Narrow\nField-of-View (NFoV) image, but they are limited in alterable input patterns,\ngeneration quality, and controllability. To address these issues, we propose a\nnovel pipeline called PanoDiff, which efficiently generates complete\n360$^\\circ$ panoramas using one or more unregistered NFoV images captured from\narbitrary angles. Our approach has two primary components to overcome the\nlimitations. Firstly, a two-stage angle prediction module to handle various\nnumbers of NFoV inputs. Secondly, a novel latent diffusion-based panorama\ngeneration model uses incomplete panorama and text prompts as control signals\nand utilizes several geometric augmentation schemes to ensure geometric\nproperties in generated panoramas. Experiments show that PanoDiff achieves\nstate-of-the-art panoramic generation quality and high controllability, making\nit suitable for applications such as content editing.",
        "translated": ""
    },
    {
        "title": "3D Adversarial Augmentations for Robust Out-of-Domain Predictions",
        "url": "http://arxiv.org/abs/2308.15479v1",
        "pub_date": "2023-08-29",
        "summary": "Since real-world training datasets cannot properly sample the long tail of\nthe underlying data distribution, corner cases and rare out-of-domain samples\ncan severely hinder the performance of state-of-the-art models. This problem\nbecomes even more severe for dense tasks, such as 3D semantic segmentation,\nwhere points of non-standard objects can be confidently associated to the wrong\nclass. In this work, we focus on improving the generalization to out-of-domain\ndata. We achieve this by augmenting the training set with adversarial examples.\nFirst, we learn a set of vectors that deform the objects in an adversarial\nfashion. To prevent the adversarial examples from being too far from the\nexisting data distribution, we preserve their plausibility through a series of\nconstraints, ensuring sensor-awareness and shapes smoothness. Then, we perform\nadversarial augmentation by applying the learned sample-independent vectors to\nthe available objects when training a model. We conduct extensive experiments\nacross a variety of scenarios on data from KITTI, Waymo, and CrashD for 3D\nobject detection, and on data from SemanticKITTI, Waymo, and nuScenes for 3D\nsemantic segmentation. Despite training on a standard single dataset, our\napproach substantially improves the robustness and generalization of both 3D\nobject detection and 3D semantic segmentation methods to out-of-domain data.",
        "translated": ""
    },
    {
        "title": "An Adaptive Tangent Feature Perspective of Neural Networks",
        "url": "http://arxiv.org/abs/2308.15478v1",
        "pub_date": "2023-08-29",
        "summary": "In order to better understand feature learning in neural networks, we propose\na framework for understanding linear models in tangent feature space where the\nfeatures are allowed to be transformed during training. We consider linear\ntransformations of features, resulting in a joint optimization over parameters\nand transformations with a bilinear interpolation constraint. We show that this\noptimization problem has an equivalent linearly constrained optimization with\nstructured regularization that encourages approximately low rank solutions.\nSpecializing to neural network structure, we gain insights into how the\nfeatures and thus the kernel function change, providing additional nuance to\nthe phenomenon of kernel alignment when the target function is poorly\nrepresented using tangent features. In addition to verifying our theoretical\nobservations in real neural networks on a simple regression problem, we\nempirically show that an adaptive feature implementation of tangent feature\nclassification has an order of magnitude lower sample complexity than the fixed\ntangent feature model on MNIST and CIFAR-10.",
        "translated": ""
    },
    {
        "title": "A General-Purpose Self-Supervised Model for Computational Pathology",
        "url": "http://arxiv.org/abs/2308.15474v1",
        "pub_date": "2023-08-29",
        "summary": "Tissue phenotyping is a fundamental computational pathology (CPath) task in\nlearning objective characterizations of histopathologic biomarkers in anatomic\npathology. However, whole-slide imaging (WSI) poses a complex computer vision\nproblem in which the large-scale image resolutions of WSIs and the enormous\ndiversity of morphological phenotypes preclude large-scale data annotation.\nCurrent efforts have proposed using pretrained image encoders with either\ntransfer learning from natural image datasets or self-supervised pretraining on\npublicly-available histopathology datasets, but have not been extensively\ndeveloped and evaluated across diverse tissue types at scale. We introduce UNI,\na general-purpose self-supervised model for pathology, pretrained using over\n100 million tissue patches from over 100,000 diagnostic haematoxylin and\neosin-stained WSIs across 20 major tissue types, and evaluated on 33\nrepresentative CPath clinical tasks in CPath of varying diagnostic\ndifficulties. In addition to outperforming previous state-of-the-art models, we\ndemonstrate new modeling capabilities in CPath such as resolution-agnostic\ntissue classification, slide classification using few-shot class prototypes,\nand disease subtyping generalization in classifying up to 108 cancer types in\nthe OncoTree code classification system. UNI advances unsupervised\nrepresentation learning at scale in CPath in terms of both pretraining data and\ndownstream evaluation, enabling data-efficient AI models that can generalize\nand transfer to a gamut of diagnostically-challenging tasks and clinical\nworkflows in anatomic pathology.",
        "translated": ""
    },
    {
        "title": "Learning Modulated Transformation in GANs",
        "url": "http://arxiv.org/abs/2308.15472v1",
        "pub_date": "2023-08-29",
        "summary": "The success of style-based generators largely benefits from style modulation,\nwhich helps take care of the cross-instance variation within data. However, the\ninstance-wise stochasticity is typically introduced via regular convolution,\nwhere kernels interact with features at some fixed locations, limiting its\ncapacity for modeling geometric variation. To alleviate this problem, we equip\nthe generator in generative adversarial networks (GANs) with a plug-and-play\nmodule, termed as modulated transformation module (MTM). This module predicts\nspatial offsets under the control of latent codes, based on which the\nconvolution operation can be applied at variable locations for different\ninstances, and hence offers the model an additional degree of freedom to handle\ngeometry deformation. Extensive experiments suggest that our approach can be\nfaithfully generalized to various generative tasks, including image generation,\n3D-aware image synthesis, and video generation, and get compatible with\nstate-of-the-art frameworks without any hyper-parameter tuning. It is\nnoteworthy that, towards human generation on the challenging TaiChi dataset, we\nimprove the FID of StyleGAN3 from 21.36 to 13.60, demonstrating the efficacy of\nlearning modulated geometry transformation.",
        "translated": ""
    },
    {
        "title": "Multimodal Contrastive Learning and Tabular Attention for Automated\n  Alzheimer's Disease Prediction",
        "url": "http://arxiv.org/abs/2308.15469v1",
        "pub_date": "2023-08-29",
        "summary": "Alongside neuroimaging such as MRI scans and PET, Alzheimer's disease (AD)\ndatasets contain valuable tabular data including AD biomarkers and clinical\nassessments. Existing computer vision approaches struggle to utilize this\nadditional information. To address these needs, we propose a generalizable\nframework for multimodal contrastive learning of image data and tabular data, a\nnovel tabular attention module for amplifying and ranking salient features in\ntables, and the application of these techniques onto Alzheimer's disease\nprediction. Experimental evaulations demonstrate the strength of our framework\nby detecting Alzheimer's disease (AD) from over 882 MR image slices from the\nADNI database. We take advantage of the high interpretability of tabular data\nand our novel tabular attention approach and through attribution of the\nattention scores for each row of the table, we note and rank the most\npredominant features. Results show that the model is capable of an accuracy of\nover 83.8%, almost a 10% increase from previous state of the art.",
        "translated": ""
    },
    {
        "title": "Input margins can predict generalization too",
        "url": "http://arxiv.org/abs/2308.15466v1",
        "pub_date": "2023-08-29",
        "summary": "Understanding generalization in deep neural networks is an active area of\nresearch. A promising avenue of exploration has been that of margin\nmeasurements: the shortest distance to the decision boundary for a given sample\nor its representation internal to the network. While margins have been shown to\nbe correlated with the generalization ability of a model when measured at its\nhidden representations (hidden margins), no such link between large margins and\ngeneralization has been established for input margins. We show that while input\nmargins are not generally predictive of generalization, they can be if the\nsearch space is appropriately constrained. We develop such a measure based on\ninput margins, which we refer to as `constrained margins'. The predictive power\nof this new measure is demonstrated on the 'Predicting Generalization in Deep\nLearning' (PGDL) dataset and contrasted with hidden representation margins. We\nfind that constrained margins achieve highly competitive scores and outperform\nother margin measurements in general. This provides a novel insight on the\nrelationship between generalization and classification margins, and highlights\nthe importance of considering the data manifold for investigations of\ngeneralization in DNNs.",
        "translated": ""
    },
    {
        "title": "Online Overexposed Pixels Hallucination in Videos with Adaptive\n  Reference Frame Selection",
        "url": "http://arxiv.org/abs/2308.15462v1",
        "pub_date": "2023-08-29",
        "summary": "Low dynamic range (LDR) cameras cannot deal with wide dynamic range inputs,\nfrequently leading to local overexposure issues. We present a learning-based\nsystem to reduce these artifacts without resorting to complex acquisition\nmechanisms like alternating exposures or costly processing that are typical of\nhigh dynamic range (HDR) imaging. We propose a transformer-based deep neural\nnetwork (DNN) to infer the missing HDR details. In an ablation study, we show\nthe importance of using a multiscale DNN and train it with the proper cost\nfunction to achieve state-of-the-art quality. To aid the reconstruction of the\noverexposed areas, our DNN takes a reference frame from the past as an\nadditional input. This leverages the commonly occurring temporal instabilities\nof autoexposure to our advantage: since well-exposed details in the current\nframe may be overexposed in the future, we use reinforcement learning to train\na reference frame selection DNN that decides whether to adopt the current frame\nas a future reference. Without resorting to alternating exposures, we obtain\ntherefore a causal, HDR hallucination algorithm with potential application in\ncommon video acquisition settings. Our demo video can be found at\nhttps://drive.google.com/file/d/1-r12BKImLOYCLUoPzdebnMyNjJ4Rk360/view",
        "translated": ""
    },
    {
        "title": "Canonical Factors for Hybrid Neural Fields",
        "url": "http://arxiv.org/abs/2308.15461v1",
        "pub_date": "2023-08-29",
        "summary": "Factored feature volumes offer a simple way to build more compact, efficient,\nand intepretable neural fields, but also introduce biases that are not\nnecessarily beneficial for real-world data. In this work, we (1) characterize\nthe undesirable biases that these architectures have for axis-aligned signals\n-- they can lead to radiance field reconstruction differences of as high as 2\nPSNR -- and (2) explore how learning a set of canonicalizing transformations\ncan improve representations by removing these biases. We prove in a\ntwo-dimensional model problem that simultaneously learning these\ntransformations together with scene appearance succeeds with drastically\nimproved efficiency. We validate the resulting architectures, which we call\nTILTED, using image, signed distance, and radiance field reconstruction tasks,\nwhere we observe improvements across quality, robustness, compactness, and\nruntime. Results demonstrate that TILTED can enable capabilities comparable to\nbaselines that are 2x larger, while highlighting weaknesses of neural field\nevaluation procedures.",
        "translated": ""
    },
    {
        "title": "Pseudo-Boolean Polynomials Approach To Edge Detection And Image\n  Segmentation",
        "url": "http://arxiv.org/abs/2308.15453v1",
        "pub_date": "2023-08-29",
        "summary": "We introduce a deterministic approach to edge detection and image\nsegmentation by formulating pseudo-Boolean polynomials on image patches. The\napproach works by applying a binary classification of blob and edge regions in\nan image based on the degrees of pseudo-Boolean polynomials calculated on\npatches extracted from the provided image. We test our method on simple images\ncontaining primitive shapes of constant and contrasting colour and establish\nthe feasibility before applying it to complex instances like aerial landscape\nimages. The proposed method is based on the exploitation of the reduction,\npolynomial degree, and equivalence properties of penalty-based pseudo-Boolean\npolynomials.",
        "translated": ""
    },
    {
        "title": "Complementing Onboard Sensors with Satellite Map: A New Perspective for\n  HD Map Construction",
        "url": "http://arxiv.org/abs/2308.15427v1",
        "pub_date": "2023-08-29",
        "summary": "High-Definition (HD) maps play a crucial role in autonomous driving systems.\nRecent methods have attempted to construct HD maps in real-time based on\ninformation obtained from vehicle onboard sensors. However, the performance of\nthese methods is significantly susceptible to the environment surrounding the\nvehicle due to the inherent limitation of onboard sensors, such as weak\ncapacity for long-range detection. In this study, we demonstrate that\nsupplementing onboard sensors with satellite maps can enhance the performance\nof HD map construction methods, leveraging the broad coverage capability of\nsatellite maps. For the purpose of further research, we release the satellite\nmap tiles as a complementary dataset of nuScenes dataset. Meanwhile, we propose\na hierarchical fusion module that enables better fusion of satellite maps\ninformation with existing methods. Specifically, we design an attention mask\nbased on segmentation and distance, applying the cross-attention mechanism to\nfuse onboard Bird's Eye View (BEV) features and satellite features in\nfeature-level fusion. An alignment module is introduced before concatenation in\nBEV-level fusion to mitigate the impact of misalignment between the two\nfeatures. The experimental results on the augmented nuScenes dataset showcase\nthe seamless integration of our module into three existing HD map construction\nmethods. It notably enhances their performance in both HD map semantic\nsegmentation and instance detection tasks.",
        "translated": ""
    },
    {
        "title": "Boosting Detection in Crowd Analysis via Underutilized Output Features",
        "url": "http://arxiv.org/abs/2308.16187v1",
        "pub_date": "2023-08-30",
        "summary": "Detection-based methods have been viewed unfavorably in crowd analysis due to\ntheir poor performance in dense crowds. However, we argue that the potential of\nthese methods has been underestimated, as they offer crucial information for\ncrowd analysis that is often ignored. Specifically, the area size and\nconfidence score of output proposals and bounding boxes provide insight into\nthe scale and density of the crowd. To leverage these underutilized features,\nwe propose Crowd Hat, a plug-and-play module that can be easily integrated with\nexisting detection models. This module uses a mixed 2D-1D compression technique\nto refine the output features and obtain the spatial and numerical distribution\nof crowd-specific information. Based on these features, we further propose\nregion-adaptive NMS thresholds and a decouple-then-align paradigm that address\nthe major limitations of detection-based methods. Our extensive evaluations on\nvarious crowd analysis tasks, including crowd counting, localization, and\ndetection, demonstrate the effectiveness of utilizing output features and the\npotential of detection-based methods in crowd analysis.",
        "translated": ""
    },
    {
        "title": "SAM-Med2D",
        "url": "http://arxiv.org/abs/2308.16184v1",
        "pub_date": "2023-08-30",
        "summary": "The Segment Anything Model (SAM) represents a state-of-the-art research\nadvancement in natural image segmentation, achieving impressive results with\ninput prompts such as points and bounding boxes. However, our evaluation and\nrecent research indicate that directly applying the pretrained SAM to medical\nimage segmentation does not yield satisfactory performance. This limitation\nprimarily arises from significant domain gap between natural images and medical\nimages. To bridge this gap, we introduce SAM-Med2D, the most comprehensive\nstudies on applying SAM to medical 2D images. Specifically, we first collect\nand curate approximately 4.6M images and 19.7M masks from public and private\ndatasets, constructing a large-scale medical image segmentation dataset\nencompassing various modalities and objects. Then, we comprehensively fine-tune\nSAM on this dataset and turn it into SAM-Med2D. Unlike previous methods that\nonly adopt bounding box or point prompts as interactive segmentation approach,\nwe adapt SAM to medical image segmentation through more comprehensive prompts\ninvolving bounding boxes, points, and masks. We additionally fine-tune the\nencoder and decoder of the original SAM to obtain a well-performed SAM-Med2D,\nleading to the most comprehensive fine-tuning strategies to date. Finally, we\nconducted a comprehensive evaluation and analysis to investigate the\nperformance of SAM-Med2D in medical image segmentation across various\nmodalities, anatomical structures, and organs. Concurrently, we validated the\ngeneralization capability of SAM-Med2D on 9 datasets from MICCAI 2023\nchallenge. Overall, our approach demonstrated significantly superior\nperformance and generalization capability compared to SAM.",
        "translated": ""
    },
    {
        "title": "GREC: Generalized Referring Expression Comprehension",
        "url": "http://arxiv.org/abs/2308.16182v1",
        "pub_date": "2023-08-30",
        "summary": "The objective of Classic Referring Expression Comprehension (REC) is to\nproduce a bounding box corresponding to the object mentioned in a given textual\ndescription. Commonly, existing datasets and techniques in classic REC are\ntailored for expressions that pertain to a single target, meaning a sole\nexpression is linked to one specific object. Expressions that refer to multiple\ntargets or involve no specific target have not been taken into account. This\nconstraint hinders the practical applicability of REC. This study introduces a\nnew benchmark termed as Generalized Referring Expression Comprehension (GREC).\nThis benchmark extends the classic REC by permitting expressions to describe\nany number of target objects. To achieve this goal, we have built the first\nlarge-scale GREC dataset named gRefCOCO. This dataset encompasses a range of\nexpressions: those referring to multiple targets, expressions with no specific\ntarget, and the single-target expressions. The design of GREC and gRefCOCO\nensures smooth compatibility with classic REC. The proposed gRefCOCO dataset, a\nGREC method implementation code, and GREC evaluation code are available at\nhttps://github.com/henghuiding/gRefCOCO.",
        "translated": ""
    },
    {
        "title": "MMVP: Motion-Matrix-based Video Prediction",
        "url": "http://arxiv.org/abs/2308.16154v2",
        "pub_date": "2023-08-30",
        "summary": "A central challenge of video prediction lies where the system has to reason\nthe objects' future motions from image frames while simultaneously maintaining\nthe consistency of their appearances across frames. This work introduces an\nend-to-end trainable two-stream video prediction framework, Motion-Matrix-based\nVideo Prediction (MMVP), to tackle this challenge. Unlike previous methods that\nusually handle motion prediction and appearance maintenance within the same set\nof modules, MMVP decouples motion and appearance information by constructing\nappearance-agnostic motion matrices. The motion matrices represent the temporal\nsimilarity of each and every pair of feature patches in the input frames, and\nare the sole input of the motion prediction module in MMVP. This design\nimproves video prediction in both accuracy and efficiency, and reduces the\nmodel size. Results of extensive experiments demonstrate that MMVP outperforms\nstate-of-the-art systems on public data sets by non-negligible large margins\n(about 1 db in PSNR, UCF Sports) in significantly smaller model sizes (84% the\nsize or smaller).",
        "translated": ""
    },
    {
        "title": "Modality Cycles with Masked Conditional Diffusion for Unsupervised\n  Anomaly Segmentation in MRI",
        "url": "http://arxiv.org/abs/2308.16150v1",
        "pub_date": "2023-08-30",
        "summary": "Unsupervised anomaly segmentation aims to detect patterns that are distinct\nfrom any patterns processed during training, commonly called abnormal or\nout-of-distribution patterns, without providing any associated manual\nsegmentations. Since anomalies during deployment can lead to model failure,\ndetecting the anomaly can enhance the reliability of models, which is valuable\nin high-risk domains like medical imaging. This paper introduces Masked\nModality Cycles with Conditional Diffusion (MMCCD), a method that enables\nsegmentation of anomalies across diverse patterns in multimodal MRI. The method\nis based on two fundamental ideas. First, we propose the use of cyclic modality\ntranslation as a mechanism for enabling abnormality detection.\nImage-translation models learn tissue-specific modality mappings, which are\ncharacteristic of tissue physiology. Thus, these learned mappings fail to\ntranslate tissues or image patterns that have never been encountered during\ntraining, and the error enables their segmentation. Furthermore, we combine\nimage translation with a masked conditional diffusion model, which attempts to\n`imagine' what tissue exists under a masked area, further exposing unknown\npatterns as the generative model fails to recreate them. We evaluate our method\non a proxy task by training on healthy-looking slices of BraTS2021\nmulti-modality MRIs and testing on slices with tumors. We show that our method\ncompares favorably to previous unsupervised approaches based on image\nreconstruction and denoising with autoencoders and diffusion models.",
        "translated": ""
    },
    {
        "title": "CircleFormer: Circular Nuclei Detection in Whole Slide Images with\n  Circle Queries and Attention",
        "url": "http://arxiv.org/abs/2308.16145v2",
        "pub_date": "2023-08-30",
        "summary": "Both CNN-based and Transformer-based object detection with bounding box\nrepresentation have been extensively studied in computer vision and medical\nimage analysis, but circular object detection in medical images is still\nunderexplored. Inspired by the recent anchor free CNN-based circular object\ndetection method (CircleNet) for ball-shape glomeruli detection in renal\npathology, in this paper, we present CircleFormer, a Transformer-based circular\nmedical object detection with dynamic anchor circles. Specifically, queries\nwith circle representation in Transformer decoder iteratively refine the\ncircular object detection results, and a circle cross attention module is\nintroduced to compute the similarity between circular queries and image\nfeatures. A generalized circle IoU (gCIoU) is proposed to serve as a new\nregression loss of circular object detection as well. Moreover, our approach is\neasy to generalize to the segmentation task by adding a simple segmentation\nbranch to CircleFormer. We evaluate our method in circular nuclei detection and\nsegmentation on the public MoNuSeg dataset, and the experimental results show\nthat our method achieves promising performance compared with the\nstate-of-the-art approaches. The effectiveness of each component is validated\nvia ablation studies as well. Our code is released at\nhttps://github.com/zhanghx-iim-ahu/CircleFormer.",
        "translated": ""
    },
    {
        "title": "MedShapeNet -- A Large-Scale Dataset of 3D Medical Shapes for Computer\n  Vision",
        "url": "http://arxiv.org/abs/2308.16139v2",
        "pub_date": "2023-08-30",
        "summary": "We present MedShapeNet, a large collection of anatomical shapes (e.g., bones,\norgans, vessels) and 3D surgical instrument models. Prior to the deep learning\nera, the broad application of statistical shape models (SSMs) in medical image\nanalysis is evidence that shapes have been commonly used to describe medical\ndata. Nowadays, however, state-of-the-art (SOTA) deep learning algorithms in\nmedical imaging are predominantly voxel-based. In computer vision, on the\ncontrary, shapes (including, voxel occupancy grids, meshes, point clouds and\nimplicit surface models) are preferred data representations in 3D, as seen from\nthe numerous shape-related publications in premier vision conferences, such as\nthe IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), as\nwell as the increasing popularity of ShapeNet (about 51,300 models) and\nPrinceton ModelNet (127,915 models) in computer vision research. MedShapeNet is\ncreated as an alternative to these commonly used shape benchmarks to facilitate\nthe translation of data-driven vision algorithms to medical applications, and\nit extends the opportunities to adapt SOTA vision algorithms to solve critical\nmedical problems. Besides, the majority of the medical shapes in MedShapeNet\nare modeled directly on the imaging data of real patients, and therefore it\ncomplements well existing shape benchmarks comprising of computer-aided design\n(CAD) models. MedShapeNet currently includes more than 100,000 medical shapes,\nand provides annotations in the form of paired data. It is therefore also a\nfreely available repository of 3D models for extended reality (virtual reality\n- VR, augmented reality - AR, mixed reality - MR) and medical 3D printing. This\nwhite paper describes in detail the motivations behind MedShapeNet, the shape\nacquisition procedures, the use cases, as well as the usage of the online shape\nsearch portal: https://medshapenet.ikim.nrw/",
        "translated": ""
    },
    {
        "title": "CorrEmbed: Evaluating Pre-trained Model Image Similarity Efficacy with a\n  Novel Metric",
        "url": "http://arxiv.org/abs/2308.16126v1",
        "pub_date": "2023-08-30",
        "summary": "Detecting visually similar images is a particularly useful attribute to look\nto when calculating product recommendations. Embedding similarity, which\nutilizes pre-trained computer vision models to extract high-level image\nfeatures, has demonstrated remarkable efficacy in identifying images with\nsimilar compositions. However, there is a lack of methods for evaluating the\nembeddings generated by these models, as conventional loss and performance\nmetrics do not adequately capture their performance in image similarity search\ntasks.\n  In this paper, we evaluate the viability of the image embeddings from\nnumerous pre-trained computer vision models using a novel approach named\nCorrEmbed. Our approach computes the correlation between distances in image\nembeddings and distances in human-generated tag vectors. We extensively\nevaluate numerous pre-trained Torchvision models using this metric, revealing\nan intuitive relationship of linear scaling between ImageNet1k accuracy scores\nand tag-correlation scores. Importantly, our method also identifies deviations\nfrom this pattern, providing insights into how different models capture\nhigh-level image features.\n  By offering a robust performance evaluation of these pre-trained models,\nCorrEmbed serves as a valuable tool for researchers and practitioners seeking\nto develop effective, data-driven approaches to similar item recommendations in\nfashion retail.",
        "translated": ""
    },
    {
        "title": "Improving Few-shot Image Generation by Structural Discrimination and\n  Textural Modulation",
        "url": "http://arxiv.org/abs/2308.16110v1",
        "pub_date": "2023-08-30",
        "summary": "Few-shot image generation, which aims to produce plausible and diverse images\nfor one category given a few images from this category, has drawn extensive\nattention. Existing approaches either globally interpolate different images or\nfuse local representations with pre-defined coefficients. However, such an\nintuitive combination of images/features only exploits the most relevant\ninformation for generation, leading to poor diversity and coarse-grained\nsemantic fusion. To remedy this, this paper proposes a novel textural\nmodulation (TexMod) mechanism to inject external semantic signals into internal\nlocal representations. Parameterized by the feedback from the discriminator,\nour TexMod enables more fined-grained semantic injection while maintaining the\nsynthesis fidelity. Moreover, a global structural discriminator (StructD) is\ndeveloped to explicitly guide the model to generate images with reasonable\nlayout and outline. Furthermore, the frequency awareness of the model is\nreinforced by encouraging the model to distinguish frequency signals. Together\nwith these techniques, we build a novel and effective model for few-shot image\ngeneration. The effectiveness of our model is identified by extensive\nexperiments on three popular datasets and various settings. Besides achieving\nstate-of-the-art synthesis performance on these datasets, our proposed\ntechniques could be seamlessly integrated into existing models for a further\nperformance boost.",
        "translated": ""
    },
    {
        "title": "Learned Image Reasoning Prior Penetrates Deep Unfolding Network for\n  Panchromatic and Multi-Spectral Image Fusion",
        "url": "http://arxiv.org/abs/2308.16083v1",
        "pub_date": "2023-08-30",
        "summary": "The success of deep neural networks for pan-sharpening is commonly in a form\nof black box, lacking transparency and interpretability. To alleviate this\nissue, we propose a novel model-driven deep unfolding framework with image\nreasoning prior tailored for the pan-sharpening task. Different from existing\nunfolding solutions that deliver the proximal operator networks as the\nuncertain and vague priors, our framework is motivated by the content reasoning\nability of masked autoencoders (MAE) with insightful designs. Specifically, the\npre-trained MAE with spatial masking strategy, acting as intrinsic reasoning\nprior, is embedded into unfolding architecture. Meanwhile, the pre-trained MAE\nwith spatial-spectral masking strategy is treated as the regularization term\nwithin loss function to constrain the spatial-spectral consistency. Such\ndesigns penetrate the image reasoning prior into deep unfolding networks while\nimproving its interpretability and representation capability. The uniqueness of\nour framework is that the holistic learning process is explicitly integrated\nwith the inherent physical mechanism underlying the pan-sharpening task.\nExtensive experiments on multiple satellite datasets demonstrate the\nsuperiority of our method over the existing state-of-the-art approaches. Code\nwill be released at \\url{https://manman1995.github.io/}.",
        "translated": ""
    },
    {
        "title": "PointLLM: Empowering Large Language Models to Understand Point Clouds",
        "url": "http://arxiv.org/abs/2308.16911v1",
        "pub_date": "2023-08-31",
        "summary": "The unprecedented advancements in Large Language Models (LLMs) have created a\nprofound impact on natural language processing but are yet to fully embrace the\nrealm of 3D understanding. This paper introduces PointLLM, a preliminary effort\nto fill this gap, thereby enabling LLMs to understand point clouds and offering\na new avenue beyond 2D visual data. PointLLM processes colored object point\nclouds with human instructions and generates contextually appropriate\nresponses, illustrating its grasp of point clouds and common sense.\nSpecifically, it leverages a point cloud encoder with a powerful LLM to\neffectively fuse geometric, appearance, and linguistic information. We collect\na novel dataset comprising 660K simple and 70K complex point-text instruction\npairs to enable a two-stage training strategy: initially aligning latent spaces\nand subsequently instruction-tuning the unified model. To rigorously evaluate\nour model's perceptual abilities and its generalization capabilities, we\nestablish two benchmarks: Generative 3D Object Classification and 3D Object\nCaptioning, assessed through three different methods, including human\nevaluation, GPT-4/ChatGPT evaluation, and traditional metrics. Experiment\nresults show that PointLLM demonstrates superior performance over existing 2D\nbaselines. Remarkably, in human-evaluated object captioning tasks, PointLLM\noutperforms human annotators in over 50% of the samples. Codes, datasets, and\nbenchmarks are available at https://github.com/OpenRobotLab/PointLLM .",
        "translated": ""
    },
    {
        "title": "StyleInV: A Temporal Style Modulated Inversion Network for Unconditional\n  Video Generation",
        "url": "http://arxiv.org/abs/2308.16909v1",
        "pub_date": "2023-08-31",
        "summary": "Unconditional video generation is a challenging task that involves\nsynthesizing high-quality videos that are both coherent and of extended\nduration. To address this challenge, researchers have used pretrained StyleGAN\nimage generators for high-quality frame synthesis and focused on motion\ngenerator design. The motion generator is trained in an autoregressive manner\nusing heavy 3D convolutional discriminators to ensure motion coherence during\nvideo generation. In this paper, we introduce a novel motion generator design\nthat uses a learning-based inversion network for GAN. The encoder in our method\ncaptures rich and smooth priors from encoding images to latents, and given the\nlatent of an initially generated frame as guidance, our method can generate\nsmooth future latent by modulating the inversion encoder temporally. Our method\nenjoys the advantage of sparse training and naturally constrains the generation\nspace of our motion generator with the inversion network guided by the initial\nframe, eliminating the need for heavy discriminators. Moreover, our method\nsupports style transfer with simple fine-tuning when the encoder is paired with\na pretrained StyleGAN generator. Extensive experiments conducted on various\nbenchmarks demonstrate the superiority of our method in generating long and\nhigh-resolution videos with decent single-frame quality and temporal\nconsistency.",
        "translated": ""
    },
    {
        "title": "Fine-Grained Cross-View Geo-Localization Using a Correlation-Aware\n  Homography Estimator",
        "url": "http://arxiv.org/abs/2308.16906v1",
        "pub_date": "2023-08-31",
        "summary": "In this paper, we introduce a novel approach to fine-grained cross-view\ngeo-localization. Our method aligns a warped ground image with a corresponding\nGPS-tagged satellite image covering the same area using homography estimation.\nWe first employ a differentiable spherical transform, adhering to geometric\nprinciples, to accurately align the perspective of the ground image with the\nsatellite map. This transformation effectively places ground and aerial images\nin the same view and on the same plane, reducing the task to an image alignment\nproblem. To address challenges such as occlusion, small overlapping range, and\nseasonal variations, we propose a robust correlation-aware homography estimator\nto align similar parts of the transformed ground image with the satellite\nimage. Our method achieves sub-pixel resolution and meter-level GPS accuracy by\nmapping the center point of the transformed ground image to the satellite image\nusing a homography matrix and determining the orientation of the ground camera\nusing a point above the central axis. Operating at a speed of 30 FPS, our\nmethod outperforms state-of-the-art techniques, reducing the mean metric\nlocalization error by 21.3% and 32.4% in same-area and cross-area\ngeneralization tasks on the VIGOR benchmark, respectively, and by 34.4% on the\nKITTI benchmark in same-area evaluation.",
        "translated": ""
    },
    {
        "title": "InterDiff: Generating 3D Human-Object Interactions with Physics-Informed\n  Diffusion",
        "url": "http://arxiv.org/abs/2308.16905v1",
        "pub_date": "2023-08-31",
        "summary": "This paper addresses a novel task of anticipating 3D human-object\ninteractions (HOIs). Most existing research on HOI synthesis lacks\ncomprehensive whole-body interactions with dynamic objects, e.g., often limited\nto manipulating small or static objects. Our task is significantly more\nchallenging, as it requires modeling dynamic objects with various shapes,\ncapturing whole-body motion, and ensuring physically valid interactions. To\nthis end, we propose InterDiff, a framework comprising two key steps: (i)\ninteraction diffusion, where we leverage a diffusion model to encode the\ndistribution of future human-object interactions; (ii) interaction correction,\nwhere we introduce a physics-informed predictor to correct denoised HOIs in a\ndiffusion step. Our key insight is to inject prior knowledge that the\ninteractions under reference with respect to contact points follow a simple\npattern and are easily predictable. Experiments on multiple human-object\ninteraction datasets demonstrate the effectiveness of our method for this task,\ncapable of producing realistic, vivid, and remarkably long-term 3D HOI\npredictions.",
        "translated": ""
    },
    {
        "title": "PointOcc: Cylindrical Tri-Perspective View for Point-based 3D Semantic\n  Occupancy Prediction",
        "url": "http://arxiv.org/abs/2308.16896v1",
        "pub_date": "2023-08-31",
        "summary": "Semantic segmentation in autonomous driving has been undergoing an evolution\nfrom sparse point segmentation to dense voxel segmentation, where the objective\nis to predict the semantic occupancy of each voxel in the concerned 3D space.\nThe dense nature of the prediction space has rendered existing efficient\n2D-projection-based methods (e.g., bird's eye view, range view, etc.)\nineffective, as they can only describe a subspace of the 3D scene. To address\nthis, we propose a cylindrical tri-perspective view to represent point clouds\neffectively and comprehensively and a PointOcc model to process them\nefficiently. Considering the distance distribution of LiDAR point clouds, we\nconstruct the tri-perspective view in the cylindrical coordinate system for\nmore fine-grained modeling of nearer areas. We employ spatial group pooling to\nmaintain structural details during projection and adopt 2D backbones to\nefficiently process each TPV plane. Finally, we obtain the features of each\npoint by aggregating its projected features on each of the processed TPV planes\nwithout the need for any post-processing. Extensive experiments on both 3D\noccupancy prediction and LiDAR segmentation benchmarks demonstrate that the\nproposed PointOcc achieves state-of-the-art performance with much faster speed.\nSpecifically, despite only using LiDAR, PointOcc significantly outperforms all\nother methods, including multi-modal methods, with a large margin on the\nOpenOccupancy benchmark. Code: https://github.com/wzzheng/PointOcc.",
        "translated": ""
    },
    {
        "title": "EMDB: The Electromagnetic Database of Global 3D Human Pose and Shape in\n  the Wild",
        "url": "http://arxiv.org/abs/2308.16894v1",
        "pub_date": "2023-08-31",
        "summary": "We present EMDB, the Electromagnetic Database of Global 3D Human Pose and\nShape in the Wild. EMDB is a novel dataset that contains high-quality 3D SMPL\npose and shape parameters with global body and camera trajectories for\nin-the-wild videos. We use body-worn, wireless electromagnetic (EM) sensors and\na hand-held iPhone to record a total of 58 minutes of motion data, distributed\nover 81 indoor and outdoor sequences and 10 participants. Together with\naccurate body poses and shapes, we also provide global camera poses and body\nroot trajectories. To construct EMDB, we propose a multi-stage optimization\nprocedure, which first fits SMPL to the 6-DoF EM measurements and then refines\nthe poses via image observations. To achieve high-quality results, we leverage\na neural implicit avatar model to reconstruct detailed human surface geometry\nand appearance, which allows for improved alignment and smoothness via a dense\npixel-level objective. Our evaluations, conducted with a multi-view volumetric\ncapture system, indicate that EMDB has an expected accuracy of 2.3 cm\npositional and 10.6 degrees angular error, surpassing the accuracy of previous\nin-the-wild datasets. We evaluate existing state-of-the-art monocular RGB\nmethods for camera-relative and global pose estimation on EMDB. EMDB is\npublicly available under https://ait.ethz.ch/emdb",
        "translated": ""
    },
    {
        "title": "Language-Conditioned Path Planning",
        "url": "http://arxiv.org/abs/2308.16893v1",
        "pub_date": "2023-08-31",
        "summary": "Contact is at the core of robotic manipulation. At times, it is desired (e.g.\nmanipulation and grasping), and at times, it is harmful (e.g. when avoiding\nobstacles). However, traditional path planning algorithms focus solely on\ncollision-free paths, limiting their applicability in contact-rich tasks. To\naddress this limitation, we propose the domain of Language-Conditioned Path\nPlanning, where contact-awareness is incorporated into the path planning\nproblem. As a first step in this domain, we propose Language-Conditioned\nCollision Functions (LACO) a novel approach that learns a collision function\nusing only a single-view image, language prompt, and robot configuration. LACO\npredicts collisions between the robot and the environment, enabling flexible,\nconditional path planning without the need for manual object annotations, point\ncloud data, or ground-truth object meshes. In both simulation and the real\nworld, we demonstrate that LACO can facilitate complex, nuanced path plans that\nallow for interaction with objects that are safe to collide, rather than\nprohibiting any collision.",
        "translated": ""
    },
    {
        "title": "GNFactor: Multi-Task Real Robot Learning with Generalizable Neural\n  Feature Fields",
        "url": "http://arxiv.org/abs/2308.16891v1",
        "pub_date": "2023-08-31",
        "summary": "It is a long-standing problem in robotics to develop agents capable of\nexecuting diverse manipulation tasks from visual observations in unstructured\nreal-world environments. To achieve this goal, the robot needs to have a\ncomprehensive understanding of the 3D structure and semantics of the scene. In\nthis work, we present $\\textbf{GNFactor}$, a visual behavior cloning agent for\nmulti-task robotic manipulation with $\\textbf{G}$eneralizable $\\textbf{N}$eural\nfeature $\\textbf{F}$ields. GNFactor jointly optimizes a generalizable neural\nfield (GNF) as a reconstruction module and a Perceiver Transformer as a\ndecision-making module, leveraging a shared deep 3D voxel representation. To\nincorporate semantics in 3D, the reconstruction module utilizes a\nvision-language foundation model ($\\textit{e.g.}$, Stable Diffusion) to distill\nrich semantic information into the deep 3D voxel. We evaluate GNFactor on 3\nreal robot tasks and perform detailed ablations on 10 RLBench tasks with a\nlimited number of demonstrations. We observe a substantial improvement of\nGNFactor over current state-of-the-art methods in seen and unseen tasks,\ndemonstrating the strong generalization ability of GNFactor. Our project\nwebsite is https://yanjieze.com/GNFactor/ .",
        "translated": ""
    },
    {
        "title": "TouchStone: Evaluating Vision-Language Models by Language Models",
        "url": "http://arxiv.org/abs/2308.16890v1",
        "pub_date": "2023-08-31",
        "summary": "Large vision-language models (LVLMs) have recently witnessed rapid\nadvancements, exhibiting a remarkable capacity for perceiving, understanding,\nand processing visual information by connecting visual receptor with large\nlanguage models (LLMs). However, current assessments mainly focus on\nrecognizing and reasoning abilities, lacking direct evaluation of\nconversational skills and neglecting visual storytelling abilities. In this\npaper, we propose an evaluation method that uses strong LLMs as judges to\ncomprehensively evaluate the various abilities of LVLMs. Firstly, we construct\na comprehensive visual dialogue dataset TouchStone, consisting of open-world\nimages and questions, covering five major categories of abilities and 27\nsubtasks. This dataset not only covers fundamental recognition and\ncomprehension but also extends to literary creation. Secondly, by integrating\ndetailed image annotations we effectively transform the multimodal input\ncontent into a form understandable by LLMs. This enables us to employ advanced\nLLMs for directly evaluating the quality of the multimodal dialogue without\nrequiring human intervention. Through validation, we demonstrate that powerful\nLVLMs, such as GPT-4, can effectively score dialogue quality by leveraging\ntheir textual capabilities alone, aligning with human preferences. We hope our\nwork can serve as a touchstone for LVLMs' evaluation and pave the way for\nbuilding stronger LVLMs. The evaluation code is available at\nhttps://github.com/OFA-Sys/TouchStone.",
        "translated": ""
    },
    {
        "title": "Text2Scene: Text-driven Indoor Scene Stylization with Part-aware Details",
        "url": "http://arxiv.org/abs/2308.16880v1",
        "pub_date": "2023-08-31",
        "summary": "We propose Text2Scene, a method to automatically create realistic textures\nfor virtual scenes composed of multiple objects. Guided by a reference image\nand text descriptions, our pipeline adds detailed texture on labeled 3D\ngeometries in the room such that the generated colors respect the hierarchical\nstructure or semantic parts that are often composed of similar materials.\nInstead of applying flat stylization on the entire scene at a single step, we\nobtain weak semantic cues from geometric segmentation, which are further\nclarified by assigning initial colors to segmented parts. Then we add texture\ndetails for individual objects such that their projections on image space\nexhibit feature embedding aligned with the embedding of the input. The\ndecomposition makes the entire pipeline tractable to a moderate amount of\ncomputation resources and memory. As our framework utilizes the existing\nresources of image and text embedding, it does not require dedicated datasets\nwith high-quality textures designed by skillful artists. To the best of our\nknowledge, it is the first practical and scalable approach that can create\ndetailed and realistic textures of the desired style that maintain structural\ncontext for scenes with multiple objects.",
        "translated": ""
    },
    {
        "title": "OpenIns3D: Snap and Lookup for 3D Open-vocabulary Instance Segmentation",
        "url": "http://arxiv.org/abs/2309.00616v1",
        "pub_date": "2023-09-01",
        "summary": "Current 3D open-vocabulary scene understanding methods mostly utilize\nwell-aligned 2D images as the bridge to learn 3D features with language.\nHowever, applying these approaches becomes challenging in scenarios where 2D\nimages are absent. In this work, we introduce a completely new pipeline,\nnamely, OpenIns3D, which requires no 2D image inputs, for 3D open-vocabulary\nscene understanding at the instance level. The OpenIns3D framework employs a\n\"Mask-Snap-Lookup\" scheme. The \"Mask\" module learns class-agnostic mask\nproposals in 3D point clouds. The \"Snap\" module generates synthetic scene-level\nimages at multiple scales and leverages 2D vision language models to extract\ninteresting objects. The \"Lookup\" module searches through the outcomes of\n\"Snap\" with the help of Mask2Pixel maps, which contain the precise\ncorrespondence between 3D masks and synthetic images, to assign category names\nto the proposed masks. This 2D input-free, easy-to-train, and flexible approach\nachieved state-of-the-art results on a wide range of indoor and outdoor\ndatasets with a large margin. Furthermore, OpenIns3D allows for effortless\nswitching of 2D detectors without re-training. When integrated with\nstate-of-the-art 2D open-world models such as ODISE and GroundingDINO, superb\nresults are observed on open-vocabulary instance segmentation. When integrated\nwith LLM-powered 2D models like LISA, it demonstrates a remarkable capacity to\nprocess highly complex text queries, including those that require intricate\nreasoning and world knowledge. The code and model will be made publicly\navailable.",
        "translated": ""
    },
    {
        "title": "Point-Bind &amp; Point-LLM: Aligning Point Cloud with Multi-modality for 3D\n  Understanding, Generation, and Instruction Following",
        "url": "http://arxiv.org/abs/2309.00615v1",
        "pub_date": "2023-09-01",
        "summary": "We introduce Point-Bind, a 3D multi-modality model aligning point clouds with\n2D image, language, audio, and video. Guided by ImageBind, we construct a joint\nembedding space between 3D and multi-modalities, enabling many promising\napplications, e.g., any-to-3D generation, 3D embedding arithmetic, and 3D\nopen-world understanding. On top of this, we further present Point-LLM, the\nfirst 3D large language model (LLM) following 3D multi-modal instructions. By\nparameter-efficient fine-tuning techniques, Point-LLM injects the semantics of\nPoint-Bind into pre-trained LLMs, e.g., LLaMA, which requires no 3D instruction\ndata, but exhibits superior 3D and multi-modal question-answering capacity. We\nhope our work may cast a light on the community for extending 3D point clouds\nto multi-modality applications. Code is available at\nhttps://github.com/ZiyuGuo99/Point-Bind_Point-LLM.",
        "translated": ""
    },
    {
        "title": "Iterative Multi-granular Image Editing using Diffusion Models",
        "url": "http://arxiv.org/abs/2309.00613v1",
        "pub_date": "2023-09-01",
        "summary": "Recent advances in text-guided image synthesis has dramatically changed how\ncreative professionals generate artistic and aesthetically pleasing visual\nassets. To fully support such creative endeavors, the process should possess\nthe ability to: 1) iteratively edit the generations and 2) control the spatial\nreach of desired changes (global, local or anything in between). We formalize\nthis pragmatic problem setting as Iterative Multi-granular Editing. While there\nhas been substantial progress with diffusion-based models for image synthesis\nand editing, they are all one shot (i.e., no iterative editing capabilities)\nand do not naturally yield multi-granular control (i.e., covering the full\nspectrum of local-to-global edits). To overcome these drawbacks, we propose\nEMILIE: Iterative Multi-granular Image Editor. EMILIE introduces a novel latent\niteration strategy, which re-purposes a pre-trained diffusion model to\nfacilitate iterative editing. This is complemented by a gradient control\noperation for multi-granular control. We introduce a new benchmark dataset to\nevaluate our newly proposed setting. We conduct exhaustive quantitatively and\nqualitatively evaluation against recent state-of-the-art approaches adapted to\nour task, to being out the mettle of EMILIE. We hope our work would attract\nattention to this newly identified, pragmatic problem setting.",
        "translated": ""
    },
    {
        "title": "CityDreamer: Compositional Generative Model of Unbounded 3D Cities",
        "url": "http://arxiv.org/abs/2309.00610v1",
        "pub_date": "2023-09-01",
        "summary": "In recent years, extensive research has focused on 3D natural scene\ngeneration, but the domain of 3D city generation has not received as much\nexploration. This is due to the greater challenges posed by 3D city generation,\nmainly because humans are more sensitive to structural distortions in urban\nenvironments. Additionally, generating 3D cities is more complex than 3D\nnatural scenes since buildings, as objects of the same class, exhibit a wider\nrange of appearances compared to the relatively consistent appearance of\nobjects like trees in natural scenes. To address these challenges, we propose\nCityDreamer, a compositional generative model designed specifically for\nunbounded 3D cities, which separates the generation of building instances from\nother background objects, such as roads, green lands, and water areas, into\ndistinct modules. Furthermore, we construct two datasets, OSM and GoogleEarth,\ncontaining a vast amount of real-world city imagery to enhance the realism of\nthe generated 3D cities both in their layouts and appearances. Through\nextensive experiments, CityDreamer has proven its superiority over\nstate-of-the-art methods in generating a wide range of lifelike 3D cities.",
        "translated": ""
    },
    {
        "title": "Time Series Analysis of Urban Liveability",
        "url": "http://arxiv.org/abs/2309.00594v1",
        "pub_date": "2023-09-01",
        "summary": "In this paper we explore deep learning models to monitor longitudinal\nliveability changes in Dutch cities at the neighbourhood level. Our liveability\nreference data is defined by a country-wise yearly survey based on a set of\nindicators combined into a liveability score, the Leefbaarometer. We pair this\nreference data with yearly-available high-resolution aerial images, which\ncreates yearly timesteps at which liveability can be monitored. We deploy a\nconvolutional neural network trained on an aerial image from 2016 and the\nLeefbaarometer score to predict liveability at new timesteps 2012 and 2020. The\nresults in a city used for training (Amsterdam) and one never seen during\ntraining (Eindhoven) show some trends which are difficult to interpret,\nespecially in light of the differences in image acquisitions at the different\ntime steps. This demonstrates the complexity of liveability monitoring across\ntime periods and the necessity for more sophisticated methods compensating for\nchanges unrelated to liveability dynamics.",
        "translated": ""
    },
    {
        "title": "Discrete Morphological Neural Networks",
        "url": "http://arxiv.org/abs/2309.00588v1",
        "pub_date": "2023-09-01",
        "summary": "A classical approach to designing binary image operators is Mathematical\nMorphology (MM). We propose the Discrete Morphological Neural Networks (DMNN)\nfor binary image analysis to represent W-operators and estimate them via\nmachine learning. A DMNN architecture, which is represented by a Morphological\nComputational Graph, is designed as in the classical heuristic design of\nmorphological operators, in which the designer should combine a set of MM\noperators and Boolean operations based on prior information and theoretical\nknowledge. Then, once the architecture is fixed, instead of adjusting its\nparameters (i.e., structural elements or maximal intervals) by hand, we propose\na lattice gradient descent algorithm (LGDA) to train these parameters based on\na sample of input and output images under the usual machine learning approach.\nWe also propose a stochastic version of the LGDA that is more efficient, is\nscalable and can obtain small error in practical problems. The class\nrepresented by a DMNN can be quite general or specialized according to expected\nproperties of the target operator, i.e., prior information, and the semantic\nexpressed by algebraic properties of classes of operators is a differential\nrelative to other methods. The main contribution of this paper is the merger of\nthe two main paradigms for designing morphological operators: classical\nheuristic design and automatic design via machine learning. Thus, conciliating\nclassical heuristic morphological operator design with machine learning. We\napply the DMNN to recognize the boundary of digits with noise, and we discuss\nmany topics for future research.",
        "translated": ""
    },
    {
        "title": "Mechanism of feature learning in convolutional neural networks",
        "url": "http://arxiv.org/abs/2309.00570v1",
        "pub_date": "2023-09-01",
        "summary": "Understanding the mechanism of how convolutional neural networks learn\nfeatures from image data is a fundamental problem in machine learning and\ncomputer vision. In this work, we identify such a mechanism. We posit the\nConvolutional Neural Feature Ansatz, which states that covariances of filters\nin any convolutional layer are proportional to the average gradient outer\nproduct (AGOP) taken with respect to patches of the input to that layer. We\npresent extensive empirical evidence for our ansatz, including identifying high\ncorrelation between covariances of filters and patch-based AGOPs for\nconvolutional layers in standard neural architectures, such as AlexNet, VGG,\nand ResNets pre-trained on ImageNet. We also provide supporting theoretical\nevidence. We then demonstrate the generality of our result by using the\npatch-based AGOP to enable deep feature learning in convolutional kernel\nmachines. We refer to the resulting algorithm as (Deep) ConvRFM and show that\nour algorithm recovers similar features to deep convolutional networks\nincluding the notable emergence of edge detectors. Moreover, we find that Deep\nConvRFM overcomes previously identified limitations of convolutional kernels,\nsuch as their inability to adapt to local signals in images and, as a result,\nleads to sizable performance improvement over fixed convolutional kernels.",
        "translated": ""
    },
    {
        "title": "Amyloid-Beta Axial Plane PET Synthesis from Structural MRI: An Image\n  Translation Approach for Screening Alzheimer's Disease",
        "url": "http://arxiv.org/abs/2309.00569v1",
        "pub_date": "2023-09-01",
        "summary": "In this work, an image translation model is implemented to produce synthetic\namyloid-beta PET images from structural MRI that are quantitatively accurate.\nImage pairs of amyloid-beta PET and structural MRI were used to train the\nmodel. We found that the synthetic PET images could be produced with a high\ndegree of similarity to truth in terms of shape, contrast and overall high SSIM\nand PSNR. This work demonstrates that performing structural to quantitative\nimage translation is feasible to enable the access amyloid-beta information\nfrom only MRI.",
        "translated": ""
    },
    {
        "title": "Impact of Image Context for Single Deep Learning Face Morphing Attack\n  Detection",
        "url": "http://arxiv.org/abs/2309.00549v1",
        "pub_date": "2023-09-01",
        "summary": "The increase in security concerns due to technological advancements has led\nto the popularity of biometric approaches that utilize physiological or\nbehavioral characteristics for enhanced recognition. Face recognition systems\n(FRSs) have become prevalent, but they are still vulnerable to image\nmanipulation techniques such as face morphing attacks. This study investigates\nthe impact of the alignment settings of input images on deep learning face\nmorphing detection performance. We analyze the interconnections between the\nface contour and image context and suggest optimal alignment conditions for\nface morphing detection.",
        "translated": ""
    },
    {
        "title": "Trust your Good Friends: Source-free Domain Adaptation by Reciprocal\n  Neighborhood Clustering",
        "url": "http://arxiv.org/abs/2309.00528v1",
        "pub_date": "2023-09-01",
        "summary": "Domain adaptation (DA) aims to alleviate the domain shift between source\ndomain and target domain. Most DA methods require access to the source data,\nbut often that is not possible (e.g. due to data privacy or intellectual\nproperty). In this paper, we address the challenging source-free domain\nadaptation (SFDA) problem, where the source pretrained model is adapted to the\ntarget domain in the absence of source data. Our method is based on the\nobservation that target data, which might not align with the source domain\nclassifier, still forms clear clusters. We capture this intrinsic structure by\ndefining local affinity of the target data, and encourage label consistency\namong data with high local affinity. We observe that higher affinity should be\nassigned to reciprocal neighbors. To aggregate information with more context,\nwe consider expanded neighborhoods with small affinity values. Furthermore, we\nconsider the density around each target sample, which can alleviate the\nnegative impact of potential outliers. In the experimental results we verify\nthat the inherent structure of the target features is an important source of\ninformation for domain adaptation. We demonstrate that this local structure can\nbe efficiently captured by considering the local neighbors, the reciprocal\nneighbors, and the expanded neighborhood. Finally, we achieve state-of-the-art\nperformance on several 2D image and 3D point cloud recognition datasets.",
        "translated": ""
    },
    {
        "title": "GO-SLAM: Global Optimization for Consistent 3D Instant Reconstruction",
        "url": "http://arxiv.org/abs/2309.02436v1",
        "pub_date": "2023-09-05",
        "summary": "Neural implicit representations have recently demonstrated compelling results\non dense Simultaneous Localization And Mapping (SLAM) but suffer from the\naccumulation of errors in camera tracking and distortion in the reconstruction.\nPurposely, we present GO-SLAM, a deep-learning-based dense visual SLAM\nframework globally optimizing poses and 3D reconstruction in real-time. Robust\npose estimation is at its core, supported by efficient loop closing and online\nfull bundle adjustment, which optimize per frame by utilizing the learned\nglobal geometry of the complete history of input frames. Simultaneously, we\nupdate the implicit and continuous surface representation on-the-fly to ensure\nglobal consistency of 3D reconstruction. Results on various synthetic and\nreal-world datasets demonstrate that GO-SLAM outperforms state-of-the-art\napproaches at tracking robustness and reconstruction accuracy. Furthermore,\nGO-SLAM is versatile and can run with monocular, stereo, and RGB-D input.",
        "translated": ""
    },
    {
        "title": "Efficient RL via Disentangled Environment and Agent Representations",
        "url": "http://arxiv.org/abs/2309.02435v1",
        "pub_date": "2023-09-05",
        "summary": "Agents that are aware of the separation between themselves and their\nenvironments can leverage this understanding to form effective representations\nof visual input. We propose an approach for learning such structured\nrepresentations for RL algorithms, using visual knowledge of the agent, such as\nits shape or mask, which is often inexpensive to obtain. This is incorporated\ninto the RL objective using a simple auxiliary loss. We show that our method,\nStructured Environment-Agent Representations, outperforms state-of-the-art\nmodel-free approaches over 18 different challenging visual simulation\nenvironments spanning 5 different robots. Website at https://sear-rl.github.io/",
        "translated": ""
    },
    {
        "title": "ReliTalk: Relightable Talking Portrait Generation from a Single Video",
        "url": "http://arxiv.org/abs/2309.02434v1",
        "pub_date": "2023-09-05",
        "summary": "Recent years have witnessed great progress in creating vivid audio-driven\nportraits from monocular videos. However, how to seamlessly adapt the created\nvideo avatars to other scenarios with different backgrounds and lighting\nconditions remains unsolved. On the other hand, existing relighting studies\nmostly rely on dynamically lighted or multi-view data, which are too expensive\nfor creating video portraits. To bridge this gap, we propose ReliTalk, a novel\nframework for relightable audio-driven talking portrait generation from\nmonocular videos. Our key insight is to decompose the portrait's reflectance\nfrom implicitly learned audio-driven facial normals and images. Specifically,\nwe involve 3D facial priors derived from audio features to predict delicate\nnormal maps through implicit functions. These initially predicted normals then\ntake a crucial part in reflectance decomposition by dynamically estimating the\nlighting condition of the given video. Moreover, the stereoscopic face\nrepresentation is refined using the identity-consistent loss under simulated\nmultiple lighting conditions, addressing the ill-posed problem caused by\nlimited views available from a single monocular video. Extensive experiments\nvalidate the superiority of our proposed framework on both real and synthetic\ndatasets. Our code is released in https://github.com/arthur-qiu/ReliTalk.",
        "translated": ""
    },
    {
        "title": "Building a Winning Team: Selecting Source Model Ensembles using a\n  Submodular Transferability Estimation Approach",
        "url": "http://arxiv.org/abs/2309.02429v1",
        "pub_date": "2023-09-05",
        "summary": "Estimating the transferability of publicly available pretrained models to a\ntarget task has assumed an important place for transfer learning tasks in\nrecent years. Existing efforts propose metrics that allow a user to choose one\nmodel from a pool of pre-trained models without having to fine-tune each model\nindividually and identify one explicitly. With the growth in the number of\navailable pre-trained models and the popularity of model ensembles, it also\nbecomes essential to study the transferability of multiple-source models for a\ngiven target task. The few existing efforts study transferability in such\nmulti-source ensemble settings using just the outputs of the classification\nlayer and neglect possible domain or task mismatch. Moreover, they overlook the\nmost important factor while selecting the source models, viz., the cohesiveness\nfactor between them, which can impact the performance and confidence in the\nprediction of the ensemble. To address these gaps, we propose a novel Optimal\ntranSport-based suBmOdular tRaNsferability metric (OSBORN) to estimate the\ntransferability of an ensemble of models to a downstream task. OSBORN\ncollectively accounts for image domain difference, task difference, and\ncohesiveness of models in the ensemble to provide reliable estimates of\ntransferability. We gauge the performance of OSBORN on both image\nclassification and semantic segmentation tasks. Our setup includes 28 source\ndatasets, 11 target datasets, 5 model architectures, and 2 pre-training\nmethods. We benchmark our method against current state-of-the-art metrics\nMS-LEEP and E-LEEP, and outperform them consistently using the proposed\napproach.",
        "translated": ""
    },
    {
        "title": "EgoPCA: A New Framework for Egocentric Hand-Object Interaction\n  Understanding",
        "url": "http://arxiv.org/abs/2309.02423v1",
        "pub_date": "2023-09-05",
        "summary": "With the surge in attention to Egocentric Hand-Object Interaction (Ego-HOI),\nlarge-scale datasets such as Ego4D and EPIC-KITCHENS have been proposed.\nHowever, most current research is built on resources derived from third-person\nvideo action recognition. This inherent domain gap between first- and\nthird-person action videos, which have not been adequately addressed before,\nmakes current Ego-HOI suboptimal. This paper rethinks and proposes a new\nframework as an infrastructure to advance Ego-HOI recognition by Probing,\nCuration and Adaption (EgoPCA). We contribute comprehensive pre-train sets,\nbalanced test sets and a new baseline, which are complete with a\ntraining-finetuning strategy. With our new framework, we not only achieve\nstate-of-the-art performance on Ego-HOI benchmarks but also build several new\nand effective mechanisms and settings to advance further research. We believe\nour data and the findings will pave a new way for Ego-HOI understanding. Code\nand data are available at https://mvig-rhos.com/ego_pca",
        "translated": ""
    },
    {
        "title": "Doppelgangers: Learning to Disambiguate Images of Similar Structures",
        "url": "http://arxiv.org/abs/2309.02420v1",
        "pub_date": "2023-09-05",
        "summary": "We consider the visual disambiguation task of determining whether a pair of\nvisually similar images depict the same or distinct 3D surfaces (e.g., the same\nor opposite sides of a symmetric building). Illusory image matches, where two\nimages observe distinct but visually similar 3D surfaces, can be challenging\nfor humans to differentiate, and can also lead 3D reconstruction algorithms to\nproduce erroneous results. We propose a learning-based approach to visual\ndisambiguation, formulating it as a binary classification task on image pairs.\nTo that end, we introduce a new dataset for this problem, Doppelgangers, which\nincludes image pairs of similar structures with ground truth labels. We also\ndesign a network architecture that takes the spatial distribution of local\nkeypoints and matches as input, allowing for better reasoning about both local\nand global cues. Our evaluation shows that our method can distinguish illusory\nmatches in difficult cases, and can be integrated into SfM pipelines to produce\ncorrect, disambiguated 3D reconstructions. See our project page for our code,\ndatasets, and more results: http://doppelgangers-3d.github.io/.",
        "translated": ""
    },
    {
        "title": "Generating Realistic Images from In-the-wild Sounds",
        "url": "http://arxiv.org/abs/2309.02405v1",
        "pub_date": "2023-09-05",
        "summary": "Representing wild sounds as images is an important but challenging task due\nto the lack of paired datasets between sound and images and the significant\ndifferences in the characteristics of these two modalities. Previous studies\nhave focused on generating images from sound in limited categories or music. In\nthis paper, we propose a novel approach to generate images from in-the-wild\nsounds. First, we convert sound into text using audio captioning. Second, we\npropose audio attention and sentence attention to represent the rich\ncharacteristics of sound and visualize the sound. Lastly, we propose a direct\nsound optimization with CLIPscore and AudioCLIP and generate images with a\ndiffusion-based model. In experiments, it shows that our model is able to\ngenerate high quality images from wild sounds and outperforms baselines in both\nquantitative and qualitative evaluations on wild audio datasets.",
        "translated": ""
    },
    {
        "title": "Voice Morphing: Two Identities in One Voice",
        "url": "http://arxiv.org/abs/2309.02404v1",
        "pub_date": "2023-09-05",
        "summary": "In a biometric system, each biometric sample or template is typically\nassociated with a single identity. However, recent research has demonstrated\nthe possibility of generating \"morph\" biometric samples that can successfully\nmatch more than a single identity. Morph attacks are now recognized as a\npotential security threat to biometric systems. However, most morph attacks\nhave been studied on biometric modalities operating in the image domain, such\nas face, fingerprint, and iris. In this preliminary work, we introduce Voice\nIdentity Morphing (VIM) - a voice-based morph attack that can synthesize speech\nsamples that impersonate the voice characteristics of a pair of individuals.\nOur experiments evaluate the vulnerabilities of two popular speaker recognition\nsystems, ECAPA-TDNN and x-vector, to VIM, with a success rate (MMPMR) of over\n80% at a false match rate of 1% on the Librispeech dataset.",
        "translated": ""
    },
    {
        "title": "Prototype-based Dataset Comparison",
        "url": "http://arxiv.org/abs/2309.02401v1",
        "pub_date": "2023-09-05",
        "summary": "Dataset summarisation is a fruitful approach to dataset inspection. However,\nwhen applied to a single dataset the discovery of visual concepts is restricted\nto those most prominent. We argue that a comparative approach can expand upon\nthis paradigm to enable richer forms of dataset inspection that go beyond the\nmost prominent concepts. To enable dataset comparison we present a module that\nlearns concept-level prototypes across datasets. We leverage self-supervised\nlearning to discover these prototypes without supervision, and we demonstrate\nthe benefits of our approach in two case-studies. Our findings show that\ndataset comparison extends dataset inspection and we hope to encourage more\nworks in this direction. Code and usage instructions available at\nhttps://github.com/Nanne/ProtoSim",
        "translated": ""
    },
    {
        "title": "STEP -- Towards Structured Scene-Text Spotting",
        "url": "http://arxiv.org/abs/2309.02356v1",
        "pub_date": "2023-09-05",
        "summary": "We introduce the structured scene-text spotting task, which requires a\nscene-text OCR system to spot text in the wild according to a query regular\nexpression. Contrary to generic scene text OCR, structured scene-text spotting\nseeks to dynamically condition both scene text detection and recognition on\nuser-provided regular expressions. To tackle this task, we propose the\nStructured TExt sPotter (STEP), a model that exploits the provided text\nstructure to guide the OCR process. STEP is able to deal with regular\nexpressions that contain spaces and it is not bound to detection at the\nword-level granularity. Our approach enables accurate zero-shot structured text\nspotting in a wide variety of real-world reading scenarios and is solely\ntrained on publicly available data. To demonstrate the effectiveness of our\napproach, we introduce a new challenging test dataset that contains several\ntypes of out-of-vocabulary structured text, reflecting important reading\napplications of fields such as prices, dates, serial numbers, license plates\netc. We demonstrate that STEP can provide specialised OCR performance on demand\nin all tested scenarios.",
        "translated": ""
    },
    {
        "title": "My Art My Choice: Adversarial Protection Against Unruly AI",
        "url": "http://arxiv.org/abs/2309.03198v1",
        "pub_date": "2023-09-06",
        "summary": "Generative AI is on the rise, enabling everyone to produce realistic content\nvia publicly available interfaces. Especially for guided image generation,\ndiffusion models are changing the creator economy by producing high quality low\ncost content. In parallel, artists are rising against unruly AI, since their\nartwork are leveraged, distributed, and dissimulated by large generative\nmodels. Our approach, My Art My Choice (MAMC), aims to empower content owners\nby protecting their copyrighted materials from being utilized by diffusion\nmodels in an adversarial fashion. MAMC learns to generate adversarially\nperturbed \"protected\" versions of images which can in turn \"break\" diffusion\nmodels. The perturbation amount is decided by the artist to balance distortion\nvs. protection of the content. MAMC is designed with a simple UNet-based\ngenerator, attacking black box diffusion models, combining several losses to\ncreate adversarial twins of the original artwork. We experiment on three\ndatasets for various image-to-image tasks, with different user control values.\nBoth protected image and diffusion output results are evaluated in visual,\nnoise, structure, pixel, and generative spaces to validate our claims. We\nbelieve that MAMC is a crucial step for preserving ownership information for AI\ngenerated content in a flawless, based-on-need, and human-centric way.",
        "translated": ""
    },
    {
        "title": "Bayes' Rays: Uncertainty Quantification for Neural Radiance Fields",
        "url": "http://arxiv.org/abs/2309.03185v1",
        "pub_date": "2023-09-06",
        "summary": "Neural Radiance Fields (NeRFs) have shown promise in applications like view\nsynthesis and depth estimation, but learning from multiview images faces\ninherent uncertainties. Current methods to quantify them are either heuristic\nor computationally demanding. We introduce BayesRays, a post-hoc framework to\nevaluate uncertainty in any pre-trained NeRF without modifying the training\nprocess. Our method establishes a volumetric uncertainty field using spatial\nperturbations and a Bayesian Laplace approximation. We derive our algorithm\nstatistically and show its superior performance in key metrics and\napplications. Additional results available at: https://bayesrays.github.io.",
        "translated": ""
    },
    {
        "title": "3D Transformer based on deformable patch location for differential\n  diagnosis between Alzheimer's disease and Frontotemporal dementia",
        "url": "http://arxiv.org/abs/2309.03183v1",
        "pub_date": "2023-09-06",
        "summary": "Alzheimer's disease and Frontotemporal dementia are common types of\nneurodegenerative disorders that present overlapping clinical symptoms, making\ntheir differential diagnosis very challenging. Numerous efforts have been done\nfor the diagnosis of each disease but the problem of multi-class differential\ndiagnosis has not been actively explored. In recent years, transformer-based\nmodels have demonstrated remarkable success in various computer vision tasks.\nHowever, their use in disease diagnostic is uncommon due to the limited amount\nof 3D medical data given the large size of such models. In this paper, we\npresent a novel 3D transformer-based architecture using a deformable patch\nlocation module to improve the differential diagnosis of Alzheimer's disease\nand Frontotemporal dementia. Moreover, to overcome the problem of data\nscarcity, we propose an efficient combination of various data augmentation\ntechniques, adapted for training transformer-based models on 3D structural\nmagnetic resonance imaging data. Finally, we propose to combine our\ntransformer-based model with a traditional machine learning model using brain\nstructure volumes to better exploit the available data. Our experiments\ndemonstrate the effectiveness of the proposed approach, showing competitive\nresults compared to state-of-the-art methods. Moreover, the deformable patch\nlocations can be visualized, revealing the most relevant brain regions used to\nestablish the diagnosis of each disease.",
        "translated": ""
    },
    {
        "title": "SLiMe: Segment Like Me",
        "url": "http://arxiv.org/abs/2309.03179v1",
        "pub_date": "2023-09-06",
        "summary": "Significant strides have been made using large vision-language models, like\nStable Diffusion (SD), for a variety of downstream tasks, including image\nediting, image correspondence, and 3D shape generation. Inspired by these\nadvancements, we explore leveraging these extensive vision-language models for\nsegmenting images at any desired granularity using as few as one annotated\nsample by proposing SLiMe. SLiMe frames this problem as an optimization task.\nSpecifically, given a single training image and its segmentation mask, we first\nextract attention maps, including our novel \"weighted accumulated\nself-attention map\" from the SD prior. Then, using the extracted attention\nmaps, the text embeddings of Stable Diffusion are optimized such that, each of\nthem, learn about a single segmented region from the training image. These\nlearned embeddings then highlight the segmented region in the attention maps,\nwhich in turn can then be used to derive the segmentation map. This enables\nSLiMe to segment any real-world image during inference with the granularity of\nthe segmented region in the training image, using just one example. Moreover,\nleveraging additional training data when available, i.e. few-shot, improves the\nperformance of SLiMe. We carried out a knowledge-rich set of experiments\nexamining various design factors and showed that SLiMe outperforms other\nexisting one-shot and few-shot segmentation methods.",
        "translated": ""
    },
    {
        "title": "3D Object Positioning Using Differentiable Multimodal Learning",
        "url": "http://arxiv.org/abs/2309.03177v1",
        "pub_date": "2023-09-06",
        "summary": "This article describes a multi-modal method using simulated Lidar data via\nray tracing and image pixel loss with differentiable rendering to optimize an\nobject's position with respect to an observer or some referential objects in a\ncomputer graphics scene. Object position optimization is completed using\ngradient descent with the loss function being influenced by both modalities.\nTypical object placement optimization is done using image pixel loss with\ndifferentiable rendering only, this work shows the use of a second modality\n(Lidar) leads to faster convergence. This method of fusing sensor input\npresents a potential usefulness for autonomous vehicles, as these methods can\nbe used to establish the locations of multiple actors in a scene. This article\nalso presents a method for the simulation of multiple types of data to be used\nin the training of autonomous vehicles.",
        "translated": ""
    },
    {
        "title": "PDiscoNet: Semantically consistent part discovery for fine-grained\n  recognition",
        "url": "http://arxiv.org/abs/2309.03173v1",
        "pub_date": "2023-09-06",
        "summary": "Fine-grained classification often requires recognizing specific object parts,\nsuch as beak shape and wing patterns for birds. Encouraging a fine-grained\nclassification model to first detect such parts and then using them to infer\nthe class could help us gauge whether the model is indeed looking at the right\ndetails better than with interpretability methods that provide a single\nattribution map. We propose PDiscoNet to discover object parts by using only\nimage-level class labels along with priors encouraging the parts to be:\ndiscriminative, compact, distinct from each other, equivariant to rigid\ntransforms, and active in at least some of the images. In addition to using the\nappropriate losses to encode these priors, we propose to use part-dropout,\nwhere full part feature vectors are dropped at once to prevent a single part\nfrom dominating in the classification, and part feature vector modulation,\nwhich makes the information coming from each part distinct from the perspective\nof the classifier. Our results on CUB, CelebA, and PartImageNet show that the\nproposed method provides substantially better part discovery performance than\nprevious methods while not requiring any additional hyper-parameter tuning and\nwithout penalizing the classification performance. The code is available at\nhttps://github.com/robertdvdk/part_detection.",
        "translated": ""
    },
    {
        "title": "ResFields: Residual Neural Fields for Spatiotemporal Signals",
        "url": "http://arxiv.org/abs/2309.03160v1",
        "pub_date": "2023-09-06",
        "summary": "Neural fields, a category of neural networks trained to represent\nhigh-frequency signals, have gained significant attention in recent years due\nto their impressive performance in modeling complex 3D data, especially large\nneural signed distance (SDFs) or radiance fields (NeRFs) via a single\nmulti-layer perceptron (MLP). However, despite the power and simplicity of\nrepresenting signals with an MLP, these methods still face challenges when\nmodeling large and complex temporal signals due to the limited capacity of\nMLPs. In this paper, we propose an effective approach to address this\nlimitation by incorporating temporal residual layers into neural fields, dubbed\nResFields, a novel class of networks specifically designed to effectively\nrepresent complex temporal signals. We conduct a comprehensive analysis of the\nproperties of ResFields and propose a matrix factorization technique to reduce\nthe number of trainable parameters and enhance generalization capabilities.\nImportantly, our formulation seamlessly integrates with existing techniques and\nconsistently improves results across various challenging tasks: 2D video\napproximation, dynamic shape modeling via temporal SDFs, and dynamic NeRF\nreconstruction. Lastly, we demonstrate the practical utility of ResFields by\nshowcasing its effectiveness in capturing dynamic 3D scenes from sparse sensory\ninputs of a lightweight capture system.",
        "translated": ""
    },
    {
        "title": "Detecting Manufacturing Defects in PCBs via Data-Centric Machine\n  Learning on Solder Paste Inspection Features",
        "url": "http://arxiv.org/abs/2309.03113v1",
        "pub_date": "2023-09-06",
        "summary": "Automated detection of defects in Printed Circuit Board (PCB) manufacturing\nusing Solder Paste Inspection (SPI) and Automated Optical Inspection (AOI)\nmachines can help improve operational efficiency and significantly reduce the\nneed for manual intervention. In this paper, using SPI-extracted features of 6\nmillion pins, we demonstrate a data-centric approach to train Machine Learning\n(ML) models to detect PCB defects at three stages of PCB manufacturing. The 6\nmillion PCB pins correspond to 2 million components that belong to 15,387 PCBs.\nUsing a base extreme gradient boosting (XGBoost) ML model, we iterate on the\ndata pre-processing step to improve detection performance. Combining pin-level\nSPI features using component and PCB IDs, we developed training instances also\nat the component and PCB level. This allows the ML model to capture any\ninter-pin, inter-component, or spatial effects that may not be apparent at the\npin level. Models are trained at the pin, component, and PCB levels, and the\ndetection results from the different models are combined to identify defective\ncomponents.",
        "translated": ""
    },
    {
        "title": "Do We Still Need Non-Maximum Suppression? Accurate Confidence Estimates\n  and Implicit Duplication Modeling with IoU-Aware Calibration",
        "url": "http://arxiv.org/abs/2309.03110v1",
        "pub_date": "2023-09-06",
        "summary": "Object detectors are at the heart of many semi- and fully autonomous decision\nsystems and are poised to become even more indispensable. They are, however,\nstill lacking in accessibility and can sometimes produce unreliable\npredictions. Especially concerning in this regard are the -- essentially\nhand-crafted -- non-maximum suppression algorithms that lead to an obfuscated\nprediction process and biased confidence estimates. We show that we can\neliminate classic NMS-style post-processing by using IoU-aware calibration.\nIoU-aware calibration is a conditional Beta calibration; this makes it\nparallelizable with no hyper-parameters. Instead of arbitrary cutoffs or\ndiscounts, it implicitly accounts for the likelihood of each detection being a\nduplicate and adjusts the confidence score accordingly, resulting in\nempirically based precision estimates for each detection. Our extensive\nexperiments on diverse detection architectures show that the proposed IoU-aware\ncalibration can successfully model duplicate detections and improve\ncalibration. Compared to the standard sequential NMS and calibration approach,\nour joint modeling can deliver performance gains over the best NMS-based\nalternative while producing consistently better-calibrated confidence\npredictions with less complexity. The\n\\hyperlink{https://github.com/Blueblue4/IoU-AwareCalibration}{code} for all our\nexperiments is publicly available.",
        "translated": ""
    },
    {
        "title": "FArMARe: a Furniture-Aware Multi-task methodology for Recommending\n  Apartments based on the user interests",
        "url": "http://arxiv.org/abs/2309.03100v1",
        "pub_date": "2023-09-06",
        "summary": "Nowadays, many people frequently have to search for new accommodation\noptions. Searching for a suitable apartment is a time-consuming process,\nespecially because visiting them is often mandatory to assess the truthfulness\nof the advertisements found on the Web. While this process could be alleviated\nby visiting the apartments in the metaverse, the Web-based recommendation\nplatforms are not suitable for the task. To address this shortcoming, in this\npaper, we define a new problem called text-to-apartment recommendation, which\nrequires ranking the apartments based on their relevance to a textual query\nexpressing the user's interests. To tackle this problem, we introduce FArMARe,\na multi-task approach that supports cross-modal contrastive training with a\nfurniture-aware objective. Since public datasets related to indoor scenes do\nnot contain detailed descriptions of the furniture, we collect and annotate a\ndataset comprising more than 6000 apartments. A thorough experimentation with\nthree different methods and two raw feature extraction procedures reveals the\neffectiveness of FArMARe in dealing with the problem at hand.",
        "translated": ""
    },
    {
        "title": "ImageBind-LLM: Multi-modality Instruction Tuning",
        "url": "http://arxiv.org/abs/2309.03905v1",
        "pub_date": "2023-09-07",
        "summary": "We present ImageBind-LLM, a multi-modality instruction tuning method of large\nlanguage models (LLMs) via ImageBind. Existing works mainly focus on language\nand image instruction tuning, different from which, our ImageBind-LLM can\nrespond to multi-modality conditions, including audio, 3D point clouds, video,\nand their embedding-space arithmetic by only image-text alignment training.\nDuring training, we adopt a learnable bind network to align the embedding space\nbetween LLaMA and ImageBind's image encoder. Then, the image features\ntransformed by the bind network are added to word tokens of all layers in\nLLaMA, which progressively injects visual instructions via an attention-free\nand zero-initialized gating mechanism. Aided by the joint embedding of\nImageBind, the simple image-text training enables our model to exhibit superior\nmulti-modality instruction-following capabilities. During inference, the\nmulti-modality inputs are fed into the corresponding ImageBind encoders, and\nprocessed by a proposed visual cache model for further cross-modal embedding\nenhancement. The training-free cache model retrieves from three million image\nfeatures extracted by ImageBind, which effectively mitigates the\ntraining-inference modality discrepancy. Notably, with our approach,\nImageBind-LLM can respond to instructions of diverse modalities and demonstrate\nsignificant language generation quality. Code is released at\nhttps://github.com/OpenGVLab/LLaMA-Adapter.",
        "translated": ""
    },
    {
        "title": "Exploring Sparse MoE in GANs for Text-conditioned Image Synthesis",
        "url": "http://arxiv.org/abs/2309.03904v1",
        "pub_date": "2023-09-07",
        "summary": "Due to the difficulty in scaling up, generative adversarial networks (GANs)\nseem to be falling from grace on the task of text-conditioned image synthesis.\nSparsely-activated mixture-of-experts (MoE) has recently been demonstrated as a\nvalid solution to training large-scale models with limited computational\nresources. Inspired by such a philosophy, we present Aurora, a GAN-based\ntext-to-image generator that employs a collection of experts to learn feature\nprocessing, together with a sparse router to help select the most suitable\nexpert for each feature point. To faithfully decode the sampling stochasticity\nand the text condition to the final synthesis, our router adaptively makes its\ndecision by taking into account the text-integrated global latent code. At\n64x64 image resolution, our model trained on LAION2B-en and COYO-700M achieves\n6.2 zero-shot FID on MS COCO. We release the code and checkpoints to facilitate\nthe community for further development.",
        "translated": ""
    },
    {
        "title": "Tracking Anything with Decoupled Video Segmentation",
        "url": "http://arxiv.org/abs/2309.03903v1",
        "pub_date": "2023-09-07",
        "summary": "Training data for video segmentation are expensive to annotate. This impedes\nextensions of end-to-end algorithms to new video segmentation tasks, especially\nin large-vocabulary settings. To 'track anything' without training on video\ndata for every individual task, we develop a decoupled video segmentation\napproach (DEVA), composed of task-specific image-level segmentation and\nclass/task-agnostic bi-directional temporal propagation. Due to this design, we\nonly need an image-level model for the target task (which is cheaper to train)\nand a universal temporal propagation model which is trained once and\ngeneralizes across tasks. To effectively combine these two modules, we use\nbi-directional propagation for (semi-)online fusion of segmentation hypotheses\nfrom different frames to generate a coherent segmentation. We show that this\ndecoupled formulation compares favorably to end-to-end approaches in several\ndata-scarce tasks including large-vocabulary video panoptic segmentation,\nopen-world video segmentation, referring video segmentation, and unsupervised\nvideo object segmentation. Code is available at:\nhttps://hkchengrex.github.io/Tracking-Anything-with-DEVA",
        "translated": ""
    },
    {
        "title": "Learning Continuous Exposure Value Representations for Single-Image HDR\n  Reconstruction",
        "url": "http://arxiv.org/abs/2309.03900v1",
        "pub_date": "2023-09-07",
        "summary": "Deep learning is commonly used to reconstruct HDR images from LDR images. LDR\nstack-based methods are used for single-image HDR reconstruction, generating an\nHDR image from a deep learning-generated LDR stack. However, current methods\ngenerate the stack with predetermined exposure values (EVs), which may limit\nthe quality of HDR reconstruction. To address this, we propose the continuous\nexposure value representation (CEVR), which uses an implicit function to\ngenerate LDR images with arbitrary EVs, including those unseen during training.\nOur approach generates a continuous stack with more images containing diverse\nEVs, significantly improving HDR reconstruction. We use a cycle training\nstrategy to supervise the model in generating continuous EV LDR images without\ncorresponding ground truths. Our CEVR model outperforms existing methods, as\ndemonstrated by experimental results.",
        "translated": ""
    },
    {
        "title": "The Making and Breaking of Camouflage",
        "url": "http://arxiv.org/abs/2309.03899v1",
        "pub_date": "2023-09-07",
        "summary": "Not all camouflages are equally effective, as even a partially visible\ncontour or a slight color difference can make the animal stand out and break\nits camouflage. In this paper, we address the question of what makes a\ncamouflage successful, by proposing three scores for automatically assessing\nits effectiveness. In particular, we show that camouflage can be measured by\nthe similarity between background and foreground features and boundary\nvisibility. We use these camouflage scores to assess and compare all available\ncamouflage datasets. We also incorporate the proposed camouflage score into a\ngenerative model as an auxiliary loss and show that effective camouflage images\nor videos can be synthesised in a scalable manner. The generated synthetic\ndataset is used to train a transformer-based model for segmenting camouflaged\nanimals in videos. Experimentally, we demonstrate state-of-the-art camouflage\nbreaking performance on the public MoCA-Mask benchmark.",
        "translated": ""
    },
    {
        "title": "ProPainter: Improving Propagation and Transformer for Video Inpainting",
        "url": "http://arxiv.org/abs/2309.03897v1",
        "pub_date": "2023-09-07",
        "summary": "Flow-based propagation and spatiotemporal Transformer are two mainstream\nmechanisms in video inpainting (VI). Despite the effectiveness of these\ncomponents, they still suffer from some limitations that affect their\nperformance. Previous propagation-based approaches are performed separately\neither in the image or feature domain. Global image propagation isolated from\nlearning may cause spatial misalignment due to inaccurate optical flow.\nMoreover, memory or computational constraints limit the temporal range of\nfeature propagation and video Transformer, preventing exploration of\ncorrespondence information from distant frames. To address these issues, we\npropose an improved framework, called ProPainter, which involves enhanced\nProPagation and an efficient Transformer. Specifically, we introduce\ndual-domain propagation that combines the advantages of image and feature\nwarping, exploiting global correspondences reliably. We also propose a\nmask-guided sparse video Transformer, which achieves high efficiency by\ndiscarding unnecessary and redundant tokens. With these components, ProPainter\noutperforms prior arts by a large margin of 1.46 dB in PSNR while maintaining\nappealing efficiency.",
        "translated": ""
    },
    {
        "title": "InstructDiffusion: A Generalist Modeling Interface for Vision Tasks",
        "url": "http://arxiv.org/abs/2309.03895v1",
        "pub_date": "2023-09-07",
        "summary": "We present InstructDiffusion, a unifying and generic framework for aligning\ncomputer vision tasks with human instructions. Unlike existing approaches that\nintegrate prior knowledge and pre-define the output space (e.g., categories and\ncoordinates) for each vision task, we cast diverse vision tasks into a\nhuman-intuitive image-manipulating process whose output space is a flexible and\ninteractive pixel space. Concretely, the model is built upon the diffusion\nprocess and is trained to predict pixels according to user instructions, such\nas encircling the man's left shoulder in red or applying a blue mask to the\nleft car. InstructDiffusion could handle a variety of vision tasks, including\nunderstanding tasks (such as segmentation and keypoint detection) and\ngenerative tasks (such as editing and enhancement). It even exhibits the\nability to handle unseen tasks and outperforms prior methods on novel datasets.\nThis represents a significant step towards a generalist modeling interface for\nvision tasks, advancing artificial general intelligence in the field of\ncomputer vision.",
        "translated": ""
    },
    {
        "title": "DiffusionEngine: Diffusion Model is Scalable Data Engine for Object\n  Detection",
        "url": "http://arxiv.org/abs/2309.03893v1",
        "pub_date": "2023-09-07",
        "summary": "Data is the cornerstone of deep learning. This paper reveals that the\nrecently developed Diffusion Model is a scalable data engine for object\ndetection. Existing methods for scaling up detection-oriented data often\nrequire manual collection or generative models to obtain target images,\nfollowed by data augmentation and labeling to produce training pairs, which are\ncostly, complex, or lacking diversity. To address these issues, we\npresentDiffusionEngine (DE), a data scaling-up engine that provides\nhigh-quality detection-oriented training pairs in a single stage. DE consists\nof a pre-trained diffusion model and an effective Detection-Adapter,\ncontributing to generating scalable, diverse and generalizable detection data\nin a plug-and-play manner. Detection-Adapter is learned to align the implicit\nsemantic and location knowledge in off-the-shelf diffusion models with\ndetection-aware signals to make better bounding-box predictions. Additionally,\nwe contribute two datasets, i.e., COCO-DE and VOC-DE, to scale up existing\ndetection benchmarks for facilitating follow-up research. Extensive experiments\ndemonstrate that data scaling-up via DE can achieve significant improvements in\ndiverse scenarios, such as various detection algorithms, self-supervised\npre-training, data-sparse, label-scarce, cross-domain, and semi-supervised\nlearning. For example, when using DE with a DINO-based adapter to scale up\ndata, mAP is improved by 3.1% on COCO, 7.6% on VOC, and 11.5% on Clipart.",
        "translated": ""
    },
    {
        "title": "ArtiGrasp: Physically Plausible Synthesis of Bi-Manual Dexterous\n  Grasping and Articulation",
        "url": "http://arxiv.org/abs/2309.03891v1",
        "pub_date": "2023-09-07",
        "summary": "We present ArtiGrasp, a novel method to synthesize bi-manual hand-object\ninteractions that include grasping and articulation. This task is challenging\ndue to the diversity of the global wrist motions and the precise finger control\nthat are necessary to articulate objects. ArtiGrasp leverages reinforcement\nlearning and physics simulations to train a policy that controls the global and\nlocal hand pose. Our framework unifies grasping and articulation within a\nsingle policy guided by a single hand pose reference. Moreover, to facilitate\nthe training of the precise finger control required for articulation, we\npresent a learning curriculum with increasing difficulty. It starts with\nsingle-hand manipulation of stationary objects and continues with multi-agent\ntraining including both hands and non-stationary objects. To evaluate our\nmethod, we introduce Dynamic Object Grasping and Articulation, a task that\ninvolves bringing an object into a target articulated pose. This task requires\ngrasping, relocation, and articulation. We show our method's efficacy towards\nthis task. We further demonstrate that our method can generate motions with\nnoisy hand-object pose estimates from an off-the-shelf image-based regressor.",
        "translated": ""
    },
    {
        "title": "Better Practices for Domain Adaptation",
        "url": "http://arxiv.org/abs/2309.03879v1",
        "pub_date": "2023-09-07",
        "summary": "Distribution shifts are all too common in real-world applications of machine\nlearning. Domain adaptation (DA) aims to address this by providing various\nframeworks for adapting models to the deployment data without using labels.\nHowever, the domain shift scenario raises a second more subtle challenge: the\ndifficulty of performing hyperparameter optimisation (HPO) for these adaptation\nalgorithms without access to a labelled validation set. The unclear validation\nprotocol for DA has led to bad practices in the literature, such as performing\nHPO using the target test labels when, in real-world scenarios, they are not\navailable. This has resulted in over-optimism about DA research progress\ncompared to reality. In this paper, we analyse the state of DA when using good\nevaluation practice, by benchmarking a suite of candidate validation criteria\nand using them to assess popular adaptation algorithms. We show that there are\nchallenges across all three branches of domain adaptation methodology including\nUnsupervised Domain Adaptation (UDA), Source-Free Domain Adaptation (SFDA), and\nTest Time Adaptation (TTA). While the results show that realistically\nachievable performance is often worse than expected, they also show that using\nproper validation splits is beneficial, as well as showing that some previously\nunexplored validation metrics provide the best options to date. Altogether, our\nimproved practices covering data, training, validation and hyperparameter\noptimisation form a new rigorous pipeline to improve benchmarking, and hence\nresearch progress, within this important field going forward.",
        "translated": ""
    },
    {
        "title": "Generalized Cross-domain Multi-label Few-shot Learning for Chest X-rays",
        "url": "http://arxiv.org/abs/2309.04462v1",
        "pub_date": "2023-09-08",
        "summary": "Real-world application of chest X-ray abnormality classification requires\ndealing with several challenges: (i) limited training data; (ii) training and\nevaluation sets that are derived from different domains; and (iii) classes that\nappear during training may have partial overlap with classes of interest during\nevaluation. To address these challenges, we present an integrated framework\ncalled Generalized Cross-Domain Multi-Label Few-Shot Learning (GenCDML-FSL).\nThe framework supports overlap in classes during training and evaluation,\ncross-domain transfer, adopts meta-learning to learn using few training\nsamples, and assumes each chest X-ray image is either normal or associated with\none or more abnormalities. Furthermore, we propose Generalized Episodic\nTraining (GenET), a training strategy that equips models to operate with\nmultiple challenges observed in the GenCDML-FSL scenario. Comparisons with\nwell-established methods such as transfer learning, hybrid transfer learning,\nand multi-label meta-learning on multiple datasets show the superiority of our\napproach.",
        "translated": ""
    },
    {
        "title": "Measuring and Improving Chain-of-Thought Reasoning in Vision-Language\n  Models",
        "url": "http://arxiv.org/abs/2309.04461v1",
        "pub_date": "2023-09-08",
        "summary": "Vision-language models (VLMs) have recently demonstrated strong efficacy as\nvisual assistants that can parse natural queries about the visual content and\ngenerate human-like outputs. In this work, we explore the ability of these\nmodels to demonstrate human-like reasoning based on the perceived information.\nTo address a crucial concern regarding the extent to which their reasoning\ncapabilities are fully consistent and grounded, we also measure the reasoning\nconsistency of these models. We achieve this by proposing a chain-of-thought\n(CoT) based consistency measure. However, such an evaluation requires a\nbenchmark that encompasses both high-level inference and detailed reasoning\nchains, which is costly. We tackle this challenge by proposing a\nLLM-Human-in-the-Loop pipeline, which notably reduces cost while simultaneously\nensuring the generation of a high-quality dataset. Based on this pipeline and\nthe existing coarse-grained annotated dataset, we build the CURE benchmark to\nmeasure both the zero-shot reasoning performance and consistency of VLMs. We\nevaluate existing state-of-the-art VLMs, and find that even the best-performing\nmodel is unable to demonstrate strong visual reasoning capabilities and\nconsistency, indicating that substantial efforts are required to enable VLMs to\nperform visual reasoning as systematically and consistently as humans. As an\nearly step, we propose a two-stage training framework aimed at improving both\nthe reasoning performance and consistency of VLMs. The first stage involves\nemploying supervised fine-tuning of VLMs using step-by-step reasoning samples\nautomatically generated by LLMs. In the second stage, we further augment the\ntraining process by incorporating feedback provided by LLMs to produce\nreasoning chains that are highly consistent and grounded. We empirically\nhighlight the effectiveness of our framework in both reasoning performance and\nconsistency.",
        "translated": ""
    },
    {
        "title": "WiSARD: A Labeled Visual and Thermal Image Dataset for Wilderness Search\n  and Rescue",
        "url": "http://arxiv.org/abs/2309.04453v1",
        "pub_date": "2023-09-08",
        "summary": "Sensor-equipped unoccupied aerial vehicles (UAVs) have the potential to help\nreduce search times and alleviate safety risks for first responders carrying\nout Wilderness Search and Rescue (WiSAR) operations, the process of finding and\nrescuing person(s) lost in wilderness areas. Unfortunately, visual sensors\nalone do not address the need for robustness across all the possible terrains,\nweather, and lighting conditions that WiSAR operations can be conducted in. The\nuse of multi-modal sensors, specifically visual-thermal cameras, is critical in\nenabling WiSAR UAVs to perform in diverse operating conditions. However, due to\nthe unique challenges posed by the wilderness context, existing dataset\nbenchmarks are inadequate for developing vision-based algorithms for autonomous\nWiSAR UAVs. To this end, we present WiSARD, a dataset with roughly 56,000\nlabeled visual and thermal images collected from UAV flights in various\nterrains, seasons, weather, and lighting conditions. To the best of our\nknowledge, WiSARD is the first large-scale dataset collected with multi-modal\nsensors for autonomous WiSAR operations. We envision that our dataset will\nprovide researchers with a diverse and challenging benchmark that can test the\nrobustness of their algorithms when applied to real-world (life-saving)\napplications.",
        "translated": ""
    },
    {
        "title": "Demographic Disparities in 1-to-Many Facial Identification",
        "url": "http://arxiv.org/abs/2309.04447v1",
        "pub_date": "2023-09-08",
        "summary": "Most studies to date that have examined demographic variations in face\nrecognition accuracy have analyzed 1-to-1 matching accuracy, using images that\ncould be described as \"government ID quality\". This paper analyzes the accuracy\nof 1-to-many facial identification across demographic groups, and in the\npresence of blur and reduced resolution in the probe image as might occur in\n\"surveillance camera quality\" images. Cumulative match characteristic\ncurves(CMC) are not appropriate for comparing propensity for rank-one\nrecognition errors across demographics, and so we introduce three metrics for\nthis: (1) d' metric between mated and non-mated score distributions, (2)\nabsolute score difference between thresholds in the high-similarity tail of the\nnon-mated and the low-similarity tail of the mated distribution, and (3)\ndistribution of (mated - non-mated rank one scores) across the set of probe\nimages. We find that demographic variation in 1-to-many accuracy does not\nentirely follow what has been observed in 1-to-1 matching accuracy. Also,\ndifferent from 1-to-1 accuracy, demographic comparison of 1-to-many accuracy\ncan be affected by different numbers of identities and images across\ndemographics. Finally, we show that increased blur in the probe image, or\nreduced resolution of the face in the probe image, can significantly increase\nthe false positive identification rate. And we show that the demographic\nvariation in these high blur or low resolution conditions is much larger for\nmale/ female than for African-American / Caucasian. The point that 1-to-many\naccuracy can potentially collapse in the context of processing \"surveillance\ncamera quality\" probe images against a \"government ID quality\" gallery is an\nimportant one.",
        "translated": ""
    },
    {
        "title": "Comparative Study of Visual SLAM-Based Mobile Robot Localization Using\n  Fiducial Markers",
        "url": "http://arxiv.org/abs/2309.04441v1",
        "pub_date": "2023-09-08",
        "summary": "This paper presents a comparative study of three modes for mobile robot\nlocalization based on visual SLAM using fiducial markers (i.e., square-shaped\nartificial landmarks with a black-and-white grid pattern): SLAM, SLAM with a\nprior map, and localization with a prior map. The reason for comparing the\nSLAM-based approaches leveraging fiducial markers is because previous work has\nshown their superior performance over feature-only methods, with less\ncomputational burden compared to methods that use both feature and marker\ndetection without compromising the localization performance. The evaluation is\nconducted using indoor image sequences captured with a hand-held camera\ncontaining multiple fiducial markers in the environment. The performance\nmetrics include absolute trajectory error and runtime for the optimization\nprocess per frame. In particular, for the last two modes (SLAM and localization\nwith a prior map), we evaluate their performances by perturbing the quality of\nprior map to study the extent to which each mode is tolerant to such\nperturbations. Hardware experiments show consistent trajectory error levels\nacross the three modes, with the localization mode exhibiting the shortest\nruntime among them. Yet, with map perturbations, SLAM with a prior map\nmaintains performance, while localization mode degrades in both aspects.",
        "translated": ""
    },
    {
        "title": "Single View Refractive Index Tomography with Neural Fields",
        "url": "http://arxiv.org/abs/2309.04437v1",
        "pub_date": "2023-09-08",
        "summary": "Refractive Index Tomography is an inverse problem in which we seek to\nreconstruct a scene's 3D refractive field from 2D projected image measurements.\nThe refractive field is not visible itself, but instead affects how the path of\na light ray is continuously curved as it travels through space. Refractive\nfields appear across a wide variety of scientific applications, from\ntranslucent cell samples in microscopy to fields of dark matter bending light\nfrom faraway galaxies. This problem poses a unique challenge because the\nrefractive field directly affects the path that light takes, making its\nrecovery a non-linear problem. In addition, in contrast with traditional\ntomography, we seek to recover the refractive field using a projected image\nfrom only a single viewpoint by leveraging knowledge of light sources scattered\nthroughout the medium. In this work, we introduce a method that uses a\ncoordinate-based neural network to model the underlying continuous refractive\nfield in a scene. We then use explicit modeling of rays' 3D spatial curvature\nto optimize the parameters of this network, reconstructing refractive fields\nwith an analysis-by-synthesis approach. The efficacy of our approach is\ndemonstrated by recovering refractive fields in simulation, and analyzing how\nrecovery is affected by the light source distribution. We then test our method\non a simulated dark matter mapping problem, where we recover the refractive\nfield underlying a realistic simulated dark matter distribution.",
        "translated": ""
    },
    {
        "title": "Create Your World: Lifelong Text-to-Image Diffusion",
        "url": "http://arxiv.org/abs/2309.04430v1",
        "pub_date": "2023-09-08",
        "summary": "Text-to-image generative models can produce diverse high-quality images of\nconcepts with a text prompt, which have demonstrated excellent ability in image\ngeneration, image translation, etc. We in this work study the problem of\nsynthesizing instantiations of a use's own concepts in a never-ending manner,\ni.e., create your world, where the new concepts from user are quickly learned\nwith a few examples. To achieve this goal, we propose a Lifelong text-to-image\nDiffusion Model (L2DM), which intends to overcome knowledge \"catastrophic\nforgetting\" for the past encountered concepts, and semantic \"catastrophic\nneglecting\" for one or more concepts in the text prompt. In respect of\nknowledge \"catastrophic forgetting\", our L2DM framework devises a task-aware\nmemory enhancement module and a elastic-concept distillation module, which\ncould respectively safeguard the knowledge of both prior concepts and each past\npersonalized concept. When generating images with a user text prompt, the\nsolution to semantic \"catastrophic neglecting\" is that a concept attention\nartist module can alleviate the semantic neglecting from concept aspect, and an\northogonal attention module can reduce the semantic binding from attribute\naspect. To the end, our model can generate more faithful image across a range\nof continual text prompts in terms of both qualitative and quantitative\nmetrics, when comparing with the related state-of-the-art models. The code will\nbe released at https://wenqiliang.github.io/.",
        "translated": ""
    },
    {
        "title": "Video Task Decathlon: Unifying Image and Video Tasks in Autonomous\n  Driving",
        "url": "http://arxiv.org/abs/2309.04422v1",
        "pub_date": "2023-09-08",
        "summary": "Performing multiple heterogeneous visual tasks in dynamic scenes is a\nhallmark of human perception capability. Despite remarkable progress in image\nand video recognition via representation learning, current research still\nfocuses on designing specialized networks for singular, homogeneous, or simple\ncombination of tasks. We instead explore the construction of a unified model\nfor major image and video recognition tasks in autonomous driving with diverse\ninput and output structures. To enable such an investigation, we design a new\nchallenge, Video Task Decathlon (VTD), which includes ten representative image\nand video tasks spanning classification, segmentation, localization, and\nassociation of objects and pixels. On VTD, we develop our unified network,\nVTDNet, that uses a single structure and a single set of weights for all ten\ntasks. VTDNet groups similar tasks and employs task interaction stages to\nexchange information within and between task groups. Given the impracticality\nof labeling all tasks on all frames, and the performance degradation associated\nwith joint training of many tasks, we design a Curriculum training,\nPseudo-labeling, and Fine-tuning (CPF) scheme to successfully train VTDNet on\nall tasks and mitigate performance loss. Armed with CPF, VTDNet significantly\noutperforms its single-task counterparts on most tasks with only 20% overall\ncomputations. VTD is a promising new direction for exploring the unification of\nperception tasks in autonomous driving.",
        "translated": ""
    },
    {
        "title": "SynthoGestures: A Novel Framework for Synthetic Dynamic Hand Gesture\n  Generation for Driving Scenarios",
        "url": "http://arxiv.org/abs/2309.04421v1",
        "pub_date": "2023-09-08",
        "summary": "Creating a diverse and comprehensive dataset of hand gestures for dynamic\nhuman-machine interfaces in the automotive domain can be challenging and\ntime-consuming. To overcome this challenge, we propose using synthetic gesture\ndatasets generated by virtual 3D models. Our framework utilizes Unreal Engine\nto synthesize realistic hand gestures, offering customization options and\nreducing the risk of overfitting. Multiple variants, including gesture speed,\nperformance, and hand shape, are generated to improve generalizability. In\naddition, we simulate different camera locations and types, such as RGB,\ninfrared, and depth cameras, without incurring additional time and cost to\nobtain these cameras. Experimental results demonstrate that our proposed\nframework,\nSynthoGestures\\footnote{\\url{https://github.com/amrgomaaelhady/SynthoGestures}},\nimproves gesture recognition accuracy and can replace or augment real-hand\ndatasets. By saving time and effort in the creation of the data set, our tool\naccelerates the development of gesture recognition systems for automotive\napplications.",
        "translated": ""
    },
    {
        "title": "DeformToon3D: Deformable 3D Toonification from Neural Radiance Fields",
        "url": "http://arxiv.org/abs/2309.04410v1",
        "pub_date": "2023-09-08",
        "summary": "In this paper, we address the challenging problem of 3D toonification, which\ninvolves transferring the style of an artistic domain onto a target 3D face\nwith stylized geometry and texture. Although fine-tuning a pre-trained 3D GAN\non the artistic domain can produce reasonable performance, this strategy has\nlimitations in the 3D domain. In particular, fine-tuning can deteriorate the\noriginal GAN latent space, which affects subsequent semantic editing, and\nrequires independent optimization and storage for each new style, limiting\nflexibility and efficient deployment. To overcome these challenges, we propose\nDeformToon3D, an effective toonification framework tailored for hierarchical 3D\nGAN. Our approach decomposes 3D toonification into subproblems of geometry and\ntexture stylization to better preserve the original latent space. Specifically,\nwe devise a novel StyleField that predicts conditional 3D deformation to align\na real-space NeRF to the style space for geometry stylization. Thanks to the\nStyleField formulation, which already handles geometry stylization well,\ntexture stylization can be achieved conveniently via adaptive style mixing that\ninjects information of the artistic domain into the decoder of the pre-trained\n3D GAN. Due to the unique design, our method enables flexible style degree\ncontrol and shape-texture-specific style swap. Furthermore, we achieve\nefficient training without any real-world 2D-3D training pairs but proxy\nsamples synthesized from off-the-shelf 2D toonification models.",
        "translated": ""
    },
    {
        "title": "Robot Parkour Learning",
        "url": "http://arxiv.org/abs/2309.05665v2",
        "pub_date": "2023-09-11",
        "summary": "Parkour is a grand challenge for legged locomotion that requires robots to\novercome various obstacles rapidly in complex environments. Existing methods\ncan generate either diverse but blind locomotion skills or vision-based but\nspecialized skills by using reference animal data or complex rewards. However,\nautonomous parkour requires robots to learn generalizable skills that are both\nvision-based and diverse to perceive and react to various scenarios. In this\nwork, we propose a system for learning a single end-to-end vision-based parkour\npolicy of diverse parkour skills using a simple reward without any reference\nmotion data. We develop a reinforcement learning method inspired by direct\ncollocation to generate parkour skills, including climbing over high obstacles,\nleaping over large gaps, crawling beneath low barriers, squeezing through thin\nslits, and running. We distill these skills into a single vision-based parkour\npolicy and transfer it to a quadrupedal robot using its egocentric depth\ncamera. We demonstrate that our system can empower two different low-cost\nrobots to autonomously select and execute appropriate parkour skills to\ntraverse challenging real-world environments.",
        "translated": ""
    },
    {
        "title": "Diffusion-Guided Reconstruction of Everyday Hand-Object Interaction\n  Clips",
        "url": "http://arxiv.org/abs/2309.05663v1",
        "pub_date": "2023-09-11",
        "summary": "We tackle the task of reconstructing hand-object interactions from short\nvideo clips. Given an input video, our approach casts 3D inference as a\nper-video optimization and recovers a neural 3D representation of the object\nshape, as well as the time-varying motion and hand articulation. While the\ninput video naturally provides some multi-view cues to guide 3D inference,\nthese are insufficient on their own due to occlusions and limited viewpoint\nvariations. To obtain accurate 3D, we augment the multi-view signals with\ngeneric data-driven priors to guide reconstruction. Specifically, we learn a\ndiffusion network to model the conditional distribution of (geometric)\nrenderings of objects conditioned on hand configuration and category label, and\nleverage it as a prior to guide the novel-view renderings of the reconstructed\nscene. We empirically evaluate our approach on egocentric videos across 6\nobject categories, and observe significant improvements over prior single-view\nand multi-view methods. Finally, we demonstrate our system's ability to\nreconstruct arbitrary clips from YouTube, showing both 1st and 3rd person\ninteractions.",
        "translated": ""
    },
    {
        "title": "ViHOPE: Visuotactile In-Hand Object 6D Pose Estimation with Shape\n  Completion",
        "url": "http://arxiv.org/abs/2309.05662v1",
        "pub_date": "2023-09-11",
        "summary": "In this letter, we introduce ViHOPE, a novel framework for estimating the 6D\npose of an in-hand object using visuotactile perception. Our key insight is\nthat the accuracy of the 6D object pose estimate can be improved by explicitly\ncompleting the shape of the object. To this end, we introduce a novel\nvisuotactile shape completion module that uses a conditional Generative\nAdversarial Network to complete the shape of an in-hand object based on\nvolumetric representation. This approach improves over prior works that\ndirectly regress visuotactile observations to a 6D pose. By explicitly\ncompleting the shape of the in-hand object and jointly optimizing the shape\ncompletion and pose estimation tasks, we improve the accuracy of the 6D object\npose estimate. We train and test our model on a synthetic dataset and compare\nit with the state-of-the-art. In the visuotactile shape completion task, we\noutperform the state-of-the-art by 265% using the Intersection of Union metric\nand achieve 88% lower Chamfer Distance. In the visuotactile pose estimation\ntask, we present results that suggest our framework reduces position and\nangular errors by 35% and 64%, respectively. Furthermore, we ablate our\nframework to confirm the gain on the 6D object pose estimate from explicitly\ncompleting the shape. Ultimately, we show that our framework produces models\nthat are robust to sim-to-real transfer on a real-world robot platform.",
        "translated": ""
    },
    {
        "title": "An Effective Two-stage Training Paradigm Detector for Small Dataset",
        "url": "http://arxiv.org/abs/2309.05652v1",
        "pub_date": "2023-09-11",
        "summary": "Learning from the limited amount of labeled data to the pre-train model has\nalways been viewed as a challenging task. In this report, an effective and\nrobust solution, the two-stage training paradigm YOLOv8 detector (TP-YOLOv8),\nis designed for the object detection track in VIPriors Challenge 2023. First,\nthe backbone of YOLOv8 is pre-trained as the encoder using the masked image\nmodeling technique. Then the detector is fine-tuned with elaborate\naugmentations. During the test stage, test-time augmentation (TTA) is used to\nenhance each model, and weighted box fusion (WBF) is implemented to further\nboost the performance. With the well-designed structure, our approach has\nachieved 30.4% average precision from 0.50 to 0.95 on the DelftBikes test set,\nranking 4th on the leaderboard.",
        "translated": ""
    },
    {
        "title": "CitDet: A Benchmark Dataset for Citrus Fruit Detection",
        "url": "http://arxiv.org/abs/2309.05645v1",
        "pub_date": "2023-09-11",
        "summary": "In this letter, we present a new dataset to advance the state of the art in\ndetecting citrus fruit and accurately estimate yield on trees affected by the\nHuanglongbing (HLB) disease in orchard environments via imaging. Despite the\nfact that significant progress has been made in solving the fruit detection\nproblem, the lack of publicly available datasets has complicated direct\ncomparison of results. For instance, citrus detection has long been of interest\nin the agricultural research community, yet there is an absence of work,\nparticularly involving public datasets of citrus affected by HLB. To address\nthis issue, we enhance state-of-the-art object detection methods for use in\ntypical orchard settings. Concretely, we provide high-resolution images of\ncitrus trees located in an area known to be highly affected by HLB, along with\nhigh-quality bounding box annotations of citrus fruit. Fruit on both the trees\nand the ground are labeled to allow for identification of fruit location, which\ncontributes to advancements in yield estimation and potential measure of HLB\nimpact via fruit drop. The dataset consists of over 32,000 bounding box\nannotations for fruit instances contained in 579 high-resolution images. In\nsummary, our contributions are the following: (i) we introduce a novel dataset\nalong with baseline performance benchmarks on multiple contemporary object\ndetection algorithms, (ii) we show the ability to accurately capture fruit\nlocation on tree or on ground, and finally (ii) we present a correlation of our\nresults with yield estimations.",
        "translated": ""
    },
    {
        "title": "Learning the Geodesic Embedding with Graph Neural Networks",
        "url": "http://arxiv.org/abs/2309.05613v1",
        "pub_date": "2023-09-11",
        "summary": "We present GeGnn, a learning-based method for computing the approximate\ngeodesic distance between two arbitrary points on discrete polyhedra surfaces\nwith constant time complexity after fast precomputation. Previous relevant\nmethods either focus on computing the geodesic distance between a single source\nand all destinations, which has linear complexity at least or require a long\nprecomputation time. Our key idea is to train a graph neural network to embed\nan input mesh into a high-dimensional embedding space and compute the geodesic\ndistance between a pair of points using the corresponding embedding vectors and\na lightweight decoding function. To facilitate the learning of the embedding,\nwe propose novel graph convolution and graph pooling modules that incorporate\nlocal geodesic information and are verified to be much more effective than\nprevious designs. After training, our method requires only one forward pass of\nthe network per mesh as precomputation. Then, we can compute the geodesic\ndistance between a pair of points using our decoding function, which requires\nonly several matrix multiplications and can be massively parallelized on GPUs.\nWe verify the efficiency and effectiveness of our method on ShapeNet and\ndemonstrate that our method is faster than existing methods by orders of\nmagnitude while achieving comparable or better accuracy. Additionally, our\nmethod exhibits robustness on noisy and incomplete meshes and strong\ngeneralization ability on out-of-distribution meshes. The code and pretrained\nmodel can be found on https://github.com/IntelligentGeometry/GeGnn.",
        "translated": ""
    },
    {
        "title": "Temporal Action Localization with Enhanced Instant Discriminability",
        "url": "http://arxiv.org/abs/2309.05590v1",
        "pub_date": "2023-09-11",
        "summary": "Temporal action detection (TAD) aims to detect all action boundaries and\ntheir corresponding categories in an untrimmed video. The unclear boundaries of\nactions in videos often result in imprecise predictions of action boundaries by\nexisting methods. To resolve this issue, we propose a one-stage framework named\nTriDet. First, we propose a Trident-head to model the action boundary via an\nestimated relative probability distribution around the boundary. Then, we\nanalyze the rank-loss problem (i.e. instant discriminability deterioration) in\ntransformer-based methods and propose an efficient scalable-granularity\nperception (SGP) layer to mitigate this issue. To further push the limit of\ninstant discriminability in the video backbone, we leverage the strong\nrepresentation capability of pretrained large models and investigate their\nperformance on TAD. Last, considering the adequate spatial-temporal context for\nclassification, we design a decoupled feature pyramid network with separate\nfeature pyramids to incorporate rich spatial context from the large model for\nlocalization. Experimental results demonstrate the robustness of TriDet and its\nstate-of-the-art performance on multiple TAD datasets, including hierarchical\n(multilabel) TAD datasets.",
        "translated": ""
    },
    {
        "title": "UniSeg: A Unified Multi-Modal LiDAR Segmentation Network and the\n  OpenPCSeg Codebase",
        "url": "http://arxiv.org/abs/2309.05573v1",
        "pub_date": "2023-09-11",
        "summary": "Point-, voxel-, and range-views are three representative forms of point\nclouds. All of them have accurate 3D measurements but lack color and texture\ninformation. RGB images are a natural complement to these point cloud views and\nfully utilizing the comprehensive information of them benefits more robust\nperceptions. In this paper, we present a unified multi-modal LiDAR segmentation\nnetwork, termed UniSeg, which leverages the information of RGB images and three\nviews of the point cloud, and accomplishes semantic segmentation and panoptic\nsegmentation simultaneously. Specifically, we first design the Learnable\ncross-Modal Association (LMA) module to automatically fuse voxel-view and\nrange-view features with image features, which fully utilize the rich semantic\ninformation of images and are robust to calibration errors. Then, the enhanced\nvoxel-view and range-view features are transformed to the point space,where\nthree views of point cloud features are further fused adaptively by the\nLearnable cross-View Association module (LVA). Notably, UniSeg achieves\npromising results in three public benchmarks, i.e., SemanticKITTI, nuScenes,\nand Waymo Open Dataset (WOD); it ranks 1st on two challenges of two benchmarks,\nincluding the LiDAR semantic segmentation challenge of nuScenes and panoptic\nsegmentation challenges of SemanticKITTI. Besides, we construct the OpenPCSeg\ncodebase, which is the largest and most comprehensive outdoor LiDAR\nsegmentation codebase. It contains most of the popular outdoor LiDAR\nsegmentation algorithms and provides reproducible implementations. The\nOpenPCSeg codebase will be made publicly available at\nhttps://github.com/PJLab-ADG/PCSeg.",
        "translated": ""
    },
    {
        "title": "ITI-GEN: Inclusive Text-to-Image Generation",
        "url": "http://arxiv.org/abs/2309.05569v1",
        "pub_date": "2023-09-11",
        "summary": "Text-to-image generative models often reflect the biases of the training\ndata, leading to unequal representations of underrepresented groups. This study\ninvestigates inclusive text-to-image generative models that generate images\nbased on human-written prompts and ensure the resulting images are uniformly\ndistributed across attributes of interest. Unfortunately, directly expressing\nthe desired attributes in the prompt often leads to sub-optimal results due to\nlinguistic ambiguity or model misrepresentation. Hence, this paper proposes a\ndrastically different approach that adheres to the maxim that \"a picture is\nworth a thousand words\". We show that, for some attributes, images can\nrepresent concepts more expressively than text. For instance, categories of\nskin tones are typically hard to specify by text but can be easily represented\nby example images. Building upon these insights, we propose a novel approach,\nITI-GEN, that leverages readily available reference images for Inclusive\nText-to-Image GENeration. The key idea is learning a set of prompt embeddings\nto generate images that can effectively represent all desired attribute\ncategories. More importantly, ITI-GEN requires no model fine-tuning, making it\ncomputationally efficient to augment existing text-to-image models. Extensive\nexperiments demonstrate that ITI-GEN largely improves over state-of-the-art\nmodels to generate inclusive images from a prompt. Project page:\nhttps://czhang0528.github.io/iti-gen.",
        "translated": ""
    },
    {
        "title": "OpenFashionCLIP: Vision-and-Language Contrastive Learning with\n  Open-Source Fashion Data",
        "url": "http://arxiv.org/abs/2309.05551v1",
        "pub_date": "2023-09-11",
        "summary": "The inexorable growth of online shopping and e-commerce demands scalable and\nrobust machine learning-based solutions to accommodate customer requirements.\nIn the context of automatic tagging classification and multimodal retrieval,\nprior works either defined a low generalizable supervised learning approach or\nmore reusable CLIP-based techniques while, however, training on closed source\ndata. In this work, we propose OpenFashionCLIP, a vision-and-language\ncontrastive learning method that only adopts open-source fashion data stemming\nfrom diverse domains, and characterized by varying degrees of specificity. Our\napproach is extensively validated across several tasks and benchmarks, and\nexperimental results highlight a significant out-of-domain generalization\ncapability and consistent improvements over state-of-the-art methods both in\nterms of accuracy and recall. Source code and trained models are publicly\navailable at: https://github.com/aimagelab/open-fashion-clip.",
        "translated": ""
    },
    {
        "title": "Learning Disentangled Avatars with Hybrid 3D Representations",
        "url": "http://arxiv.org/abs/2309.06441v1",
        "pub_date": "2023-09-12",
        "summary": "Tremendous efforts have been made to learn animatable and photorealistic\nhuman avatars. Towards this end, both explicit and implicit 3D representations\nare heavily studied for a holistic modeling and capture of the whole human\n(e.g., body, clothing, face and hair), but neither representation is an optimal\nchoice in terms of representation efficacy since different parts of the human\navatar have different modeling desiderata. For example, meshes are generally\nnot suitable for modeling clothing and hair. Motivated by this, we present\nDisentangled Avatars~(DELTA), which models humans with hybrid explicit-implicit\n3D representations. DELTA takes a monocular RGB video as input, and produces a\nhuman avatar with separate body and clothing/hair layers. Specifically, we\ndemonstrate two important applications for DELTA. For the first one, we\nconsider the disentanglement of the human body and clothing and in the second,\nwe disentangle the face and hair. To do so, DELTA represents the body or face\nwith an explicit mesh-based parametric 3D model and the clothing or hair with\nan implicit neural radiance field. To make this possible, we design an\nend-to-end differentiable renderer that integrates meshes into volumetric\nrendering, enabling DELTA to learn directly from monocular videos without any\n3D supervision. Finally, we show that how these two applications can be easily\ncombined to model full-body avatars, such that the hair, face, body and\nclothing can be fully disentangled yet jointly rendered. Such a disentanglement\nenables hair and clothing transfer to arbitrary body shapes. We empirically\nvalidate the effectiveness of DELTA's disentanglement by demonstrating its\npromising performance on disentangled reconstruction, virtual clothing try-on\nand hairstyle transfer. To facilitate future research, we also release an\nopen-sourced pipeline for the study of hybrid human avatar modeling.",
        "translated": ""
    },
    {
        "title": "LEAP Hand: Low-Cost, Efficient, and Anthropomorphic Hand for Robot\n  Learning",
        "url": "http://arxiv.org/abs/2309.06440v1",
        "pub_date": "2023-09-12",
        "summary": "Dexterous manipulation has been a long-standing challenge in robotics. While\nmachine learning techniques have shown some promise, results have largely been\ncurrently limited to simulation. This can be mostly attributed to the lack of\nsuitable hardware. In this paper, we present LEAP Hand, a low-cost dexterous\nand anthropomorphic hand for machine learning research. In contrast to previous\nhands, LEAP Hand has a novel kinematic structure that allows maximal dexterity\nregardless of finger pose. LEAP Hand is low-cost and can be assembled in 4\nhours at a cost of 2000 USD from readily available parts. It is capable of\nconsistently exerting large torques over long durations of time. We show that\nLEAP Hand can be used to perform several manipulation tasks in the real world\n-- from visual teleoperation to learning from passive video data and sim2real.\nLEAP Hand significantly outperforms its closest competitor Allegro Hand in all\nour experiments while being 1/8th of the cost. We release detailed assembly\ninstructions, the Sim2Real pipeline and a development platform with useful APIs\non our website at https://leap-hand.github.io/",
        "translated": ""
    },
    {
        "title": "Attention De-sparsification Matters: Inducing Diversity in Digital\n  Pathology Representation Learning",
        "url": "http://arxiv.org/abs/2309.06439v1",
        "pub_date": "2023-09-12",
        "summary": "We propose DiRL, a Diversity-inducing Representation Learning technique for\nhistopathology imaging. Self-supervised learning techniques, such as\ncontrastive and non-contrastive approaches, have been shown to learn rich and\neffective representations of digitized tissue samples with limited pathologist\nsupervision. Our analysis of vanilla SSL-pretrained models' attention\ndistribution reveals an insightful observation: sparsity in attention, i.e,\nmodels tends to localize most of their attention to some prominent patterns in\nthe image. Although attention sparsity can be beneficial in natural images due\nto these prominent patterns being the object of interest itself, this can be\nsub-optimal in digital pathology; this is because, unlike natural images,\ndigital pathology scans are not object-centric, but rather a complex phenotype\nof various spatially intermixed biological components. Inadequate\ndiversification of attention in these complex images could result in crucial\ninformation loss. To address this, we leverage cell segmentation to densely\nextract multiple histopathology-specific representations, and then propose a\nprior-guided dense pretext task for SSL, designed to match the multiple\ncorresponding representations between the views. Through this, the model learns\nto attend to various components more closely and evenly, thus inducing adequate\ndiversification in attention for capturing context rich representations.\nThrough quantitative and qualitative analysis on multiple tasks across cancer\ntypes, we demonstrate the efficacy of our method and observe that the attention\nis more globally distributed.",
        "translated": ""
    },
    {
        "title": "Exploring Non-additive Randomness on ViT against Query-Based Black-Box\n  Attacks",
        "url": "http://arxiv.org/abs/2309.06438v1",
        "pub_date": "2023-09-12",
        "summary": "Deep Neural Networks can be easily fooled by small and imperceptible\nperturbations. The query-based black-box attack (QBBA) is able to create the\nperturbations using model output probabilities of image queries requiring no\naccess to the underlying models. QBBA poses realistic threats to real-world\napplications. Recently, various types of robustness have been explored to\ndefend against QBBA. In this work, we first taxonomize the stochastic defense\nstrategies against QBBA. Following our taxonomy, we propose to explore\nnon-additive randomness in models to defend against QBBA. Specifically, we\nfocus on underexplored Vision Transformers based on their flexible\narchitectures. Extensive experiments show that the proposed defense approach\nachieves effective defense, without much sacrifice in performance.",
        "translated": ""
    },
    {
        "title": "AGMDT: Virtual Staining of Renal Histology Images with Adjacency-Guided\n  Multi-Domain Transfer",
        "url": "http://arxiv.org/abs/2309.06421v1",
        "pub_date": "2023-09-12",
        "summary": "Renal pathology, as the gold standard of kidney disease diagnosis, requires\ndoctors to analyze a serial of tissue slices stained by H\\&amp;E staining and\nspecial staining like Masson, PASM, and PAS, respectively. These special\nstaining methods are costly, time-consuming, and hard to standardize for wide\nuse especially in primary hospitals. Advances of supervised learning methods\ncan virtually convert H\\&amp;E images into special staining images, but the\npixel-to-pixel alignment is hard to achieve for training. As contrast,\nunsupervised learning methods regarding different stains as different style\ntransferring domains can use unpaired data, but they ignore the spatial\ninter-domain correlations and thus decrease the trustworthiness of structural\ndetails for diagnosis. In this paper, we propose a novel virtual staining\nframework AGMDT to translate images into other domains by avoiding pixel-level\nalignment and meanwhile utilizing the correlations among adjacent tissue\nslices. We first build a high-quality multi-domain renal histological dataset\nwhere each specimen case comprises a series of slices stained in various ways.\nBased on it, the proposed framework AGMDT discovers patch-level aligned pairs\nacross the serial slices of multi-domains through glomerulus detection and\nbipartite graph matching, and utilizes such correlations to supervise the\nend-to-end model for multi-domain staining transformation. Experimental results\nshow that the proposed AGMDT achieves a good balance between the precise\npixel-level alignment and unpaired domain transfer by exploiting correlations\nacross multi-domain serial pathological slices, and outperforms the\nstate-of-the-art methods in both quantitative measure and morphological\ndetails.",
        "translated": ""
    },
    {
        "title": "InstaFlow: One Step is Enough for High-Quality Diffusion-Based\n  Text-to-Image Generation",
        "url": "http://arxiv.org/abs/2309.06380v1",
        "pub_date": "2023-09-12",
        "summary": "Diffusion models have revolutionized text-to-image generation with its\nexceptional quality and creativity. However, its multi-step sampling process is\nknown to be slow, often requiring tens of inference steps to obtain\nsatisfactory results. Previous attempts to improve its sampling speed and\nreduce computational costs through distillation have been unsuccessful in\nachieving a functional one-step model. In this paper, we explore a recent\nmethod called Rectified Flow, which, thus far, has only been applied to small\ndatasets. The core of Rectified Flow lies in its \\emph{reflow} procedure, which\nstraightens the trajectories of probability flows, refines the coupling between\nnoises and images, and facilitates the distillation process with student\nmodels. We propose a novel text-conditioned pipeline to turn Stable Diffusion\n(SD) into an ultra-fast one-step model, in which we find reflow plays a\ncritical role in improving the assignment between noise and images. Leveraging\nour new pipeline, we create, to the best of our knowledge, the first one-step\ndiffusion-based text-to-image generator with SD-level image quality, achieving\nan FID (Frechet Inception Distance) of $23.3$ on MS COCO 2017-5k, surpassing\nthe previous state-of-the-art technique, progressive distillation, by a\nsignificant margin ($37.2$ $\\rightarrow$ $23.3$ in FID). By utilizing an\nexpanded network with 1.7B parameters, we further improve the FID to $22.4$. We\ncall our one-step models \\emph{InstaFlow}. On MS COCO 2014-30k, InstaFlow\nyields an FID of $13.1$ in just $0.09$ second, the best in $\\leq 0.1$ second\nregime, outperforming the recent StyleGAN-T ($13.9$ in $0.1$ second). Notably,\nthe training of InstaFlow only costs 199 A100 GPU days. Project\npage:~\\url{https://github.com/gnobitab/InstaFlow}.",
        "translated": ""
    },
    {
        "title": "Padding-free Convolution based on Preservation of Differential\n  Characteristics of Kernels",
        "url": "http://arxiv.org/abs/2309.06370v1",
        "pub_date": "2023-09-12",
        "summary": "Convolution is a fundamental operation in image processing and machine\nlearning. Aimed primarily at maintaining image size, padding is a key\ningredient of convolution, which, however, can introduce undesirable boundary\neffects. We present a non-padding-based method for size-keeping convolution\nbased on the preservation of differential characteristics of kernels. The main\nidea is to make convolution over an incomplete sliding window \"collapse\" to a\nlinear differential operator evaluated locally at its central pixel, which no\nlonger requires information from the neighbouring missing pixels. While the\nunderlying theory is rigorous, our final formula turns out to be simple: the\nconvolution over an incomplete window is achieved by convolving its nearest\ncomplete window with a transformed kernel. This formula is computationally\nlightweight, involving neither interpolation or extrapolation nor restrictions\non image and kernel sizes. Our method favours data with smooth boundaries, such\nas high-resolution images and fields from physics. Our experiments include: i)\nfiltering analytical and non-analytical fields from computational physics and,\nii) training convolutional neural networks (CNNs) for the tasks of image\nclassification, semantic segmentation and super-resolution reconstruction. In\nall these experiments, our method has exhibited visible superiority over the\ncompared ones.",
        "translated": ""
    },
    {
        "title": "Exploring Flat Minima for Domain Generalization with Large Learning\n  Rates",
        "url": "http://arxiv.org/abs/2309.06337v1",
        "pub_date": "2023-09-12",
        "summary": "Domain Generalization (DG) aims to generalize to arbitrary unseen domains. A\npromising approach to improve model generalization in DG is the identification\nof flat minima. One typical method for this task is SWAD, which involves\naveraging weights along the training trajectory. However, the success of weight\naveraging depends on the diversity of weights, which is limited when training\nwith a small learning rate. Instead, we observe that leveraging a large\nlearning rate can simultaneously promote weight diversity and facilitate the\nidentification of flat regions in the loss landscape. However, employing a\nlarge learning rate suffers from the convergence problem, which cannot be\nresolved by simply averaging the training weights. To address this issue, we\nintroduce a training strategy called Lookahead which involves the weight\ninterpolation, instead of average, between fast and slow weights. The fast\nweight explores the weight space with a large learning rate, which is not\nconverged while the slow weight interpolates with it to ensure the convergence.\nBesides, weight interpolation also helps identify flat minima by implicitly\noptimizing the local entropy loss that measures flatness. To further prevent\noverfitting during training, we propose two variants to regularize the training\nweight with weighted averaged weight or with accumulated history weight. Taking\nadvantage of this new perspective, our methods achieve state-of-the-art\nperformance on both classification and semantic segmentation domain\ngeneralization benchmarks. The code is available at\nhttps://github.com/koncle/DG-with-Large-LR.",
        "translated": ""
    },
    {
        "title": "Grounded Language Acquisition From Object and Action Imagery",
        "url": "http://arxiv.org/abs/2309.06335v1",
        "pub_date": "2023-09-12",
        "summary": "Deep learning approaches to natural language processing have made great\nstrides in recent years. While these models produce symbols that convey vast\namounts of diverse knowledge, it is unclear how such symbols are grounded in\ndata from the world. In this paper, we explore the development of a private\nlanguage for visual data representation by training emergent language (EL)\nencoders/decoders in both i) a traditional referential game environment and ii)\na contrastive learning environment utilizing a within-class matching training\nparadigm. An additional classification layer utilizing neural machine\ntranslation and random forest classification was used to transform symbolic\nrepresentations (sequences of integer symbols) to class labels. These methods\nwere applied in two experiments focusing on object recognition and action\nrecognition. For object recognition, a set of sketches produced by human\nparticipants from real imagery was used (Sketchy dataset) and for action\nrecognition, 2D trajectories were generated from 3D motion capture systems\n(MOVI dataset). In order to interpret the symbols produced for data in each\nexperiment, gradient-weighted class activation mapping (Grad-CAM) methods were\nused to identify pixel regions indicating semantic features which contribute\nevidence towards symbols in learned languages. Additionally, a t-distributed\nstochastic neighbor embedding (t-SNE) method was used to investigate embeddings\nlearned by CNN feature extractors.",
        "translated": ""
    },
    {
        "title": "SAMPLING: Scene-adaptive Hierarchical Multiplane Images Representation\n  for Novel View Synthesis from a Single Image",
        "url": "http://arxiv.org/abs/2309.06323v2",
        "pub_date": "2023-09-12",
        "summary": "Recent novel view synthesis methods obtain promising results for relatively\nsmall scenes, e.g., indoor environments and scenes with a few objects, but tend\nto fail for unbounded outdoor scenes with a single image as input. In this\npaper, we introduce SAMPLING, a Scene-adaptive Hierarchical Multiplane Images\nRepresentation for Novel View Synthesis from a Single Image based on improved\nmultiplane images (MPI). Observing that depth distribution varies significantly\nfor unbounded outdoor scenes, we employ an adaptive-bins strategy for MPI to\narrange planes in accordance with each scene image. To represent intricate\ngeometry and multi-scale details, we further introduce a hierarchical\nrefinement branch, which results in high-quality synthesized novel views. Our\nmethod demonstrates considerable performance gains in synthesizing large-scale\nunbounded outdoor scenes using a single image on the KITTI dataset and\ngeneralizes well to the unseen Tanks and Temples dataset.The code and models\nwill soon be made available.",
        "translated": ""
    },
    {
        "title": "Text-Guided Generation and Editing of Compositional 3D Avatars",
        "url": "http://arxiv.org/abs/2309.07125v1",
        "pub_date": "2023-09-13",
        "summary": "Our goal is to create a realistic 3D facial avatar with hair and accessories\nusing only a text description. While this challenge has attracted significant\nrecent interest, existing methods either lack realism, produce unrealistic\nshapes, or do not support editing, such as modifications to the hairstyle. We\nargue that existing methods are limited because they employ a monolithic\nmodeling approach, using a single representation for the head, face, hair, and\naccessories. Our observation is that the hair and face, for example, have very\ndifferent structural qualities that benefit from different representations.\nBuilding on this insight, we generate avatars with a compositional model, in\nwhich the head, face, and upper body are represented with traditional 3D\nmeshes, and the hair, clothing, and accessories with neural radiance fields\n(NeRF). The model-based mesh representation provides a strong geometric prior\nfor the face region, improving realism while enabling editing of the person's\nappearance. By using NeRFs to represent the remaining components, our method is\nable to model and synthesize parts with complex geometry and appearance, such\nas curly hair and fluffy scarves. Our novel system synthesizes these\nhigh-quality compositional avatars from text descriptions. The experimental\nresults demonstrate that our method, Text-guided generation and Editing of\nCompositional Avatars (TECA), produces avatars that are more realistic than\nthose of recent methods while being editable because of their compositional\nnature. For example, our TECA enables the seamless transfer of compositional\nfeatures like hairstyles, scarves, and other accessories between avatars. This\ncapability supports applications such as virtual try-on.",
        "translated": ""
    },
    {
        "title": "Tree-Structured Shading Decomposition",
        "url": "http://arxiv.org/abs/2309.07122v1",
        "pub_date": "2023-09-13",
        "summary": "We study inferring a tree-structured representation from a single image for\nobject shading. Prior work typically uses the parametric or measured\nrepresentation to model shading, which is neither interpretable nor easily\neditable. We propose using the shade tree representation, which combines basic\nshading nodes and compositing methods to factorize object surface shading. The\nshade tree representation enables novice users who are unfamiliar with the\nphysical shading process to edit object shading in an efficient and intuitive\nmanner. A main challenge in inferring the shade tree is that the inference\nproblem involves both the discrete tree structure and the continuous parameters\nof the tree nodes. We propose a hybrid approach to address this issue. We\nintroduce an auto-regressive inference model to generate a rough estimation of\nthe tree structure and node parameters, and then we fine-tune the inferred\nshade tree through an optimization algorithm. We show experiments on synthetic\nimages, captured reflectance, real images, and non-realistic vector drawings,\nallowing downstream applications such as material editing, vectorized shading,\nand relighting. Project website: https://chen-geng.com/inv-shade-trees",
        "translated": ""
    },
    {
        "title": "Sight Beyond Text: Multi-Modal Training Enhances LLMs in Truthfulness\n  and Ethics",
        "url": "http://arxiv.org/abs/2309.07120v1",
        "pub_date": "2023-09-13",
        "summary": "Multi-modal large language models (MLLMs) are trained based on large language\nmodels (LLM), with an enhanced capability to comprehend multi-modal inputs and\ngenerate textual responses. While they excel in multi-modal tasks, the pure NLP\nabilities of MLLMs are often underestimated and left untested. In this study,\nwe get out of the box and unveil an intriguing characteristic of MLLMs -- our\npreliminary results suggest that visual instruction tuning, a prevailing\nstrategy for transitioning LLMs into MLLMs, unexpectedly and interestingly\nhelps models attain both improved truthfulness and ethical alignment in the\npure NLP context. For example, a visual-instruction-tuned LLaMA2 7B model\nsurpasses the performance of the LLaMA2-chat 7B model, fine-tuned with over one\nmillion human annotations, on TruthfulQA-mc and Ethics benchmarks. Further\nanalysis reveals that the improved alignment can be attributed to the superior\ninstruction quality inherent to visual-text data. In releasing our code at\ngithub.com/UCSC-VLAA/Sight-Beyond-Text, we aspire to foster further exploration\ninto the intrinsic value of visual-text synergies and, in a broader scope,\nmulti-modal interactions in alignment research.",
        "translated": ""
    },
    {
        "title": "PILOT: A Pre-Trained Model-Based Continual Learning Toolbox",
        "url": "http://arxiv.org/abs/2309.07117v1",
        "pub_date": "2023-09-13",
        "summary": "While traditional machine learning can effectively tackle a wide range of\nproblems, it primarily operates within a closed-world setting, which presents\nlimitations when dealing with streaming data. As a solution, incremental\nlearning emerges to address real-world scenarios involving new data's arrival.\nRecently, pre-training has made significant advancements and garnered the\nattention of numerous researchers. The strong performance of these pre-trained\nmodels (PTMs) presents a promising avenue for developing continual learning\nalgorithms that can effectively adapt to real-world scenarios. Consequently,\nexploring the utilization of PTMs in incremental learning has become essential.\nThis paper introduces a pre-trained model-based continual learning toolbox\nknown as PILOT. On the one hand, PILOT implements some state-of-the-art\nclass-incremental learning algorithms based on pre-trained models, such as L2P,\nDualPrompt, and CODA-Prompt. On the other hand, PILOT also fits typical\nclass-incremental learning algorithms (e.g., DER, FOSTER, and MEMO) within the\ncontext of pre-trained models to evaluate their effectiveness.",
        "translated": ""
    },
    {
        "title": "Weakly-Supervised Multi-Task Learning for Audio-Visual Speaker\n  Verification",
        "url": "http://arxiv.org/abs/2309.07115v1",
        "pub_date": "2023-09-13",
        "summary": "In this paper, we present a methodology for achieving robust multimodal\nperson representations optimized for open-set audio-visual speaker\nverification. Distance Metric Learning (DML) approaches have typically\ndominated this problem space, owing to strong performance on new and unseen\nclasses. In our work, we explored multitask learning techniques to further\nboost performance of the DML approach and show that an auxiliary task with weak\nlabels can increase the compactness of the learned speaker representation. We\nalso extend the Generalized end-to-end loss (GE2E) to multimodal inputs and\ndemonstrate that it can achieve competitive performance in an audio-visual\nspace. Finally, we introduce a non-synchronous audio-visual sampling random\nstrategy during training time that has shown to improve generalization. Our\nnetwork achieves state of the art performance for speaker verification,\nreporting 0.244%, 0.252%, 0.441% Equal Error Rate (EER) on the three official\ntrial lists of VoxCeleb1-O/E/H, which is to our knowledge, the best published\nresults on VoxCeleb1-E and VoxCeleb1-H.",
        "translated": ""
    },
    {
        "title": "Contrastive Deep Encoding Enables Uncertainty-aware\n  Machine-learning-assisted Histopathology",
        "url": "http://arxiv.org/abs/2309.07113v1",
        "pub_date": "2023-09-13",
        "summary": "Deep neural network models can learn clinically relevant features from\nmillions of histopathology images. However generating high-quality annotations\nto train such models for each hospital, each cancer type, and each diagnostic\ntask is prohibitively laborious. On the other hand, terabytes of training data\n-- while lacking reliable annotations -- are readily available in the public\ndomain in some cases. In this work, we explore how these large datasets can be\nconsciously utilized to pre-train deep networks to encode informative\nrepresentations. We then fine-tune our pre-trained models on a fraction of\nannotated training data to perform specific downstream tasks. We show that our\napproach can reach the state-of-the-art (SOTA) for patch-level classification\nwith only 1-10% randomly selected annotations compared to other SOTA\napproaches. Moreover, we propose an uncertainty-aware loss function, to\nquantify the model confidence during inference. Quantified uncertainty helps\nexperts select the best instances to label for further training. Our\nuncertainty-aware labeling reaches the SOTA with significantly fewer\nannotations compared to random labeling. Last, we demonstrate how our\npre-trained encoders can surpass current SOTA for whole-slide image\nclassification with weak supervision. Our work lays the foundation for data and\ntask-agnostic pre-trained deep networks with quantified uncertainty.",
        "translated": ""
    },
    {
        "title": "Hardening RGB-D Object Recognition Systems against Adversarial Patch\n  Attacks",
        "url": "http://arxiv.org/abs/2309.07106v1",
        "pub_date": "2023-09-13",
        "summary": "RGB-D object recognition systems improve their predictive performances by\nfusing color and depth information, outperforming neural network architectures\nthat rely solely on colors. While RGB-D systems are expected to be more robust\nto adversarial examples than RGB-only systems, they have also been proven to be\nhighly vulnerable. Their robustness is similar even when the adversarial\nexamples are generated by altering only the original images' colors. Different\nworks highlighted the vulnerability of RGB-D systems; however, there is a\nlacking of technical explanations for this weakness. Hence, in our work, we\nbridge this gap by investigating the learned deep representation of RGB-D\nsystems, discovering that color features make the function learned by the\nnetwork more complex and, thus, more sensitive to small perturbations. To\nmitigate this problem, we propose a defense based on a detection mechanism that\nmakes RGB-D systems more robust against adversarial examples. We empirically\nshow that this defense improves the performances of RGB-D systems against\nadversarial examples even when they are computed ad-hoc to circumvent this\ndetection mechanism, and that is also more effective than adversarial training.",
        "translated": ""
    },
    {
        "title": "Polygon Intersection-over-Union Loss for Viewpoint-Agnostic Monocular 3D\n  Vehicle Detection",
        "url": "http://arxiv.org/abs/2309.07104v1",
        "pub_date": "2023-09-13",
        "summary": "Monocular 3D object detection is a challenging task because depth information\nis difficult to obtain from 2D images. A subset of viewpoint-agnostic monocular\n3D detection methods also do not explicitly leverage scene homography or\ngeometry during training, meaning that a model trained thusly can detect\nobjects in images from arbitrary viewpoints. Such works predict the projections\nof the 3D bounding boxes on the image plane to estimate the location of the 3D\nboxes, but these projections are not rectangular so the calculation of IoU\nbetween these projected polygons is not straightforward. This work proposes an\nefficient, fully differentiable algorithm for the calculation of IoU between\ntwo convex polygons, which can be utilized to compute the IoU between two 3D\nbounding box footprints viewed from an arbitrary angle. We test the performance\nof the proposed polygon IoU loss (PIoU loss) on three state-of-the-art\nviewpoint-agnostic 3D detection models. Experiments demonstrate that the\nproposed PIoU loss converges faster than L1 loss and that in 3D detection\nmodels, a combination of PIoU loss and L1 loss gives better results than L1\nloss alone (+1.64% AP70 for MonoCon on cars, +0.18% AP70 for RTM3D on cars, and\n+0.83%/+2.46% AP50/AP25 for MonoRCNN on cyclists).",
        "translated": ""
    },
    {
        "title": "RadarLCD: Learnable Radar-based Loop Closure Detection Pipeline",
        "url": "http://arxiv.org/abs/2309.07094v1",
        "pub_date": "2023-09-13",
        "summary": "Loop Closure Detection (LCD) is an essential task in robotics and computer\nvision, serving as a fundamental component for various applications across\ndiverse domains. These applications encompass object recognition, image\nretrieval, and video analysis. LCD consists in identifying whether a robot has\nreturned to a previously visited location, referred to as a loop, and then\nestimating the related roto-translation with respect to the analyzed location.\nDespite the numerous advantages of radar sensors, such as their ability to\noperate under diverse weather conditions and provide a wider range of view\ncompared to other commonly used sensors (e.g., cameras or LiDARs), integrating\nradar data remains an arduous task due to intrinsic noise and distortion. To\naddress this challenge, this research introduces RadarLCD, a novel supervised\ndeep learning pipeline specifically designed for Loop Closure Detection using\nthe FMCW Radar (Frequency Modulated Continuous Wave) sensor. RadarLCD, a\nlearning-based LCD methodology explicitly designed for radar systems, makes a\nsignificant contribution by leveraging the pre-trained HERO (Hybrid Estimation\nRadar Odometry) model. Being originally developed for radar odometry, HERO's\nfeatures are used to select key points crucial for LCD tasks. The methodology\nundergoes evaluation across a variety of FMCW Radar dataset scenes, and it is\ncompared to state-of-the-art systems such as Scan Context for Place Recognition\nand ICP for Loop Closure. The results demonstrate that RadarLCD surpasses the\nalternatives in multiple aspects of Loop Closure Detection.",
        "translated": ""
    },
    {
        "title": "Developing a Novel Image Marker to Predict the Responses of Neoadjuvant\n  Chemotherapy (NACT) for Ovarian Cancer Patients",
        "url": "http://arxiv.org/abs/2309.07087v1",
        "pub_date": "2023-09-13",
        "summary": "Objective: Neoadjuvant chemotherapy (NACT) is one kind of treatment for\nadvanced stage ovarian cancer patients. However, due to the nature of tumor\nheterogeneity, the patients' responses to NACT varies significantly among\ndifferent subgroups. To address this clinical challenge, the purpose of this\nstudy is to develop a novel image marker to achieve high accuracy response\nprediction of the NACT at an early stage. Methods: For this purpose, we first\ncomputed a total of 1373 radiomics features to quantify the tumor\ncharacteristics, which can be grouped into three categories: geometric,\nintensity, and texture features. Second, all these features were optimized by\nprincipal component analysis algorithm to generate a compact and informative\nfeature cluster. Using this cluster as the input, an SVM based classifier was\ndeveloped and optimized to create a final marker, indicating the likelihood of\nthe patient being responsive to the NACT treatment. To validate this scheme, a\ntotal of 42 ovarian cancer patients were retrospectively collected. A nested\nleave-one-out cross-validation was adopted for model performance assessment.\nResults: The results demonstrate that the new method yielded an AUC (area under\nthe ROC [receiver characteristic operation] curve) of 0.745. Meanwhile, the\nmodel achieved overall accuracy of 76.2%, positive predictive value of 70%, and\nnegative predictive value of 78.1%. Conclusion: This study provides meaningful\ninformation for the development of radiomics based image markers in NACT\nresponse prediction.",
        "translated": ""
    },
    {
        "title": "Large-Vocabulary 3D Diffusion Model with Transformer",
        "url": "http://arxiv.org/abs/2309.07920v1",
        "pub_date": "2023-09-14",
        "summary": "Creating diverse and high-quality 3D assets with an automatic generative\nmodel is highly desirable. Despite extensive efforts on 3D generation, most\nexisting works focus on the generation of a single category or a few\ncategories. In this paper, we introduce a diffusion-based feed-forward\nframework for synthesizing massive categories of real-world 3D objects with a\nsingle generative model. Notably, there are three major challenges for this\nlarge-vocabulary 3D generation: a) the need for expressive yet efficient 3D\nrepresentation; b) large diversity in geometry and texture across categories;\nc) complexity in the appearances of real-world objects. To this end, we propose\na novel triplane-based 3D-aware Diffusion model with TransFormer, DiffTF, for\nhandling challenges via three aspects. 1) Considering efficiency and\nrobustness, we adopt a revised triplane representation and improve the fitting\nspeed and accuracy. 2) To handle the drastic variations in geometry and\ntexture, we regard the features of all 3D objects as a combination of\ngeneralized 3D knowledge and specialized 3D features. To extract generalized 3D\nknowledge from diverse categories, we propose a novel 3D-aware transformer with\nshared cross-plane attention. It learns the cross-plane relations across\ndifferent planes and aggregates the generalized 3D knowledge with specialized\n3D features. 3) In addition, we devise the 3D-aware encoder/decoder to enhance\nthe generalized 3D knowledge in the encoded triplanes for handling categories\nwith complex appearances. Extensive experiments on ShapeNet and OmniObject3D\n(over 200 diverse real-world categories) convincingly demonstrate that a single\nDiffTF model achieves state-of-the-art large-vocabulary 3D object generation\nperformance with large diversity, rich semantics, and high quality.",
        "translated": ""
    },
    {
        "title": "OpenIllumination: A Multi-Illumination Dataset for Inverse Rendering\n  Evaluation on Real Objects",
        "url": "http://arxiv.org/abs/2309.07921v1",
        "pub_date": "2023-09-14",
        "summary": "We introduce OpenIllumination, a real-world dataset containing over 108K\nimages of 64 objects with diverse materials, captured under 72 camera views and\na large number of different illuminations. For each image in the dataset, we\nprovide accurate camera parameters, illumination ground truth, and foreground\nsegmentation masks. Our dataset enables the quantitative evaluation of most\ninverse rendering and material decomposition methods for real objects. We\nexamine several state-of-the-art inverse rendering methods on our dataset and\ncompare their performances. The dataset and code can be found on the project\npage: https://oppo-us-research.github.io/OpenIllumination.",
        "translated": ""
    },
    {
        "title": "Unified Human-Scene Interaction via Prompted Chain-of-Contacts",
        "url": "http://arxiv.org/abs/2309.07918v1",
        "pub_date": "2023-09-14",
        "summary": "Human-Scene Interaction (HSI) is a vital component of fields like embodied AI\nand virtual reality. Despite advancements in motion quality and physical\nplausibility, two pivotal factors, versatile interaction control and the\ndevelopment of a user-friendly interface, require further exploration before\nthe practical application of HSI. This paper presents a unified HSI framework,\nUniHSI, which supports unified control of diverse interactions through language\ncommands. This framework is built upon the definition of interaction as Chain\nof Contacts (CoC): steps of human joint-object part pairs, which is inspired by\nthe strong correlation between interaction types and human-object contact\nregions. Based on the definition, UniHSI constitutes a Large Language Model\n(LLM) Planner to translate language prompts into task plans in the form of CoC,\nand a Unified Controller that turns CoC into uniform task execution. To\nfacilitate training and evaluation, we collect a new dataset named ScenePlan\nthat encompasses thousands of task plans generated by LLMs based on diverse\nscenarios. Comprehensive experiments demonstrate the effectiveness of our\nframework in versatile task execution and generalizability to real scanned\nscenes. The project page is at https://github.com/OpenRobotLab/UniHSI .",
        "translated": ""
    },
    {
        "title": "Looking at words and points with attention: a benchmark for\n  text-to-shape coherence",
        "url": "http://arxiv.org/abs/2309.07917v1",
        "pub_date": "2023-09-14",
        "summary": "While text-conditional 3D object generation and manipulation have seen rapid\nprogress, the evaluation of coherence between generated 3D shapes and input\ntextual descriptions lacks a clear benchmark. The reason is twofold: a) the low\nquality of the textual descriptions in the only publicly available dataset of\ntext-shape pairs; b) the limited effectiveness of the metrics used to\nquantitatively assess such coherence. In this paper, we propose a comprehensive\nsolution that addresses both weaknesses. Firstly, we employ large language\nmodels to automatically refine textual descriptions associated with shapes.\nSecondly, we propose a quantitative metric to assess text-to-shape coherence,\nthrough cross-attention mechanisms. To validate our approach, we conduct a user\nstudy and compare quantitatively our metric with existing ones. The refined\ndataset, the new metric and a set of text-shape pairs validated by the user\nstudy comprise a novel, fine-grained benchmark that we publicly release to\nfoster research on text-to-shape coherence of text-conditioned 3D generative\nmodels. Benchmark available at\nhttps://cvlab-unibo.github.io/CrossCoherence-Web/.",
        "translated": ""
    },
    {
        "title": "MMICL: Empowering Vision-language Model with Multi-Modal In-Context\n  Learning",
        "url": "http://arxiv.org/abs/2309.07915v1",
        "pub_date": "2023-09-14",
        "summary": "Starting from the resurgence of deep learning, vision-language models (VLMs)\nbenefiting from large language models (LLMs) have never been so popular.\nHowever, while LLMs can utilize extensive background knowledge and task\ninformation with in-context learning, most VLMs still struggle with\nunderstanding complex multi-modal prompts with multiple images. The issue can\ntraced back to the architectural design of VLMs or pre-training data.\nSpecifically, the current VLMs primarily emphasize utilizing multi-modal data\nwith a single image some, rather than multi-modal prompts with interleaved\nmultiple images and text. Even though some newly proposed VLMs could handle\nuser prompts with multiple images, pre-training data does not provide more\nsophisticated multi-modal prompts than interleaved image and text crawled from\nthe web. We propose MMICL to address the issue by considering both the model\nand data perspectives. We introduce a well-designed architecture capable of\nseamlessly integrating visual and textual context in an interleaved manner and\nMIC dataset to reduce the gap between the training data and the complex user\nprompts in real-world applications, including: 1) multi-modal context with\ninterleaved images and text, 2) textual references for each image, and 3)\nmulti-image data with spatial, logical, or temporal relationships. Our\nexperiments confirm that MMICL achieves new stat-of-the-art zero-shot and\nfew-shot performance on a wide range of general vision-language tasks,\nespecially for complex reasoning benchmarks including MME and MMBench. Our\nanalysis demonstrates that MMICL effectively deals with the challenge of\ncomplex multi-modal prompt understanding. The experiments on ScienceQA-IMG also\nshow that MMICL successfully alleviates the issue of language bias in VLMs,\nwhich we believe is the reason behind the advanced performance of MMICL.",
        "translated": ""
    },
    {
        "title": "ALWOD: Active Learning for Weakly-Supervised Object Detection",
        "url": "http://arxiv.org/abs/2309.07914v1",
        "pub_date": "2023-09-14",
        "summary": "Object detection (OD), a crucial vision task, remains challenged by the lack\nof large training datasets with precise object localization labels. In this\nwork, we propose ALWOD, a new framework that addresses this problem by fusing\nactive learning (AL) with weakly and semi-supervised object detection\nparadigms. Because the performance of AL critically depends on the model\ninitialization, we propose a new auxiliary image generator strategy that\nutilizes an extremely small labeled set, coupled with a large weakly tagged set\nof images, as a warm-start for AL. We then propose a new AL acquisition\nfunction, another critical factor in AL success, that leverages the\nstudent-teacher OD pair disagreement and uncertainty to effectively propose the\nmost informative images to annotate. Finally, to complete the AL loop, we\nintroduce a new labeling task delegated to human annotators, based on selection\nand correction of model-proposed detections, which is both rapid and effective\nin labeling the informative images. We demonstrate, across several challenging\nbenchmarks, that ALWOD significantly narrows the gap between the ODs trained on\nfew partially labeled but strategically selected image instances and those that\nrely on the fully-labeled data. Our code is publicly available on\nhttps://github.com/seqam-lab/ALWOD.",
        "translated": ""
    },
    {
        "title": "Disentangling Spatial and Temporal Learning for Efficient Image-to-Video\n  Transfer Learning",
        "url": "http://arxiv.org/abs/2309.07911v1",
        "pub_date": "2023-09-14",
        "summary": "Recently, large-scale pre-trained language-image models like CLIP have shown\nextraordinary capabilities for understanding spatial contents, but naively\ntransferring such models to video recognition still suffers from unsatisfactory\ntemporal modeling capabilities. Existing methods insert tunable structures into\nor in parallel with the pre-trained model, which either requires\nback-propagation through the whole pre-trained model and is thus\nresource-demanding, or is limited by the temporal reasoning capability of the\npre-trained structure. In this work, we present DiST, which disentangles the\nlearning of spatial and temporal aspects of videos. Specifically, DiST uses a\ndual-encoder structure, where a pre-trained foundation model acts as the\nspatial encoder, and a lightweight network is introduced as the temporal\nencoder. An integration branch is inserted between the encoders to fuse\nspatio-temporal information. The disentangled spatial and temporal learning in\nDiST is highly efficient because it avoids the back-propagation of massive\npre-trained parameters. Meanwhile, we empirically show that disentangled\nlearning with an extra network for integration benefits both spatial and\ntemporal understanding. Extensive experiments on five benchmarks show that DiST\ndelivers better performance than existing state-of-the-art methods by\nconvincing gaps. When pre-training on the large-scale Kinetics-710, we achieve\n89.7% on Kinetics-400 with a frozen ViT-L model, which verifies the scalability\nof DiST. Codes and models can be found in\nhttps://github.com/alibaba-mmai-research/DiST.",
        "translated": ""
    },
    {
        "title": "TEMPO: Efficient Multi-View Pose Estimation, Tracking, and Forecasting",
        "url": "http://arxiv.org/abs/2309.07910v1",
        "pub_date": "2023-09-14",
        "summary": "Existing volumetric methods for predicting 3D human pose estimation are\naccurate, but computationally expensive and optimized for single time-step\nprediction. We present TEMPO, an efficient multi-view pose estimation model\nthat learns a robust spatiotemporal representation, improving pose accuracy\nwhile also tracking and forecasting human pose. We significantly reduce\ncomputation compared to the state-of-the-art by recurrently computing\nper-person 2D pose features, fusing both spatial and temporal information into\na single representation. In doing so, our model is able to use spatiotemporal\ncontext to predict more accurate human poses without sacrificing efficiency. We\nfurther use this representation to track human poses over time as well as\npredict future poses. Finally, we demonstrate that our model is able to\ngeneralize across datasets without scene-specific fine-tuning. TEMPO achieves\n10$\\%$ better MPJPE with a 33$\\times$ improvement in FPS compared to TesseTrack\non the challenging CMU Panoptic Studio dataset.",
        "translated": ""
    },
    {
        "title": "Physically Plausible Full-Body Hand-Object Interaction Synthesis",
        "url": "http://arxiv.org/abs/2309.07907v1",
        "pub_date": "2023-09-14",
        "summary": "We propose a physics-based method for synthesizing dexterous hand-object\ninteractions in a full-body setting. While recent advancements have addressed\nspecific facets of human-object interactions, a comprehensive physics-based\napproach remains a challenge. Existing methods often focus on isolated segments\nof the interaction process and rely on data-driven techniques that may result\nin artifacts. In contrast, our proposed method embraces reinforcement learning\n(RL) and physics simulation to mitigate the limitations of data-driven\napproaches. Through a hierarchical framework, we first learn skill priors for\nboth body and hand movements in a decoupled setting. The generic skill priors\nlearn to decode a latent skill embedding into the motion of the underlying\npart. A high-level policy then controls hand-object interactions in these\npretrained latent spaces, guided by task objectives of grasping and 3D target\ntrajectory following. It is trained using a novel reward function that combines\nan adversarial style term with a task reward, encouraging natural motions while\nfulfilling the task incentives. Our method successfully accomplishes the\ncomplete interaction task, from approaching an object to grasping and\nsubsequent manipulation. We compare our approach against kinematics-based\nbaselines and show that it leads to more physically plausible motions.",
        "translated": ""
    },
    {
        "title": "Generative Image Dynamics",
        "url": "http://arxiv.org/abs/2309.07906v1",
        "pub_date": "2023-09-14",
        "summary": "We present an approach to modeling an image-space prior on scene dynamics.\nOur prior is learned from a collection of motion trajectories extracted from\nreal video sequences containing natural, oscillating motion such as trees,\nflowers, candles, and clothes blowing in the wind. Given a single image, our\ntrained model uses a frequency-coordinated diffusion sampling process to\npredict a per-pixel long-term motion representation in the Fourier domain,\nwhich we call a neural stochastic motion texture. This representation can be\nconverted into dense motion trajectories that span an entire video. Along with\nan image-based rendering module, these trajectories can be used for a number of\ndownstream applications, such as turning still images into seamlessly looping\ndynamic videos, or allowing users to realistically interact with objects in\nreal pictures.",
        "translated": ""
    },
    {
        "title": "Robust e-NeRF: NeRF from Sparse &amp; Noisy Events under Non-Uniform Motion",
        "url": "http://arxiv.org/abs/2309.08596v1",
        "pub_date": "2023-09-15",
        "summary": "Event cameras offer many advantages over standard cameras due to their\ndistinctive principle of operation: low power, low latency, high temporal\nresolution and high dynamic range. Nonetheless, the success of many downstream\nvisual applications also hinges on an efficient and effective scene\nrepresentation, where Neural Radiance Field (NeRF) is seen as the leading\ncandidate. Such promise and potential of event cameras and NeRF inspired recent\nworks to investigate on the reconstruction of NeRF from moving event cameras.\nHowever, these works are mainly limited in terms of the dependence on dense and\nlow-noise event streams, as well as generalization to arbitrary contrast\nthreshold values and camera speed profiles. In this work, we propose Robust\ne-NeRF, a novel method to directly and robustly reconstruct NeRFs from moving\nevent cameras under various real-world conditions, especially from sparse and\nnoisy events generated under non-uniform motion. It consists of two key\ncomponents: a realistic event generation model that accounts for various\nintrinsic parameters (e.g. time-independent, asymmetric threshold and\nrefractory period) and non-idealities (e.g. pixel-to-pixel threshold\nvariation), as well as a complementary pair of normalized reconstruction losses\nthat can effectively generalize to arbitrary speed profiles and intrinsic\nparameter values without such prior knowledge. Experiments on real and novel\nrealistically simulated sequences verify our effectiveness. Our code, synthetic\ndataset and improved event simulator are public.",
        "translated": ""
    },
    {
        "title": "Robust Frame-to-Frame Camera Rotation Estimation in Crowded Scenes",
        "url": "http://arxiv.org/abs/2309.08588v1",
        "pub_date": "2023-09-15",
        "summary": "We present an approach to estimating camera rotation in crowded, real-world\nscenes from handheld monocular video. While camera rotation estimation is a\nwell-studied problem, no previous methods exhibit both high accuracy and\nacceptable speed in this setting. Because the setting is not addressed well by\nother datasets, we provide a new dataset and benchmark, with high-accuracy,\nrigorously verified ground truth, on 17 video sequences. Methods developed for\nwide baseline stereo (e.g., 5-point methods) perform poorly on monocular video.\nOn the other hand, methods used in autonomous driving (e.g., SLAM) leverage\nspecific sensor setups, specific motion models, or local optimization\nstrategies (lagging batch processing) and do not generalize well to handheld\nvideo. Finally, for dynamic scenes, commonly used robustification techniques\nlike RANSAC require large numbers of iterations, and become prohibitively slow.\nWe introduce a novel generalization of the Hough transform on SO(3) to\nefficiently and robustly find the camera rotation most compatible with optical\nflow. Among comparably fast methods, ours reduces error by almost 50\\% over the\nnext best, and is more accurate than any method, irrespective of speed. This\nrepresents a strong new performance point for crowded scenes, an important\nsetting for computer vision. The code and the dataset are available at\nhttps://fabiendelattre.com/robust-rotation-estimation.",
        "translated": ""
    },
    {
        "title": "Replacing softmax with ReLU in Vision Transformers",
        "url": "http://arxiv.org/abs/2309.08586v1",
        "pub_date": "2023-09-15",
        "summary": "Previous research observed accuracy degradation when replacing the attention\nsoftmax with a point-wise activation such as ReLU. In the context of vision\ntransformers, we find that this degradation is mitigated when dividing by\nsequence length. Our experiments training small to large vision transformers on\nImageNet-21k indicate that ReLU-attention can approach or match the performance\nof softmax-attention in terms of scaling behavior as a function of compute.",
        "translated": ""
    },
    {
        "title": "Viewpoint Integration and Registration with Vision Language Foundation\n  Model for Image Change Understanding",
        "url": "http://arxiv.org/abs/2309.08585v1",
        "pub_date": "2023-09-15",
        "summary": "Recently, the development of pre-trained vision language foundation models\n(VLFMs) has led to remarkable performance in many tasks. However, these models\ntend to have strong single-image understanding capability but lack the ability\nto understand multiple images. Therefore, they cannot be directly applied to\ncope with image change understanding (ICU), which requires models to capture\nactual changes between multiple images and describe them in language. In this\npaper, we discover that existing VLFMs perform poorly when applied directly to\nICU because of the following problems: (1) VLFMs generally learn the global\nrepresentation of a single image, while ICU requires capturing nuances between\nmultiple images. (2) The ICU performance of VLFMs is significantly affected by\nviewpoint variations, which is caused by the altered relationships between\nobjects when viewpoint changes. To address these problems, we propose a\nViewpoint Integration and Registration method. Concretely, we introduce a fused\nadapter image encoder that fine-tunes pre-trained encoders by inserting\ndesigned trainable adapters and fused adapters, to effectively capture nuances\nbetween image pairs. Additionally, a viewpoint registration flow and a semantic\nemphasizing module are designed to reduce the performance degradation caused by\nviewpoint variations in the visual and semantic space, respectively.\nExperimental results on CLEVR-Change and Spot-the-Diff demonstrate that our\nmethod achieves state-of-the-art performance in all metrics.",
        "translated": ""
    },
    {
        "title": "The Impact of Different Backbone Architecture on Autonomous Vehicle\n  Dataset",
        "url": "http://arxiv.org/abs/2309.08564v1",
        "pub_date": "2023-09-15",
        "summary": "Object detection is a crucial component of autonomous driving, and many\ndetection applications have been developed to address this task. These\napplications often rely on backbone architectures, which extract representation\nfeatures from inputs to perform the object detection task. The quality of the\nfeatures extracted by the backbone architecture can have a significant impact\non the overall detection performance. Many researchers have focused on\ndeveloping new and improved backbone architectures to enhance the efficiency\nand accuracy of object detection applications. While these backbone\narchitectures have shown state-of-the-art performance on generic object\ndetection datasets like MS-COCO and PASCAL-VOC, evaluating their performance\nunder an autonomous driving environment has not been previously explored. To\naddress this, our study evaluates three well-known autonomous vehicle datasets,\nnamely KITTI, NuScenes, and BDD, to compare the performance of different\nbackbone architectures on object detection tasks.",
        "translated": ""
    },
    {
        "title": "Visual Speech Recognition for Low-resource Languages with Automatic\n  Labels From Whisper Model",
        "url": "http://arxiv.org/abs/2309.08535v1",
        "pub_date": "2023-09-15",
        "summary": "This paper proposes a powerful Visual Speech Recognition (VSR) method for\nmultiple languages, especially for low-resource languages that have a limited\nnumber of labeled data. Different from previous methods that tried to improve\nthe VSR performance for the target language by using knowledge learned from\nother languages, we explore whether we can increase the amount of training data\nitself for the different languages without human intervention. To this end, we\nemploy a Whisper model which can conduct both language identification and\naudio-based speech recognition. It serves to filter data of the desired\nlanguages and transcribe labels from the unannotated, multilingual audio-visual\ndata pool. By comparing the performances of VSR models trained on automatic\nlabels and the human-annotated labels, we show that we can achieve similar VSR\nperformance to that of human-annotated labels even without utilizing human\nannotations. Through the automated labeling process, we label large-scale\nunlabeled multilingual databases, VoxCeleb2 and AVSpeech, producing 1,002 hours\nof data for four low VSR resource languages, French, Italian, Spanish, and\nPortuguese. With the automatic labels, we achieve new state-of-the-art\nperformance on mTEDx in four languages, significantly surpassing the previous\nmethods. The automatic labels are available online:\nhttps://github.com/JeongHun0716/Visual-Speech-Recognition-for-Low-Resource-Languages",
        "translated": ""
    },
    {
        "title": "Automated dermatoscopic pattern discovery by clustering neural network\n  output for human-computer interaction",
        "url": "http://arxiv.org/abs/2309.08533v1",
        "pub_date": "2023-09-15",
        "summary": "Background: As available medical image datasets increase in size, it becomes\ninfeasible for clinicians to review content manually for knowledge extraction.\nThe objective of this study was to create an automated clustering resulting in\nhuman-interpretable pattern discovery.\n  Methods: Images from the public HAM10000 dataset, including 7 common\npigmented skin lesion diagnoses, were tiled into 29420 tiles and clustered via\nk-means using neural network-extracted image features. The final number of\nclusters per diagnosis was chosen by either the elbow method or a compactness\nmetric balancing intra-lesion variance and cluster numbers. The amount of\nresulting non-informative clusters, defined as those containing less than six\nimage tiles, was compared between the two methods.\n  Results: Applying k-means, the optimal elbow cutoff resulted in a mean of\n24.7 (95%-CI: 16.4-33) clusters for every included diagnosis, including 14.9%\n(95% CI: 0.8-29.0) non-informative clusters. The optimal cutoff, as estimated\nby the compactness metric, resulted in significantly fewer clusters (13.4;\n95%-CI 11.8-15.1; p=0.03) and less non-informative ones (7.5%; 95% CI: 0-19.5;\np=0.017). The majority of clusters (93.6%) from the compactness metric could be\nmanually mapped to previously described dermatoscopic diagnostic patterns.\n  Conclusions: Automatically constraining unsupervised clustering can produce\nan automated extraction of diagnostically relevant and human-interpretable\nclusters of visual patterns from a large image dataset.",
        "translated": ""
    },
    {
        "title": "Towards Practical and Efficient Image-to-Speech Captioning with\n  Vision-Language Pre-training and Multi-modal Tokens",
        "url": "http://arxiv.org/abs/2309.08531v1",
        "pub_date": "2023-09-15",
        "summary": "In this paper, we propose methods to build a powerful and efficient\nImage-to-Speech captioning (Im2Sp) model. To this end, we start with importing\nthe rich knowledge related to image comprehension and language modeling from a\nlarge-scale pre-trained vision-language model into Im2Sp. We set the output of\nthe proposed Im2Sp as discretized speech units, i.e., the quantized speech\nfeatures of a self-supervised speech model. The speech units mainly contain\nlinguistic information while suppressing other characteristics of speech. This\nallows us to incorporate the language modeling capability of the pre-trained\nvision-language model into the spoken language modeling of Im2Sp. With the\nvision-language pre-training strategy, we set new state-of-the-art Im2Sp\nperformances on two widely used benchmark databases, COCO and Flickr8k. Then,\nwe further improve the efficiency of the Im2Sp model. Similar to the speech\nunit case, we convert the original image into image units, which are derived\nthrough vector quantization of the raw image. With these image units, we can\ndrastically reduce the required data storage for saving image data to just 0.8%\nwhen compared to the original image data in terms of bits. Demo page:\nhttps://ms-dot-k.github.io/Image-to-Speech-Captioning.",
        "translated": ""
    },
    {
        "title": "Breathing New Life into 3D Assets with Generative Repainting",
        "url": "http://arxiv.org/abs/2309.08523v1",
        "pub_date": "2023-09-15",
        "summary": "Diffusion-based text-to-image models ignited immense attention from the\nvision community, artists, and content creators. Broad adoption of these models\nis due to significant improvement in the quality of generations and efficient\nconditioning on various modalities, not just text. However, lifting the rich\ngenerative priors of these 2D models into 3D is challenging. Recent works have\nproposed various pipelines powered by the entanglement of diffusion models and\nneural fields. We explore the power of pretrained 2D diffusion models and\nstandard 3D neural radiance fields as independent, standalone tools and\ndemonstrate their ability to work together in a non-learned fashion. Such\nmodularity has the intrinsic advantage of eased partial upgrades, which became\nan important property in such a fast-paced domain. Our pipeline accepts any\nlegacy renderable geometry, such as textured or untextured meshes, orchestrates\nthe interaction between 2D generative refinement and 3D consistency enforcement\ntools, and outputs a painted input geometry in several formats. We conduct a\nlarge-scale study on a wide range of objects and categories from the\nShapeNetSem dataset and demonstrate the advantages of our approach, both\nqualitatively and quantitatively. Project page:\nhttps://www.obukhov.ai/repainting_3d_assets",
        "translated": ""
    },
    {
        "title": "SCT: A Simple Baseline for Parameter-Efficient Fine-Tuning via Salient\n  Channels",
        "url": "http://arxiv.org/abs/2309.08513v1",
        "pub_date": "2023-09-15",
        "summary": "Pre-trained vision transformers have strong representation benefits to\nvarious downstream tasks. Recently, many parameter-efficient fine-tuning (PEFT)\nmethods have been proposed, and their experiments demonstrate that tuning only\n1% of extra parameters could surpass full fine-tuning in low-data resource\nscenarios. However, these methods overlook the task-specific information when\nfine-tuning diverse downstream tasks. In this paper, we propose a simple yet\neffective method called \"Salient Channel Tuning\" (SCT) to leverage the\ntask-specific information by forwarding the model with the task images to\nselect partial channels in a feature map that enables us to tune only 1/8\nchannels leading to significantly lower parameter costs. Experiments outperform\nfull fine-tuning on 18 out of 19 tasks in the VTAB-1K benchmark by adding only\n0.11M parameters of the ViT-B, which is 780$\\times$ fewer than its full\nfine-tuning counterpart. Furthermore, experiments on domain generalization and\nfew-shot learning surpass other PEFT methods with lower parameter costs,\ndemonstrating our proposed tuning technique's strong capability and\neffectiveness in the low-data regime.",
        "translated": ""
    },
    {
        "title": "General In-Hand Object Rotation with Vision and Touch",
        "url": "http://arxiv.org/abs/2309.09979v1",
        "pub_date": "2023-09-18",
        "summary": "We introduce RotateIt, a system that enables fingertip-based object rotation\nalong multiple axes by leveraging multimodal sensory inputs. Our system is\ntrained in simulation, where it has access to ground-truth object shapes and\nphysical properties. Then we distill it to operate on realistic yet noisy\nsimulated visuotactile and proprioceptive sensory inputs. These multimodal\ninputs are fused via a visuotactile transformer, enabling online inference of\nobject shapes and physical properties during deployment. We show significant\nperformance improvements over prior methods and the importance of visual and\ntactile sensing.",
        "translated": ""
    },
    {
        "title": "GEDepth: Ground Embedding for Monocular Depth Estimation",
        "url": "http://arxiv.org/abs/2309.09975v1",
        "pub_date": "2023-09-18",
        "summary": "Monocular depth estimation is an ill-posed problem as the same 2D image can\nbe projected from infinite 3D scenes. Although the leading algorithms in this\nfield have reported significant improvement, they are essentially geared to the\nparticular compound of pictorial observations and camera parameters (i.e.,\nintrinsics and extrinsics), strongly limiting their generalizability in\nreal-world scenarios. To cope with this challenge, this paper proposes a novel\nground embedding module to decouple camera parameters from pictorial cues, thus\npromoting the generalization capability. Given camera parameters, the proposed\nmodule generates the ground depth, which is stacked with the input image and\nreferenced in the final depth prediction. A ground attention is designed in the\nmodule to optimally combine ground depth with residual depth. Our ground\nembedding is highly flexible and lightweight, leading to a plug-in module that\nis amenable to be integrated into various depth estimation networks.\nExperiments reveal that our approach achieves the state-of-the-art results on\npopular benchmarks, and more importantly, renders significant generalization\nimprovement on a wide range of cross-domain tests.",
        "translated": ""
    },
    {
        "title": "An Empirical Study of Scaling Instruct-Tuned Large Multimodal Models",
        "url": "http://arxiv.org/abs/2309.09958v1",
        "pub_date": "2023-09-18",
        "summary": "Visual instruction tuning has recently shown encouraging progress with\nopen-source large multimodal models (LMM) such as LLaVA and MiniGPT-4. However,\nmost existing studies of open-source LMM are performed using models with 13B\nparameters or smaller. In this paper we present an empirical study of scaling\nLLaVA up to 33B and 65B/70B, and share our findings from our explorations in\nimage resolution, data mixing and parameter-efficient training methods such as\nLoRA/QLoRA. These are evaluated by their impact on the multi-modal and language\ncapabilities when completing real-world tasks in the wild.\n  We find that scaling LMM consistently enhances model performance and improves\nlanguage capabilities, and performance of LoRA/QLoRA tuning of LMM are\ncomparable to the performance of full-model fine-tuning. Additionally, the\nstudy highlights the importance of higher image resolutions and mixing\nmultimodal-language data to improve LMM performance, and visual instruction\ntuning can sometimes improve LMM's pure language capability. We hope that this\nstudy makes state-of-the-art LMM research at a larger scale more accessible,\nthus helping establish stronger baselines for future research. Code and\ncheckpoints will be made public.",
        "translated": ""
    },
    {
        "title": "vSHARP: variable Splitting Half-quadratic ADMM algorithm for\n  Reconstruction of inverse-Problems",
        "url": "http://arxiv.org/abs/2309.09954v1",
        "pub_date": "2023-09-18",
        "summary": "Medical Imaging (MI) tasks, such as accelerated Parallel Magnetic Resonance\nImaging (MRI), often involve reconstructing an image from noisy or incomplete\nmeasurements. This amounts to solving ill-posed inverse problems, where a\nsatisfactory closed-form analytical solution is not available. Traditional\nmethods such as Compressed Sensing (CS) in MRI reconstruction can be\ntime-consuming or prone to obtaining low-fidelity images. Recently, a plethora\nof supervised and self-supervised Deep Learning (DL) approaches have\ndemonstrated superior performance in inverse-problem solving, surpassing\nconventional methods. In this study, we propose vSHARP (variable Splitting\nHalf-quadratic ADMM algorithm for Reconstruction of inverse Problems), a novel\nDL-based method for solving ill-posed inverse problems arising in MI. vSHARP\nutilizes the Half-Quadratic Variable Splitting method and employs the\nAlternating Direction Method of Multipliers (ADMM) to unroll the optimization\nprocess. For data consistency, vSHARP unrolls a differentiable gradient descent\nprocess in the image domain, while a DL-based denoiser, such as a U-Net\narchitecture, is applied to enhance image quality. vSHARP also employs a\ndilated-convolution DL-based model to predict the Lagrange multipliers for the\nADMM initialization. We evaluate the proposed model by applying it to the task\nof accelerated Parallel MRI Reconstruction on two distinct datasets. We present\na comparative analysis of our experimental results with state-of-the-art\napproaches, highlighting the superior performance of vSHARP.",
        "translated": ""
    },
    {
        "title": "End-to-End Learned Event- and Image-based Visual Odometry",
        "url": "http://arxiv.org/abs/2309.09947v1",
        "pub_date": "2023-09-18",
        "summary": "Visual Odometry (VO) is crucial for autonomous robotic navigation, especially\nin GPS-denied environments like planetary terrains. While standard RGB cameras\nstruggle in low-light or high-speed motion, event-based cameras offer high\ndynamic range and low latency. However, seamlessly integrating asynchronous\nevent data with synchronous frames remains challenging. We introduce RAMP-VO,\nthe first end-to-end learned event- and image-based VO system. It leverages\nnovel Recurrent, Asynchronous, and Massively Parallel (RAMP) encoders that are\n8x faster and 20% more accurate than existing asynchronous encoders. RAMP-VO\nfurther employs a novel pose forecasting technique to predict future poses for\ninitialization. Despite being trained only in simulation, RAMP-VO outperforms\nimage- and event-based methods by 52% and 20%, respectively, on traditional,\nreal-world benchmarks as well as newly introduced Apollo and Malapert landing\nsequences, paving the way for robust and asynchronous VO in space.",
        "translated": ""
    },
    {
        "title": "What is a Fair Diffusion Model? Designing Generative Text-To-Image\n  Models to Incorporate Various Worldviews",
        "url": "http://arxiv.org/abs/2309.09944v1",
        "pub_date": "2023-09-18",
        "summary": "Generative text-to-image (GTI) models produce high-quality images from short\ntextual descriptions and are widely used in academic and creative domains.\nHowever, GTI models frequently amplify biases from their training data, often\nproducing prejudiced or stereotypical images. Yet, current bias mitigation\nstrategies are limited and primarily focus on enforcing gender parity across\noccupations. To enhance GTI bias mitigation, we introduce DiffusionWorldViewer,\na tool to analyze and manipulate GTI models' attitudes, values, stories, and\nexpectations of the world that impact its generated images. Through an\ninteractive interface deployed as a web-based GUI and Jupyter Notebook plugin,\nDiffusionWorldViewer categorizes existing demographics of GTI-generated images\nand provides interactive methods to align image demographics with user\nworldviews. In a study with 13 GTI users, we find that DiffusionWorldViewer\nallows users to represent their varied viewpoints about what GTI outputs are\nfair and, in doing so, challenges current notions of fairness that assume a\nuniversal worldview.",
        "translated": ""
    },
    {
        "title": "Hierarchical Attention and Graph Neural Networks: Toward Drift-Free Pose\n  Estimation",
        "url": "http://arxiv.org/abs/2309.09934v1",
        "pub_date": "2023-09-18",
        "summary": "The most commonly used method for addressing 3D geometric registration is the\niterative closet-point algorithm, this approach is incremental and prone to\ndrift over multiple consecutive frames. The Common strategy to address the\ndrift is the pose graph optimization subsequent to frame-to-frame registration,\nincorporating a loop closure process that identifies previously visited places.\nIn this paper, we explore a framework that replaces traditional geometric\nregistration and pose graph optimization with a learned model utilizing\nhierarchical attention mechanisms and graph neural networks. We propose a\nstrategy to condense the data flow, preserving essential information required\nfor the precise estimation of rigid poses. Our results, derived from tests on\nthe KITTI Odometry dataset, demonstrate a significant improvement in pose\nestimation accuracy. This improvement is especially notable in determining\nrotational components when compared with results obtained through conventional\nmulti-way registration via pose graph optimization. The code will be made\navailable upon completion of the review process.",
        "translated": ""
    },
    {
        "title": "Quantum Vision Clustering",
        "url": "http://arxiv.org/abs/2309.09907v1",
        "pub_date": "2023-09-18",
        "summary": "Unsupervised visual clustering has recently received considerable attention.\nIt aims to explain distributions of unlabeled visual images by clustering them\nvia a parameterized appearance model. From a different perspective, the\nclustering algorithms can be treated as assignment problems, often NP-hard.\nThey can be solved precisely for small instances on current hardware. Adiabatic\nquantum computing (AQC) offers a solution, as it can soon provide a\nconsiderable speedup on a range of NP-hard optimization problems. However,\ncurrent clustering formulations are unsuitable for quantum computing due to\ntheir scaling properties. Consequently, in this work, we propose the first\nclustering formulation designed to be solved with AQC. We employ an Ising model\nrepresenting the quantum mechanical system implemented on the AQC. Our approach\nis competitive compared to state-of-the-art optimization-based approaches, even\nusing of-the-shelf integer programming solvers. Finally, we demonstrate that\nour clustering problem is already solvable on the current generation of real\nquantum computers for small examples and analyze the properties of the measured\nsolutions.",
        "translated": ""
    },
    {
        "title": "On Model Explanations with Transferable Neural Pathways",
        "url": "http://arxiv.org/abs/2309.09887v1",
        "pub_date": "2023-09-18",
        "summary": "Neural pathways as model explanations consist of a sparse set of neurons that\nprovide the same level of prediction performance as the whole model. Existing\nmethods primarily focus on accuracy and sparsity but the generated pathways may\noffer limited interpretability thus fall short in explaining the model\nbehavior. In this paper, we suggest two interpretability criteria of neural\npathways: (i) same-class neural pathways should primarily consist of\nclass-relevant neurons; (ii) each instance's neural pathway sparsity should be\noptimally determined. To this end, we propose a Generative Class-relevant\nNeural Pathway (GEN-CNP) model that learns to predict the neural pathways from\nthe target model's feature maps. We propose to learn class-relevant information\nfrom features of deep and shallow layers such that same-class neural pathways\nexhibit high similarity. We further impose a faithfulness criterion for GEN-CNP\nto generate pathways with instance-specific sparsity. We propose to transfer\nthe class-relevant neural pathways to explain samples of the same class and\nshow experimentally and qualitatively their faithfulness and interpretability.",
        "translated": ""
    },
    {
        "title": "RaLF: Flow-based Global and Metric Radar Localization in LiDAR Maps",
        "url": "http://arxiv.org/abs/2309.09875v1",
        "pub_date": "2023-09-18",
        "summary": "Localization is paramount for autonomous robots. While camera and LiDAR-based\napproaches have been extensively investigated, they are affected by adverse\nillumination and weather conditions. Therefore, radar sensors have recently\ngained attention due to their intrinsic robustness to such conditions. In this\npaper, we propose RaLF, a novel deep neural network-based approach for\nlocalizing radar scans in a LiDAR map of the environment, by jointly learning\nto address both place recognition and metric localization. RaLF is composed of\nradar and LiDAR feature encoders, a place recognition head that generates\nglobal descriptors, and a metric localization head that predicts the 3-DoF\ntransformation between the radar scan and the map. We tackle the place\nrecognition task by learning a shared embedding space between the two\nmodalities via cross-modal metric learning. Additionally, we perform metric\nlocalization by predicting pixel-level flow vectors that align the query radar\nscan with the LiDAR map. We extensively evaluate our approach on multiple\nreal-world driving datasets and show that RaLF achieves state-of-the-art\nperformance for both place recognition and metric localization. Moreover, we\ndemonstrate that our approach can effectively generalize to different cities\nand sensor setups than the ones used during training. We make the code and\ntrained models publicly available at http://ralf.cs.uni-freiburg.de.",
        "translated": ""
    },
    {
        "title": "Assessing the capacity of a denoising diffusion probabilistic model to\n  reproduce spatial context",
        "url": "http://arxiv.org/abs/2309.10817v1",
        "pub_date": "2023-09-19",
        "summary": "Diffusion models have emerged as a popular family of deep generative models\n(DGMs). In the literature, it has been claimed that one class of diffusion\nmodels -- denoising diffusion probabilistic models (DDPMs) -- demonstrate\nsuperior image synthesis performance as compared to generative adversarial\nnetworks (GANs). To date, these claims have been evaluated using either\nensemble-based methods designed for natural images, or conventional measures of\nimage quality such as structural similarity. However, there remains an\nimportant need to understand the extent to which DDPMs can reliably learn\nmedical imaging domain-relevant information, which is referred to as `spatial\ncontext' in this work. To address this, a systematic assessment of the ability\nof DDPMs to learn spatial context relevant to medical imaging applications is\nreported for the first time. A key aspect of the studies is the use of\nstochastic context models (SCMs) to produce training data. In this way, the\nability of the DDPMs to reliably reproduce spatial context can be\nquantitatively assessed by use of post-hoc image analyses. Error-rates in\nDDPM-generated ensembles are reported, and compared to those corresponding to a\nmodern GAN. The studies reveal new and important insights regarding the\ncapacity of DDPMs to learn spatial context. Notably, the results demonstrate\nthat DDPMs hold significant capacity for generating contextually correct images\nthat are `interpolated' between training samples, which may benefit\ndata-augmentation tasks in ways that GANs cannot.",
        "translated": ""
    },
    {
        "title": "PanopticNeRF-360: Panoramic 3D-to-2D Label Transfer in Urban Scenes",
        "url": "http://arxiv.org/abs/2309.10815v1",
        "pub_date": "2023-09-19",
        "summary": "Training perception systems for self-driving cars requires substantial\nannotations. However, manual labeling in 2D images is highly labor-intensive.\nWhile existing datasets provide rich annotations for pre-recorded sequences,\nthey fall short in labeling rarely encountered viewpoints, potentially\nhampering the generalization ability for perception models. In this paper, we\npresent PanopticNeRF-360, a novel approach that combines coarse 3D annotations\nwith noisy 2D semantic cues to generate consistent panoptic labels and\nhigh-quality images from any viewpoint. Our key insight lies in exploiting the\ncomplementarity of 3D and 2D priors to mutually enhance geometry and semantics.\nSpecifically, we propose to leverage noisy semantic and instance labels in both\n3D and 2D spaces to guide geometry optimization. Simultaneously, the improved\ngeometry assists in filtering noise present in the 3D and 2D annotations by\nmerging them in 3D space via a learned semantic field. To further enhance\nappearance, we combine MLP and hash grids to yield hybrid scene features,\nstriking a balance between high-frequency appearance and predominantly\ncontiguous semantics. Our experiments demonstrate PanopticNeRF-360's\nstate-of-the-art performance over existing label transfer methods on the\nchallenging urban scenes of the KITTI-360 dataset. Moreover, PanopticNeRF-360\nenables omnidirectional rendering of high-fidelity, multi-view and\nspatiotemporally consistent appearance, semantic and instance labels. We make\nour code and data available at https://github.com/fuxiao0719/PanopticNeRF",
        "translated": ""
    },
    {
        "title": "PGDiff: Guiding Diffusion Models for Versatile Face Restoration via\n  Partial Guidance",
        "url": "http://arxiv.org/abs/2309.10810v1",
        "pub_date": "2023-09-19",
        "summary": "Exploiting pre-trained diffusion models for restoration has recently become a\nfavored alternative to the traditional task-specific training approach.\nPrevious works have achieved noteworthy success by limiting the solution space\nusing explicit degradation models. However, these methods often fall short when\nfaced with complex degradations as they generally cannot be precisely modeled.\nIn this paper, we propose PGDiff by introducing partial guidance, a fresh\nperspective that is more adaptable to real-world degradations compared to\nexisting works. Rather than specifically defining the degradation process, our\napproach models the desired properties, such as image structure and color\nstatistics of high-quality images, and applies this guidance during the reverse\ndiffusion process. These properties are readily available and make no\nassumptions about the degradation process. When combined with a diffusion\nprior, this partial guidance can deliver appealing results across a range of\nrestoration tasks. Additionally, PGDiff can be extended to handle composite\ntasks by consolidating multiple high-quality image properties, achieved by\nintegrating the guidance from respective tasks. Experimental results\ndemonstrate that our method not only outperforms existing diffusion-prior-based\napproaches but also competes favorably with task-specific models.",
        "translated": ""
    },
    {
        "title": "Multi-Context Dual Hyper-Prior Neural Image Compression",
        "url": "http://arxiv.org/abs/2309.10799v1",
        "pub_date": "2023-09-19",
        "summary": "Transform and entropy models are the two core components in deep image\ncompression neural networks. Most existing learning-based image compression\nmethods utilize convolutional-based transform, which lacks the ability to model\nlong-range dependencies, primarily due to the limited receptive field of the\nconvolution operation. To address this limitation, we propose a\nTransformer-based nonlinear transform. This transform has the remarkable\nability to efficiently capture both local and global information from the input\nimage, leading to a more decorrelated latent representation. In addition, we\nintroduce a novel entropy model that incorporates two different hyperpriors to\nmodel cross-channel and spatial dependencies of the latent representation. To\nfurther improve the entropy model, we add a global context that leverages\ndistant relationships to predict the current latent more accurately. This\nglobal context employs a causal attention mechanism to extract long-range\ninformation in a content-dependent manner. Our experiments show that our\nproposed framework performs better than the state-of-the-art methods in terms\nof rate-distortion performance.",
        "translated": ""
    },
    {
        "title": "Multi-spectral Entropy Constrained Neural Compression of Solar Imagery",
        "url": "http://arxiv.org/abs/2309.10791v1",
        "pub_date": "2023-09-19",
        "summary": "Missions studying the dynamic behaviour of the Sun are defined to capture\nmulti-spectral images of the sun and transmit them to the ground station in a\ndaily basis. To make transmission efficient and feasible, image compression\nsystems need to be exploited. Recently successful end-to-end optimized neural\nnetwork-based image compression systems have shown great potential to be used\nin an ad-hoc manner. In this work we have proposed a transformer-based\nmulti-spectral neural image compressor to efficiently capture redundancies both\nintra/inter-wavelength. To unleash the locality of window-based self attention\nmechanism, we propose an inter-window aggregated token multi head self\nattention. Additionally to make the neural compressor autoencoder shift\ninvariant, a randomly shifted window attention mechanism is used which makes\nthe transformer blocks insensitive to translations in their input domain. We\ndemonstrate that the proposed approach not only outperforms the conventional\ncompression algorithms but also it is able to better decorrelates images along\nthe multiple wavelengths compared to single spectral compression.",
        "translated": ""
    },
    {
        "title": "Guide Your Agent with Adaptive Multimodal Rewards",
        "url": "http://arxiv.org/abs/2309.10790v1",
        "pub_date": "2023-09-19",
        "summary": "Developing an agent capable of adapting to unseen environments remains a\ndifficult challenge in imitation learning. In this work, we present Adaptive\nReturn-conditioned Policy (ARP), an efficient framework designed to enhance the\nagent's generalization ability using natural language task descriptions and\npre-trained multimodal encoders. Our key idea is to calculate a similarity\nbetween visual observations and natural language instructions in the\npre-trained multimodal embedding space (such as CLIP) and use it as a reward\nsignal. We then train a return-conditioned policy using expert demonstrations\nlabeled with multimodal rewards. Because the multimodal rewards provide\nadaptive signals at each timestep, our ARP effectively mitigates the goal\nmisgeneralization. This results in superior generalization performances even\nwhen faced with unseen text instructions, compared to existing text-conditioned\npolicies. To improve the quality of rewards, we also introduce a fine-tuning\nmethod for pre-trained multimodal encoders, further enhancing the performance.\nVideo demonstrations and source code are available on the project website:\nhttps://sites.google.com/view/2023arp.",
        "translated": ""
    },
    {
        "title": "AV-SUPERB: A Multi-Task Evaluation Benchmark for Audio-Visual\n  Representation Models",
        "url": "http://arxiv.org/abs/2309.10787v1",
        "pub_date": "2023-09-19",
        "summary": "Audio-visual representation learning aims to develop systems with human-like\nperception by utilizing correlation between auditory and visual information.\nHowever, current models often focus on a limited set of tasks, and\ngeneralization abilities of learned representations are unclear. To this end,\nwe propose the AV-SUPERB benchmark that enables general-purpose evaluation of\nunimodal audio/visual and bimodal fusion representations on 7 datasets covering\n5 audio-visual tasks in speech and audio processing. We evaluate 5 recent\nself-supervised models and show that none of these models generalize to all\ntasks, emphasizing the need for future study on improving universal model\nperformance. In addition, we show that representations may be improved with\nintermediate-task fine-tuning and audio event classification with AudioSet\nserves as a strong intermediate task. We release our benchmark with evaluation\ncode and a model submission platform to encourage further research in\naudio-visual learning.",
        "translated": ""
    },
    {
        "title": "Context-Aware Neural Video Compression on Solar Dynamics Observatory",
        "url": "http://arxiv.org/abs/2309.10784v1",
        "pub_date": "2023-09-19",
        "summary": "NASA's Solar Dynamics Observatory (SDO) mission collects large data volumes\nof the Sun's daily activity. Data compression is crucial for space missions to\nreduce data storage and video bandwidth requirements by eliminating\nredundancies in the data. In this paper, we present a novel neural\nTransformer-based video compression approach specifically designed for the SDO\nimages. Our primary objective is to efficiently exploit the temporal and\nspatial redundancies inherent in solar images to obtain a high compression\nratio. Our proposed architecture benefits from a novel Transformer block called\nFused Local-aware Window (FLaWin), which incorporates window-based\nself-attention modules and an efficient fused local-aware feed-forward (FLaFF)\nnetwork. This architectural design allows us to simultaneously capture\nshort-range and long-range information while facilitating the extraction of\nrich and diverse contextual representations. Moreover, this design choice\nresults in reduced computational complexity. Experimental results demonstrate\nthe significant contribution of the FLaWin Transformer block to the compression\nperformance, outperforming conventional hand-engineered video codecs such as\nH.264 and H.265 in terms of rate-distortion trade-off.",
        "translated": ""
    },
    {
        "title": "Language as the Medium: Multimodal Video Classification through text\n  only",
        "url": "http://arxiv.org/abs/2309.10783v1",
        "pub_date": "2023-09-19",
        "summary": "Despite an exciting new wave of multimodal machine learning models, current\napproaches still struggle to interpret the complex contextual relationships\nbetween the different modalities present in videos. Going beyond existing\nmethods that emphasize simple activities or objects, we propose a new\nmodel-agnostic approach for generating detailed textual descriptions that\ncaptures multimodal video information. Our method leverages the extensive\nknowledge learnt by large language models, such as GPT-3.5 or Llama2, to reason\nabout textual descriptions of the visual and aural modalities, obtained from\nBLIP-2, Whisper and ImageBind. Without needing additional finetuning of\nvideo-text models or datasets, we demonstrate that available LLMs have the\nability to use these multimodal textual descriptions as proxies for ``sight''\nor ``hearing'' and perform zero-shot multimodal classification of videos\nin-context. Our evaluations on popular action recognition benchmarks, such as\nUCF-101 or Kinetics, show these context-rich descriptions can be successfully\nused in video understanding tasks. This method points towards a promising new\nresearch direction in multimodal classification, demonstrating how an interplay\nbetween textual, visual and auditory machine learning models can enable more\nholistic video understanding.",
        "translated": ""
    },
    {
        "title": "MAGIC-TBR: Multiview Attention Fusion for Transformer-based Bodily\n  Behavior Recognition in Group Settings",
        "url": "http://arxiv.org/abs/2309.10765v1",
        "pub_date": "2023-09-19",
        "summary": "Bodily behavioral language is an important social cue, and its automated\nanalysis helps in enhancing the understanding of artificial intelligence\nsystems. Furthermore, behavioral language cues are essential for active\nengagement in social agent-based user interactions. Despite the progress made\nin computer vision for tasks like head and body pose estimation, there is still\na need to explore the detection of finer behaviors such as gesturing, grooming,\nor fumbling. This paper proposes a multiview attention fusion method named\nMAGIC-TBR that combines features extracted from videos and their corresponding\nDiscrete Cosine Transform coefficients via a transformer-based approach. The\nexperiments are conducted on the BBSI dataset and the results demonstrate the\neffectiveness of the proposed feature fusion with multiview attention. The code\nis available at: https://github.com/surbhimadan92/MAGIC-TBR",
        "translated": ""
    },
    {
        "title": "A Large-scale Dataset for Audio-Language Representation Learning",
        "url": "http://arxiv.org/abs/2309.11500v1",
        "pub_date": "2023-09-20",
        "summary": "The AI community has made significant strides in developing powerful\nfoundation models, driven by large-scale multimodal datasets. However, in the\naudio representation learning community, the present audio-language datasets\nsuffer from limitations such as insufficient volume, simplistic content, and\narduous collection procedures. To tackle these challenges, we present an\ninnovative and automatic audio caption generation pipeline based on a series of\npublic tools or APIs, and construct a large-scale, high-quality, audio-language\ndataset, named as Auto-ACD, comprising over 1.9M audio-text pairs. To\ndemonstrate the effectiveness of the proposed dataset, we train popular models\non our dataset and show performance improvement on various downstream tasks,\nnamely, audio-language retrieval, audio captioning, environment classification.\nIn addition, we establish a novel test set and provide a benchmark for\naudio-text tasks. The proposed dataset will be released at\nhttps://auto-acd.github.io/.",
        "translated": ""
    },
    {
        "title": "DreamLLM: Synergistic Multimodal Comprehension and Creation",
        "url": "http://arxiv.org/abs/2309.11499v1",
        "pub_date": "2023-09-20",
        "summary": "This paper presents DreamLLM, a learning framework that first achieves\nversatile Multimodal Large Language Models (MLLMs) empowered with frequently\noverlooked synergy between multimodal comprehension and creation. DreamLLM\noperates on two fundamental principles. The first focuses on the generative\nmodeling of both language and image posteriors by direct sampling in the raw\nmultimodal space. This approach circumvents the limitations and information\nloss inherent to external feature extractors like CLIP, and a more thorough\nmultimodal understanding is obtained. Second, DreamLLM fosters the generation\nof raw, interleaved documents, modeling both text and image contents, along\nwith unstructured layouts. This allows DreamLLM to learn all conditional,\nmarginal, and joint multimodal distributions effectively. As a result, DreamLLM\nis the first MLLM capable of generating free-form interleaved content.\nComprehensive experiments highlight DreamLLM's superior performance as a\nzero-shot multimodal generalist, reaping from the enhanced learning synergy.",
        "translated": ""
    },
    {
        "title": "FreeU: Free Lunch in Diffusion U-Net",
        "url": "http://arxiv.org/abs/2309.11497v1",
        "pub_date": "2023-09-20",
        "summary": "In this paper, we uncover the untapped potential of diffusion U-Net, which\nserves as a \"free lunch\" that substantially improves the generation quality on\nthe fly. We initially investigate the key contributions of the U-Net\narchitecture to the denoising process and identify that its main backbone\nprimarily contributes to denoising, whereas its skip connections mainly\nintroduce high-frequency features into the decoder module, causing the network\nto overlook the backbone semantics. Capitalizing on this discovery, we propose\na simple yet effective method-termed \"FreeU\" - that enhances generation quality\nwithout additional training or finetuning. Our key insight is to strategically\nre-weight the contributions sourced from the U-Net's skip connections and\nbackbone feature maps, to leverage the strengths of both components of the\nU-Net architecture. Promising results on image and video generation tasks\ndemonstrate that our FreeU can be readily integrated to existing diffusion\nmodels, e.g., Stable Diffusion, DreamBooth, ModelScope, Rerender and ReVersion,\nto improve the generation quality with only a few lines of code. All you need\nis to adjust two scaling factors during inference. Project page:\nhttps://chenyangsi.top/FreeU/.",
        "translated": ""
    },
    {
        "title": "Budget-Aware Pruning: Handling Multiple Domains with Less Parameters",
        "url": "http://arxiv.org/abs/2309.11464v1",
        "pub_date": "2023-09-20",
        "summary": "Deep learning has achieved state-of-the-art performance on several computer\nvision tasks and domains. Nevertheless, it still has a high computational cost\nand demands a significant amount of parameters. Such requirements hinder the\nuse in resource-limited environments and demand both software and hardware\noptimization. Another limitation is that deep models are usually specialized\ninto a single domain or task, requiring them to learn and store new parameters\nfor each new one. Multi-Domain Learning (MDL) attempts to solve this problem by\nlearning a single model that is capable of performing well in multiple domains.\nNevertheless, the models are usually larger than the baseline for a single\ndomain. This work tackles both of these problems: our objective is to prune\nmodels capable of handling multiple domains according to a user-defined budget,\nmaking them more computationally affordable while keeping a similar\nclassification performance. We achieve this by encouraging all domains to use a\nsimilar subset of filters from the baseline model, up to the amount defined by\nthe user's budget. Then, filters that are not used by any domain are pruned\nfrom the network. The proposed approach innovates by better adapting to\nresource-limited devices while, to our knowledge, being the only work that\nhandles multiple domains at test time with fewer parameters and lower\ncomputational complexity than the baseline model for a single domain.",
        "translated": ""
    },
    {
        "title": "Weight Averaging Improves Knowledge Distillation under Domain Shift",
        "url": "http://arxiv.org/abs/2309.11446v1",
        "pub_date": "2023-09-20",
        "summary": "Knowledge distillation (KD) is a powerful model compression technique broadly\nused in practical deep learning applications. It is focused on training a small\nstudent network to mimic a larger teacher network. While it is widely known\nthat KD can offer an improvement to student generalization in i.i.d setting,\nits performance under domain shift, i.e. the performance of student networks on\ndata from domains unseen during training, has received little attention in the\nliterature. In this paper we make a step towards bridging the research fields\nof knowledge distillation and domain generalization. We show that weight\naveraging techniques proposed in domain generalization literature, such as SWAD\nand SMA, also improve the performance of knowledge distillation under domain\nshift. In addition, we propose a simplistic weight averaging strategy that does\nnot require evaluation on validation data during training and show that it\nperforms on par with SWAD and SMA when applied to KD. We name our final\ndistillation approach Weight-Averaged Knowledge Distillation (WAKD).",
        "translated": ""
    },
    {
        "title": "SkeleTR: Towrads Skeleton-based Action Recognition in the Wild",
        "url": "http://arxiv.org/abs/2309.11445v1",
        "pub_date": "2023-09-20",
        "summary": "We present SkeleTR, a new framework for skeleton-based action recognition. In\ncontrast to prior work, which focuses mainly on controlled environments, we\ntarget more general scenarios that typically involve a variable number of\npeople and various forms of interaction between people. SkeleTR works with a\ntwo-stage paradigm. It first models the intra-person skeleton dynamics for each\nskeleton sequence with graph convolutions, and then uses stacked Transformer\nencoders to capture person interactions that are important for action\nrecognition in general scenarios. To mitigate the negative impact of inaccurate\nskeleton associations, SkeleTR takes relative short skeleton sequences as input\nand increases the number of sequences. As a unified solution, SkeleTR can be\ndirectly applied to multiple skeleton-based action tasks, including video-level\naction classification, instance-level action detection, and group-level\nactivity recognition. It also enables transfer learning and joint training\nacross different action tasks and datasets, which result in performance\nimprovement. When evaluated on various skeleton-based action recognition\nbenchmarks, SkeleTR achieves the state-of-the-art performance.",
        "translated": ""
    },
    {
        "title": "Signature Activation: A Sparse Signal View for Holistic Saliency",
        "url": "http://arxiv.org/abs/2309.11443v1",
        "pub_date": "2023-09-20",
        "summary": "The adoption of machine learning in healthcare calls for model transparency\nand explainability. In this work, we introduce Signature Activation, a saliency\nmethod that generates holistic and class-agnostic explanations for\nConvolutional Neural Network (CNN) outputs. Our method exploits the fact that\ncertain kinds of medical images, such as angiograms, have clear foreground and\nbackground objects. We give theoretical explanation to justify our methods. We\nshow the potential use of our method in clinical settings through evaluating\nits efficacy for aiding the detection of lesions in coronary angiograms.",
        "translated": ""
    },
    {
        "title": "A Systematic Review of Few-Shot Learning in Medical Imaging",
        "url": "http://arxiv.org/abs/2309.11433v1",
        "pub_date": "2023-09-20",
        "summary": "The lack of annotated medical images limits the performance of deep learning\nmodels, which usually need large-scale labelled datasets. Few-shot learning\ntechniques can reduce data scarcity issues and enhance medical image analysis,\nespecially with meta-learning. This systematic review gives a comprehensive\noverview of few-shot learning in medical imaging. We searched the literature\nsystematically and selected 80 relevant articles published from 2018 to 2023.\nWe clustered the articles based on medical outcomes, such as tumour\nsegmentation, disease classification, and image registration; anatomical\nstructure investigated (i.e. heart, lung, etc.); and the meta-learning method\nused. For each cluster, we examined the papers' distributions and the results\nprovided by the state-of-the-art. In addition, we identified a generic pipeline\nshared among all the studies. The review shows that few-shot learning can\novercome data scarcity in most outcomes and that meta-learning is a popular\nchoice to perform few-shot learning because it can adapt to new tasks with few\nlabelled samples. In addition, following meta-learning, supervised learning and\nsemi-supervised learning stand out as the predominant techniques employed to\ntackle few-shot learning challenges in medical imaging and also best\nperforming. Lastly, we observed that the primary application areas\npredominantly encompass cardiac, pulmonary, and abdominal domains. This\nsystematic review aims to inspire further research to improve medical image\nanalysis and patient care.",
        "translated": ""
    },
    {
        "title": "CalibFPA: A Focal Plane Array Imaging System based on Online\n  Deep-Learning Calibration",
        "url": "http://arxiv.org/abs/2309.11421v1",
        "pub_date": "2023-09-20",
        "summary": "Compressive focal plane arrays (FPA) enable cost-effective high-resolution\n(HR) imaging by acquisition of several multiplexed measurements on a\nlow-resolution (LR) sensor. Multiplexed encoding of the visual scene is\ntypically performed via electronically controllable spatial light modulators\n(SLM). An HR image is then reconstructed from the encoded measurements by\nsolving an inverse problem that involves the forward model of the imaging\nsystem. To capture system non-idealities such as optical aberrations, a\nmainstream approach is to conduct an offline calibration scan to measure the\nsystem response for a point source at each spatial location on the imaging\ngrid. However, it is challenging to run calibration scans when using structured\nSLMs as they cannot encode individual grid locations. In this study, we propose\na novel compressive FPA system based on online deep-learning calibration of\nmultiplexed LR measurements (CalibFPA). We introduce a piezo-stage that\nlocomotes a pre-printed fixed coded aperture. A deep neural network is then\nleveraged to correct for the influences of system non-idealities in multiplexed\nmeasurements without the need for offline calibration scans. Finally, a deep\nplug-and-play algorithm is used to reconstruct images from corrected\nmeasurements. On simulated and experimental datasets, we demonstrate that\nCalibFPA outperforms state-of-the-art compressive FPA methods. We also report\nanalyses to validate the design elements in CalibFPA and assess computational\ncomplexity.",
        "translated": ""
    },
    {
        "title": "Kosmos-2.5: A Multimodal Literate Model",
        "url": "http://arxiv.org/abs/2309.11419v1",
        "pub_date": "2023-09-20",
        "summary": "We present Kosmos-2.5, a multimodal literate model for machine reading of\ntext-intensive images. Pre-trained on large-scale text-intensive images,\nKosmos-2.5 excels in two distinct yet cooperative transcription tasks: (1)\ngenerating spatially-aware text blocks, where each block of text is assigned\nits spatial coordinates within the image, and (2) producing structured text\noutput that captures styles and structures into the markdown format. This\nunified multimodal literate capability is achieved through a shared Transformer\narchitecture, task-specific prompts, and flexible text representations. We\nevaluate Kosmos-2.5 on end-to-end document-level text recognition and\nimage-to-markdown text generation. Furthermore, the model can be readily\nadapted for any text-intensive image understanding task with different prompts\nthrough supervised fine-tuning, making it a general-purpose tool for real-world\napplications involving text-rich images. This work also paves the way for the\nfuture scaling of multimodal large language models.",
        "translated": ""
    },
    {
        "title": "Active Stereo Without Pattern Projector",
        "url": "http://arxiv.org/abs/2309.12315v1",
        "pub_date": "2023-09-21",
        "summary": "This paper proposes a novel framework integrating the principles of active\nstereo in standard passive camera systems without a physical pattern projector.\nWe virtually project a pattern over the left and right images according to the\nsparse measurements obtained from a depth sensor. Any such devices can be\nseamlessly plugged into our framework, allowing for the deployment of a virtual\nactive stereo setup in any possible environment, overcoming the limitation of\npattern projectors, such as limited working range or environmental conditions.\nExperiments on indoor/outdoor datasets, featuring both long and close-range,\nsupport the seamless effectiveness of our approach, boosting the accuracy of\nboth stereo algorithms and deep networks.",
        "translated": ""
    },
    {
        "title": "TinyCLIP: CLIP Distillation via Affinity Mimicking and Weight\n  Inheritance",
        "url": "http://arxiv.org/abs/2309.12314v1",
        "pub_date": "2023-09-21",
        "summary": "In this paper, we propose a novel cross-modal distillation method, called\nTinyCLIP, for large-scale language-image pre-trained models. The method\nintroduces two core techniques: affinity mimicking and weight inheritance.\nAffinity mimicking explores the interaction between modalities during\ndistillation, enabling student models to mimic teachers' behavior of learning\ncross-modal feature alignment in a visual-linguistic affinity space. Weight\ninheritance transmits the pre-trained weights from the teacher models to their\nstudent counterparts to improve distillation efficiency. Moreover, we extend\nthe method into a multi-stage progressive distillation to mitigate the loss of\ninformative weights during extreme compression. Comprehensive experiments\ndemonstrate the efficacy of TinyCLIP, showing that it can reduce the size of\nthe pre-trained CLIP ViT-B/32 by 50%, while maintaining comparable zero-shot\nperformance. While aiming for comparable performance, distillation with weight\ninheritance can speed up the training by 1.4 - 7.8 $\\times$ compared to\ntraining from scratch. Moreover, our TinyCLIP ViT-8M/16, trained on YFCC-15M,\nachieves an impressive zero-shot top-1 accuracy of 41.1% on ImageNet,\nsurpassing the original CLIP ViT-B/16 by 3.5% while utilizing only 8.9%\nparameters. Finally, we demonstrate the good transferability of TinyCLIP in\nvarious downstream tasks. Code and models will be open-sourced at\nhttps://aka.ms/tinyclip.",
        "translated": ""
    },
    {
        "title": "ForceSight: Text-Guided Mobile Manipulation with Visual-Force Goals",
        "url": "http://arxiv.org/abs/2309.12312v1",
        "pub_date": "2023-09-21",
        "summary": "We present ForceSight, a system for text-guided mobile manipulation that\npredicts visual-force goals using a deep neural network. Given a single RGBD\nimage combined with a text prompt, ForceSight determines a target end-effector\npose in the camera frame (kinematic goal) and the associated forces (force\ngoal). Together, these two components form a visual-force goal. Prior work has\ndemonstrated that deep models outputting human-interpretable kinematic goals\ncan enable dexterous manipulation by real robots. Forces are critical to\nmanipulation, yet have typically been relegated to lower-level execution in\nthese systems. When deployed on a mobile manipulator equipped with an\neye-in-hand RGBD camera, ForceSight performed tasks such as precision grasps,\ndrawer opening, and object handovers with an 81% success rate in unseen\nenvironments with object instances that differed significantly from the\ntraining data. In a separate experiment, relying exclusively on visual servoing\nand ignoring force goals dropped the success rate from 90% to 45%,\ndemonstrating that force goals can significantly enhance performance. The\nappendix, videos, code, and trained models are available at\nhttps://force-sight.github.io/.",
        "translated": ""
    },
    {
        "title": "LLM-Grounder: Open-Vocabulary 3D Visual Grounding with Large Language\n  Model as an Agent",
        "url": "http://arxiv.org/abs/2309.12311v1",
        "pub_date": "2023-09-21",
        "summary": "3D visual grounding is a critical skill for household robots, enabling them\nto navigate, manipulate objects, and answer questions based on their\nenvironment. While existing approaches often rely on extensive labeled data or\nexhibit limitations in handling complex language queries, we propose\nLLM-Grounder, a novel zero-shot, open-vocabulary, Large Language Model\n(LLM)-based 3D visual grounding pipeline. LLM-Grounder utilizes an LLM to\ndecompose complex natural language queries into semantic constituents and\nemploys a visual grounding tool, such as OpenScene or LERF, to identify objects\nin a 3D scene. The LLM then evaluates the spatial and commonsense relations\namong the proposed objects to make a final grounding decision. Our method does\nnot require any labeled training data and can generalize to novel 3D scenes and\narbitrary text queries. We evaluate LLM-Grounder on the ScanRefer benchmark and\ndemonstrate state-of-the-art zero-shot grounding accuracy. Our findings\nindicate that LLMs significantly improve the grounding capability, especially\nfor complex language queries, making LLM-Grounder an effective approach for 3D\nvision-language tasks in robotics. Videos and interactive demos can be found on\nthe project website https://chat-with-nerf.github.io/ .",
        "translated": ""
    },
    {
        "title": "TalkNCE: Improving Active Speaker Detection with Talk-Aware Contrastive\n  Learning",
        "url": "http://arxiv.org/abs/2309.12306v1",
        "pub_date": "2023-09-21",
        "summary": "The goal of this work is Active Speaker Detection (ASD), a task to determine\nwhether a person is speaking or not in a series of video frames. Previous works\nhave dealt with the task by exploring network architectures while learning\neffective representations has been less explored. In this work, we propose\nTalkNCE, a novel talk-aware contrastive loss. The loss is only applied to part\nof the full segments where a person on the screen is actually speaking. This\nencourages the model to learn effective representations through the natural\ncorrespondence of speech and facial movements. Our loss can be jointly\noptimized with the existing objectives for training ASD models without the need\nfor additional supervision or training data. The experiments demonstrate that\nour loss can be easily integrated into the existing ASD frameworks, improving\ntheir performance. Our method achieves state-of-the-art performances on\nAVA-ActiveSpeaker and ASW datasets.",
        "translated": ""
    },
    {
        "title": "SlowFast Network for Continuous Sign Language Recognition",
        "url": "http://arxiv.org/abs/2309.12304v1",
        "pub_date": "2023-09-21",
        "summary": "The objective of this work is the effective extraction of spatial and dynamic\nfeatures for Continuous Sign Language Recognition (CSLR). To accomplish this,\nwe utilise a two-pathway SlowFast network, where each pathway operates at\ndistinct temporal resolutions to separately capture spatial (hand shapes,\nfacial expressions) and dynamic (movements) information. In addition, we\nintroduce two distinct feature fusion methods, carefully designed for the\ncharacteristics of CSLR: (1) Bi-directional Feature Fusion (BFF), which\nfacilitates the transfer of dynamic semantics into spatial semantics and vice\nversa; and (2) Pathway Feature Enhancement (PFE), which enriches dynamic and\nspatial representations through auxiliary subnetworks, while avoiding the need\nfor extra inference time. As a result, our model further strengthens spatial\nand dynamic representations in parallel. We demonstrate that the proposed\nframework outperforms the current state-of-the-art performance on popular CSLR\ndatasets, including PHOENIX14, PHOENIX14-T, and CSL-Daily.",
        "translated": ""
    },
    {
        "title": "PanoVOS:Bridging Non-panoramic and Panoramic Views with Transformer for\n  Video Segmentation",
        "url": "http://arxiv.org/abs/2309.12303v1",
        "pub_date": "2023-09-21",
        "summary": "Panoramic videos contain richer spatial information and have attracted\ntremendous amounts of attention due to their exceptional experience in some\nfields such as autonomous driving and virtual reality. However, existing\ndatasets for video segmentation only focus on conventional planar images. To\naddress the challenge, in this paper, we present a panoramic video dataset,\nPanoVOS. The dataset provides 150 videos with high video resolutions and\ndiverse motions. To quantify the domain gap between 2D planar videos and\npanoramic videos, we evaluate 15 off-the-shelf video object segmentation (VOS)\nmodels on PanoVOS. Through error analysis, we found that all of them fail to\ntackle pixel-level content discontinues of panoramic videos. Thus, we present a\nPanoramic Space Consistency Transformer (PSCFormer), which can effectively\nutilize the semantic boundary information of the previous frame for pixel-level\nmatching with the current frame. Extensive experiments demonstrate that\ncompared with the previous SOTA models, our PSCFormer network exhibits a great\nadvantage in terms of segmentation results under the panoramic setting. Our\ndataset poses new challenges in panoramic VOS and we hope that our PanoVOS can\nadvance the development of panoramic segmentation/tracking.",
        "translated": ""
    },
    {
        "title": "Text-Guided Vector Graphics Customization",
        "url": "http://arxiv.org/abs/2309.12302v1",
        "pub_date": "2023-09-21",
        "summary": "Vector graphics are widely used in digital art and valued by designers for\ntheir scalability and layer-wise topological properties. However, the creation\nand editing of vector graphics necessitate creativity and design expertise,\nleading to a time-consuming process. In this paper, we propose a novel pipeline\nthat generates high-quality customized vector graphics based on textual prompts\nwhile preserving the properties and layer-wise information of a given exemplar\nSVG. Our method harnesses the capabilities of large pre-trained text-to-image\nmodels. By fine-tuning the cross-attention layers of the model, we generate\ncustomized raster images guided by textual prompts. To initialize the SVG, we\nintroduce a semantic-based path alignment method that preserves and transforms\ncrucial paths from the exemplar SVG. Additionally, we optimize path parameters\nusing both image-level and vector-level losses, ensuring smooth shape\ndeformation while aligning with the customized raster image. We extensively\nevaluate our method using multiple metrics from vector-level, image-level, and\ntext-level perspectives. The evaluation results demonstrate the effectiveness\nof our pipeline in generating diverse customizations of vector graphics with\nexceptional quality. The project page is\nhttps://intchous.github.io/SVGCustomization.",
        "translated": ""
    },
    {
        "title": "Environment-biased Feature Ranking for Novelty Detection Robustness",
        "url": "http://arxiv.org/abs/2309.12301v1",
        "pub_date": "2023-09-21",
        "summary": "We tackle the problem of robust novelty detection, where we aim to detect\nnovelties in terms of semantic content while being invariant to changes in\nother, irrelevant factors. Specifically, we operate in a setup with multiple\nenvironments, where we determine the set of features that are associated more\nwith the environments, rather than to the content relevant for the task. Thus,\nwe propose a method that starts with a pretrained embedding and a multi-env\nsetup and manages to rank the features based on their environment-focus. First,\nwe compute a per-feature score based on the feature distribution variance\nbetween envs. Next, we show that by dropping the highly scored ones, we manage\nto remove spurious correlations and improve the overall performance by up to\n6%, both in covariance and sub-population shift cases, both for a real and a\nsynthetic benchmark, that we introduce for this task.",
        "translated": ""
    },
    {
        "title": "See to Touch: Learning Tactile Dexterity through Visual Incentives",
        "url": "http://arxiv.org/abs/2309.12300v1",
        "pub_date": "2023-09-21",
        "summary": "Equipping multi-fingered robots with tactile sensing is crucial for achieving\nthe precise, contact-rich, and dexterous manipulation that humans excel at.\nHowever, relying solely on tactile sensing fails to provide adequate cues for\nreasoning about objects' spatial configurations, limiting the ability to\ncorrect errors and adapt to changing situations. In this paper, we present\nTactile Adaptation from Visual Incentives (TAVI), a new framework that enhances\ntactile-based dexterity by optimizing dexterous policies using vision-based\nrewards. First, we use a contrastive-based objective to learn visual\nrepresentations. Next, we construct a reward function using these visual\nrepresentations through optimal-transport based matching on one human\ndemonstration. Finally, we use online reinforcement learning on our robot to\noptimize tactile-based policies that maximize the visual reward. On six\nchallenging tasks, such as peg pick-and-place, unstacking bowls, and flipping\nslender objects, TAVI achieves a success rate of 73% using our four-fingered\nAllegro robot hand. The increase in performance is 108% higher than policies\nusing tactile and vision-based rewards and 135% higher than policies without\ntactile observational input. Robot videos are best viewed on our project\nwebsite: https://see-to-touch.github.io/.",
        "translated": ""
    },
    {
        "title": "PanoVOS: Bridging Non-panoramic and Panoramic Views with Transformer for\n  Video Segmentation",
        "url": "http://arxiv.org/abs/2309.12303v2",
        "pub_date": "2023-09-21",
        "summary": "Panoramic videos contain richer spatial information and have attracted\ntremendous amounts of attention due to their exceptional experience in some\nfields such as autonomous driving and virtual reality. However, existing\ndatasets for video segmentation only focus on conventional planar images. To\naddress the challenge, in this paper, we present a panoramic video dataset,\nPanoVOS. The dataset provides 150 videos with high video resolutions and\ndiverse motions. To quantify the domain gap between 2D planar videos and\npanoramic videos, we evaluate 15 off-the-shelf video object segmentation (VOS)\nmodels on PanoVOS. Through error analysis, we found that all of them fail to\ntackle pixel-level content discontinues of panoramic videos. Thus, we present a\nPanoramic Space Consistency Transformer (PSCFormer), which can effectively\nutilize the semantic boundary information of the previous frame for pixel-level\nmatching with the current frame. Extensive experiments demonstrate that\ncompared with the previous SOTA models, our PSCFormer network exhibits a great\nadvantage in terms of segmentation results under the panoramic setting. Our\ndataset poses new challenges in panoramic VOS and we hope that our PanoVOS can\nadvance the development of panoramic segmentation/tracking.",
        "translated": ""
    },
    {
        "title": "MosaicFusion: Diffusion Models as Data Augmenters for Large Vocabulary\n  Instance Segmentation",
        "url": "http://arxiv.org/abs/2309.13042v1",
        "pub_date": "2023-09-22",
        "summary": "We present MosaicFusion, a simple yet effective diffusion-based data\naugmentation approach for large vocabulary instance segmentation. Our method is\ntraining-free and does not rely on any label supervision. Two key designs\nenable us to employ an off-the-shelf text-to-image diffusion model as a useful\ndataset generator for object instances and mask annotations. First, we divide\nan image canvas into several regions and perform a single round of diffusion\nprocess to generate multiple instances simultaneously, conditioning on\ndifferent text prompts. Second, we obtain corresponding instance masks by\naggregating cross-attention maps associated with object prompts across layers\nand diffusion time steps, followed by simple thresholding and edge-aware\nrefinement processing. Without bells and whistles, our MosaicFusion can produce\na significant amount of synthetic labeled data for both rare and novel\ncategories. Experimental results on the challenging LVIS long-tailed and\nopen-vocabulary benchmarks demonstrate that MosaicFusion can significantly\nimprove the performance of existing instance segmentation models, especially\nfor rare and novel categories. Code will be released at\nhttps://github.com/Jiahao000/MosaicFusion.",
        "translated": ""
    },
    {
        "title": "Robotic Offline RL from Internet Videos via Value-Function Pre-Training",
        "url": "http://arxiv.org/abs/2309.13041v1",
        "pub_date": "2023-09-22",
        "summary": "Pre-training on Internet data has proven to be a key ingredient for broad\ngeneralization in many modern ML systems. What would it take to enable such\ncapabilities in robotic reinforcement learning (RL)? Offline RL methods, which\nlearn from datasets of robot experience, offer one way to leverage prior data\ninto the robotic learning pipeline. However, these methods have a \"type\nmismatch\" with video data (such as Ego4D), the largest prior datasets available\nfor robotics, since video offers observation-only experience without the action\nor reward annotations needed for RL methods. In this paper, we develop a system\nfor leveraging large-scale human video datasets in robotic offline RL, based\nentirely on learning value functions via temporal-difference learning. We show\nthat value learning on video datasets learns representations that are more\nconducive to downstream robotic offline RL than other approaches for learning\nfrom video data. Our system, called V-PTR, combines the benefits of\npre-training on video data with robotic offline RL approaches that train on\ndiverse robot data, resulting in value functions and policies for manipulation\ntasks that perform better, act robustly, and generalize broadly. On several\nmanipulation tasks on a real WidowX robot, our framework produces policies that\ngreatly improve over prior methods. Our video and additional details can be\nfound at https://dibyaghosh.com/vptr/",
        "translated": ""
    },
    {
        "title": "NeRRF: 3D Reconstruction and View Synthesis for Transparent and Specular\n  Objects with Neural Refractive-Reflective Fields",
        "url": "http://arxiv.org/abs/2309.13039v1",
        "pub_date": "2023-09-22",
        "summary": "Neural radiance fields (NeRF) have revolutionized the field of image-based\nview synthesis. However, NeRF uses straight rays and fails to deal with\ncomplicated light path changes caused by refraction and reflection. This\nprevents NeRF from successfully synthesizing transparent or specular objects,\nwhich are ubiquitous in real-world robotics and A/VR applications. In this\npaper, we introduce the refractive-reflective field. Taking the object\nsilhouette as input, we first utilize marching tetrahedra with a progressive\nencoding to reconstruct the geometry of non-Lambertian objects and then model\nrefraction and reflection effects of the object in a unified framework using\nFresnel terms. Meanwhile, to achieve efficient and effective anti-aliasing, we\npropose a virtual cone supersampling technique. We benchmark our method on\ndifferent shapes, backgrounds and Fresnel terms on both real-world and\nsynthetic datasets. We also qualitatively and quantitatively benchmark the\nrendering results of various editing applications, including material editing,\nobject replacement/insertion, and environment illumination estimation. Codes\nand data are publicly available at https://github.com/dawning77/NeRRF.",
        "translated": ""
    },
    {
        "title": "Privacy Assessment on Reconstructed Images: Are Existing Evaluation\n  Metrics Faithful to Human Perception?",
        "url": "http://arxiv.org/abs/2309.13038v1",
        "pub_date": "2023-09-22",
        "summary": "Hand-crafted image quality metrics, such as PSNR and SSIM, are commonly used\nto evaluate model privacy risk under reconstruction attacks. Under these\nmetrics, reconstructed images that are determined to resemble the original one\ngenerally indicate more privacy leakage. Images determined as overall\ndissimilar, on the other hand, indicate higher robustness against attack.\nHowever, there is no guarantee that these metrics well reflect human opinions,\nwhich, as a judgement for model privacy leakage, are more trustworthy. In this\npaper, we comprehensively study the faithfulness of these hand-crafted metrics\nto human perception of privacy information from the reconstructed images. On 5\ndatasets ranging from natural images, faces, to fine-grained classes, we use 4\nexisting attack methods to reconstruct images from many different\nclassification models and, for each reconstructed image, we ask multiple human\nannotators to assess whether this image is recognizable. Our studies reveal\nthat the hand-crafted metrics only have a weak correlation with the human\nevaluation of privacy leakage and that even these metrics themselves often\ncontradict each other. These observations suggest risks of current metrics in\nthe community. To address this potential risk, we propose a learning-based\nmeasure called SemSim to evaluate the Semantic Similarity between the original\nand reconstructed images. SemSim is trained with a standard triplet loss, using\nan original image as an anchor, one of its recognizable reconstructed images as\na positive sample, and an unrecognizable one as a negative. By training on\nhuman annotations, SemSim exhibits a greater reflection of privacy leakage on\nthe semantic level. We show that SemSim has a significantly higher correlation\nwith human judgment compared with existing metrics. Moreover, this strong\ncorrelation generalizes to unseen datasets, models and attack methods.",
        "translated": ""
    },
    {
        "title": "Performance Analysis of UNet and Variants for Medical Image Segmentation",
        "url": "http://arxiv.org/abs/2309.13013v1",
        "pub_date": "2023-09-22",
        "summary": "Medical imaging plays a crucial role in modern healthcare by providing\nnon-invasive visualisation of internal structures and abnormalities, enabling\nearly disease detection, accurate diagnosis, and treatment planning. This study\naims to explore the application of deep learning models, particularly focusing\non the UNet architecture and its variants, in medical image segmentation. We\nseek to evaluate the performance of these models across various challenging\nmedical image segmentation tasks, addressing issues such as image\nnormalization, resizing, architecture choices, loss function design, and\nhyperparameter tuning. The findings reveal that the standard UNet, when\nextended with a deep network layer, is a proficient medical image segmentation\nmodel, while the Res-UNet and Attention Res-UNet architectures demonstrate\nsmoother convergence and superior performance, particularly when handling fine\nimage details. The study also addresses the challenge of high class imbalance\nthrough careful preprocessing and loss function definitions. We anticipate that\nthe results of this study will provide useful insights for researchers seeking\nto apply these models to new medical imaging problems and offer guidance and\nbest practices for their implementation.",
        "translated": ""
    },
    {
        "title": "Deep3DSketch+: Rapid 3D Modeling from Single Free-hand Sketches",
        "url": "http://arxiv.org/abs/2309.13006v1",
        "pub_date": "2023-09-22",
        "summary": "The rapid development of AR/VR brings tremendous demands for 3D content.\nWhile the widely-used Computer-Aided Design (CAD) method requires a\ntime-consuming and labor-intensive modeling process, sketch-based 3D modeling\noffers a potential solution as a natural form of computer-human interaction.\nHowever, the sparsity and ambiguity of sketches make it challenging to generate\nhigh-fidelity content reflecting creators' ideas. Precise drawing from multiple\nviews or strategic step-by-step drawings is often required to tackle the\nchallenge but is not friendly to novice users. In this work, we introduce a\nnovel end-to-end approach, Deep3DSketch+, which performs 3D modeling using only\na single free-hand sketch without inputting multiple sketches or view\ninformation. Specifically, we introduce a lightweight generation network for\nefficient inference in real-time and a structural-aware adversarial training\napproach with a Stroke Enhancement Module (SEM) to capture the structural\ninformation to facilitate learning of the realistic and fine-detailed shape\nstructures for high-fidelity performance. Extensive experiments demonstrated\nthe effectiveness of our approach with the state-of-the-art (SOTA) performance\non both synthetic and real datasets.",
        "translated": ""
    },
    {
        "title": "Point Cloud Network: An Order of Magnitude Improvement in Linear Layer\n  Parameter Count",
        "url": "http://arxiv.org/abs/2309.12996v1",
        "pub_date": "2023-09-22",
        "summary": "This paper introduces the Point Cloud Network (PCN) architecture, a novel\nimplementation of linear layers in deep learning networks, and provides\nempirical evidence to advocate for its preference over the Multilayer\nPerceptron (MLP) in linear layers. We train several models, including the\noriginal AlexNet, using both MLP and PCN architectures for direct comparison of\nlinear layers (Krizhevsky et al., 2012). The key results collected are model\nparameter count and top-1 test accuracy over the CIFAR-10 and CIFAR-100\ndatasets (Krizhevsky, 2009). AlexNet-PCN16, our PCN equivalent to AlexNet,\nachieves comparable efficacy (test accuracy) to the original architecture with\na 99.5% reduction of parameters in its linear layers. All training is done on\ncloud RTX 4090 GPUs, leveraging pytorch for model construction and training.\nCode is provided for anyone to reproduce the trials from this paper.",
        "translated": ""
    },
    {
        "title": "License Plate Recognition Based On Multi-Angle View Model",
        "url": "http://arxiv.org/abs/2309.12972v1",
        "pub_date": "2023-09-22",
        "summary": "In the realm of research, the detection/recognition of text within\nimages/videos captured by cameras constitutes a highly challenging problem for\nresearchers. Despite certain advancements achieving high accuracy, current\nmethods still require substantial improvements to be applicable in practical\nscenarios. Diverging from text detection in images/videos, this paper addresses\nthe issue of text detection within license plates by amalgamating multiple\nframes of distinct perspectives. For each viewpoint, the proposed method\nextracts descriptive features characterizing the text components of the license\nplate, specifically corner points and area. Concretely, we present three\nviewpoints: view-1, view-2, and view-3, to identify the nearest neighboring\ncomponents facilitating the restoration of text components from the same\nlicense plate line based on estimations of similarity levels and distance\nmetrics. Subsequently, we employ the CnOCR method for text recognition within\nlicense plates. Experimental results on the self-collected dataset\n(PTITPlates), comprising pairs of images in various scenarios, and the publicly\navailable Stanford Cars Dataset, demonstrate the superiority of the proposed\nmethod over existing approaches.",
        "translated": ""
    },
    {
        "title": "PI-RADS v2 Compliant Automated Segmentation of Prostate Zones Using\n  co-training Motivated Multi-task Dual-Path CNN",
        "url": "http://arxiv.org/abs/2309.12970v1",
        "pub_date": "2023-09-22",
        "summary": "The detailed images produced by Magnetic Resonance Imaging (MRI) provide\nlife-critical information for the diagnosis and treatment of prostate cancer.\nTo provide standardized acquisition, interpretation and usage of the complex\nMRI images, the PI-RADS v2 guideline was proposed. An automated segmentation\nfollowing the guideline facilitates consistent and precise lesion detection,\nstaging and treatment. The guideline recommends a division of the prostate into\nfour zones, PZ (peripheral zone), TZ (transition zone), DPU (distal prostatic\nurethra) and AFS (anterior fibromuscular stroma). Not every zone shares a\nboundary with the others and is present in every slice. Further, the\nrepresentations captured by a single model might not suffice for all zones.\nThis motivated us to design a dual-branch convolutional neural network (CNN),\nwhere each branch captures the representations of the connected zones\nseparately. Further, the representations from different branches act\ncomplementary to each other at the second stage of training, where they are\nfine-tuned through an unsupervised loss. The loss penalises the difference in\npredictions from the two branches for the same class. We also incorporate\nmulti-task learning in our framework to further improve the segmentation\naccuracy. The proposed approach improves the segmentation accuracy of the\nbaseline (mean absolute symmetric distance) by 7.56%, 11.00%, 58.43% and 19.67%\nfor PZ, TZ, DPU and AFS zones respectively.",
        "translated": ""
    },
    {
        "title": "Detect Every Thing with Few Examples",
        "url": "http://arxiv.org/abs/2309.12969v1",
        "pub_date": "2023-09-22",
        "summary": "Open-set object detection aims at detecting arbitrary categories beyond those\nseen during training. Most recent advancements have adopted the open-vocabulary\nparadigm, utilizing vision-language backbones to represent categories with\nlanguage. In this paper, we introduce DE-ViT, an open-set object detector that\nemploys vision-only DINOv2 backbones and learns new categories through example\nimages instead of language. To improve general detection ability, we transform\nmulti-classification tasks into binary classification tasks while bypassing\nper-class inference, and propose a novel region propagation technique for\nlocalization. We evaluate DE-ViT on open-vocabulary, few-shot, and one-shot\nobject detection benchmark with COCO and LVIS. For COCO, DE-ViT outperforms the\nopen-vocabulary SoTA by 6.9 AP50 and achieves 50 AP50 in novel classes. DE-ViT\nsurpasses the few-shot SoTA by 15 mAP on 10-shot and 7.2 mAP on 30-shot and\none-shot SoTA by 2.8 AP50. For LVIS, DE-ViT outperforms the open-vocabulary\nSoTA by 2.2 mask AP and reaches 34.3 mask APr. Code is available at\nhttps://github.com/mlzxy/devit.",
        "translated": ""
    },
    {
        "title": "Extreme Parkour with Legged Robots",
        "url": "http://arxiv.org/abs/2309.14341v1",
        "pub_date": "2023-09-25",
        "summary": "Humans can perform parkour by traversing obstacles in a highly dynamic\nfashion requiring precise eye-muscle coordination and movement. Getting robots\nto do the same task requires overcoming similar challenges. Classically, this\nis done by independently engineering perception, actuation, and control systems\nto very low tolerances. This restricts them to tightly controlled settings such\nas a predetermined obstacle course in labs. In contrast, humans are able to\nlearn parkour through practice without significantly changing their underlying\nbiology. In this paper, we take a similar approach to developing robot parkour\non a small low-cost robot with imprecise actuation and a single front-facing\ndepth camera for perception which is low-frequency, jittery, and prone to\nartifacts. We show how a single neural net policy operating directly from a\ncamera image, trained in simulation with large-scale RL, can overcome imprecise\nsensing and actuation to output highly precise control behavior end-to-end. We\nshow our robot can perform a high jump on obstacles 2x its height, long jump\nacross gaps 2x its length, do a handstand and run across tilted ramps, and\ngeneralize to novel obstacle courses with different physical properties.\nParkour videos at https://extreme-parkour.github.io/",
        "translated": ""
    },
    {
        "title": "Chop &amp; Learn: Recognizing and Generating Object-State Compositions",
        "url": "http://arxiv.org/abs/2309.14339v1",
        "pub_date": "2023-09-25",
        "summary": "Recognizing and generating object-state compositions has been a challenging\ntask, especially when generalizing to unseen compositions. In this paper, we\nstudy the task of cutting objects in different styles and the resulting object\nstate changes. We propose a new benchmark suite Chop &amp; Learn, to accommodate\nthe needs of learning objects and different cut styles using multiple\nviewpoints. We also propose a new task of Compositional Image Generation, which\ncan transfer learned cut styles to different objects, by generating novel\nobject-state images. Moreover, we also use the videos for Compositional Action\nRecognition, and show valuable uses of this dataset for multiple video tasks.\nProject website: https://chopnlearn.github.io.",
        "translated": ""
    },
    {
        "title": "3D Indoor Instance Segmentation in an Open-World",
        "url": "http://arxiv.org/abs/2309.14338v1",
        "pub_date": "2023-09-25",
        "summary": "Existing 3D instance segmentation methods typically assume that all semantic\nclasses to be segmented would be available during training and only seen\ncategories are segmented at inference. We argue that such a closed-world\nassumption is restrictive and explore for the first time 3D indoor instance\nsegmentation in an open-world setting, where the model is allowed to\ndistinguish a set of known classes as well as identify an unknown object as\nunknown and then later incrementally learning the semantic category of the\nunknown when the corresponding category labels are available. To this end, we\nintroduce an open-world 3D indoor instance segmentation method, where an\nauto-labeling scheme is employed to produce pseudo-labels during training and\ninduce separation to separate known and unknown category labels. We further\nimprove the pseudo-labels quality at inference by adjusting the unknown class\nprobability based on the objectness score distribution. We also introduce\ncarefully curated open-world splits leveraging realistic scenarios based on\ninherent object distribution, region-based indoor scene exploration and\nrandomness aspect of open-world classes. Extensive experiments reveal the\nefficacy of the proposed contributions leading to promising open-world 3D\ninstance segmentation performance.",
        "translated": ""
    },
    {
        "title": "UnitedHuman: Harnessing Multi-Source Data for High-Resolution Human\n  Generation",
        "url": "http://arxiv.org/abs/2309.14335v1",
        "pub_date": "2023-09-25",
        "summary": "Human generation has achieved significant progress. Nonetheless, existing\nmethods still struggle to synthesize specific regions such as faces and hands.\nWe argue that the main reason is rooted in the training data. A holistic human\ndataset inevitably has insufficient and low-resolution information on local\nparts. Therefore, we propose to use multi-source datasets with various\nresolution images to jointly learn a high-resolution human generative model.\nHowever, multi-source data inherently a) contains different parts that do not\nspatially align into a coherent human, and b) comes with different scales. To\ntackle these challenges, we propose an end-to-end framework, UnitedHuman, that\nempowers continuous GAN with the ability to effectively utilize multi-source\ndata for high-resolution human generation. Specifically, 1) we design a\nMulti-Source Spatial Transformer that spatially aligns multi-source images to\nfull-body space with a human parametric model. 2) Next, a continuous GAN is\nproposed with global-structural guidance and CutMix consistency. Patches from\ndifferent datasets are then sampled and transformed to supervise the training\nof this scale-invariant generative model. Extensive experiments demonstrate\nthat our model jointly learned from multi-source data achieves superior quality\nthan those learned from a holistic dataset.",
        "translated": ""
    },
    {
        "title": "Noise-in, Bias-out: Balanced and Real-time MoCap Solving",
        "url": "http://arxiv.org/abs/2309.14330v1",
        "pub_date": "2023-09-25",
        "summary": "Real-time optical Motion Capture (MoCap) systems have not benefited from the\nadvances in modern data-driven modeling. In this work we apply machine learning\nto solve noisy unstructured marker estimates in real-time and deliver robust\nmarker-based MoCap even when using sparse affordable sensors. To achieve this\nwe focus on a number of challenges related to model training, namely the\nsourcing of training data and their long-tailed distribution. Leveraging\nrepresentation learning we design a technique for imbalanced regression that\nrequires no additional data or labels and improves the performance of our model\nin rare and challenging poses. By relying on a unified representation, we show\nthat training such a model is not bound to high-end MoCap training data\nacquisition, and exploit the advances in marker-less MoCap to acquire the\nnecessary data. Finally, we take a step towards richer and affordable MoCap by\nadapting a body model-based inverse kinematics solution to account for\nmeasurement and inference uncertainty, further improving performance and\nrobustness. Project page: https://moverseai.github.io/noise-tail",
        "translated": ""
    },
    {
        "title": "DeepSpeed-VisualChat: Multi-Round Multi-Image Interleave Chat via\n  Multi-Modal Causal Attention",
        "url": "http://arxiv.org/abs/2309.14327v1",
        "pub_date": "2023-09-25",
        "summary": "Most of the existing multi-modal models, hindered by their incapacity to\nadeptly manage interleaved image-and-text inputs in multi-image, multi-round\ndialogues, face substantial constraints in resource allocation for training and\ndata accessibility, impacting their adaptability and scalability across varied\ninteraction realms. To address this, we present the DeepSpeed-VisualChat\nframework, designed to optimize Large Language Models (LLMs) by incorporating\nmulti-modal capabilities, with a focus on enhancing the proficiency of Large\nVision and Language Models in handling interleaved inputs. Our framework is\nnotable for (1) its open-source support for multi-round and multi-image\ndialogues, (2) introducing an innovative multi-modal causal attention\nmechanism, and (3) utilizing data blending techniques on existing datasets to\nassure seamless interactions in multi-round, multi-image conversations.\nCompared to existing frameworks, DeepSpeed-VisualChat shows superior\nscalability up to 70B parameter language model size, representing a significant\nadvancement in multi-modal language models and setting a solid foundation for\nfuture explorations.",
        "translated": ""
    },
    {
        "title": "Multiple Different Explanations for Image Classifiers",
        "url": "http://arxiv.org/abs/2309.14309v1",
        "pub_date": "2023-09-25",
        "summary": "Existing explanation tools for image classifiers usually give only one single\nexplanation for an image. For many images, however, both humans and image\nclassifiers accept more than one explanation for the image label. Thus,\nrestricting the number of explanations to just one severely limits the insight\ninto the behavior of the classifier. In this paper, we describe an algorithm\nand a tool, REX, for computing multiple explanations of the output of a\nblack-box image classifier for a given image. Our algorithm uses a principled\napproach based on causal theory. We analyse its theoretical complexity and\nprovide experimental results showing that REX finds multiple explanations on 7\ntimes more images than the previous work on the ImageNet-mini benchmark.",
        "translated": ""
    },
    {
        "title": "DeepMesh: Mesh-based Cardiac Motion Tracking using Deep Learning",
        "url": "http://arxiv.org/abs/2309.14306v1",
        "pub_date": "2023-09-25",
        "summary": "3D motion estimation from cine cardiac magnetic resonance (CMR) images is\nimportant for the assessment of cardiac function and the diagnosis of\ncardiovascular diseases. Current state-of-the art methods focus on estimating\ndense pixel-/voxel-wise motion fields in image space, which ignores the fact\nthat motion estimation is only relevant and useful within the anatomical\nobjects of interest, e.g., the heart. In this work, we model the heart as a 3D\nmesh consisting of epi- and endocardial surfaces. We propose a novel learning\nframework, DeepMesh, which propagates a template heart mesh to a subject space\nand estimates the 3D motion of the heart mesh from CMR images for individual\nsubjects. In DeepMesh, the heart mesh of the end-diastolic frame of an\nindividual subject is first reconstructed from the template mesh. Mesh-based 3D\nmotion fields with respect to the end-diastolic frame are then estimated from\n2D short- and long-axis CMR images. By developing a differentiable\nmesh-to-image rasterizer, DeepMesh is able to leverage 2D shape information\nfrom multiple anatomical views for 3D mesh reconstruction and mesh motion\nestimation. The proposed method estimates vertex-wise displacement and thus\nmaintains vertex correspondences between time frames, which is important for\nthe quantitative assessment of cardiac function across different subjects and\npopulations. We evaluate DeepMesh on CMR images acquired from the UK Biobank.\nWe focus on 3D motion estimation of the left ventricle in this work.\nExperimental results show that the proposed method quantitatively and\nqualitatively outperforms other image-based and mesh-based cardiac motion\ntracking methods.",
        "translated": ""
    },
    {
        "title": "Overview of Class Activation Maps for Visualization Explainability",
        "url": "http://arxiv.org/abs/2309.14304v1",
        "pub_date": "2023-09-25",
        "summary": "Recent research in deep learning methodology has led to a variety of complex\nmodelling techniques in computer vision (CV) that reach or even outperform\nhuman performance. Although these black-box deep learning models have obtained\nastounding results, they are limited in their interpretability and transparency\nwhich are critical to take learning machines to the next step to include them\nin sensitive decision-support systems involving human supervision. Hence, the\ndevelopment of explainable techniques for computer vision (XCV) has recently\nattracted increasing attention. In the realm of XCV, Class Activation Maps\n(CAMs) have become widely recognized and utilized for enhancing\ninterpretability and insights into the decision-making process of deep learning\nmodels. This work presents a comprehensive overview of the evolution of Class\nActivation Map methods over time. It also explores the metrics used for\nevaluating CAMs and introduces auxiliary techniques to improve the saliency of\nthese methods. The overview concludes by proposing potential avenues for future\nresearch in this evolving field.",
        "translated": ""
    },
    {
        "title": "Dataset Diffusion: Diffusion-based Synthetic Dataset Generation for\n  Pixel-Level Semantic Segmentation",
        "url": "http://arxiv.org/abs/2309.14303v1",
        "pub_date": "2023-09-25",
        "summary": "Preparing training data for deep vision models is a labor-intensive task. To\naddress this, generative models have emerged as an effective solution for\ngenerating synthetic data. While current generative models produce image-level\ncategory labels, we propose a novel method for generating pixel-level semantic\nsegmentation labels using the text-to-image generative model Stable Diffusion\n(SD). By utilizing the text prompts, cross-attention, and self-attention of SD,\nwe introduce three new techniques: \\textit{class-prompt appending},\n\\textit{class-prompt cross-attention}, and \\textit{self-attention\nexponentiation}. These techniques enable us to generate segmentation maps\ncorresponding to synthetic images. These maps serve as pseudo-labels for\ntraining semantic segmenters, eliminating the need for labor-intensive\npixel-wise annotation. To account for the imperfections in our pseudo-labels,\nwe incorporate uncertainty regions into the segmentation, allowing us to\ndisregard loss from those regions. We conduct evaluations on two datasets,\nPASCAL VOC and MSCOCO, and our approach significantly outperforms concurrent\nwork. Our benchmarks and code will be released at\nhttps://github.com/VinAIResearch/Dataset-Diffusion",
        "translated": ""
    },
    {
        "title": "Generating Visual Scenes from Touch",
        "url": "http://arxiv.org/abs/2309.15117v1",
        "pub_date": "2023-09-26",
        "summary": "An emerging line of work has sought to generate plausible imagery from touch.\nExisting approaches, however, tackle only narrow aspects of the visuo-tactile\nsynthesis problem, and lag significantly behind the quality of cross-modal\nsynthesis methods in other domains. We draw on recent advances in latent\ndiffusion to create a model for synthesizing images from tactile signals (and\nvice versa) and apply it to a number of visuo-tactile synthesis tasks. Using\nthis model, we significantly outperform prior work on the tactile-driven\nstylization problem, i.e., manipulating an image to match a touch signal, and\nwe are the first to successfully generate images from touch without additional\nsources of information about the scene. We also successfully use our model to\naddress two novel synthesis problems: generating images that do not contain the\ntouch sensor or the hand holding it, and estimating an image's shading from its\nreflectance and touch.",
        "translated": ""
    },
    {
        "title": "InternLM-XComposer: A Vision-Language Large Model for Advanced\n  Text-image Comprehension and Composition",
        "url": "http://arxiv.org/abs/2309.15112v1",
        "pub_date": "2023-09-26",
        "summary": "We propose InternLM-XComposer, a vision-language large model that enables\nadvanced image-text comprehension and composition. The innovative nature of our\nmodel is highlighted by three appealing properties: 1) Interleaved Text-Image\nComposition: InternLM-XComposer can effortlessly generate coherent and\ncontextual articles that seamlessly integrate images, providing a more engaging\nand immersive reading experience. Simply provide a title, and our system will\ngenerate the corresponding manuscript. It can intelligently identify the areas\nin the text where images would enhance the content and automatically insert the\nmost appropriate visual candidates. 2) Comprehension with Rich Multilingual\nKnowledge: The text-image comprehension is empowered by training on extensive\nmulti-modal multilingual concepts with carefully crafted strategies, resulting\nin a deep understanding of visual content. 3) State-of-the-art Performance: Our\nmodel consistently achieves state-of-the-art results across various mainstream\nbenchmarks for vision-language foundational models, including MME Benchmark,\nMMBench, MMBench-CN, Seed-Bench, and CCBench (Chinese Cultural Benchmark).\nCollectively, InternLM-XComposer seamlessly blends advanced text-image\ncomprehension and composition, revolutionizing vision-language interaction and\noffering new insights and opportunities. The InternLM-XComposer models with 7B\nparameters are publicly available at\nhttps://github.com/InternLM/InternLM-XComposer.",
        "translated": ""
    },
    {
        "title": "Doduo: Learning Dense Visual Correspondence from Unsupervised\n  Semantic-Aware Flow",
        "url": "http://arxiv.org/abs/2309.15110v1",
        "pub_date": "2023-09-26",
        "summary": "Dense visual correspondence plays a vital role in robotic perception. This\nwork focuses on establishing the dense correspondence between a pair of images\nthat captures dynamic scenes undergoing substantial transformations. We\nintroduce Doduo to learn general dense visual correspondence from in-the-wild\nimages and videos without ground truth supervision. Given a pair of images, it\nestimates the dense flow field encoding the displacement of each pixel in one\nimage to its corresponding pixel in the other image. Doduo uses flow-based\nwarping to acquire supervisory signals for the training. Incorporating semantic\npriors with self-supervised flow training, Doduo produces accurate dense\ncorrespondence robust to the dynamic changes of the scenes. Trained on an\nin-the-wild video dataset, Doduo illustrates superior performance on\npoint-level correspondence estimation over existing self-supervised\ncorrespondence learning baselines. We also apply Doduo to articulation\nestimation and zero-shot goal-conditioned manipulation, underlining its\npractical applications in robotics. Code and additional visualizations are\navailable at https://ut-austin-rpl.github.io/Doduo",
        "translated": ""
    },
    {
        "title": "DistillBEV: Boosting Multi-Camera 3D Object Detection with Cross-Modal\n  Knowledge Distillation",
        "url": "http://arxiv.org/abs/2309.15109v1",
        "pub_date": "2023-09-26",
        "summary": "3D perception based on the representations learned from multi-camera\nbird's-eye-view (BEV) is trending as cameras are cost-effective for mass\nproduction in autonomous driving industry. However, there exists a distinct\nperformance gap between multi-camera BEV and LiDAR based 3D object detection.\nOne key reason is that LiDAR captures accurate depth and other geometry\nmeasurements, while it is notoriously challenging to infer such 3D information\nfrom merely image input. In this work, we propose to boost the representation\nlearning of a multi-camera BEV based student detector by training it to imitate\nthe features of a well-trained LiDAR based teacher detector. We propose\neffective balancing strategy to enforce the student to focus on learning the\ncrucial features from the teacher, and generalize knowledge transfer to\nmulti-scale layers with temporal fusion. We conduct extensive evaluations on\nmultiple representative models of multi-camera BEV. Experiments reveal that our\napproach renders significant improvement over the student models, leading to\nthe state-of-the-art performance on the popular benchmark nuScenes.",
        "translated": ""
    },
    {
        "title": "LAVIE: High-Quality Video Generation with Cascaded Latent Diffusion\n  Models",
        "url": "http://arxiv.org/abs/2309.15103v2",
        "pub_date": "2023-09-26",
        "summary": "This work aims to learn a high-quality text-to-video (T2V) generative model\nby leveraging a pre-trained text-to-image (T2I) model as a basis. It is a\nhighly desirable yet challenging task to simultaneously a) accomplish the\nsynthesis of visually realistic and temporally coherent videos while b)\npreserving the strong creative generation nature of the pre-trained T2I model.\nTo this end, we propose LaVie, an integrated video generation framework that\noperates on cascaded video latent diffusion models, comprising a base T2V\nmodel, a temporal interpolation model, and a video super-resolution model. Our\nkey insights are two-fold: 1) We reveal that the incorporation of simple\ntemporal self-attentions, coupled with rotary positional encoding, adequately\ncaptures the temporal correlations inherent in video data. 2) Additionally, we\nvalidate that the process of joint image-video fine-tuning plays a pivotal role\nin producing high-quality and creative outcomes. To enhance the performance of\nLaVie, we contribute a comprehensive and diverse video dataset named Vimeo25M,\nconsisting of 25 million text-video pairs that prioritize quality, diversity,\nand aesthetic appeal. Extensive experiments demonstrate that LaVie achieves\nstate-of-the-art performance both quantitatively and qualitatively.\nFurthermore, we showcase the versatility of pre-trained LaVie models in various\nlong video generation and personalized video synthesis applications.",
        "translated": ""
    },
    {
        "title": "Case Study: Ensemble Decision-Based Annotation of Unconstrained Real\n  Estate Images",
        "url": "http://arxiv.org/abs/2309.15097v1",
        "pub_date": "2023-09-26",
        "summary": "We describe a proof-of-concept for annotating real estate images using simple\niterative rule-based semi-supervised learning. In this study, we have gained\nimportant insights into the content characteristics and uniqueness of\nindividual image classes as well as essential requirements for a practical\nimplementation.",
        "translated": ""
    },
    {
        "title": "VideoDirectorGPT: Consistent Multi-scene Video Generation via LLM-Guided\n  Planning",
        "url": "http://arxiv.org/abs/2309.15091v1",
        "pub_date": "2023-09-26",
        "summary": "Although recent text-to-video (T2V) generation methods have seen significant\nadvancements, most of these works focus on producing short video clips of a\nsingle event with a single background (i.e., single-scene videos). Meanwhile,\nrecent large language models (LLMs) have demonstrated their capability in\ngenerating layouts and programs to control downstream visual modules such as\nimage generation models. This raises an important question: can we leverage the\nknowledge embedded in these LLMs for temporally consistent long video\ngeneration? In this paper, we propose VideoDirectorGPT, a novel framework for\nconsistent multi-scene video generation that uses the knowledge of LLMs for\nvideo content planning and grounded video generation. Specifically, given a\nsingle text prompt, we first ask our video planner LLM (GPT-4) to expand it\ninto a 'video plan', which involves generating the scene descriptions, the\nentities with their respective layouts, the background for each scene, and\nconsistency groupings of the entities and backgrounds. Next, guided by this\noutput from the video planner, our video generator, Layout2Vid, has explicit\ncontrol over spatial layouts and can maintain temporal consistency of\nentities/backgrounds across scenes, while only trained with image-level\nannotations. Our experiments demonstrate that VideoDirectorGPT framework\nsubstantially improves layout and movement control in both single- and\nmulti-scene video generation and can generate multi-scene videos with visual\nconsistency across scenes, while achieving competitive performance with SOTAs\nin open-domain single-scene T2V generation. We also demonstrate that our\nframework can dynamically control the strength for layout guidance and can also\ngenerate videos with user-provided images. We hope our framework can inspire\nfuture work on better integrating the planning ability of LLMs into consistent\nlong video generation.",
        "translated": ""
    },
    {
        "title": "Video-adverb retrieval with compositional adverb-action embeddings",
        "url": "http://arxiv.org/abs/2309.15086v1",
        "pub_date": "2023-09-26",
        "summary": "Retrieving adverbs that describe an action in a video poses a crucial step\ntowards fine-grained video understanding. We propose a framework for\nvideo-to-adverb retrieval (and vice versa) that aligns video embeddings with\ntheir matching compositional adverb-action text embedding in a joint embedding\nspace. The compositional adverb-action text embedding is learned using a\nresidual gating mechanism, along with a novel training objective consisting of\ntriplet losses and a regression target. Our method achieves state-of-the-art\nperformance on five recent benchmarks for video-adverb retrieval. Furthermore,\nwe introduce dataset splits to benchmark video-adverb retrieval for unseen\nadverb-action compositions on subsets of the MSR-VTT Adverbs and ActivityNet\nAdverbs datasets. Our proposed framework outperforms all prior works for the\ngeneralisation task of retrieving adverbs from videos for unseen adverb-action\ncompositions. Code and dataset splits are available at\nhttps://hummelth.github.io/ReGaDa/.",
        "translated": ""
    },
    {
        "title": "The Surveillance AI Pipeline",
        "url": "http://arxiv.org/abs/2309.15084v1",
        "pub_date": "2023-09-26",
        "summary": "A rapidly growing number of voices have argued that AI research, and computer\nvision in particular, is closely tied to mass surveillance. Yet the direct path\nfrom computer vision research to surveillance has remained obscured and\ndifficult to assess. This study reveals the Surveillance AI pipeline. We obtain\nthree decades of computer vision research papers and downstream patents (more\nthan 20,000 documents) and present a rich qualitative and quantitative\nanalysis. This analysis exposes the nature and extent of the Surveillance AI\npipeline, its institutional roots and evolution, and ongoing patterns of\nobfuscation. We first perform an in-depth content analysis of computer vision\npapers and downstream patents, identifying and quantifying key features and the\nmany, often subtly expressed, forms of surveillance that appear. On the basis\nof this analysis, we present a topology of Surveillance AI that characterizes\nthe prevalent targeting of human data, practices of data transferal, and\ninstitutional data use. We find stark evidence of close ties between computer\nvision and surveillance. The majority (68%) of annotated computer vision papers\nand patents self-report their technology enables data extraction about human\nbodies and body parts and even more (90%) enable data extraction about humans\nin general.",
        "translated": ""
    },
    {
        "title": "RPEFlow: Multimodal Fusion of RGB-PointCloud-Event for Joint Optical\n  Flow and Scene Flow Estimation",
        "url": "http://arxiv.org/abs/2309.15082v1",
        "pub_date": "2023-09-26",
        "summary": "Recently, the RGB images and point clouds fusion methods have been proposed\nto jointly estimate 2D optical flow and 3D scene flow. However, as both\nconventional RGB cameras and LiDAR sensors adopt a frame-based data acquisition\nmechanism, their performance is limited by the fixed low sampling rates,\nespecially in highly-dynamic scenes. By contrast, the event camera can\nasynchronously capture the intensity changes with a very high temporal\nresolution, providing complementary dynamic information of the observed scenes.\nIn this paper, we incorporate RGB images, Point clouds and Events for joint\noptical flow and scene flow estimation with our proposed multi-stage multimodal\nfusion model, RPEFlow. First, we present an attention fusion module with a\ncross-attention mechanism to implicitly explore the internal cross-modal\ncorrelation for 2D and 3D branches, respectively. Second, we introduce a mutual\ninformation regularization term to explicitly model the complementary\ninformation of three modalities for effective multimodal feature learning. We\nalso contribute a new synthetic dataset to advocate further research.\nExperiments on both synthetic and real datasets show that our model outperforms\nthe existing state-of-the-art by a wide margin. Code and dataset is available\nat https://npucvr.github.io/RPEFlow.",
        "translated": ""
    },
    {
        "title": "SHACIRA: Scalable HAsh-grid Compression for Implicit Neural\n  Representations",
        "url": "http://arxiv.org/abs/2309.15848v1",
        "pub_date": "2023-09-27",
        "summary": "Implicit Neural Representations (INR) or neural fields have emerged as a\npopular framework to encode multimedia signals such as images and radiance\nfields while retaining high-quality. Recently, learnable feature grids proposed\nby Instant-NGP have allowed significant speed-up in the training as well as the\nsampling of INRs by replacing a large neural network with a multi-resolution\nlook-up table of feature vectors and a much smaller neural network. However,\nthese feature grids come at the expense of large memory consumption which can\nbe a bottleneck for storage and streaming applications. In this work, we\npropose SHACIRA, a simple yet effective task-agnostic framework for compressing\nsuch feature grids with no additional post-hoc pruning/quantization stages. We\nreparameterize feature grids with quantized latent weights and apply entropy\nregularization in the latent space to achieve high levels of compression across\nvarious domains. Quantitative and qualitative results on diverse datasets\nconsisting of images, videos, and radiance fields, show that our approach\noutperforms existing INR approaches without the need for any large datasets or\ndomain-specific heuristics. Our project page is available at\nhttp://shacira.github.io .",
        "translated": ""
    },
    {
        "title": "Exploiting the Signal-Leak Bias in Diffusion Models",
        "url": "http://arxiv.org/abs/2309.15842v1",
        "pub_date": "2023-09-27",
        "summary": "There is a bias in the inference pipeline of most diffusion models. This bias\narises from a signal leak whose distribution deviates from the noise\ndistribution, creating a discrepancy between training and inference processes.\nWe demonstrate that this signal-leak bias is particularly significant when\nmodels are tuned to a specific style, causing sub-optimal style matching.\nRecent research tries to avoid the signal leakage during training. We instead\nshow how we can exploit this signal-leak bias in existing diffusion models to\nallow more control over the generated images. This enables us to generate\nimages with more varied brightness, and images that better match a desired\nstyle or color. By modeling the distribution of the signal leak in the spatial\nfrequency and pixel domains, and including a signal leak in the initial latent,\nwe generate images that better match expected results without any additional\ntraining.",
        "translated": ""
    },
    {
        "title": "OrthoPlanes: A Novel Representation for Better 3D-Awareness of GANs",
        "url": "http://arxiv.org/abs/2309.15830v1",
        "pub_date": "2023-09-27",
        "summary": "We present a new method for generating realistic and view-consistent images\nwith fine geometry from 2D image collections. Our method proposes a hybrid\nexplicit-implicit representation called \\textbf{OrthoPlanes}, which encodes\nfine-grained 3D information in feature maps that can be efficiently generated\nby modifying 2D StyleGANs. Compared to previous representations, our method has\nbetter scalability and expressiveness with clear and explicit information. As a\nresult, our method can handle more challenging view-angles and synthesize\narticulated objects with high spatial degree of freedom. Experiments\ndemonstrate that our method achieves state-of-the-art results on FFHQ and SHHQ\ndatasets, both quantitatively and qualitatively. Project page:\n\\url{https://orthoplanes.github.io/}.",
        "translated": ""
    },
    {
        "title": "Show-1: Marrying Pixel and Latent Diffusion Models for Text-to-Video\n  Generation",
        "url": "http://arxiv.org/abs/2309.15818v1",
        "pub_date": "2023-09-27",
        "summary": "Significant advancements have been achieved in the realm of large-scale\npre-trained text-to-video Diffusion Models (VDMs). However, previous methods\neither rely solely on pixel-based VDMs, which come with high computational\ncosts, or on latent-based VDMs, which often struggle with precise text-video\nalignment. In this paper, we are the first to propose a hybrid model, dubbed as\nShow-1, which marries pixel-based and latent-based VDMs for text-to-video\ngeneration. Our model first uses pixel-based VDMs to produce a low-resolution\nvideo of strong text-video correlation. After that, we propose a novel expert\ntranslation method that employs the latent-based VDMs to further upsample the\nlow-resolution video to high resolution. Compared to latent VDMs, Show-1 can\nproduce high-quality videos of precise text-video alignment; Compared to pixel\nVDMs, Show-1 is much more efficient (GPU memory usage during inference is 15G\nvs 72G). We also validate our model on standard video generation benchmarks.\nOur code and model weights are publicly available at\n\\url{https://github.com/showlab/Show-1}.",
        "translated": ""
    },
    {
        "title": "Convolutional Networks with Oriented 1D Kernels",
        "url": "http://arxiv.org/abs/2309.15812v1",
        "pub_date": "2023-09-27",
        "summary": "In computer vision, 2D convolution is arguably the most important operation\nperformed by a ConvNet. Unsurprisingly, it has been the focus of intense\nsoftware and hardware optimization and enjoys highly efficient implementations.\nIn this work, we ask an intriguing question: can we make a ConvNet work without\n2D convolutions? Surprisingly, we find that the answer is yes -- we show that a\nConvNet consisting entirely of 1D convolutions can do just as well as 2D on\nImageNet classification. Specifically, we find that one key ingredient to a\nhigh-performing 1D ConvNet is oriented 1D kernels: 1D kernels that are oriented\nnot just horizontally or vertically, but also at other angles. Our experiments\nshow that oriented 1D convolutions can not only replace 2D convolutions but\nalso augment existing architectures with large kernels, leading to improved\naccuracy with minimal FLOPs increase. A key contribution of this work is a\nhighly-optimized custom CUDA implementation of oriented 1D kernels, specialized\nto the depthwise convolution setting. Our benchmarks demonstrate that our\ncustom CUDA implementation almost perfectly realizes the theoretical advantage\nof 1D convolution: it is faster than a native horizontal convolution for any\narbitrary angle. Code is available at\nhttps://github.com/princeton-vl/Oriented1D.",
        "translated": ""
    },
    {
        "title": "Emu: Enhancing Image Generation Models Using Photogenic Needles in a\n  Haystack",
        "url": "http://arxiv.org/abs/2309.15807v1",
        "pub_date": "2023-09-27",
        "summary": "Training text-to-image models with web scale image-text pairs enables the\ngeneration of a wide range of visual concepts from text. However, these\npre-trained models often face challenges when it comes to generating highly\naesthetic images. This creates the need for aesthetic alignment post\npre-training. In this paper, we propose quality-tuning to effectively guide a\npre-trained model to exclusively generate highly visually appealing images,\nwhile maintaining generality across visual concepts. Our key insight is that\nsupervised fine-tuning with a set of surprisingly small but extremely visually\nappealing images can significantly improve the generation quality. We pre-train\na latent diffusion model on $1.1$ billion image-text pairs and fine-tune it\nwith only a few thousand carefully selected high-quality images. The resulting\nmodel, Emu, achieves a win rate of $82.9\\%$ compared with its pre-trained only\ncounterpart. Compared to the state-of-the-art SDXLv1.0, Emu is preferred\n$68.4\\%$ and $71.3\\%$ of the time on visual appeal on the standard PartiPrompts\nand our Open User Input benchmark based on the real-world usage of\ntext-to-image models. In addition, we show that quality-tuning is a generic\napproach that is also effective for other architectures, including pixel\ndiffusion and masked generative transformer models.",
        "translated": ""
    },
    {
        "title": "A Quantum-Classical Hybrid Block-Matching Algorithm in Noisy Environment\n  using Dissimilarity Measure",
        "url": "http://arxiv.org/abs/2309.15792v1",
        "pub_date": "2023-09-27",
        "summary": "A block-matching algorithm finds a group of similar image patches inside a\nsearch area. Similarity/dissimilarity measures can help to solve this problem.\nIn different practical applications, finding groups of similar image blocks\nwithin an ample search area is often necessary, such as video compression,\nimage clustering, vector quantization, and nonlocal noise reduction. In this\nwork, classical image processing is performed using Gaussian noise and image\nsize reduction with a fit of a Low-Pass Filter or Domain Transform. A\nhierarchical search technique is implemented to encode the images by phase\noperator. Using phase image coding with the quantum Fourier transform and the\nSwap test, we propose a dissimilarity measure. Results were obtained with\nperfect and noisy simulations and in the case of the Swap test with the IBM and\nIonq quantum devices.",
        "translated": ""
    },
    {
        "title": "Partial Transport for Point-Cloud Registration",
        "url": "http://arxiv.org/abs/2309.15787v1",
        "pub_date": "2023-09-27",
        "summary": "Point cloud registration plays a crucial role in various fields, including\nrobotics, computer graphics, and medical imaging. This process involves\ndetermining spatial relationships between different sets of points, typically\nwithin a 3D space. In real-world scenarios, complexities arise from non-rigid\nmovements and partial visibility, such as occlusions or sensor noise, making\nnon-rigid registration a challenging problem. Classic non-rigid registration\nmethods are often computationally demanding, suffer from unstable performance,\nand, importantly, have limited theoretical guarantees. The optimal transport\nproblem and its unbalanced variations (e.g., the optimal partial transport\nproblem) have emerged as powerful tools for point-cloud registration,\nestablishing a strong benchmark in this field. These methods view point clouds\nas empirical measures and provide a mathematically rigorous way to quantify the\n`correspondence' between (the transformed) source and target points. In this\npaper, we approach the point-cloud registration problem through the lens of\noptimal transport theory and first propose a comprehensive set of non-rigid\nregistration methods based on the optimal partial transportation problem.\nSubsequently, leveraging the emerging work on efficient solutions to the\none-dimensional optimal partial transport problem, we extend our proposed\nalgorithms via slicing to gain significant computational efficiency, resulting\nin fast and robust non-rigid registration algorithms. We demonstrate the\neffectiveness of our proposed methods and compare them against baselines on\nvarious 3D and 2D non-rigid registration problems where the source and target\npoint clouds are corrupted by random noise.",
        "translated": ""
    },
    {
        "title": "One For All: Video Conversation is Feasible Without Video Instruction\n  Tuning",
        "url": "http://arxiv.org/abs/2309.15785v1",
        "pub_date": "2023-09-27",
        "summary": "The recent progress in Large Language Models (LLM) has spurred various\nadvancements in image-language conversation agents, while how to build a\nproficient video-based dialogue system is still under exploration. Considering\nthe extensive scale of LLM and visual backbone, minimal GPU memory is left for\nfacilitating effective temporal modeling, which is crucial for comprehending\nand providing feedback on videos. To this end, we propose Branching Temporal\nAdapter (BT-Adapter), a novel method for extending image-language pretrained\nmodels into the video domain. Specifically, BT-Adapter serves as a plug-and-use\ntemporal modeling branch alongside the pretrained visual encoder, which is\ntuned while keeping the backbone frozen. Just pretrained once, BT-Adapter can\nbe seamlessly integrated into all image conversation models using this version\nof CLIP, enabling video conversations without the need for video instructions.\nBesides, we develop a unique asymmetric token masking strategy inside the\nbranch with tailor-made training tasks for BT-Adapter, facilitating faster\nconvergence and better results. Thanks to BT-Adapter, we are able to empower\nexisting multimodal dialogue models with strong video understanding\ncapabilities without incurring excessive GPU costs. Without bells and whistles,\nBT-Adapter achieves (1) state-of-the-art zero-shot results on various video\ntasks using thousands of fewer GPU hours. (2) better performance than current\nvideo chatbots without any video instruction tuning. (3) state-of-the-art\nresults of video chatting using video instruction tuning, outperforming\nprevious SOTAs by a large margin.",
        "translated": ""
    },
    {
        "title": "Joint-YODNet: A Light-weight Object Detector for UAVs to Achieve Above\n  100fps",
        "url": "http://arxiv.org/abs/2309.15782v1",
        "pub_date": "2023-09-27",
        "summary": "Small object detection via UAV (Unmanned Aerial Vehicle) images captured from\ndrones and radar is a complex task with several formidable challenges. This\ndomain encompasses numerous complexities that impede the accurate detection and\nlocalization of small objects. To address these challenges, we propose a novel\nmethod called JointYODNet for UAVs to detect small objects, leveraging a joint\nloss function specifically designed for this task. Our method revolves around\nthe development of a joint loss function tailored to enhance the detection\nperformance of small objects. Through extensive experimentation on a diverse\ndataset of UAV images captured under varying environmental conditions, we\nevaluated different variations of the loss function and determined the most\neffective formulation. The results demonstrate that our proposed joint loss\nfunction outperforms existing methods in accurately localizing small objects.\nSpecifically, our method achieves a recall of 0.971, and a F1Score of 0.975,\nsurpassing state-of-the-art techniques. Additionally, our method achieves a\nmAP@.5(%) of 98.6, indicating its robustness in detecting small objects across\nvarying scales",
        "translated": ""
    },
    {
        "title": "Learning to Transform for Generalizable Instance-wise Invariance",
        "url": "http://arxiv.org/abs/2309.16672v1",
        "pub_date": "2023-09-28",
        "summary": "Computer vision research has long aimed to build systems that are robust to\nspatial transformations found in natural data. Traditionally, this is done\nusing data augmentation or hard-coding invariances into the architecture.\nHowever, too much or too little invariance can hurt, and the correct amount is\nunknown a priori and dependent on the instance. Ideally, the appropriate\ninvariance would be learned from data and inferred at test-time.\n  We treat invariance as a prediction problem. Given any image, we use a\nnormalizing flow to predict a distribution over transformations and average the\npredictions over them. Since this distribution only depends on the instance, we\ncan align instances before classifying them and generalize invariance across\nclasses. The same distribution can also be used to adapt to out-of-distribution\nposes. This normalizing flow is trained end-to-end and can learn a much larger\nrange of transformations than Augerino and InstaAug. When used as data\naugmentation, our method shows accuracy and robustness gains on CIFAR 10,\nCIFAR10-LT, and TinyImageNet.",
        "translated": ""
    },
    {
        "title": "Demystifying CLIP Data",
        "url": "http://arxiv.org/abs/2309.16671v1",
        "pub_date": "2023-09-28",
        "summary": "Contrastive Language-Image Pre-training (CLIP) is an approach that has\nadvanced research and applications in computer vision, fueling modern\nrecognition systems and generative models. We believe that the main ingredient\nto the success of CLIP is its data and not the model architecture or\npre-training objective. However, CLIP only provides very limited information\nabout its data and how it has been collected, leading to works that aim to\nreproduce CLIP's data by filtering with its model parameters. In this work, we\nintend to reveal CLIP's data curation approach and in our pursuit of making it\nopen to the community introduce Metadata-Curated Language-Image Pre-training\n(MetaCLIP). MetaCLIP takes a raw data pool and metadata (derived from CLIP's\nconcepts) and yields a balanced subset over the metadata distribution. Our\nexperimental study rigorously isolates the model and training settings,\nconcentrating solely on data. MetaCLIP applied to CommonCrawl with 400M\nimage-text data pairs outperforms CLIP's data on multiple standard benchmarks.\nIn zero-shot ImageNet classification, MetaCLIP achieves 70.8% accuracy,\nsurpassing CLIP's 68.3% on ViT-B models. Scaling to 1B data, while maintaining\nthe same training budget, attains 72.4%. Our observations hold across various\nmodel sizes, exemplified by ViT-H achieving 80.5%, without any\nbells-and-whistles. Curation code and training data distribution on metadata is\nmade available at https://github.com/facebookresearch/MetaCLIP.",
        "translated": ""
    },
    {
        "title": "Decaf: Monocular Deformation Capture for Face and Hand Interactions",
        "url": "http://arxiv.org/abs/2309.16670v1",
        "pub_date": "2023-09-28",
        "summary": "Existing methods for 3D tracking from monocular RGB videos predominantly\nconsider articulated and rigid objects. Modelling dense non-rigid object\ndeformations in this setting remained largely unaddressed so far, although such\neffects can improve the realism of the downstream applications such as AR/VR\nand avatar communications. This is due to the severe ill-posedness of the\nmonocular view setting and the associated challenges. While it is possible to\nnaively track multiple non-rigid objects independently using 3D templates or\nparametric 3D models, such an approach would suffer from multiple artefacts in\nthe resulting 3D estimates such as depth ambiguity, unnatural intra-object\ncollisions and missing or implausible deformations. Hence, this paper\nintroduces the first method that addresses the fundamental challenges depicted\nabove and that allows tracking human hands interacting with human faces in 3D\nfrom single monocular RGB videos. We model hands as articulated objects\ninducing non-rigid face deformations during an active interaction. Our method\nrelies on a new hand-face motion and interaction capture dataset with realistic\nface deformations acquired with a markerless multi-view camera system. As a\npivotal step in its creation, we process the reconstructed raw 3D shapes with\nposition-based dynamics and an approach for non-uniform stiffness estimation of\nthe head tissues, which results in plausible annotations of the surface\ndeformations, hand-face contact regions and head-hand positions. At the core of\nour neural approach are a variational auto-encoder supplying the hand-face\ndepth prior and modules that guide the 3D tracking by estimating the contacts\nand the deformations. Our final 3D hand and face reconstructions are realistic\nand more plausible compared to several baselines applicable in our setting,\nboth quantitatively and qualitatively.\nhttps://vcai.mpi-inf.mpg.de/projects/Decaf",
        "translated": ""
    },
    {
        "title": "Training a Large Video Model on a Single Machine in a Day",
        "url": "http://arxiv.org/abs/2309.16669v1",
        "pub_date": "2023-09-28",
        "summary": "Videos are big, complex to pre-process, and slow to train on.\nState-of-the-art large-scale video models are trained on clusters of 32 or more\nGPUs for several days. As a consequence, academia largely ceded the training of\nlarge video models to industry. In this paper, we show how to still train a\nstate-of-the-art video model on a single machine with eight consumer-grade GPUs\nin a day. We identify three bottlenecks, IO, CPU, and GPU computation, and\noptimize each. The result is a highly efficient video training pipeline. For\ncomparable architectures, our pipeline achieves higher accuracies with\n$\\frac{1}{8}$ of the computation compared to prior work. Code is available at\nhttps://github.com/zhaoyue-zephyrus/AVION.",
        "translated": ""
    },
    {
        "title": "RealFill: Reference-Driven Generation for Authentic Image Completion",
        "url": "http://arxiv.org/abs/2309.16668v1",
        "pub_date": "2023-09-28",
        "summary": "Recent advances in generative imagery have brought forth outpainting and\ninpainting models that can produce high-quality, plausible image content in\nunknown regions, but the content these models hallucinate is necessarily\ninauthentic, since the models lack sufficient context about the true scene. In\nthis work, we propose RealFill, a novel generative approach for image\ncompletion that fills in missing regions of an image with the content that\nshould have been there. RealFill is a generative inpainting model that is\npersonalized using only a few reference images of a scene. These reference\nimages do not have to be aligned with the target image, and can be taken with\ndrastically varying viewpoints, lighting conditions, camera apertures, or image\nstyles. Once personalized, RealFill is able to complete a target image with\nvisually compelling contents that are faithful to the original scene. We\nevaluate RealFill on a new image completion benchmark that covers a set of\ndiverse and challenging scenarios, and find that it outperforms existing\napproaches by a large margin. See more results on our project page:\nhttps://realfill.github.io",
        "translated": ""
    },
    {
        "title": "Geodesic Regression Characterizes 3D Shape Changes in the Female Brain\n  During Menstruation",
        "url": "http://arxiv.org/abs/2309.16662v1",
        "pub_date": "2023-09-28",
        "summary": "Women are at higher risk of Alzheimer's and other neurological diseases after\nmenopause, and yet research connecting female brain health to sex hormone\nfluctuations is limited. We seek to investigate this connection by developing\ntools that quantify 3D shape changes that occur in the brain during sex hormone\nfluctuations. Geodesic regression on the space of 3D discrete surfaces offers a\nprincipled way to characterize the evolution of a brain's shape. However, in\nits current form, this approach is too computationally expensive for practical\nuse. In this paper, we propose approximation schemes that accelerate geodesic\nregression on shape spaces of 3D discrete surfaces. We also provide rules of\nthumb for when each approximation can be used. We test our approach on\nsynthetic data to quantify the speed-accuracy trade-off of these approximations\nand show that practitioners can expect very significant speed-up while only\nsacrificing little accuracy. Finally, we apply the method to real brain shape\ndata and produce the first characterization of how the female hippocampus\nchanges shape during the menstrual cycle as a function of progesterone: a\ncharacterization made (practically) possible by our approximation schemes. Our\nwork paves the way for comprehensive, practical shape analyses in the fields of\nbio-medicine and computer vision. Our implementation is publicly available on\nGitHub: https://github.com/bioshape-lab/my28brains.",
        "translated": ""
    },
    {
        "title": "SA2-Net: Scale-aware Attention Network for Microscopic Image\n  Segmentation",
        "url": "http://arxiv.org/abs/2309.16661v1",
        "pub_date": "2023-09-28",
        "summary": "Microscopic image segmentation is a challenging task, wherein the objective\nis to assign semantic labels to each pixel in a given microscopic image. While\nconvolutional neural networks (CNNs) form the foundation of many existing\nframeworks, they often struggle to explicitly capture long-range dependencies.\nAlthough transformers were initially devised to address this issue using\nself-attention, it has been proven that both local and global features are\ncrucial for addressing diverse challenges in microscopic images, including\nvariations in shape, size, appearance, and target region density. In this\npaper, we introduce SA2-Net, an attention-guided method that leverages\nmulti-scale feature learning to effectively handle diverse structures within\nmicroscopic images. Specifically, we propose scale-aware attention (SA2) module\ndesigned to capture inherent variations in scales and shapes of microscopic\nregions, such as cells, for accurate segmentation. This module incorporates\nlocal attention at each level of multi-stage features, as well as global\nattention across multiple resolutions. Furthermore, we address the issue of\nblurred region boundaries (e.g., cell boundaries) by introducing a novel\nupsampling strategy called the Adaptive Up-Attention (AuA) module. This module\nenhances the discriminative ability for improved localization of microscopic\nregions using an explicit attention mechanism. Extensive experiments on five\nchallenging datasets demonstrate the benefits of our SA2-Net model. Our source\ncode is publicly available at \\url{https://github.com/mustansarfiaz/SA2-Net}.",
        "translated": ""
    },
    {
        "title": "Visual In-Context Learning for Few-Shot Eczema Segmentation",
        "url": "http://arxiv.org/abs/2309.16656v1",
        "pub_date": "2023-09-28",
        "summary": "Automated diagnosis of eczema from digital camera images is crucial for\ndeveloping applications that allow patients to self-monitor their recovery. An\nimportant component of this is the segmentation of eczema region from such\nimages. Current methods for eczema segmentation rely on deep neural networks\nsuch as convolutional (CNN)-based U-Net or transformer-based Swin U-Net. While\neffective, these methods require high volume of annotated data, which can be\ndifficult to obtain. Here, we investigate the capabilities of visual in-context\nlearning that can perform few-shot eczema segmentation with just a handful of\nexamples and without any need for retraining models. Specifically, we propose a\nstrategy for applying in-context learning for eczema segmentation with a\ngeneralist vision model called SegGPT. When benchmarked on a dataset of\nannotated eczema images, we show that SegGPT with just 2 representative example\nimages from the training dataset performs better (mIoU: 36.69) than a CNN U-Net\ntrained on 428 images (mIoU: 32.60). We also discover that using more number of\nexamples for SegGPT may in fact be harmful to its performance. Our result\nhighlights the importance of visual in-context learning in developing faster\nand better solutions to skin imaging tasks. Our result also paves the way for\ndeveloping inclusive solutions that can cater to minorities in the demographics\nwho are typically heavily under-represented in the training data.",
        "translated": ""
    },
    {
        "title": "Novel Deep Learning Pipeline for Automatic Weapon Detection",
        "url": "http://arxiv.org/abs/2309.16654v1",
        "pub_date": "2023-09-28",
        "summary": "Weapon and gun violence have recently become a pressing issue today. The\ndegree of these crimes and activities has risen to the point of being termed as\nan epidemic. This prevalent misuse of weapons calls for an automatic system\nthat detects weapons in real-time. Real-time surveillance video is captured and\nrecorded in almost all public forums and places. These videos contain abundant\nraw data which can be extracted and processed into meaningful information. This\npaper proposes a novel pipeline consisting of an ensemble of convolutional\nneural networks with distinct architectures. Each neural network is trained\nwith a unique mini-batch with little to no overlap in the training samples.\nThis paper will present several promising results using multiple datasets\nassociated with comparing the proposed architecture and state-of-the-art (SoA)\nmodels. The proposed pipeline produced an average increase of 5% in accuracy,\nspecificity, and recall compared to the SoA systems.",
        "translated": ""
    },
    {
        "title": "DreamGaussian: Generative Gaussian Splatting for Efficient 3D Content\n  Creation",
        "url": "http://arxiv.org/abs/2309.16653v1",
        "pub_date": "2023-09-28",
        "summary": "Recent advances in 3D content creation mostly leverage optimization-based 3D\ngeneration via score distillation sampling (SDS). Though promising results have\nbeen exhibited, these methods often suffer from slow per-sample optimization,\nlimiting their practical usage. In this paper, we propose DreamGaussian, a\nnovel 3D content generation framework that achieves both efficiency and quality\nsimultaneously. Our key insight is to design a generative 3D Gaussian Splatting\nmodel with companioned mesh extraction and texture refinement in UV space. In\ncontrast to the occupancy pruning used in Neural Radiance Fields, we\ndemonstrate that the progressive densification of 3D Gaussians converges\nsignificantly faster for 3D generative tasks. To further enhance the texture\nquality and facilitate downstream applications, we introduce an efficient\nalgorithm to convert 3D Gaussians into textured meshes and apply a fine-tuning\nstage to refine the details. Extensive experiments demonstrate the superior\nefficiency and competitive generation quality of our proposed approach.\nNotably, DreamGaussian produces high-quality textured meshes in just 2 minutes\nfrom a single-view image, achieving approximately 10 times acceleration\ncompared to existing methods.",
        "translated": ""
    },
    {
        "title": "Multi-task View Synthesis with Neural Radiance Fields",
        "url": "http://arxiv.org/abs/2309.17450v1",
        "pub_date": "2023-09-29",
        "summary": "Multi-task visual learning is a critical aspect of computer vision. Current\nresearch, however, predominantly concentrates on the multi-task dense\nprediction setting, which overlooks the intrinsic 3D world and its multi-view\nconsistent structures, and lacks the capability for versatile imagination. In\nresponse to these limitations, we present a novel problem setting -- multi-task\nview synthesis (MTVS), which reinterprets multi-task prediction as a set of\nnovel-view synthesis tasks for multiple scene properties, including RGB. To\ntackle the MTVS problem, we propose MuvieNeRF, a framework that incorporates\nboth multi-task and cross-view knowledge to simultaneously synthesize multiple\nscene properties. MuvieNeRF integrates two key modules, the Cross-Task\nAttention (CTA) and Cross-View Attention (CVA) modules, enabling the efficient\nuse of information across multiple views and tasks. Extensive evaluation on\nboth synthetic and realistic benchmarks demonstrates that MuvieNeRF is capable\nof simultaneously synthesizing different scene properties with promising visual\nquality, even outperforming conventional discriminative models in various\nsettings. Notably, we show that MuvieNeRF exhibits universal applicability\nacross a range of NeRF backbones. Our code is available at\nhttps://github.com/zsh2000/MuvieNeRF.",
        "translated": ""
    },
    {
        "title": "SMPLer-X: Scaling Up Expressive Human Pose and Shape Estimation",
        "url": "http://arxiv.org/abs/2309.17448v1",
        "pub_date": "2023-09-29",
        "summary": "Expressive human pose and shape estimation (EHPS) unifies body, hands, and\nface motion capture with numerous applications. Despite encouraging progress,\ncurrent state-of-the-art methods still depend largely on confined training\ndatasets. In this work, we investigate scaling up EHPS towards the first\ngeneralist foundation model (dubbed SMPLer-X), with up to ViT-Huge as the\nbackbone and training with up to 4.5M instances from diverse data sources. With\nbig data and the large model, SMPLer-X exhibits strong performance across\ndiverse test benchmarks and excellent transferability to even unseen\nenvironments. 1) For the data scaling, we perform a systematic investigation on\n32 EHPS datasets, encompassing a wide range of scenarios that a model trained\non any single dataset cannot handle. More importantly, capitalizing on insights\nobtained from the extensive benchmarking process, we optimize our training\nscheme and select datasets that lead to a significant leap in EHPS\ncapabilities. 2) For the model scaling, we take advantage of vision\ntransformers to study the scaling law of model sizes in EHPS. Moreover, our\nfinetuning strategy turn SMPLer-X into specialist models, allowing them to\nachieve further performance boosts. Notably, our foundation model SMPLer-X\nconsistently delivers state-of-the-art results on seven benchmarks such as\nAGORA (107.2 mm NMVE), UBody (57.4 mm PVE), EgoBody (63.6 mm PVE), and EHF\n(62.3 mm PVE without finetuning).",
        "translated": ""
    },
    {
        "title": "LLM-grounded Video Diffusion Models",
        "url": "http://arxiv.org/abs/2309.17444v1",
        "pub_date": "2023-09-29",
        "summary": "Text-conditioned diffusion models have emerged as a promising tool for neural\nvideo generation. However, current models still struggle with intricate\nspatiotemporal prompts and often generate restricted or incorrect motion (e.g.,\neven lacking the ability to be prompted for objects moving from left to right).\nTo address these limitations, we introduce LLM-grounded Video Diffusion (LVD).\nInstead of directly generating videos from the text inputs, LVD first leverages\na large language model (LLM) to generate dynamic scene layouts based on the\ntext inputs and subsequently uses the generated layouts to guide a diffusion\nmodel for video generation. We show that LLMs are able to understand complex\nspatiotemporal dynamics from text alone and generate layouts that align closely\nwith both the prompts and the object motion patterns typically observed in the\nreal world. We then propose to guide video diffusion models with these layouts\nby adjusting the attention maps. Our approach is training-free and can be\nintegrated into any video diffusion model that admits classifier guidance. Our\nresults demonstrate that LVD significantly outperforms its base video diffusion\nmodel and several strong baseline methods in faithfully generating videos with\nthe desired attributes and motion patterns.",
        "translated": ""
    },
    {
        "title": "FACTS: First Amplify Correlations and Then Slice to Discover Bias",
        "url": "http://arxiv.org/abs/2309.17430v1",
        "pub_date": "2023-09-29",
        "summary": "Computer vision datasets frequently contain spurious correlations between\ntask-relevant labels and (easy to learn) latent task-irrelevant attributes\n(e.g. context). Models trained on such datasets learn \"shortcuts\" and\nunderperform on bias-conflicting slices of data where the correlation does not\nhold. In this work, we study the problem of identifying such slices to inform\ndownstream bias mitigation strategies. We propose First Amplify Correlations\nand Then Slice to Discover Bias (FACTS), wherein we first amplify correlations\nto fit a simple bias-aligned hypothesis via strongly regularized empirical risk\nminimization. Next, we perform correlation-aware slicing via mixture modeling\nin bias-aligned feature space to discover underperforming data slices that\ncapture distinct correlations. Despite its simplicity, our method considerably\nimproves over prior work (by as much as 35% precision@10) in correlation bias\nidentification across a range of diverse evaluation settings. Our code is\navailable at: https://github.com/yvsriram/FACTS.",
        "translated": ""
    },
    {
        "title": "Classification of Potholes Based on Surface Area Using Pre-Trained\n  Models of Convolutional Neural Network",
        "url": "http://arxiv.org/abs/2309.17426v1",
        "pub_date": "2023-09-29",
        "summary": "Potholes are fatal and can cause severe damage to vehicles as well as can\ncause deadly accidents. In South Asian countries, pavement distresses are the\nprimary cause due to poor subgrade conditions, lack of subsurface drainage, and\nexcessive rainfalls. The present research compares the performance of three\npre-trained Convolutional Neural Network (CNN) models, i.e., ResNet 50, ResNet\n18, and MobileNet. At first, pavement images are classified to find whether\nimages contain potholes, i.e., Potholes or Normal. Secondly, pavements images\nare classi-fied into three categories, i.e., Small Pothole, Large Pothole, and\nNormal. Pavement images are taken from 3.5 feet (waist height) and 2 feet.\nMobileNet v2 has an accuracy of 98% for detecting a pothole. The classification\nof images taken at the height of 2 feet has an accuracy value of 87.33%,\n88.67%, and 92% for classifying the large, small, and normal pavement,\nrespectively. Similarly, the classification of the images taken from full of\nwaist (FFW) height has an accuracy value of 98.67%, 98.67%, and 100%.",
        "translated": ""
    },
    {
        "title": "The Dawn of LMMs: Preliminary Explorations with GPT-4V(ision)",
        "url": "http://arxiv.org/abs/2309.17421v1",
        "pub_date": "2023-09-29",
        "summary": "Large multimodal models (LMMs) extend large language models (LLMs) with\nmulti-sensory skills, such as visual understanding, to achieve stronger generic\nintelligence. In this paper, we analyze the latest model, GPT-4V(ision), to\ndeepen the understanding of LMMs. The analysis focuses on the intriguing tasks\nthat GPT-4V can perform, containing test samples to probe the quality and\ngenericity of GPT-4V's capabilities, its supported inputs and working modes,\nand the effective ways to prompt the model. In our approach to exploring\nGPT-4V, we curate and organize a collection of carefully designed qualitative\nsamples spanning a variety of domains and tasks. Observations from these\nsamples demonstrate that GPT-4V's unprecedented ability in processing\narbitrarily interleaved multimodal inputs and the genericity of its\ncapabilities together make GPT-4V a powerful multimodal generalist system.\nFurthermore, GPT-4V's unique capability of understanding visual markers drawn\non input images can give rise to new human-computer interaction methods such as\nvisual referring prompting. We conclude the report with in-depth discussions on\nthe emerging application scenarios and the future research directions for\nGPT-4V-based systems. We hope that this preliminary exploration will inspire\nfuture research on the next-generation multimodal task formulation, new ways to\nexploit and enhance LMMs to solve real-world problems, and gaining better\nunderstanding of multimodal foundation models.",
        "translated": ""
    },
    {
        "title": "Directly Fine-Tuning Diffusion Models on Differentiable Rewards",
        "url": "http://arxiv.org/abs/2309.17400v1",
        "pub_date": "2023-09-29",
        "summary": "We present Direct Reward Fine-Tuning (DRaFT), a simple and effective method\nfor fine-tuning diffusion models to maximize differentiable reward functions,\nsuch as scores from human preference models. We first show that it is possible\nto backpropagate the reward function gradient through the full sampling\nprocedure, and that doing so achieves strong performance on a variety of\nrewards, outperforming reinforcement learning-based approaches. We then propose\nmore efficient variants of DRaFT: DRaFT-K, which truncates backpropagation to\nonly the last K steps of sampling, and DRaFT-LV, which obtains lower-variance\ngradient estimates for the case when K=1. We show that our methods work well\nfor a variety of reward functions and can be used to substantially improve the\naesthetic quality of images generated by Stable Diffusion 1.4. Finally, we draw\nconnections between our approach and prior work, providing a unifying\nperspective on the design space of gradient-based fine-tuning algorithms.",
        "translated": ""
    },
    {
        "title": "IFAST: Weakly Supervised Interpretable Face Anti-spoofing from\n  Single-shot Binocular NIR Images",
        "url": "http://arxiv.org/abs/2309.17399v1",
        "pub_date": "2023-09-29",
        "summary": "Single-shot face anti-spoofing (FAS) is a key technique for securing face\nrecognition systems, and it requires only static images as input. However,\nsingle-shot FAS remains a challenging and under-explored problem due to two\nmain reasons: 1) on the data side, learning FAS from RGB images is largely\ncontext-dependent, and single-shot images without additional annotations\ncontain limited semantic information. 2) on the model side, existing\nsingle-shot FAS models are infeasible to provide proper evidence for their\ndecisions, and FAS methods based on depth estimation require expensive\nper-pixel annotations. To address these issues, a large binocular NIR image\ndataset (BNI-FAS) is constructed and published, which contains more than\n300,000 real face and plane attack images, and an Interpretable FAS Transformer\n(IFAST) is proposed that requires only weak supervision to produce\ninterpretable predictions. Our IFAST can produce pixel-wise disparity maps by\nthe proposed disparity estimation Transformer with Dynamic Matching Attention\n(DMA) block. Besides, a well-designed confidence map generator is adopted to\ncooperate with the proposed dual-teacher distillation module to obtain the\nfinal discriminant results. The comprehensive experiments show that our IFAST\ncan achieve state-of-the-art results on BNI-FAS, proving the effectiveness of\nthe single-shot FAS based on binocular NIR images.",
        "translated": ""
    },
    {
        "title": "Forward Flow for Novel View Synthesis of Dynamic Scenes",
        "url": "http://arxiv.org/abs/2309.17390v1",
        "pub_date": "2023-09-29",
        "summary": "This paper proposes a neural radiance field (NeRF) approach for novel view\nsynthesis of dynamic scenes using forward warping. Existing methods often adopt\na static NeRF to represent the canonical space, and render dynamic images at\nother time steps by mapping the sampled 3D points back to the canonical space\nwith the learned backward flow field. However, this backward flow field is\nnon-smooth and discontinuous, which is difficult to be fitted by commonly used\nsmooth motion models. To address this problem, we propose to estimate the\nforward flow field and directly warp the canonical radiance field to other time\nsteps. Such forward flow field is smooth and continuous within the object\nregion, which benefits the motion model learning. To achieve this goal, we\nrepresent the canonical radiance field with voxel grids to enable efficient\nforward warping, and propose a differentiable warping process, including an\naverage splatting operation and an inpaint network, to resolve the many-to-one\nand one-to-many mapping issues. Thorough experiments show that our method\noutperforms existing methods in both novel view rendering and motion modeling,\ndemonstrating the effectiveness of our forward flow motion modeling. Project\npage: https://npucvr.github.io/ForwardFlowDNeRF",
        "translated": ""
    },
    {
        "title": "Prompt-based test-time real image dehazing: a novel pipeline",
        "url": "http://arxiv.org/abs/2309.17389v1",
        "pub_date": "2023-09-29",
        "summary": "Existing methods attempt to improve models' generalization ability on\nreal-world hazy images by exploring well-designed training schemes (e.g.,\ncycleGAN, prior loss). However, most of them need very complicated training\nprocedures to achieve satisfactory results. In this work, we present a totally\nnovel testing pipeline called Prompt-based Test-Time Dehazing (PTTD) to help\ngenerate visually pleasing results of real-captured hazy images during the\ninference phase. We experimentally find that given a dehazing model trained on\nsynthetic data, by fine-tuning the statistics (i.e., mean and standard\ndeviation) of encoding features, PTTD is able to narrow the domain gap,\nboosting the performance of real image dehazing. Accordingly, we first apply a\nprompt generation module (PGM) to generate a visual prompt, which is the source\nof appropriate statistical perturbations for mean and standard deviation. And\nthen, we employ the feature adaptation module (FAM) into the existing dehazing\nmodels for adjusting the original statistics with the guidance of the generated\nprompt. Note that, PTTD is model-agnostic and can be equipped with various\nstate-of-the-art dehazing models trained on synthetic hazy-clean pairs.\nExtensive experimental results demonstrate that our PTTD is flexible meanwhile\nachieves superior performance against state-of-the-art dehazing methods in\nreal-world scenarios.",
        "translated": ""
    },
    {
        "title": "DREAM: Visual Decoding from Reversing Human Visual System",
        "url": "http://arxiv.org/abs/2310.02265v1",
        "pub_date": "2023-10-03",
        "summary": "In this work we present DREAM, an fMRI-to-image method for reconstructing\nviewed images from brain activities, grounded on fundamental knowledge of the\nhuman visual system. We craft reverse pathways that emulate the hierarchical\nand parallel nature of how humans perceive the visual world. These tailored\npathways are specialized to decipher semantics, color, and depth cues from fMRI\ndata, mirroring the forward pathways from visual stimuli to fMRI recordings. To\ndo so, two components mimic the inverse processes within the human visual\nsystem: the Reverse Visual Association Cortex (R-VAC) which reverses pathways\nof this brain region, extracting semantics from fMRI data; the Reverse Parallel\nPKM (R-PKM) component simultaneously predicting color and depth from fMRI\nsignals. The experiments indicate that our method outperforms the current\nstate-of-the-art models in terms of the consistency of appearance, structure,\nand semantics. Code will be made publicly available to facilitate further\nresearch in this field.",
        "translated": ""
    },
    {
        "title": "Generalizable Long-Horizon Manipulations with Large Language Models",
        "url": "http://arxiv.org/abs/2310.02264v1",
        "pub_date": "2023-10-03",
        "summary": "This work introduces a framework harnessing the capabilities of Large\nLanguage Models (LLMs) to generate primitive task conditions for generalizable\nlong-horizon manipulations with novel objects and unseen tasks. These task\nconditions serve as guides for the generation and adjustment of Dynamic\nMovement Primitives (DMP) trajectories for long-horizon task execution. We\nfurther create a challenging robotic manipulation task suite based on Pybullet\nfor long-horizon task evaluation. Extensive experiments in both simulated and\nreal-world environments demonstrate the effectiveness of our framework on both\nfamiliar tasks involving new objects and novel but related tasks, highlighting\nthe potential of LLMs in enhancing robotic system versatility and adaptability.\nProject website: https://object814.github.io/Task-Condition-With-LLM/",
        "translated": ""
    },
    {
        "title": "RSRD: A Road Surface Reconstruction Dataset and Benchmark for Safe and\n  Comfortable Autonomous Driving",
        "url": "http://arxiv.org/abs/2310.02262v1",
        "pub_date": "2023-10-03",
        "summary": "This paper addresses the growing demands for safety and comfort in\nintelligent robot systems, particularly autonomous vehicles, where road\nconditions play a pivotal role in overall driving performance. For example,\nreconstructing road surfaces helps to enhance the analysis and prediction of\nvehicle responses for motion planning and control systems. We introduce the\nRoad Surface Reconstruction Dataset (RSRD), a real-world, high-resolution, and\nhigh-precision dataset collected with a specialized platform in diverse driving\nconditions. It covers common road types containing approximately 16,000 pairs\nof stereo images, original point clouds, and ground-truth depth/disparity maps,\nwith accurate post-processing pipelines to ensure its quality. Based on RSRD,\nwe further build a comprehensive benchmark for recovering road profiles through\ndepth estimation and stereo matching. Preliminary evaluations with various\nstate-of-the-art methods reveal the effectiveness of our dataset and the\nchallenge of the task, underscoring substantial opportunities of RSRD as a\nvaluable resource for advancing techniques, e.g., multi-view stereo towards\nsafe autonomous driving. The dataset and demo videos are available at\nhttps://thu-rsxd.com/rsrd/",
        "translated": ""
    },
    {
        "title": "TransRadar: Adaptive-Directional Transformer for Real-Time Multi-View\n  Radar Semantic Segmentation",
        "url": "http://arxiv.org/abs/2310.02260v1",
        "pub_date": "2023-10-03",
        "summary": "Scene understanding plays an essential role in enabling autonomous driving\nand maintaining high standards of performance and safety. To address this task,\ncameras and laser scanners (LiDARs) have been the most commonly used sensors,\nwith radars being less popular. Despite that, radars remain low-cost,\ninformation-dense, and fast-sensing techniques that are resistant to adverse\nweather conditions. While multiple works have been previously presented for\nradar-based scene semantic segmentation, the nature of the radar data still\nposes a challenge due to the inherent noise and sparsity, as well as the\ndisproportionate foreground and background. In this work, we propose a novel\napproach to the semantic segmentation of radar scenes using a multi-input\nfusion of radar data through a novel architecture and loss functions that are\ntailored to tackle the drawbacks of radar perception. Our novel architecture\nincludes an efficient attention block that adaptively captures important\nfeature information. Our method, TransRadar, outperforms state-of-the-art\nmethods on the CARRADA and RADIal datasets while having smaller model sizes.\nhttps://github.com/YahiDar/TransRadar",
        "translated": ""
    },
    {
        "title": "MathVista: Evaluating Mathematical Reasoning of Foundation Models in\n  Visual Contexts",
        "url": "http://arxiv.org/abs/2310.02255v1",
        "pub_date": "2023-10-03",
        "summary": "Although Large Language Models (LLMs) and Large Multimodal Models (LMMs)\nexhibit impressive skills in various domains, their ability for mathematical\nreasoning within visual contexts has not been formally examined. Equipping LLMs\nand LMMs with this capability is vital for general-purpose AI assistants and\nshowcases promising potential in education, data analysis, and scientific\ndiscovery. To bridge this gap, we present MathVista, a benchmark designed to\namalgamate challenges from diverse mathematical and visual tasks. We first\ntaxonomize the key task types, reasoning skills, and visual contexts from the\nliterature to guide our selection from 28 existing math-focused and visual\nquestion answering datasets. Then, we construct three new datasets, IQTest,\nFunctionQA, and PaperQA, to accommodate for missing types of visual contexts.\nThe problems featured often require deep visual understanding beyond OCR or\nimage captioning, and compositional reasoning with rich domain-specific tools,\nthus posing a notable challenge to existing models. We conduct a comprehensive\nevaluation of 11 prominent open-source and proprietary foundation models (LLMs,\nLLMs augmented with tools, and LMMs), and early experiments with GPT-4V. The\nbest-performing model, Multimodal Bard, achieves only 58% of human performance\n(34.8% vs 60.3%), indicating ample room for further improvement. Given this\nsignificant gap, MathVista fuels future research in the development of\ngeneral-purpose AI agents capable of tackling mathematically intensive and\nvisually rich real-world tasks. Preliminary tests show that MathVista also\npresents challenges to GPT-4V, underscoring the benchmark's importance. The\nproject is available at https://mathvista.github.io/.",
        "translated": ""
    },
    {
        "title": "Talk2BEV: Language-enhanced Bird's-eye View Maps for Autonomous Driving",
        "url": "http://arxiv.org/abs/2310.02251v1",
        "pub_date": "2023-10-03",
        "summary": "Talk2BEV is a large vision-language model (LVLM) interface for bird's-eye\nview (BEV) maps in autonomous driving contexts. While existing perception\nsystems for autonomous driving scenarios have largely focused on a pre-defined\n(closed) set of object categories and driving scenarios, Talk2BEV blends recent\nadvances in general-purpose language and vision models with BEV-structured map\nrepresentations, eliminating the need for task-specific models. This enables a\nsingle system to cater to a variety of autonomous driving tasks encompassing\nvisual and spatial reasoning, predicting the intents of traffic actors, and\ndecision-making based on visual cues. We extensively evaluate Talk2BEV on a\nlarge number of scene understanding tasks that rely on both the ability to\ninterpret free-form natural language queries, and in grounding these queries to\nthe visual context embedded into the language-enhanced BEV map. To enable\nfurther research in LVLMs for autonomous driving scenarios, we develop and\nrelease Talk2BEV-Bench, a benchmark encompassing 1000 human-annotated BEV\nscenarios, with more than 20,000 questions and ground-truth responses from the\nNuScenes dataset.",
        "translated": ""
    },
    {
        "title": "Hierarchical Generation of Human-Object Interactions with Diffusion\n  Probabilistic Models",
        "url": "http://arxiv.org/abs/2310.02242v1",
        "pub_date": "2023-10-03",
        "summary": "This paper presents a novel approach to generating the 3D motion of a human\ninteracting with a target object, with a focus on solving the challenge of\nsynthesizing long-range and diverse motions, which could not be fulfilled by\nexisting auto-regressive models or path planning-based methods. We propose a\nhierarchical generation framework to solve this challenge. Specifically, our\nframework first generates a set of milestones and then synthesizes the motion\nalong them. Therefore, the long-range motion generation could be reduced to\nsynthesizing several short motion sequences guided by milestones. The\nexperiments on the NSM, COUCH, and SAMP datasets show that our approach\noutperforms previous methods by a large margin in both quality and diversity.\nThe source code is available on our project page\nhttps://zju3dv.github.io/hghoi.",
        "translated": ""
    },
    {
        "title": "MiniGPT-5: Interleaved Vision-and-Language Generation via Generative\n  Vokens",
        "url": "http://arxiv.org/abs/2310.02239v1",
        "pub_date": "2023-10-03",
        "summary": "Large Language Models (LLMs) have garnered significant attention for their\nadvancements in natural language processing, demonstrating unparalleled prowess\nin text comprehension and generation. Yet, the simultaneous generation of\nimages with coherent textual narratives remains an evolving frontier. In\nresponse, we introduce an innovative interleaved vision-and-language generation\ntechnique anchored by the concept of \"generative vokens,\" acting as the bridge\nfor harmonized image-text outputs. Our approach is characterized by a\ndistinctive two-staged training strategy focusing on description-free\nmultimodal generation, where the training requires no comprehensive\ndescriptions of images. To bolster model integrity, classifier-free guidance is\nincorporated, enhancing the effectiveness of vokens on image generation. Our\nmodel, MiniGPT-5, exhibits substantial improvement over the baseline Divter\nmodel on the MMDialog dataset and consistently delivers superior or comparable\nmultimodal outputs in human evaluations on the VIST dataset, highlighting its\nefficacy across diverse benchmarks.",
        "translated": ""
    },
    {
        "title": "Exploring Model Learning Heterogeneity for Boosting Ensemble Robustness",
        "url": "http://arxiv.org/abs/2310.02237v1",
        "pub_date": "2023-10-03",
        "summary": "Deep neural network ensembles hold the potential of improving generalization\nperformance for complex learning tasks. This paper presents formal analysis and\nempirical evaluation to show that heterogeneous deep ensembles with high\nensemble diversity can effectively leverage model learning heterogeneity to\nboost ensemble robustness. We first show that heterogeneous DNN models trained\nfor solving the same learning problem, e.g., object detection, can\nsignificantly strengthen the mean average precision (mAP) through our weighted\nbounding box ensemble consensus method. Second, we further compose ensembles of\nheterogeneous models for solving different learning problems, e.g., object\ndetection and semantic segmentation, by introducing the connected component\nlabeling (CCL) based alignment. We show that this two-tier heterogeneity driven\nensemble construction method can compose an ensemble team that promotes high\nensemble diversity and low negative correlation among member models of the\nensemble, strengthening ensemble robustness against both negative examples and\nadversarial attacks. Third, we provide a formal analysis of the ensemble\nrobustness in terms of negative correlation. Extensive experiments validate the\nenhanced robustness of heterogeneous ensembles in both benign and adversarial\nsettings. The source codes are available on GitHub at\nhttps://github.com/git-disl/HeteRobust.",
        "translated": ""
    },
    {
        "title": "MIS-AVioDD: Modality Invariant and Specific Representation for\n  Audio-Visual Deepfake Detection",
        "url": "http://arxiv.org/abs/2310.02234v1",
        "pub_date": "2023-10-03",
        "summary": "Deepfakes are synthetic media generated using deep generative algorithms and\nhave posed a severe societal and political threat. Apart from facial\nmanipulation and synthetic voice, recently, a novel kind of deepfakes has\nemerged with either audio or visual modalities manipulated. In this regard, a\nnew generation of multimodal audio-visual deepfake detectors is being\ninvestigated to collectively focus on audio and visual data for multimodal\nmanipulation detection. Existing multimodal (audio-visual) deepfake detectors\nare often based on the fusion of the audio and visual streams from the video.\nExisting studies suggest that these multimodal detectors often obtain\nequivalent performances with unimodal audio and visual deepfake detectors. We\nconjecture that the heterogeneous nature of the audio and visual signals\ncreates distributional modality gaps and poses a significant challenge to\neffective fusion and efficient performance. In this paper, we tackle the\nproblem at the representation level to aid the fusion of audio and visual\nstreams for multimodal deepfake detection. Specifically, we propose the joint\nuse of modality (audio and visual) invariant and specific representations. This\nensures that the common patterns and patterns specific to each modality\nrepresenting pristine or fake content are preserved and fused for multimodal\ndeepfake manipulation detection. Our experimental results on FakeAVCeleb and\nKoDF audio-visual deepfake datasets suggest the enhanced accuracy of our\nproposed method over SOTA unimodal and multimodal audio-visual deepfake\ndetectors by $17.8$% and $18.4$%, respectively. Thus, obtaining\nstate-of-the-art performance.",
        "translated": ""
    },
    {
        "title": "LanguageMPC: Large Language Models as Decision Makers for Autonomous\n  Driving",
        "url": "http://arxiv.org/abs/2310.03026v1",
        "pub_date": "2023-10-04",
        "summary": "Existing learning-based autonomous driving (AD) systems face challenges in\ncomprehending high-level information, generalizing to rare events, and\nproviding interpretability. To address these problems, this work employs Large\nLanguage Models (LLMs) as a decision-making component for complex AD scenarios\nthat require human commonsense understanding. We devise cognitive pathways to\nenable comprehensive reasoning with LLMs, and develop algorithms for\ntranslating LLM decisions into actionable driving commands. Through this\napproach, LLM decisions are seamlessly integrated with low-level controllers by\nguided parameter matrix adaptation. Extensive experiments demonstrate that our\nproposed method not only consistently surpasses baseline approaches in\nsingle-vehicle tasks, but also helps handle complex driving behaviors even\nmulti-vehicle coordination, thanks to the commonsense reasoning capabilities of\nLLMs. This paper presents an initial step toward leveraging LLMs as effective\ndecision-makers for intricate AD scenarios in terms of safety, efficiency,\ngeneralizability, and interoperability. We aspire for it to serve as\ninspiration for future research in this field. Project page:\nhttps://sites.google.com/view/llm-mpc",
        "translated": ""
    },
    {
        "title": "Human-oriented Representation Learning for Robotic Manipulation",
        "url": "http://arxiv.org/abs/2310.03023v1",
        "pub_date": "2023-10-04",
        "summary": "Humans inherently possess generalizable visual representations that empower\nthem to efficiently explore and interact with the environments in manipulation\ntasks. We advocate that such a representation automatically arises from\nsimultaneously learning about multiple simple perceptual skills that are\ncritical for everyday scenarios (e.g., hand detection, state estimate, etc.)\nand is better suited for learning robot manipulation policies compared to\ncurrent state-of-the-art visual representations purely based on self-supervised\nobjectives. We formalize this idea through the lens of human-oriented\nmulti-task fine-tuning on top of pre-trained visual encoders, where each task\nis a perceptual skill tied to human-environment interactions. We introduce Task\nFusion Decoder as a plug-and-play embedding translator that utilizes the\nunderlying relationships among these perceptual skills to guide the\nrepresentation learning towards encoding meaningful structure for what's\nimportant for all perceptual skills, ultimately empowering learning of\ndownstream robotic manipulation tasks. Extensive experiments across a range of\nrobotic tasks and embodiments, in both simulations and real-world environments,\nshow that our Task Fusion Decoder consistently improves the representation of\nthree state-of-the-art visual encoders including R3M, MVP, and EgoVLP, for\ndownstream manipulation policy-learning. Project page:\nhttps://sites.google.com/view/human-oriented-robot-learning",
        "translated": ""
    },
    {
        "title": "Consistent-1-to-3: Consistent Image to 3D View Synthesis via\n  Geometry-aware Diffusion Models",
        "url": "http://arxiv.org/abs/2310.03020v1",
        "pub_date": "2023-10-04",
        "summary": "Zero-shot novel view synthesis (NVS) from a single image is an essential\nproblem in 3D object understanding. While recent approaches that leverage\npre-trained generative models can synthesize high-quality novel views from\nin-the-wild inputs, they still struggle to maintain 3D consistency across\ndifferent views. In this paper, we present Consistent-1-to-3, which is a\ngenerative framework that significantly mitigate this issue. Specifically, we\ndecompose the NVS task into two stages: (i) transforming observed regions to a\nnovel view, and (ii) hallucinating unseen regions. We design a scene\nrepresentation transformer and view-conditioned diffusion model for performing\nthese two stages respectively. Inside the models, to enforce 3D consistency, we\npropose to employ epipolor-guided attention to incorporate geometry\nconstraints, and multi-view attention to better aggregate multi-view\ninformation. Finally, we design a hierarchy generation paradigm to generate\nlong sequences of consistent views, allowing a full 360 observation of the\nprovided object image. Qualitative and quantitative evaluation over multiple\ndatasets demonstrate the effectiveness of the proposed mechanisms against\nstate-of-the-art approaches. Our project page is at\nhttps://jianglongye.com/consistent123/",
        "translated": ""
    },
    {
        "title": "Efficient-3DiM: Learning a Generalizable Single-image Novel-view\n  Synthesizer in One Day",
        "url": "http://arxiv.org/abs/2310.03015v1",
        "pub_date": "2023-10-04",
        "summary": "The task of novel view synthesis aims to generate unseen perspectives of an\nobject or scene from a limited set of input images. Nevertheless, synthesizing\nnovel views from a single image still remains a significant challenge in the\nrealm of computer vision. Previous approaches tackle this problem by adopting\nmesh prediction, multi-plain image construction, or more advanced techniques\nsuch as neural radiance fields. Recently, a pre-trained diffusion model that is\nspecifically designed for 2D image synthesis has demonstrated its capability in\nproducing photorealistic novel views, if sufficiently optimized on a 3D\nfinetuning task. Although the fidelity and generalizability are greatly\nimproved, training such a powerful diffusion model requires a vast volume of\ntraining data and model parameters, resulting in a notoriously long time and\nhigh computational costs. To tackle this issue, we propose Efficient-3DiM, a\nsimple but effective framework to learn a single-image novel-view synthesizer.\nMotivated by our in-depth analysis of the inference process of diffusion\nmodels, we propose several pragmatic strategies to reduce the training overhead\nto a manageable scale, including a crafted timestep sampling strategy, a\nsuperior 3D feature extractor, and an enhanced training scheme. When combined,\nour framework is able to reduce the total training time from 10 days to less\nthan 1 day, significantly accelerating the training process under the same\ncomputational platform (one instance with 8 Nvidia A100 GPUs). Comprehensive\nexperiments are conducted to demonstrate the efficiency and generalizability of\nour proposed method.",
        "translated": ""
    },
    {
        "title": "Towards Domain-Specific Features Disentanglement for Domain\n  Generalization",
        "url": "http://arxiv.org/abs/2310.03007v1",
        "pub_date": "2023-10-04",
        "summary": "Distributional shift between domains poses great challenges to modern machine\nlearning algorithms. The domain generalization (DG) signifies a popular line\ntargeting this issue, where these methods intend to uncover universal patterns\nacross disparate distributions. Noted, the crucial challenge behind DG is the\nexistence of irrelevant domain features, and most prior works overlook this\ninformation. Motivated by this, we propose a novel contrastive-based\ndisentanglement method CDDG, to effectively utilize the disentangled features\nto exploit the over-looked domain-specific features, and thus facilitating the\nextraction of the desired cross-domain category features for DG tasks.\nSpecifically, CDDG learns to decouple inherent mutually exclusive features by\nleveraging them in the latent space, thus making the learning discriminative.\nExtensive experiments conducted on various benchmark datasets demonstrate the\nsuperiority of our method compared to other state-of-the-art approaches.\nFurthermore, visualization evaluations confirm the potential of our method in\nachieving effective feature disentanglement.",
        "translated": ""
    },
    {
        "title": "COOLer: Class-Incremental Learning for Appearance-Based Multiple Object\n  Tracking",
        "url": "http://arxiv.org/abs/2310.03006v2",
        "pub_date": "2023-10-04",
        "summary": "Continual learning allows a model to learn multiple tasks sequentially while\nretaining the old knowledge without the training data of the preceding tasks.\nThis paper extends the scope of continual learning research to\nclass-incremental learning for multiple object tracking (MOT), which is\ndesirable to accommodate the continuously evolving needs of autonomous systems.\nPrevious solutions for continual learning of object detectors do not address\nthe data association stage of appearance-based trackers, leading to\ncatastrophic forgetting of previous classes' re-identification features. We\nintroduce COOLer, a COntrastive- and cOntinual-Learning-based tracker, which\nincrementally learns to track new categories while preserving past knowledge by\ntraining on a combination of currently available ground truth labels and\npseudo-labels generated by the past tracker. To further exacerbate the\ndisentanglement of instance representations, we introduce a novel contrastive\nclass-incremental instance representation learning technique. Finally, we\npropose a practical evaluation protocol for continual learning for MOT and\nconduct experiments on the BDD100K and SHIFT datasets. Experimental results\ndemonstrate that COOLer continually learns while effectively addressing\ncatastrophic forgetting of both tracking and detection. The code is available\nat https://github.com/BoSmallEar/COOLer.",
        "translated": ""
    },
    {
        "title": "Reversing Deep Face Embeddings with Probable Privacy Protection",
        "url": "http://arxiv.org/abs/2310.03005v1",
        "pub_date": "2023-10-04",
        "summary": "Generally, privacy-enhancing face recognition systems are designed to offer\npermanent protection of face embeddings. Recently, so-called soft-biometric\nprivacy-enhancement approaches have been introduced with the aim of canceling\nsoft-biometric attributes. These methods limit the amount of soft-biometric\ninformation (gender or skin-colour) that can be inferred from face embeddings.\nPrevious work has underlined the need for research into rigorous evaluations\nand standardised evaluation protocols when assessing privacy protection\ncapabilities. Motivated by this fact, this paper explores to what extent the\nnon-invertibility requirement can be met by methods that claim to provide\nsoft-biometric privacy protection. Additionally, a detailed vulnerability\nassessment of state-of-the-art face embedding extractors is analysed in terms\nof the transformation complexity used for privacy protection. In this context,\na well-known state-of-the-art face image reconstruction approach has been\nevaluated on protected face embeddings to break soft biometric privacy\nprotection. Experimental results show that biometric privacy-enhanced face\nembeddings can be reconstructed with an accuracy of up to approximately 98%,\ndepending on the complexity of the protection algorithm.",
        "translated": ""
    },
    {
        "title": "Soft Convex Quantization: Revisiting Vector Quantization with Convex\n  Optimization",
        "url": "http://arxiv.org/abs/2310.03004v1",
        "pub_date": "2023-10-04",
        "summary": "Vector Quantization (VQ) is a well-known technique in deep learning for\nextracting informative discrete latent representations. VQ-embedded models have\nshown impressive results in a range of applications including image and speech\ngeneration. VQ operates as a parametric K-means algorithm that quantizes inputs\nusing a single codebook vector in the forward pass. While powerful, this\ntechnique faces practical challenges including codebook collapse,\nnon-differentiability and lossy compression. To mitigate the aforementioned\nissues, we propose Soft Convex Quantization (SCQ) as a direct substitute for\nVQ. SCQ works like a differentiable convex optimization (DCO) layer: in the\nforward pass, we solve for the optimal convex combination of codebook vectors\nthat quantize the inputs. In the backward pass, we leverage differentiability\nthrough the optimality conditions of the forward solution. We then introduce a\nscalable relaxation of the SCQ optimization and demonstrate its efficacy on the\nCIFAR-10, GTSRB and LSUN datasets. We train powerful SCQ autoencoder models\nthat significantly outperform matched VQ-based architectures, observing an\norder of magnitude better image reconstruction and codebook usage with\ncomparable quantization runtime.",
        "translated": ""
    },
    {
        "title": "ECoFLaP: Efficient Coarse-to-Fine Layer-Wise Pruning for Vision-Language\n  Models",
        "url": "http://arxiv.org/abs/2310.02998v1",
        "pub_date": "2023-10-04",
        "summary": "Large Vision-Language Models (LVLMs) can understand the world comprehensively\nby integrating rich information from different modalities, achieving remarkable\nperformance improvements on various multimodal downstream tasks. However,\ndeploying LVLMs is often problematic due to their massive computational/energy\ncosts and carbon consumption. Such issues make it infeasible to adopt\nconventional iterative global pruning, which is costly due to computing the\nHessian matrix of the entire large model for sparsification. Alternatively,\nseveral studies have recently proposed layer-wise pruning approaches to avoid\nthe expensive computation of global pruning and efficiently compress model\nweights according to their importance within a layer. However, these methods\noften suffer from suboptimal model compression due to their lack of a global\nperspective. To address this limitation in recent efficient pruning methods for\nlarge models, we propose Efficient Coarse-to-Fine Layer-Wise Pruning (ECoFLaP),\na two-stage coarse-to-fine weight pruning approach for LVLMs. We first\ndetermine the sparsity ratios of different layers or blocks by leveraging the\nglobal importance score, which is efficiently computed based on the\nzeroth-order approximation of the global model gradients. Then, the multimodal\nmodel performs local layer-wise unstructured weight pruning based on\nglobally-informed sparsity ratios. We validate our proposed method across\nvarious multimodal and unimodal models and datasets, demonstrating significant\nperformance improvements over prevalent pruning techniques in the high-sparsity\nregime.",
        "translated": ""
    },
    {
        "title": "Optimizing Key-Selection for Face-based One-Time Biometrics via Morphing",
        "url": "http://arxiv.org/abs/2310.02997v1",
        "pub_date": "2023-10-04",
        "summary": "Nowadays, facial recognition systems are still vulnerable to adversarial\nattacks. These attacks vary from simple perturbations of the input image to\nmodifying the parameters of the recognition model to impersonate an authorised\nsubject. So-called privacy-enhancing facial recognition systems have been\nmostly developed to provide protection of stored biometric reference data, i.e.\ntemplates. In the literature, privacy-enhancing facial recognition approaches\nhave focused solely on conventional security threats at the template level,\nignoring the growing concern related to adversarial attacks. Up to now, few\nworks have provided mechanisms to protect face recognition against adversarial\nattacks while maintaining high security at the template level. In this paper,\nwe propose different key selection strategies to improve the security of a\ncompetitive cancelable scheme operating at the signal level. Experimental\nresults show that certain strategies based on signal-level key selection can\nlead to complete blocking of the adversarial attack based on an iterative\noptimization for the most secure threshold, while for the most practical\nthreshold, the attack success chance can be decreased to approximately 5.0%.",
        "translated": ""
    },
    {
        "title": "Improved Baselines with Visual Instruction Tuning",
        "url": "http://arxiv.org/abs/2310.03744v1",
        "pub_date": "2023-10-05",
        "summary": "Large multimodal models (LMM) have recently shown encouraging progress with\nvisual instruction tuning. In this note, we show that the fully-connected\nvision-language cross-modal connector in LLaVA is surprisingly powerful and\ndata-efficient. With simple modifications to LLaVA, namely, using\nCLIP-ViT-L-336px with an MLP projection and adding academic-task-oriented VQA\ndata with simple response formatting prompts, we establish stronger baselines\nthat achieve state-of-the-art across 11 benchmarks. Our final 13B checkpoint\nuses merely 1.2M publicly available data, and finishes full training in ~1 day\non a single 8-A100 node. We hope this can make state-of-the-art LMM research\nmore accessible. Code and model will be publicly available.",
        "translated": ""
    },
    {
        "title": "ContactGen: Generative Contact Modeling for Grasp Generation",
        "url": "http://arxiv.org/abs/2310.03740v1",
        "pub_date": "2023-10-05",
        "summary": "This paper presents a novel object-centric contact representation ContactGen\nfor hand-object interaction. The ContactGen comprises three components: a\ncontact map indicates the contact location, a part map represents the contact\nhand part, and a direction map tells the contact direction within each part.\nGiven an input object, we propose a conditional generative model to predict\nContactGen and adopt model-based optimization to predict diverse and\ngeometrically feasible grasps. Experimental results demonstrate our method can\ngenerate high-fidelity and diverse human grasps for various objects. Project\npage: https://stevenlsw.github.io/contactgen/",
        "translated": ""
    },
    {
        "title": "Aligning Text-to-Image Diffusion Models with Reward Backpropagation",
        "url": "http://arxiv.org/abs/2310.03739v1",
        "pub_date": "2023-10-05",
        "summary": "Text-to-image diffusion models have recently emerged at the forefront of\nimage generation, powered by very large-scale unsupervised or weakly supervised\ntext-to-image training datasets. Due to their unsupervised training,\ncontrolling their behavior in downstream tasks, such as maximizing\nhuman-perceived image quality, image-text alignment, or ethical image\ngeneration, is difficult. Recent works finetune diffusion models to downstream\nreward functions using vanilla reinforcement learning, notorious for the high\nvariance of the gradient estimators. In this paper, we propose AlignProp, a\nmethod that aligns diffusion models to downstream reward functions using\nend-to-end backpropagation of the reward gradient through the denoising\nprocess. While naive implementation of such backpropagation would require\nprohibitive memory resources for storing the partial derivatives of modern\ntext-to-image models, AlignProp finetunes low-rank adapter weight modules and\nuses gradient checkpointing, to render its memory usage viable. We test\nAlignProp in finetuning diffusion models to various objectives, such as\nimage-text semantic alignment, aesthetics, compressibility and controllability\nof the number of objects present, as well as their combinations. We show\nAlignProp achieves higher rewards in fewer training steps than alternatives,\nwhile being conceptually simpler, making it a straightforward choice for\noptimizing diffusion models for differentiable reward functions of interest.\nCode and Visualization results are available at https://align-prop.github.io/.",
        "translated": ""
    },
    {
        "title": "Stylist: Style-Driven Feature Ranking for Robust Novelty Detection",
        "url": "http://arxiv.org/abs/2310.03738v1",
        "pub_date": "2023-10-05",
        "summary": "Novelty detection aims at finding samples that differ in some form from the\ndistribution of seen samples. But not all changes are created equal. Data can\nsuffer a multitude of distribution shifts, and we might want to detect only\nsome types of relevant changes. Similar to works in out-of-distribution\ngeneralization, we propose to use the formalization of separating into semantic\nor content changes, that are relevant to our task, and style changes, that are\nirrelevant. Within this formalization, we define the robust novelty detection\nas the task of finding semantic changes while being robust to style\ndistributional shifts. Leveraging pretrained, large-scale model\nrepresentations, we introduce Stylist, a novel method that focuses on dropping\nenvironment-biased features. First, we compute a per-feature score based on the\nfeature distribution distances between environments. Next, we show that our\nselection manages to remove features responsible for spurious correlations and\nimprove novelty detection performance. For evaluation, we adapt domain\ngeneralization datasets to our task and analyze the methods behaviors. We\nadditionally built a large synthetic dataset where we have control over the\nspurious correlations degree. We prove that our selection mechanism improves\nnovelty detection algorithms across multiple datasets, containing both\nstylistic and content shifts.",
        "translated": ""
    },
    {
        "title": "Leveraging Unpaired Data for Vision-Language Generative Models via Cycle\n  Consistency",
        "url": "http://arxiv.org/abs/2310.03734v1",
        "pub_date": "2023-10-05",
        "summary": "Current vision-language generative models rely on expansive corpora of paired\nimage-text data to attain optimal performance and generalization capabilities.\nHowever, automatically collecting such data (e.g. via large-scale web scraping)\nleads to low quality and poor image-text correlation, while human annotation is\nmore accurate but requires significant manual effort and expense. We introduce\n$\\textbf{ITIT}$ ($\\textbf{I}$n$\\textbf{T}$egrating $\\textbf{I}$mage\n$\\textbf{T}$ext): an innovative training paradigm grounded in the concept of\ncycle consistency which allows vision-language training on unpaired image and\ntext data. ITIT is comprised of a joint image-text encoder with disjoint image\nand text decoders that enable bidirectional image-to-text and text-to-image\ngeneration in a single framework. During training, ITIT leverages a small set\nof paired image-text data to ensure its output matches the input reasonably\nwell in both directions. Simultaneously, the model is also trained on much\nlarger datasets containing only images or texts. This is achieved by enforcing\ncycle consistency between the original unpaired samples and the cycle-generated\ncounterparts. For instance, it generates a caption for a given input image and\nthen uses the caption to create an output image, and enforces similarity\nbetween the input and output images. Our experiments show that ITIT with\nunpaired datasets exhibits similar scaling behavior as using high-quality\npaired data. We demonstrate image generation and captioning performance on par\nwith state-of-the-art text-to-image and image-to-text models with orders of\nmagnitude fewer (only 3M) paired image-text data.",
        "translated": ""
    },
    {
        "title": "MathCoder: Seamless Code Integration in LLMs for Enhanced Mathematical\n  Reasoning",
        "url": "http://arxiv.org/abs/2310.03731v1",
        "pub_date": "2023-10-05",
        "summary": "The recently released GPT-4 Code Interpreter has demonstrated remarkable\nproficiency in solving challenging math problems, primarily attributed to its\nability to seamlessly reason with natural language, generate code, execute\ncode, and continue reasoning based on the execution output. In this paper, we\npresent a method to fine-tune open-source language models, enabling them to use\ncode for modeling and deriving math equations and, consequently, enhancing\ntheir mathematical reasoning abilities. We propose a method of generating novel\nand high-quality datasets with math problems and their code-based solutions,\nreferred to as MathCodeInstruct. Each solution interleaves natural language,\ncode, and execution results. We also introduce a customized supervised\nfine-tuning and inference approach. This approach yields the MathCoder models,\na family of models capable of generating code-based solutions for solving\nchallenging math problems. Impressively, the MathCoder models achieve\nstate-of-the-art scores among open-source LLMs on the MATH (45.2%) and GSM8K\n(83.9%) datasets, substantially outperforming other open-source alternatives.\nNotably, the MathCoder model not only surpasses ChatGPT-3.5 and PaLM-2 on GSM8K\nand MATH but also outperforms GPT-4 on the competition-level MATH dataset. The\ndataset and models will be released at https://github.com/mathllm/MathCoder.",
        "translated": ""
    },
    {
        "title": "OMG-ATTACK: Self-Supervised On-Manifold Generation of Transferable\n  Evasion Attacks",
        "url": "http://arxiv.org/abs/2310.03707v1",
        "pub_date": "2023-10-05",
        "summary": "Evasion Attacks (EA) are used to test the robustness of trained neural\nnetworks by distorting input data to misguide the model into incorrect\nclassifications. Creating these attacks is a challenging task, especially with\nthe ever-increasing complexity of models and datasets. In this work, we\nintroduce a self-supervised, computationally economical method for generating\nadversarial examples, designed for the unseen black-box setting. Adapting\ntechniques from representation learning, our method generates on-manifold EAs\nthat are encouraged to resemble the data distribution. These attacks are\ncomparable in effectiveness compared to the state-of-the-art when attacking the\nmodel trained on, but are significantly more effective when attacking unseen\nmodels, as the attacks are more related to the data rather than the model\nitself. Our experiments consistently demonstrate the method is effective across\nvarious models, unseen data categories, and even defended models, suggesting a\nsignificant role for on-manifold EAs when targeting unseen models.",
        "translated": ""
    },
    {
        "title": "Drag View: Generalizable Novel View Synthesis with Unposed Imagery",
        "url": "http://arxiv.org/abs/2310.03704v1",
        "pub_date": "2023-10-05",
        "summary": "We introduce DragView, a novel and interactive framework for generating novel\nviews of unseen scenes. DragView initializes the new view from a single source\nimage, and the rendering is supported by a sparse set of unposed multi-view\nimages, all seamlessly executed within a single feed-forward pass. Our approach\nbegins with users dragging a source view through a local relative coordinate\nsystem. Pixel-aligned features are obtained by projecting the sampled 3D points\nalong the target ray onto the source view. We then incorporate a view-dependent\nmodulation layer to effectively handle occlusion during the projection.\nAdditionally, we broaden the epipolar attention mechanism to encompass all\nsource pixels, facilitating the aggregation of initialized coordinate-aligned\npoint features from other unposed views. Finally, we employ another transformer\nto decode ray features into final pixel intensities. Crucially, our framework\ndoes not rely on either 2D prior models or the explicit estimation of camera\nposes. During testing, DragView showcases the capability to generalize to new\nscenes unseen during training, also utilizing only unposed support images,\nenabling the generation of photo-realistic new views characterized by flexible\ncamera trajectories. In our experiments, we conduct a comprehensive comparison\nof the performance of DragView with recent scene representation networks\noperating under pose-free conditions, as well as with generalizable NeRFs\nsubject to noisy test camera poses. DragView consistently demonstrates its\nsuperior performance in view synthesis quality, while also being more\nuser-friendly. Project page: https://zhiwenfan.github.io/DragView/.",
        "translated": ""
    },
    {
        "title": "LumiNet: The Bright Side of Perceptual Knowledge Distillation",
        "url": "http://arxiv.org/abs/2310.03669v1",
        "pub_date": "2023-10-05",
        "summary": "In knowledge distillation research, feature-based methods have dominated due\nto their ability to effectively tap into extensive teacher models. In contrast,\nlogit-based approaches are considered to be less adept at extracting hidden\n'dark knowledge' from teachers. To bridge this gap, we present LumiNet, a novel\nknowledge-transfer algorithm designed to enhance logit-based distillation. We\nintroduce a perception matrix that aims to recalibrate logits through\nadjustments based on the model's representation capability. By meticulously\nanalyzing intra-class dynamics, LumiNet reconstructs more granular inter-class\nrelationships, enabling the student model to learn a richer breadth of\nknowledge. Both teacher and student models are mapped onto this refined matrix,\nwith the student's goal being to minimize representational discrepancies.\nRigorous testing on benchmark datasets (CIFAR-100, ImageNet, and MSCOCO)\nattests to LumiNet's efficacy, revealing its competitive edge over leading\nfeature-based methods. Moreover, in exploring the realm of transfer learning,\nwe assess how effectively the student model, trained using our method, adapts\nto downstream tasks. Notably, when applied to Tiny ImageNet, the transferred\nfeatures exhibit remarkable performance, further underscoring LumiNet's\nversatility and robustness in diverse settings. With LumiNet, we hope to steer\nthe research discourse towards a renewed interest in the latent capabilities of\nlogit-based knowledge distillation.",
        "translated": ""
    },
    {
        "title": "Certification of Deep Learning Models for Medical Image Segmentation",
        "url": "http://arxiv.org/abs/2310.03664v1",
        "pub_date": "2023-10-05",
        "summary": "In medical imaging, segmentation models have known a significant improvement\nin the past decade and are now used daily in clinical practice. However,\nsimilar to classification models, segmentation models are affected by\nadversarial attacks. In a safety-critical field like healthcare, certifying\nmodel predictions is of the utmost importance. Randomized smoothing has been\nintroduced lately and provides a framework to certify models and obtain\ntheoretical guarantees. In this paper, we present for the first time a\ncertified segmentation baseline for medical imaging based on randomized\nsmoothing and diffusion models. Our results show that leveraging the power of\ndenoising diffusion probabilistic models helps us overcome the limits of\nrandomized smoothing. We conduct extensive experiments on five public datasets\nof chest X-rays, skin lesions, and colonoscopies, and empirically show that we\nare able to maintain high certified Dice scores even for highly perturbed\nimages. Our work represents the first attempt to certify medical image\nsegmentation models, and we aspire for it to set a foundation for future\nbenchmarks in this crucial and largely uncharted area.",
        "translated": ""
    },
    {
        "title": "Alice Benchmarks: Connecting Real World Object Re-Identification with\n  the Synthetic",
        "url": "http://arxiv.org/abs/2310.04416v1",
        "pub_date": "2023-10-06",
        "summary": "For object re-identification (re-ID), learning from synthetic data has become\na promising strategy to cheaply acquire large-scale annotated datasets and\neffective models, with few privacy concerns. Many interesting research problems\narise from this strategy, e.g., how to reduce the domain gap between synthetic\nsource and real-world target. To facilitate developing more new approaches in\nlearning from synthetic data, we introduce the Alice benchmarks, large-scale\ndatasets providing benchmarks as well as evaluation protocols to the research\ncommunity. Within the Alice benchmarks, two object re-ID tasks are offered:\nperson and vehicle re-ID. We collected and annotated two challenging real-world\ntarget datasets: AlicePerson and AliceVehicle, captured under various\nilluminations, image resolutions, etc. As an important feature of our real\ntarget, the clusterability of its training set is not manually guaranteed to\nmake it closer to a real domain adaptation test scenario. Correspondingly, we\nreuse existing PersonX and VehicleX as synthetic source domains. The primary\ngoal is to train models from synthetic data that can work effectively in the\nreal world. In this paper, we detail the settings of Alice benchmarks, provide\nan analysis of existing commonly-used domain adaptation methods, and discuss\nsome interesting future directions. An online server will be set up for the\ncommunity to evaluate methods conveniently and fairly.",
        "translated": ""
    },
    {
        "title": "CIFAR-10-Warehouse: Broad and More Realistic Testbeds in Model\n  Generalization Analysis",
        "url": "http://arxiv.org/abs/2310.04414v1",
        "pub_date": "2023-10-06",
        "summary": "Analyzing model performance in various unseen environments is a critical\nresearch problem in the machine learning community. To study this problem, it\nis important to construct a testbed with out-of-distribution test sets that\nhave broad coverage of environmental discrepancies. However, existing testbeds\ntypically either have a small number of domains or are synthesized by image\ncorruptions, hindering algorithm design that demonstrates real-world\neffectiveness. In this paper, we introduce CIFAR-10-Warehouse, consisting of\n180 datasets collected by prompting image search engines and diffusion models\nin various ways. Generally sized between 300 and 8,000 images, the datasets\ncontain natural images, cartoons, certain colors, or objects that do not\nnaturally appear. With CIFAR-10-W, we aim to enhance the evaluation and deepen\nthe understanding of two generalization tasks: domain generalization and model\naccuracy prediction in various out-of-distribution environments. We conduct\nextensive benchmarking and comparison experiments and show that CIFAR-10-W\noffers new and interesting insights inherent to these tasks. We also discuss\nother fields that would benefit from CIFAR-10-W.",
        "translated": ""
    },
    {
        "title": "FedConv: Enhancing Convolutional Neural Networks for Handling Data\n  Heterogeneity in Federated Learning",
        "url": "http://arxiv.org/abs/2310.04412v1",
        "pub_date": "2023-10-06",
        "summary": "Federated learning (FL) is an emerging paradigm in machine learning, where a\nshared model is collaboratively learned using data from multiple devices to\nmitigate the risk of data leakage. While recent studies posit that Vision\nTransformer (ViT) outperforms Convolutional Neural Networks (CNNs) in\naddressing data heterogeneity in FL, the specific architectural components that\nunderpin this advantage have yet to be elucidated. In this paper, we\nsystematically investigate the impact of different architectural elements, such\nas activation functions and normalization layers, on the performance within\nheterogeneous FL. Through rigorous empirical analyses, we are able to offer the\nfirst-of-its-kind general guidance on micro-architecture design principles for\nheterogeneous FL.\n  Intriguingly, our findings indicate that with strategic architectural\nmodifications, pure CNNs can achieve a level of robustness that either matches\nor even exceeds that of ViTs when handling heterogeneous data clients in FL.\nAdditionally, our approach is compatible with existing FL techniques and\ndelivers state-of-the-art solutions across a broad spectrum of FL benchmarks.\nThe code is publicly available at https://github.com/UCSC-VLAA/FedConv",
        "translated": ""
    },
    {
        "title": "Language Agent Tree Search Unifies Reasoning Acting and Planning in\n  Language Models",
        "url": "http://arxiv.org/abs/2310.04406v1",
        "pub_date": "2023-10-06",
        "summary": "While large language models (LLMs) have demonstrated impressive performance\non a range of decision-making tasks, they rely on simple acting processes and\nfall short of broad deployment as autonomous agents. We introduce LATS\n(Language Agent Tree Search), a general framework that synergizes the\ncapabilities of LLMs in planning, acting, and reasoning. Drawing inspiration\nfrom Monte Carlo tree search in model-based reinforcement learning, LATS\nemploys LLMs as agents, value functions, and optimizers, repurposing their\nlatent strengths for enhanced decision-making. What is crucial in this method\nis the use of an environment for external feedback, which offers a more\ndeliberate and adaptive problem-solving mechanism that moves beyond the\nlimitations of existing techniques. Our experimental evaluation across diverse\ndomains, such as programming, HotPotQA, and WebShop, illustrates the\napplicability of LATS for both reasoning and acting. In particular, LATS\nachieves 94.4\\% for programming on HumanEval with GPT-4 and an average score of\n75.9 for web browsing on WebShop with GPT-3.5, demonstrating the effectiveness\nand generality of our method.",
        "translated": ""
    },
    {
        "title": "Latent Consistency Models: Synthesizing High-Resolution Images with\n  Few-Step Inference",
        "url": "http://arxiv.org/abs/2310.04378v1",
        "pub_date": "2023-10-06",
        "summary": "Latent Diffusion models (LDMs) have achieved remarkable results in\nsynthesizing high-resolution images. However, the iterative sampling process is\ncomputationally intensive and leads to slow generation. Inspired by Consistency\nModels (song et al.), we propose Latent Consistency Models (LCMs), enabling\nswift inference with minimal steps on any pre-trained LDMs, including Stable\nDiffusion (rombach et al). Viewing the guided reverse diffusion process as\nsolving an augmented probability flow ODE (PF-ODE), LCMs are designed to\ndirectly predict the solution of such ODE in latent space, mitigating the need\nfor numerous iterations and allowing rapid, high-fidelity sampling. Efficiently\ndistilled from pre-trained classifier-free guided diffusion models, a\nhigh-quality 768 x 768 2~4-step LCM takes only 32 A100 GPU hours for training.\nFurthermore, we introduce Latent Consistency Fine-tuning (LCF), a novel method\nthat is tailored for fine-tuning LCMs on customized image datasets. Evaluation\non the LAION-5B-Aesthetics dataset demonstrates that LCMs achieve\nstate-of-the-art text-to-image generation performance with few-step inference.\nProject Page: https://latent-consistency-models.github.io/",
        "translated": ""
    },
    {
        "title": "SwimXYZ: A large-scale dataset of synthetic swimming motions and videos",
        "url": "http://arxiv.org/abs/2310.04360v1",
        "pub_date": "2023-10-06",
        "summary": "Technologies play an increasingly important role in sports and become a real\ncompetitive advantage for the athletes who benefit from it. Among them, the use\nof motion capture is developing in various sports to optimize sporting\ngestures. Unfortunately, traditional motion capture systems are expensive and\nconstraining. Recently developed computer vision-based approaches also struggle\nin certain sports, like swimming, due to the aquatic environment. One of the\nreasons for the gap in performance is the lack of labeled datasets with\nswimming videos. In an attempt to address this issue, we introduce SwimXYZ, a\nsynthetic dataset of swimming motions and videos. SwimXYZ contains 3.4 million\nframes annotated with ground truth 2D and 3D joints, as well as 240 sequences\nof swimming motions in the SMPL parameters format. In addition to making this\ndataset publicly available, we present use cases for SwimXYZ in swimming stroke\nclustering and 2D pose estimation.",
        "translated": ""
    },
    {
        "title": "Distributed Deep Joint Source-Channel Coding with Decoder-Only Side\n  Information",
        "url": "http://arxiv.org/abs/2310.04311v1",
        "pub_date": "2023-10-06",
        "summary": "We consider low-latency image transmission over a noisy wireless channel when\ncorrelated side information is present only at the receiver side (the Wyner-Ziv\nscenario). In particular, we are interested in developing practical schemes\nusing a data-driven joint source-channel coding (JSCC) approach, which has been\npreviously shown to outperform conventional separation-based approaches in the\npractical finite blocklength regimes, and to provide graceful degradation with\nchannel quality. We propose a novel neural network architecture that\nincorporates the decoder-only side information at multiple stages at the\nreceiver side. Our results demonstrate that the proposed method succeeds in\nintegrating the side information, yielding improved performance at all channel\nnoise levels in terms of the various distortion criteria considered here,\nespecially at low channel signal-to-noise ratios (SNRs) and small bandwidth\nratios (BRs). We also provide the source code of the proposed method to enable\nfurther research and reproducibility of the results.",
        "translated": ""
    },
    {
        "title": "Towards A Robust Group-level Emotion Recognition via Uncertainty-Aware\n  Learning",
        "url": "http://arxiv.org/abs/2310.04306v1",
        "pub_date": "2023-10-06",
        "summary": "Group-level emotion recognition (GER) is an inseparable part of human\nbehavior analysis, aiming to recognize an overall emotion in a multi-person\nscene. However, the existing methods are devoted to combing diverse emotion\ncues while ignoring the inherent uncertainties under unconstrained\nenvironments, such as congestion and occlusion occurring within a group.\nAdditionally, since only group-level labels are available, inconsistent emotion\npredictions among individuals in one group can confuse the network. In this\npaper, we propose an uncertainty-aware learning (UAL) method to extract more\nrobust representations for GER. By explicitly modeling the uncertainty of each\nindividual, we utilize stochastic embedding drawn from a Gaussian distribution\ninstead of deterministic point embedding. This representation captures the\nprobabilities of different emotions and generates diverse predictions through\nthis stochasticity during the inference stage. Furthermore,\nuncertainty-sensitive scores are adaptively assigned as the fusion weights of\nindividuals' face within each group. Moreover, we develop an image enhancement\nmodule to enhance the model's robustness against severe noise. The overall\nthree-branch model, encompassing face, object, and scene component, is guided\nby a proportional-weighted fusion strategy and integrates the proposed\nuncertainty-aware method to produce the final group-level output. Experimental\nresults demonstrate the effectiveness and generalization ability of our method\nacross three widely used databases.",
        "translated": ""
    },
    {
        "title": "Convergent ADMM Plug and Play PET Image Reconstruction",
        "url": "http://arxiv.org/abs/2310.04299v1",
        "pub_date": "2023-10-06",
        "summary": "In this work, we investigate hybrid PET reconstruction algorithms based on\ncoupling a model-based variational reconstruction and the application of a\nseparately learnt Deep Neural Network operator (DNN) in an ADMM Plug and Play\nframework. Following recent results in optimization, fixed point convergence of\nthe scheme can be achieved by enforcing an additional constraint on network\nparameters during learning. We propose such an ADMM algorithm and show in a\nrealistic [18F]-FDG synthetic brain exam that the proposed scheme indeed lead\nexperimentally to convergence to a meaningful fixed point. When the proposed\nconstraint is not enforced during learning of the DNN, the proposed ADMM\nalgorithm was observed experimentally not to converge.",
        "translated": ""
    },
    {
        "title": "Graph learning in robotics: a survey",
        "url": "http://arxiv.org/abs/2310.04294v1",
        "pub_date": "2023-10-06",
        "summary": "Deep neural networks for graphs have emerged as a powerful tool for learning\non complex non-euclidean data, which is becoming increasingly common for a\nvariety of different applications. Yet, although their potential has been\nwidely recognised in the machine learning community, graph learning is largely\nunexplored for downstream tasks such as robotics applications. To fully unlock\ntheir potential, hence, we propose a review of graph neural architectures from\na robotics perspective. The paper covers the fundamentals of graph-based\nmodels, including their architecture, training procedures, and applications. It\nalso discusses recent advancements and challenges that arise in applied\nsettings, related for example to the integration of perception,\ndecision-making, and control. Finally, the paper provides an extensive review\nof various robotic applications that benefit from learning on graph structures,\nsuch as bodies and contacts modelling, robotic manipulation, action\nrecognition, fleet motion planning, and many more. This survey aims to provide\nreaders with a thorough understanding of the capabilities and limitations of\ngraph neural architectures in robotics, and to highlight potential avenues for\nfuture research.",
        "translated": ""
    },
    {
        "title": "FLATTEN: optical FLow-guided ATTENtion for consistent text-to-video\n  editing",
        "url": "http://arxiv.org/abs/2310.05922v1",
        "pub_date": "2023-10-09",
        "summary": "Text-to-video editing aims to edit the visual appearance of a source video\nconditional on textual prompts. A major challenge in this task is to ensure\nthat all frames in the edited video are visually consistent. Most recent works\napply advanced text-to-image diffusion models to this task by inflating 2D\nspatial attention in the U-Net into spatio-temporal attention. Although\ntemporal context can be added through spatio-temporal attention, it may\nintroduce some irrelevant information for each patch and therefore cause\ninconsistency in the edited video. In this paper, for the first time, we\nintroduce optical flow into the attention module in the diffusion model's U-Net\nto address the inconsistency issue for text-to-video editing. Our method,\nFLATTEN, enforces the patches on the same flow path across different frames to\nattend to each other in the attention module, thus improving the visual\nconsistency in the edited videos. Additionally, our method is training-free and\ncan be seamlessly integrated into any diffusion-based text-to-video editing\nmethods and improve their visual consistency. Experiment results on existing\ntext-to-video editing benchmarks show that our proposed method achieves the new\nstate-of-the-art performance. In particular, our method excels in maintaining\nthe visual consistency in the edited videos.",
        "translated": ""
    },
    {
        "title": "SimPLR: A Simple and Plain Transformer for Object Detection and\n  Segmentation",
        "url": "http://arxiv.org/abs/2310.05920v1",
        "pub_date": "2023-10-09",
        "summary": "The ability to detect objects in images at varying scales has played a\npivotal role in the design of modern object detectors. Despite considerable\nprogress in removing handcrafted components using transformers, multi-scale\nfeature maps remain a key factor for their empirical success, even with a plain\nbackbone like the Vision Transformer (ViT). In this paper, we show that this\nreliance on feature pyramids is unnecessary and a transformer-based detector\nwith scale-aware attention enables the plain detector `SimPLR' whose backbone\nand detection head both operate on single-scale features. The plain\narchitecture allows SimPLR to effectively take advantages of self-supervised\nlearning and scaling approaches with ViTs, yielding strong performance compared\nto multi-scale counterparts. We demonstrate through our experiments that when\nscaling to larger backbones, SimPLR indicates better performance than\nend-to-end detectors (Mask2Former) and plain-backbone detectors (ViTDet), while\nconsistently being faster. The code will be released.",
        "translated": ""
    },
    {
        "title": "Drivable Avatar Clothing: Faithful Full-Body Telepresence with Dynamic\n  Clothing Driven by Sparse RGB-D Input",
        "url": "http://arxiv.org/abs/2310.05917v1",
        "pub_date": "2023-10-09",
        "summary": "Clothing is an important part of human appearance but challenging to model in\nphotorealistic avatars. In this work we present avatars with dynamically moving\nloose clothing that can be faithfully driven by sparse RGB-D inputs as well as\nbody and face motion. We propose a Neural Iterative Closest Point (N-ICP)\nalgorithm that can efficiently track the coarse garment shape given sparse\ndepth input. Given the coarse tracking results, the input RGB-D images are then\nremapped to texel-aligned features, which are fed into the drivable avatar\nmodels to faithfully reconstruct appearance details. We evaluate our method\nagainst recent image-driven synthesis baselines, and conduct a comprehensive\nanalysis of the N-ICP algorithm. We demonstrate that our method can generalize\nto a novel testing environment, while preserving the ability to produce\nhigh-fidelity and faithful clothing dynamics and appearance.",
        "translated": ""
    },
    {
        "title": "Interpreting CLIP's Image Representation via Text-Based Decomposition",
        "url": "http://arxiv.org/abs/2310.05916v2",
        "pub_date": "2023-10-09",
        "summary": "We investigate the CLIP image encoder by analyzing how individual model\ncomponents affect the final representation. We decompose the image\nrepresentation as a sum across individual image patches, model layers, and\nattention heads, and use CLIP's text representation to interpret the summands.\nInterpreting the attention heads, we characterize each head's role by\nautomatically finding text representations that span its output space, which\nreveals property-specific roles for many heads (e.g. location or shape). Next,\ninterpreting the image patches, we uncover an emergent spatial localization\nwithin CLIP. Finally, we use this understanding to remove spurious features\nfrom CLIP and to create a strong zero-shot image segmenter. Our results\nindicate that a scalable understanding of transformer models is attainable and\ncan be used to repair and improve models.",
        "translated": ""
    },
    {
        "title": "Streaming Anchor Loss: Augmenting Supervision with Temporal Significance",
        "url": "http://arxiv.org/abs/2310.05886v1",
        "pub_date": "2023-10-09",
        "summary": "Streaming neural network models for fast frame-wise responses to various\nspeech and sensory signals are widely adopted on resource-constrained\nplatforms. Hence, increasing the learning capacity of such streaming models\n(i.e., by adding more parameters) to improve the predictive power may not be\nviable for real-world tasks. In this work, we propose a new loss, Streaming\nAnchor Loss (SAL), to better utilize the given learning capacity by encouraging\nthe model to learn more from essential frames. More specifically, our SAL and\nits focal variations dynamically modulate the frame-wise cross entropy loss\nbased on the importance of the corresponding frames so that a higher loss\npenalty is assigned for frames within the temporal proximity of semantically\ncritical events. Therefore, our loss ensures that the model training focuses on\npredicting the relatively rare but task-relevant frames. Experimental results\nwith standard lightweight convolutional and recurrent streaming networks on\nthree different speech based detection tasks demonstrate that SAL enables the\nmodel to learn the overall task more effectively with improved accuracy and\nlatency, without any additional data, model parameters, or architectural\nchanges.",
        "translated": ""
    },
    {
        "title": "Controllable Chest X-Ray Report Generation from Longitudinal\n  Representations",
        "url": "http://arxiv.org/abs/2310.05881v1",
        "pub_date": "2023-10-09",
        "summary": "Radiology reports are detailed text descriptions of the content of medical\nscans. Each report describes the presence/absence and location of relevant\nclinical findings, commonly including comparison with prior exams of the same\npatient to describe how they evolved. Radiology reporting is a time-consuming\nprocess, and scan results are often subject to delays. One strategy to speed up\nreporting is to integrate automated reporting systems, however clinical\ndeployment requires high accuracy and interpretability. Previous approaches to\nautomated radiology reporting generally do not provide the prior study as\ninput, precluding comparison which is required for clinical accuracy in some\ntypes of scans, and offer only unreliable methods of interpretability.\nTherefore, leveraging an existing visual input format of anatomical tokens, we\nintroduce two novel aspects: (1) longitudinal representation learning -- we\ninput the prior scan as an additional input, proposing a method to align,\nconcatenate and fuse the current and prior visual information into a joint\nlongitudinal representation which can be provided to the multimodal report\ngeneration model; (2) sentence-anatomy dropout -- a training strategy for\ncontrollability in which the report generator model is trained to predict only\nsentences from the original report which correspond to the subset of anatomical\nregions given as input. We show through in-depth experiments on the MIMIC-CXR\ndataset how the proposed approach achieves state-of-the-art results while\nenabling anatomy-wise controllable report generation.",
        "translated": ""
    },
    {
        "title": "Geom-Erasing: Geometry-Driven Removal of Implicit Concept in Diffusion\n  Models",
        "url": "http://arxiv.org/abs/2310.05873v2",
        "pub_date": "2023-10-09",
        "summary": "Fine-tuning diffusion models through personalized datasets is an acknowledged\nmethod for improving generation quality across downstream tasks, which,\nhowever, often inadvertently generates unintended concepts such as watermarks\nand QR codes, attributed to the limitations in image sources and collecting\nmethods within specific downstream tasks. Existing solutions suffer from\neliminating these unintentionally learned implicit concepts, primarily due to\nthe dependency on the model's ability to recognize concepts that it actually\ncannot discern. In this work, we introduce Geom-Erasing, a novel approach that\nsuccessfully removes the implicit concepts with either an additional accessible\nclassifier or detector model to encode geometric information of these concepts\ninto text domain. Moreover, we propose Implicit Concept, a novel image-text\ndataset imbued with three implicit concepts (i.e., watermarks, QR codes, and\ntext) for training and evaluation. Experimental results demonstrate that\nGeom-Erasing not only identifies but also proficiently eradicates implicit\nconcepts, revealing a significant improvement over the existing methods. The\nintegration of geometric information marks a substantial progression in the\nprecise removal of implicit concepts in diffusion models.",
        "translated": ""
    },
    {
        "title": "ViCor: Bridging Visual Understanding and Commonsense Reasoning with\n  Large Language Models",
        "url": "http://arxiv.org/abs/2310.05872v1",
        "pub_date": "2023-10-09",
        "summary": "In our work, we explore the synergistic capabilities of pre-trained\nvision-and-language models (VLMs) and large language models (LLMs) for visual\ncommonsense reasoning (VCR). We categorize the problem of VCR into visual\ncommonsense understanding (VCU) and visual commonsense inference (VCI). For\nVCU, which involves perceiving the literal visual content, pre-trained VLMs\nexhibit strong cross-dataset generalization. On the other hand, in VCI, where\nthe goal is to infer conclusions beyond image content, VLMs face difficulties.\nWe find that a baseline where VLMs provide perception results (image captions)\nto LLMs leads to improved performance on VCI. However, we identify a challenge\nwith VLMs' passive perception, which often misses crucial context information,\nleading to incorrect or uncertain reasoning by LLMs. To mitigate this issue, we\nsuggest a collaborative approach where LLMs, when uncertain about their\nreasoning, actively direct VLMs to concentrate on and gather relevant visual\nelements to support potential commonsense inferences. In our method, named\nViCor, pre-trained LLMs serve as problem classifiers to analyze the problem\ncategory, VLM commanders to leverage VLMs differently based on the problem\nclassification, and visual commonsense reasoners to answer the question. VLMs\nwill perform visual recognition and understanding. We evaluate our framework on\ntwo VCR benchmark datasets and outperform all other methods that do not require\nin-domain supervised fine-tuning.",
        "translated": ""
    },
    {
        "title": "Domain-wise Invariant Learning for Panoptic Scene Graph Generation",
        "url": "http://arxiv.org/abs/2310.05867v1",
        "pub_date": "2023-10-09",
        "summary": "Panoptic Scene Graph Generation (PSG) involves the detection of objects and\nthe prediction of their corresponding relationships (predicates). However, the\npresence of biased predicate annotations poses a significant challenge for PSG\nmodels, as it hinders their ability to establish a clear decision boundary\namong different predicates. This issue substantially impedes the practical\nutility and real-world applicability of PSG models. To address the intrinsic\nbias above, we propose a novel framework to infer potentially biased\nannotations by measuring the predicate prediction risks within each\nsubject-object pair (domain), and adaptively transfer the biased annotations to\nconsistent ones by learning invariant predicate representation embeddings.\nExperiments show that our method significantly improves the performance of\nbenchmark models, achieving a new state-of-the-art performance, and shows great\ngeneralization and effectiveness on PSG dataset.",
        "translated": ""
    },
    {
        "title": "Fine-grained Audio-Visual Joint Representations for Multimodal Large\n  Language Models",
        "url": "http://arxiv.org/abs/2310.05863v2",
        "pub_date": "2023-10-09",
        "summary": "Audio-visual large language models (LLM) have drawn significant attention,\nyet the fine-grained combination of both input streams is rather\nunder-explored, which is challenging but necessary for LLMs to understand\ngeneral video inputs. To this end, a fine-grained audio-visual joint\nrepresentation (FAVOR) learning framework for multimodal LLMs is proposed in\nthis paper, which extends a text-based LLM to simultaneously perceive speech\nand audio events in the audio input stream and images or videos in the visual\ninput stream, at the frame level. To fuse the audio and visual feature streams\ninto joint representations and to align the joint space with the LLM input\nembedding space, we propose a causal Q-Former structure with a causal attention\nmodule to enhance the capture of causal relations of the audio-visual frames\nacross time. An audio-visual evaluation benchmark (AVEB) is also proposed which\ncomprises six representative single-modal tasks with five cross-modal tasks\nreflecting audio-visual co-reasoning abilities. While achieving competitive\nsingle-modal performance on audio, speech and image tasks in AVEB, FAVOR\nachieved over 20% accuracy improvements on the video question-answering task\nwhen fine-grained information or temporal causal reasoning is required. FAVOR,\nin addition, demonstrated remarkable video comprehension and reasoning\nabilities on tasks that are unprecedented by other multimodal LLMs. An\ninteractive demo of FAVOR is available at\nhttps://github.com/BriansIDP/AudioVisualLLM.git, and the training code and\nmodel checkpoints will be released soon.",
        "translated": ""
    },
    {
        "title": "AutoAD II: The Sequel -- Who, When, and What in Movie Audio Description",
        "url": "http://arxiv.org/abs/2310.06838v1",
        "pub_date": "2023-10-10",
        "summary": "Audio Description (AD) is the task of generating descriptions of visual\ncontent, at suitable time intervals, for the benefit of visually impaired\naudiences. For movies, this presents notable challenges -- AD must occur only\nduring existing pauses in dialogue, should refer to characters by name, and\nought to aid understanding of the storyline as a whole. To this end, we develop\na new model for automatically generating movie AD, given CLIP visual features\nof the frames, the cast list, and the temporal locations of the speech;\naddressing all three of the 'who', 'when', and 'what' questions: (i) who -- we\nintroduce a character bank consisting of the character's name, the actor that\nplayed the part, and a CLIP feature of their face, for the principal cast of\neach movie, and demonstrate how this can be used to improve naming in the\ngenerated AD; (ii) when -- we investigate several models for determining\nwhether an AD should be generated for a time interval or not, based on the\nvisual content of the interval and its neighbours; and (iii) what -- we\nimplement a new vision-language model for this task, that can ingest the\nproposals from the character bank, whilst conditioning on the visual features\nusing cross-attention, and demonstrate how this improves over previous\narchitectures for AD text generation in an apples-to-apples comparison.",
        "translated": ""
    },
    {
        "title": "What Does Stable Diffusion Know about the 3D Scene?",
        "url": "http://arxiv.org/abs/2310.06836v1",
        "pub_date": "2023-10-10",
        "summary": "Recent advances in generative models like Stable Diffusion enable the\ngeneration of highly photo-realistic images. Our objective in this paper is to\nprobe the diffusion network to determine to what extent it 'understands'\ndifferent properties of the 3D scene depicted in an image. To this end, we make\nthe following contributions: (i) We introduce a protocol to evaluate whether a\nnetwork models a number of physical 'properties' of the 3D scene by probing for\nexplicit features that represent these properties. The probes are applied on\ndatasets of real images with annotations for the property. (ii) We apply this\nprotocol to properties covering scene geometry, scene material, support\nrelations, lighting, and view dependent measures. (iii) We find that Stable\nDiffusion is good at a number of properties including scene geometry, support\nrelations, shadows and depth, but less performant for occlusion. (iv) We also\napply the probes to other models trained at large-scale, including DINO and\nCLIP, and find their performance inferior to that of Stable Diffusion.",
        "translated": ""
    },
    {
        "title": "NECO: NEural Collapse Based Out-of-distribution detection",
        "url": "http://arxiv.org/abs/2310.06823v1",
        "pub_date": "2023-10-10",
        "summary": "Detecting out-of-distribution (OOD) data is a critical challenge in machine\nlearning due to model overconfidence, often without awareness of their\nepistemological limits. We hypothesize that ``neural collapse'', a phenomenon\naffecting in-distribution data for models trained beyond loss convergence, also\ninfluences OOD data. To benefit from this interplay, we introduce NECO, a novel\npost-hoc method for OOD detection, which leverages the geometric properties of\n``neural collapse'' and of principal component spaces to identify OOD data. Our\nextensive experiments demonstrate that NECO achieves state-of-the-art results\non both small and large-scale OOD detection tasks while exhibiting strong\ngeneralization capabilities across different network architectures.\nFurthermore, we provide a theoretical explanation for the effectiveness of our\nmethod in OOD detection. We plan to release the code after the anonymity\nperiod.",
        "translated": ""
    },
    {
        "title": "Neural Bounding",
        "url": "http://arxiv.org/abs/2310.06822v1",
        "pub_date": "2023-10-10",
        "summary": "Bounding volumes are an established concept in computer graphics and vision\ntasks but have seen little change since their early inception. In this work, we\nstudy the use of neural networks as bounding volumes. Our key observation is\nthat bounding, which so far has primarily been considered a problem of\ncomputational geometry, can be redefined as a problem of learning to classify\nspace into free and empty. This learning-based approach is particularly\nadvantageous in high-dimensional spaces, such as animated scenes with complex\nqueries, where neural networks are known to excel. However, unlocking neural\nbounding requires a twist: allowing -- but also limiting -- false positives,\nwhile ensuring that the number of false negatives is strictly zero. We enable\nsuch tight and conservative results using a dynamically-weighted asymmetric\nloss function. Our results show that our neural bounding produces up to an\norder of magnitude fewer false positives than traditional methods.",
        "translated": ""
    },
    {
        "title": "Uni3D: Exploring Unified 3D Representation at Scale",
        "url": "http://arxiv.org/abs/2310.06773v1",
        "pub_date": "2023-10-10",
        "summary": "Scaling up representations for images or text has been extensively\ninvestigated in the past few years and has led to revolutions in learning\nvision and language. However, scalable representation for 3D objects and scenes\nis relatively unexplored. In this work, we present Uni3D, a 3D foundation model\nto explore the unified 3D representation at scale. Uni3D uses a 2D initialized\nViT end-to-end pretrained to align the 3D point cloud features with the\nimage-text aligned features. Via the simple architecture and pretext task,\nUni3D can leverage abundant 2D pretrained models as initialization and\nimage-text aligned models as the target, unlocking the great potential of 2D\nmodels and scaling-up strategies to the 3D world. We efficiently scale up Uni3D\nto one billion parameters, and set new records on a broad range of 3D tasks,\nsuch as zero-shot classification, few-shot classification, open-world\nunderstanding and part segmentation. We show that the strong Uni3D\nrepresentation also enables applications such as 3D painting and retrieval in\nthe wild. We believe that Uni3D provides a new direction for exploring both\nscaling up and efficiency of the representation in 3D domain.",
        "translated": ""
    },
    {
        "title": "TopoMLP: An Simple yet Strong Pipeline for Driving Topology Reasoning",
        "url": "http://arxiv.org/abs/2310.06753v1",
        "pub_date": "2023-10-10",
        "summary": "Topology reasoning aims to comprehensively understand road scenes and present\ndrivable routes in autonomous driving. It requires detecting road centerlines\n(lane) and traffic elements, further reasoning their topology relationship,\ni.e., lane-lane topology, and lane-traffic topology. In this work, we first\npresent that the topology score relies heavily on detection performance on lane\nand traffic elements. Therefore, we introduce a powerful 3D lane detector and\nan improved 2D traffic element detector to extend the upper limit of topology\nperformance. Further, we propose TopoMLP, a simple yet high-performance\npipeline for driving topology reasoning. Based on the impressive detection\nperformance, we develop two simple MLP-based heads for topology generation.\nTopoMLP achieves state-of-the-art performance on OpenLane-V2 benchmark, i.e.,\n41.2% OLS with ResNet-50 backbone. It is also the 1st solution for 1st OpenLane\nTopology in Autonomous Driving Challenge. We hope such simple and strong\npipeline can provide some new insights to the community. Code is at\nhttps://github.com/wudongming97/TopoMLP.",
        "translated": ""
    },
    {
        "title": "HiFi-123: Towards High-fidelity One Image to 3D Content Generation",
        "url": "http://arxiv.org/abs/2310.06744v1",
        "pub_date": "2023-10-10",
        "summary": "Recent advances in text-to-image diffusion models have enabled 3D generation\nfrom a single image. However, current image-to-3D methods often produce\nsuboptimal results for novel views, with blurred textures and deviations from\nthe reference image, limiting their practical applications. In this paper, we\nintroduce HiFi-123, a method designed for high-fidelity and multi-view\nconsistent 3D generation. Our contributions are twofold: First, we propose a\nreference-guided novel view enhancement technique that substantially reduces\nthe quality gap between synthesized and reference views. Second, capitalizing\non the novel view enhancement, we present a novel reference-guided state\ndistillation loss. When incorporated into the optimization-based image-to-3D\npipeline, our method significantly improves 3D generation quality, achieving\nstate-of-the-art performance. Comprehensive evaluations demonstrate the\neffectiveness of our approach over existing methods, both qualitatively and\nquantitatively.",
        "translated": ""
    },
    {
        "title": "Multi-domain improves out-of-distribution and data-limited scenarios for\n  medical image analysis",
        "url": "http://arxiv.org/abs/2310.06737v1",
        "pub_date": "2023-10-10",
        "summary": "Current machine learning methods for medical image analysis primarily focus\non developing models tailored for their specific tasks, utilizing data within\ntheir target domain. These specialized models tend to be data-hungry and often\nexhibit limitations in generalizing to out-of-distribution samples. Recently,\nfoundation models have been proposed, which combine data from various domains\nand demonstrate excellent generalization capabilities. Building upon this, this\nwork introduces the incorporation of diverse medical image domains, including\ndifferent imaging modalities like X-ray, MRI, CT, and ultrasound images, as\nwell as various viewpoints such as axial, coronal, and sagittal views. We refer\nto this approach as multi-domain model and compare its performance to that of\nspecialized models. Our findings underscore the superior generalization\ncapabilities of multi-domain models, particularly in scenarios characterized by\nlimited data availability and out-of-distribution, frequently encountered in\nhealthcare applications. The integration of diverse data allows multi-domain\nmodels to utilize shared information across domains, enhancing the overall\noutcomes significantly. To illustrate, for organ recognition, multi-domain\nmodel can enhance accuracy by up to 10% compared to conventional specialized\nmodels.",
        "translated": ""
    },
    {
        "title": "Domain Generalization by Rejecting Extreme Augmentations",
        "url": "http://arxiv.org/abs/2310.06670v1",
        "pub_date": "2023-10-10",
        "summary": "Data augmentation is one of the most effective techniques for regularizing\ndeep learning models and improving their recognition performance in a variety\nof tasks and domains. However, this holds for standard in-domain settings, in\nwhich the training and test data follow the same distribution. For the\nout-of-domain case, where the test data follow a different and unknown\ndistribution, the best recipe for data augmentation is unclear. In this paper,\nwe show that for out-of-domain and domain generalization settings, data\naugmentation can provide a conspicuous and robust improvement in performance.\nTo do that, we propose a simple training procedure: (i) use uniform sampling on\nstandard data augmentation transformations; (ii) increase the strength\ntransformations to account for the higher data variance expected when working\nout-of-domain, and (iii) devise a new reward function to reject extreme\ntransformations that can harm the training. With this procedure, our data\naugmentation scheme achieves a level of accuracy that is comparable to or\nbetter than state-of-the-art methods on benchmark domain generalization\ndatasets. Code: \\url{https://github.com/Masseeh/DCAug}",
        "translated": ""
    },
    {
        "title": "Latent Diffusion Counterfactual Explanations",
        "url": "http://arxiv.org/abs/2310.06668v1",
        "pub_date": "2023-10-10",
        "summary": "Counterfactual explanations have emerged as a promising method for\nelucidating the behavior of opaque black-box models. Recently, several works\nleveraged pixel-space diffusion models for counterfactual generation. To handle\nnoisy, adversarial gradients during counterfactual generation -- causing\nunrealistic artifacts or mere adversarial perturbations -- they required either\nauxiliary adversarially robust models or computationally intensive guidance\nschemes. However, such requirements limit their applicability, e.g., in\nscenarios with restricted access to the model's training data. To address these\nlimitations, we introduce Latent Diffusion Counterfactual Explanations (LDCE).\nLDCE harnesses the capabilities of recent class- or text-conditional foundation\nlatent diffusion models to expedite counterfactual generation and focus on the\nimportant, semantic parts of the data. Furthermore, we propose a novel\nconsensus guidance mechanism to filter out noisy, adversarial gradients that\nare misaligned with the diffusion model's implicit classifier. We demonstrate\nthe versatility of LDCE across a wide spectrum of models trained on diverse\ndatasets with different learning paradigms. Finally, we showcase how LDCE can\nprovide insights into model errors, enhancing our understanding of black-box\nmodel behavior.",
        "translated": ""
    },
    {
        "title": "PAD: A Dataset and Benchmark for Pose-agnostic Anomaly Detection",
        "url": "http://arxiv.org/abs/2310.07716v1",
        "pub_date": "2023-10-11",
        "summary": "Object anomaly detection is an important problem in the field of machine\nvision and has seen remarkable progress recently. However, two significant\nchallenges hinder its research and application. First, existing datasets lack\ncomprehensive visual information from various pose angles. They usually have an\nunrealistic assumption that the anomaly-free training dataset is pose-aligned,\nand the testing samples have the same pose as the training data. However, in\npractice, anomaly may exist in any regions on a object, the training and query\nsamples may have different poses, calling for the study on pose-agnostic\nanomaly detection. Second, the absence of a consensus on experimental protocols\nfor pose-agnostic anomaly detection leads to unfair comparisons of different\nmethods, hindering the research on pose-agnostic anomaly detection. To address\nthese issues, we develop Multi-pose Anomaly Detection (MAD) dataset and\nPose-agnostic Anomaly Detection (PAD) benchmark, which takes the first step to\naddress the pose-agnostic anomaly detection problem. Specifically, we build MAD\nusing 20 complex-shaped LEGO toys including 4K views with various poses, and\nhigh-quality and diverse 3D anomalies in both simulated and real environments.\nAdditionally, we propose a novel method OmniposeAD, trained using MAD,\nspecifically designed for pose-agnostic anomaly detection. Through\ncomprehensive evaluations, we demonstrate the relevance of our dataset and\nmethod. Furthermore, we provide an open-source benchmark library, including\ndataset and baseline methods that cover 8 anomaly detection paradigms, to\nfacilitate future research and application in this domain. Code, data, and\nmodels are publicly available at https://github.com/EricLee0224/PAD.",
        "translated": ""
    },
    {
        "title": "MatFormer: Nested Transformer for Elastic Inference",
        "url": "http://arxiv.org/abs/2310.07707v1",
        "pub_date": "2023-10-11",
        "summary": "Transformer models are deployed in a wide range of settings, from\nmulti-accelerator clusters to standalone mobile phones. The diverse inference\nconstraints in these scenarios necessitate practitioners to train foundation\nmodels such as PaLM 2, Llama, &amp; ViTs as a series of models of varying sizes.\nDue to significant training costs, only a select few model sizes are trained\nand supported, limiting more fine-grained control over relevant tradeoffs,\nincluding latency, cost, and accuracy. This work introduces MatFormer, a nested\nTransformer architecture designed to offer elasticity in a variety of\ndeployment constraints. Each Feed Forward Network (FFN) block of a MatFormer\nmodel is jointly optimized with a few nested smaller FFN blocks. This training\nprocedure allows for the Mix'n'Match of model granularities across layers --\ni.e., a trained universal MatFormer model enables extraction of hundreds of\naccurate smaller models, which were never explicitly optimized. We empirically\ndemonstrate MatFormer's effectiveness across different model classes (decoders\n&amp; encoders), modalities (language &amp; vision), and scales (up to 2.6B\nparameters). We find that a 2.6B decoder-only MatFormer language model (MatLM)\nallows us to extract smaller models spanning from 1.5B to 2.6B, each exhibiting\ncomparable validation loss and one-shot downstream evaluations to their\nindependently trained counterparts. Furthermore, we observe that smaller\nencoders extracted from a universal MatFormer-based ViT (MatViT) encoder\npreserve the metric-space structure for adaptive large-scale retrieval.\nFinally, we showcase that speculative decoding with the accurate and consistent\nsubmodels extracted from MatFormer can further reduce inference latency.",
        "translated": ""
    },
    {
        "title": "Ferret: Refer and Ground Anything Anywhere at Any Granularity",
        "url": "http://arxiv.org/abs/2310.07704v1",
        "pub_date": "2023-10-11",
        "summary": "We introduce Ferret, a new Multimodal Large Language Model (MLLM) capable of\nunderstanding spatial referring of any shape or granularity within an image and\naccurately grounding open-vocabulary descriptions. To unify referring and\ngrounding in the LLM paradigm, Ferret employs a novel and powerful hybrid\nregion representation that integrates discrete coordinates and continuous\nfeatures jointly to represent a region in the image. To extract the continuous\nfeatures of versatile regions, we propose a spatial-aware visual sampler, adept\nat handling varying sparsity across different shapes. Consequently, Ferret can\naccept diverse region inputs, such as points, bounding boxes, and free-form\nshapes. To bolster the desired capability of Ferret, we curate GRIT, a\ncomprehensive refer-and-ground instruction tuning dataset including 1.1M\nsamples that contain rich hierarchical spatial knowledge, with 95K hard\nnegative data to promote model robustness. The resulting model not only\nachieves superior performance in classical referring and grounding tasks, but\nalso greatly outperforms existing MLLMs in region-based and\nlocalization-demanded multimodal chatting. Our evaluations also reveal a\nsignificantly improved capability of describing image details and a remarkable\nalleviation in object hallucination. Code and data will be available at\nhttps://github.com/apple/ml-ferret",
        "translated": ""
    },
    {
        "title": "ScaleCrafter: Tuning-free Higher-Resolution Visual Generation with\n  Diffusion Models",
        "url": "http://arxiv.org/abs/2310.07702v1",
        "pub_date": "2023-10-11",
        "summary": "In this work, we investigate the capability of generating images from\npre-trained diffusion models at much higher resolutions than the training image\nsizes. In addition, the generated images should have arbitrary image aspect\nratios. When generating images directly at a higher resolution, 1024 x 1024,\nwith the pre-trained Stable Diffusion using training images of resolution 512 x\n512, we observe persistent problems of object repetition and unreasonable\nobject structures. Existing works for higher-resolution generation, such as\nattention-based and joint-diffusion approaches, cannot well address these\nissues. As a new perspective, we examine the structural components of the U-Net\nin diffusion models and identify the crucial cause as the limited perception\nfield of convolutional kernels. Based on this key observation, we propose a\nsimple yet effective re-dilation that can dynamically adjust the convolutional\nperception field during inference. We further propose the dispersed convolution\nand noise-damped classifier-free guidance, which can enable\nultra-high-resolution image generation (e.g., 4096 x 4096). Notably, our\napproach does not require any training or optimization. Extensive experiments\ndemonstrate that our approach can address the repetition issue well and achieve\nstate-of-the-art performance on higher-resolution image synthesis, especially\nin texture details. Our work also suggests that a pre-trained diffusion model\ntrained on low-resolution images can be directly used for high-resolution\nvisual generation without further tuning, which may provide insights for future\nresearch on ultra-high-resolution image and video synthesis.",
        "translated": ""
    },
    {
        "title": "From Scarcity to Efficiency: Improving CLIP Training via Visual-enriched\n  Captions",
        "url": "http://arxiv.org/abs/2310.07699v1",
        "pub_date": "2023-10-11",
        "summary": "Web-crawled datasets are pivotal to the success of pre-training\nvision-language models, exemplified by CLIP. However, web-crawled AltTexts can\nbe noisy and potentially irrelevant to images, thereby undermining the crucial\nimage-text alignment. Existing methods for rewriting captions using large\nlanguage models (LLMs) have shown promise on small, curated datasets like CC3M\nand CC12M. Nevertheless, their efficacy on massive web-captured captions is\nconstrained by the inherent noise and randomness in such data. In this study,\nwe address this limitation by focusing on two key aspects: data quality and\ndata variety. Unlike recent LLM rewriting techniques, we emphasize exploiting\nvisual concepts and their integration into the captions to improve data\nquality. For data variety, we propose a novel mixed training scheme that\noptimally leverages AltTexts alongside newly generated Visual-enriched Captions\n(VeC). We use CLIP as one example and adapt the method for CLIP training on\nlarge-scale web-crawled datasets, named VeCLIP. We conduct a comprehensive\nevaluation of VeCLIP across small, medium, and large scales of raw data. Our\nresults show significant advantages in image-text alignment and overall model\nperformance, underscoring the effectiveness of VeCLIP in improving CLIP\ntraining. For example, VeCLIP achieves a remarkable over 20% improvement in\nCOCO and Flickr30k retrieval tasks under the 12M setting. For data efficiency,\nwe also achieve a notable over 3% improvement while using only 14% of the data\nemployed in the vanilla CLIP and 11% in ALIGN.",
        "translated": ""
    },
    {
        "title": "ConditionVideo: Training-Free Condition-Guided Text-to-Video Generation",
        "url": "http://arxiv.org/abs/2310.07697v1",
        "pub_date": "2023-10-11",
        "summary": "Recent works have successfully extended large-scale text-to-image models to\nthe video domain, producing promising results but at a high computational cost\nand requiring a large amount of video data. In this work, we introduce\nConditionVideo, a training-free approach to text-to-video generation based on\nthe provided condition, video, and input text, by leveraging the power of\noff-the-shelf text-to-image generation methods (e.g., Stable Diffusion).\nConditionVideo generates realistic dynamic videos from random noise or given\nscene videos. Our method explicitly disentangles the motion representation into\ncondition-guided and scenery motion components. To this end, the ConditionVideo\nmodel is designed with a UNet branch and a control branch. To improve temporal\ncoherence, we introduce sparse bi-directional spatial-temporal attention\n(sBiST-Attn). The 3D control network extends the conventional 2D controlnet\nmodel, aiming to strengthen conditional generation accuracy by additionally\nleveraging the bi-directional frames in the temporal domain. Our method\nexhibits superior performance in terms of frame consistency, clip score, and\nconditional accuracy, outperforming other compared methods.",
        "translated": ""
    },
    {
        "title": "Orbital Polarimetric Tomography of a Flare Near the Sagittarius A*\n  Supermassive Black Hole",
        "url": "http://arxiv.org/abs/2310.07687v1",
        "pub_date": "2023-10-11",
        "summary": "The interaction between the supermassive black hole at the center of the\nMilky Way, Sagittarius A$^*$, and its accretion disk, occasionally produces\nhigh energy flares seen in X-ray, infrared and radio. One mechanism for\nobserved flares is the formation of compact bright regions that appear within\nthe accretion disk and close to the event horizon. Understanding these flares\ncan provide a window into black hole accretion processes. Although\nsophisticated simulations predict the formation of these flares, their\nstructure has yet to be recovered by observations. Here we show the first\nthree-dimensional (3D) reconstruction of an emission flare in orbit recovered\nfrom ALMA light curves observed on April 11, 2017. Our recovery results show\ncompact bright regions at a distance of roughly 6 times the event horizon.\nMoreover, our recovery suggests a clockwise rotation in a low-inclination\norbital plane, a result consistent with prior studies by EHT and GRAVITY\ncollaborations. To recover this emission structure we solve a highly ill-posed\ntomography problem by integrating a neural 3D representation (an emergent\nartificial intelligence approach for 3D reconstruction) with a gravitational\nmodel for black holes. Although the recovered 3D structure is subject, and\nsometimes sensitive, to the model assumptions, under physically motivated\nchoices we find that our results are stable and our approach is successful on\nsimulated data. We anticipate that in the future, this approach could be used\nto analyze a richer collection of time-series data that could shed light on the\nmechanisms governing black hole and plasma dynamics.",
        "translated": ""
    },
    {
        "title": "Prediction of MET Overexpression in Non-Small Cell Lung Adenocarcinomas\n  from Hematoxylin and Eosin Images",
        "url": "http://arxiv.org/abs/2310.07682v1",
        "pub_date": "2023-10-11",
        "summary": "MET protein overexpression is a targetable event in non-small cell lung\ncancer (NSCLC) and is the subject of active drug development. Challenges in\nidentifying patients for these therapies include lack of access to validated\ntesting, such as standardized immunohistochemistry (IHC) assessment, and\nconsumption of valuable tissue for a single gene/protein assay. Development of\npre-screening algorithms using routinely available digitized hematoxylin and\neosin (H&amp;E)-stained slides to predict MET overexpression could promote testing\nfor those who will benefit most. While assessment of MET expression using IHC\nis currently not routinely performed in NSCLC, next-generation sequencing is\ncommon and in some cases includes RNA expression panel testing. In this work,\nwe leveraged a large database of matched H&amp;E slides and RNA expression data to\ntrain a weakly supervised model to predict MET RNA overexpression directly from\nH&amp;E images. This model was evaluated on an independent holdout test set of 300\nover-expressed and 289 normal patients, demonstrating an ROC-AUC of 0.70 (95th\npercentile interval: 0.66 - 0.74) with stable performance characteristics\nacross different patient clinical variables and robust to synthetic noise on\nthe test set. These results suggest that H&amp;E-based predictive models could be\nuseful to prioritize patients for confirmatory testing of MET protein or MET\ngene expression status.",
        "translated": ""
    },
    {
        "title": "Explainable Image Similarity: Integrating Siamese Networks and Grad-CAM",
        "url": "http://arxiv.org/abs/2310.07678v1",
        "pub_date": "2023-10-11",
        "summary": "With the proliferation of image-based applications in various domains, the\nneed for accurate and interpretable image similarity measures has become\nincreasingly critical. Existing image similarity models often lack\ntransparency, making it challenging to understand the reasons why two images\nare considered similar. In this paper, we propose the concept of explainable\nimage similarity, where the goal is the development of an approach, which is\ncapable of providing similarity scores along with visual factual and\ncounterfactual explanations. Along this line, we present a new framework, which\nintegrates Siamese Networks and Grad-CAM for providing explainable image\nsimilarity and discuss the potential benefits and challenges of adopting this\napproach. In addition, we provide a comprehensive discussion about factual and\ncounterfactual explanations provided by the proposed framework for assisting\ndecision making. The proposed approach has the potential to enhance the\ninterpretability, trustworthiness and user acceptance of image-based systems in\nreal-world image similarity applications. The implementation code can be found\nin https://github.com/ioannislivieris/Grad_CAM_Siamese.git.",
        "translated": ""
    },
    {
        "title": "HaarNet: Large-scale Linear-Morphological Hybrid Network for RGB-D\n  Semantic Segmentation",
        "url": "http://arxiv.org/abs/2310.07669v1",
        "pub_date": "2023-10-11",
        "summary": "Signals from different modalities each have their own combination algebra\nwhich affects their sampling processing. RGB is mostly linear; depth is a\ngeometric signal following the operations of mathematical morphology. If a\nnetwork obtaining RGB-D input has both kinds of operators available in its\nlayers, it should be able to give effective output with fewer parameters. In\nthis paper, morphological elements in conjunction with more familiar linear\nmodules are used to construct a mixed linear-morphological network called\nHaarNet. This is the first large-scale linear-morphological hybrid, evaluated\non a set of sizeable real-world datasets. In the network, morphological Haar\nsampling is applied to both feature channels in several layers, which splits\nextreme values and high-frequency information such that both can be processed\nto improve both modalities. Moreover, morphologically parameterised ReLU is\nused, and morphologically-sound up-sampling is applied to obtain a\nfull-resolution output. Experiments show that HaarNet is competitive with a\nstate-of-the-art CNN, implying that morphological networks are a promising\nresearch direction for geometry-based learning tasks.",
        "translated": ""
    },
    {
        "title": "Is Generalized Dynamic Novel View Synthesis from Monocular Videos\n  Possible Today?",
        "url": "http://arxiv.org/abs/2310.08587v1",
        "pub_date": "2023-10-12",
        "summary": "Rendering scenes observed in a monocular video from novel viewpoints is a\nchallenging problem. For static scenes the community has studied both\nscene-specific optimization techniques, which optimize on every test scene, and\ngeneralized techniques, which only run a deep net forward pass on a test scene.\nIn contrast, for dynamic scenes, scene-specific optimization techniques exist,\nbut, to our best knowledge, there is currently no generalized method for\ndynamic novel view synthesis from a given monocular video. To answer whether\ngeneralized dynamic novel view synthesis from monocular videos is possible\ntoday, we establish an analysis framework based on existing techniques and work\ntoward the generalized approach. We find a pseudo-generalized process without\nscene-specific appearance optimization is possible, but geometrically and\ntemporally consistent depth estimates are needed. Despite no scene-specific\nappearance optimization, the pseudo-generalized approach improves upon some\nscene-specific methods.",
        "translated": ""
    },
    {
        "title": "Octopus: Embodied Vision-Language Programmer from Environmental Feedback",
        "url": "http://arxiv.org/abs/2310.08588v1",
        "pub_date": "2023-10-12",
        "summary": "Large vision-language models (VLMs) have achieved substantial progress in\nmultimodal perception and reasoning. Furthermore, when seamlessly integrated\ninto an embodied agent, it signifies a crucial stride towards the creation of\nautonomous and context-aware systems capable of formulating plans and executing\ncommands with precision. In this paper, we introduce Octopus, a novel VLM\ndesigned to proficiently decipher an agent's vision and textual task objectives\nand to formulate intricate action sequences and generate executable code. Our\ndesign allows the agent to adeptly handle a wide spectrum of tasks, ranging\nfrom mundane daily chores in simulators to sophisticated interactions in\ncomplex video games. Octopus is trained by leveraging GPT-4 to control an\nexplorative agent to generate training data, i.e., action blueprints and the\ncorresponding executable code, within our experimental environment called\nOctoVerse. We also collect the feedback that allows the enhanced training\nscheme of Reinforcement Learning with Environmental Feedback (RLEF). Through a\nseries of experiments, we illuminate Octopus's functionality and present\ncompelling results, and the proposed RLEF turns out to refine the agent's\ndecision-making. By open-sourcing our model architecture, simulator, and\ndataset, we aspire to ignite further innovation and foster collaborative\napplications within the broader embodied AI community.",
        "translated": ""
    },
    {
        "title": "Im4D: High-Fidelity and Real-Time Novel View Synthesis for Dynamic\n  Scenes",
        "url": "http://arxiv.org/abs/2310.08585v1",
        "pub_date": "2023-10-12",
        "summary": "This paper aims to tackle the challenge of dynamic view synthesis from\nmulti-view videos. The key observation is that while previous grid-based\nmethods offer consistent rendering, they fall short in capturing appearance\ndetails of a complex dynamic scene, a domain where multi-view image-based\nrendering methods demonstrate the opposite properties. To combine the best of\ntwo worlds, we introduce Im4D, a hybrid scene representation that consists of a\ngrid-based geometry representation and a multi-view image-based appearance\nrepresentation. Specifically, the dynamic geometry is encoded as a 4D density\nfunction composed of spatiotemporal feature planes and a small MLP network,\nwhich globally models the scene structure and facilitates the rendering\nconsistency. We represent the scene appearance by the original multi-view\nvideos and a network that learns to predict the color of a 3D point from image\nfeatures, instead of memorizing detailed appearance totally with networks,\nthereby naturally making the learning of networks easier. Our method is\nevaluated on five dynamic view synthesis datasets including DyNeRF, ZJU-MoCap,\nNHR, DNA-Rendering and ENeRF-Outdoor datasets. The results show that Im4D\nexhibits state-of-the-art performance in rendering quality and can be trained\nefficiently, while realizing real-time rendering with a speed of 79.8 FPS for\n512x512 images, on a single RTX 3090 GPU.",
        "translated": ""
    },
    {
        "title": "PonderV2: Pave the Way for 3D Foundataion Model with A Universal\n  Pre-training Paradigm",
        "url": "http://arxiv.org/abs/2310.08586v1",
        "pub_date": "2023-10-12",
        "summary": "In contrast to numerous NLP and 2D computer vision foundational models, the\nlearning of a robust and highly generalized 3D foundational model poses\nconsiderably greater challenges. This is primarily due to the inherent data\nvariability and the diversity of downstream tasks. In this paper, we introduce\na comprehensive 3D pre-training framework designed to facilitate the\nacquisition of efficient 3D representations, thereby establishing a pathway to\n3D foundational models. Motivated by the fact that informative 3D features\nshould be able to encode rich geometry and appearance cues that can be utilized\nto render realistic images, we propose a novel universal paradigm to learn\npoint cloud representations by differentiable neural rendering, serving as a\nbridge between 3D and 2D worlds. We train a point cloud encoder within a\ndevised volumetric neural renderer by comparing the rendered images with the\nreal images. Notably, our approach demonstrates the seamless integration of the\nlearned 3D encoder into diverse downstream tasks. These tasks encompass not\nonly high-level challenges such as 3D detection and segmentation but also\nlow-level objectives like 3D reconstruction and image synthesis, spanning both\nindoor and outdoor scenarios. Besides, we also illustrate the capability of\npre-training a 2D backbone using the proposed universal methodology, surpassing\nconventional pre-training methods by a large margin. For the first time,\n\\sexyname achieves state-of-the-art performance on 11 indoor and outdoor\nbenchmarks. The consistent improvements in various settings imply the\neffectiveness of the proposed method. Code and models will be made available at\nhttps://github.com/Pointcept/Pointcept.",
        "translated": ""
    },
    {
        "title": "Is ImageNet worth 1 video? Learning strong image encoders from 1 long\n  unlabelled video",
        "url": "http://arxiv.org/abs/2310.08584v1",
        "pub_date": "2023-10-12",
        "summary": "Self-supervised learning has unlocked the potential of scaling up pretraining\nto billions of images, since annotation is unnecessary. But are we making the\nbest use of data? How more economical can we be? In this work, we attempt to\nanswer this question by making two contributions. First, we investigate\nfirst-person videos and introduce a \"Walking Tours\" dataset. These videos are\nhigh-resolution, hours-long, captured in a single uninterrupted take, depicting\na large number of objects and actions with natural scene transitions. They are\nunlabeled and uncurated, thus realistic for self-supervision and comparable\nwith human learning.\n  Second, we introduce a novel self-supervised image pretraining method\ntailored for learning from continuous videos. Existing methods typically adapt\nimage-based pretraining approaches to incorporate more frames. Instead, we\nadvocate a \"tracking to learn to recognize\" approach. Our method called DoRA,\nleads to attention maps that Discover and tRAck objects over time in an\nend-to-end manner, using transformer cross-attention. We derive multiple views\nfrom the tracks and use them in a classical self-supervised distillation loss.\nUsing our novel approach, a single Walking Tours video remarkably becomes a\nstrong competitor to ImageNet for several image and video downstream tasks.",
        "translated": ""
    },
    {
        "title": "Universal Visual Decomposer: Long-Horizon Manipulation Made Easy",
        "url": "http://arxiv.org/abs/2310.08581v1",
        "pub_date": "2023-10-12",
        "summary": "Real-world robotic tasks stretch over extended horizons and encompass\nmultiple stages. Learning long-horizon manipulation tasks, however, is a\nlong-standing challenge, and demands decomposing the overarching task into\nseveral manageable subtasks to facilitate policy learning and generalization to\nunseen tasks. Prior task decomposition methods require task-specific knowledge,\nare computationally intensive, and cannot readily be applied to new tasks. To\naddress these shortcomings, we propose Universal Visual Decomposer (UVD), an\noff-the-shelf task decomposition method for visual long horizon manipulation\nusing pre-trained visual representations designed for robotic control. At a\nhigh level, UVD discovers subgoals by detecting phase shifts in the embedding\nspace of the pre-trained representation. Operating purely on visual\ndemonstrations without auxiliary information, UVD can effectively extract\nvisual subgoals embedded in the videos, while incurring zero additional\ntraining cost on top of standard visuomotor policy training. Goal-conditioned\npolicies learned with UVD-discovered subgoals exhibit significantly improved\ncompositional generalization at test time to unseen tasks. Furthermore,\nUVD-discovered subgoals can be used to construct goal-based reward shaping that\njump-starts temporally extended exploration for reinforcement learning. We\nextensively evaluate UVD on both simulation and real-world tasks, and in all\ncases, UVD substantially outperforms baselines across imitation and\nreinforcement learning settings on in-domain and out-of-domain task sequences\nalike, validating the clear advantage of automated visual task decomposition\nwithin the simple, compact UVD framework.",
        "translated": ""
    },
    {
        "title": "OmniControl: Control Any Joint at Any Time for Human Motion Generation",
        "url": "http://arxiv.org/abs/2310.08580v1",
        "pub_date": "2023-10-12",
        "summary": "We present a novel approach named OmniControl for incorporating flexible\nspatial control signals into a text-conditioned human motion generation model\nbased on the diffusion process. Unlike previous methods that can only control\nthe pelvis trajectory, OmniControl can incorporate flexible spatial control\nsignals over different joints at different times with only one model.\nSpecifically, we propose analytic spatial guidance that ensures the generated\nmotion can tightly conform to the input control signals. At the same time,\nrealism guidance is introduced to refine all the joints to generate more\ncoherent motion. Both the spatial and realism guidance are essential and they\nare highly complementary for balancing control accuracy and motion realism. By\ncombining them, OmniControl generates motions that are realistic, coherent, and\nconsistent with the spatial constraints. Experiments on HumanML3D and KIT-ML\ndatasets show that OmniControl not only achieves significant improvement over\nstate-of-the-art methods on pelvis control but also shows promising results\nwhen incorporating the constraints over other joints.",
        "translated": ""
    },
    {
        "title": "HyperHuman: Hyper-Realistic Human Generation with Latent Structural\n  Diffusion",
        "url": "http://arxiv.org/abs/2310.08579v1",
        "pub_date": "2023-10-12",
        "summary": "Despite significant advances in large-scale text-to-image models, achieving\nhyper-realistic human image generation remains a desirable yet unsolved task.\nExisting models like Stable Diffusion and DALL-E 2 tend to generate human\nimages with incoherent parts or unnatural poses. To tackle these challenges,\nour key insight is that human image is inherently structural over multiple\ngranularities, from the coarse-level body skeleton to fine-grained spatial\ngeometry. Therefore, capturing such correlations between the explicit\nappearance and latent structure in one model is essential to generate coherent\nand natural human images. To this end, we propose a unified framework,\nHyperHuman, that generates in-the-wild human images of high realism and diverse\nlayouts. Specifically, 1) we first build a large-scale human-centric dataset,\nnamed HumanVerse, which consists of 340M images with comprehensive annotations\nlike human pose, depth, and surface normal. 2) Next, we propose a Latent\nStructural Diffusion Model that simultaneously denoises the depth and surface\nnormal along with the synthesized RGB image. Our model enforces the joint\nlearning of image appearance, spatial relationship, and geometry in a unified\nnetwork, where each branch in the model complements to each other with both\nstructural awareness and textural richness. 3) Finally, to further boost the\nvisual quality, we propose a Structure-Guided Refiner to compose the predicted\nconditions for more detailed generation of higher resolution. Extensive\nexperiments demonstrate that our framework yields the state-of-the-art\nperformance, generating hyper-realistic human images under diverse scenarios.\nProject Page: https://snap-research.github.io/HyperHuman/",
        "translated": ""
    },
    {
        "title": "Visual Data-Type Understanding does not emerge from Scaling\n  Vision-Language Models",
        "url": "http://arxiv.org/abs/2310.08577v1",
        "pub_date": "2023-10-12",
        "summary": "Recent advances in the development of vision-language models (VLMs) are\nyielding remarkable success in recognizing visual semantic content, including\nimpressive instances of compositional image understanding. Here, we introduce\nthe novel task of \\textit{Visual Data-Type Identification}, a basic perceptual\nskill with implications for data curation (e.g., noisy data-removal from large\ndatasets, domain-specific retrieval) and autonomous vision (e.g.,\ndistinguishing changing weather conditions from camera lens staining). We\ndevelop two datasets consisting of animal images altered across a diverse set\nof 27 visual \\textit{data-types}, spanning four broad categories. An extensive\nzero-shot evaluation of 39 VLMs, ranging from 100M to 80B parameters, shows a\nnuanced performance landscape. While VLMs are reasonably good at identifying\ncertain stylistic \\textit{data-types}, such as cartoons and sketches, they\nstruggle with simpler \\textit{data-types} arising from basic manipulations like\nimage rotations or additive noise. Our findings reveal that (i) model scaling\nalone yields marginal gains for contrastively-trained models like CLIP, and\n(ii) there is a pronounced drop in performance for the largest\nauto-regressively trained VLMs like OpenFlamingo. This finding points to a\nblind spot in current frontier VLMs: they excel in recognizing semantic content\nbut fail to acquire an understanding of visual \\textit{data-types} through\nscaling. By analyzing the pre-training distributions of these models and\nincorporating \\textit{data-type} information into the captions during\nfine-tuning, we achieve a significant enhancement in performance. By exploring\nthis previously uncharted task, we aim to set the stage for further advancing\nVLMs to equip them with visual data-type understanding. Code and datasets are\nreleased \\href{https://github.com/bethgelab/DataTypeIdentification}{here}.",
        "translated": ""
    },
    {
        "title": "Learning to Act from Actionless Videos through Dense Correspondences",
        "url": "http://arxiv.org/abs/2310.08576v1",
        "pub_date": "2023-10-12",
        "summary": "In this work, we present an approach to construct a video-based robot policy\ncapable of reliably executing diverse tasks across different robots and\nenvironments from few video demonstrations without using any action\nannotations. Our method leverages images as a task-agnostic representation,\nencoding both the state and action information, and text as a general\nrepresentation for specifying robot goals. By synthesizing videos that\n``hallucinate'' robot executing actions and in combination with dense\ncorrespondences between frames, our approach can infer the closed-formed action\nto execute to an environment without the need of any explicit action labels.\nThis unique capability allows us to train the policy solely based on RGB videos\nand deploy learned policies to various robotic tasks. We demonstrate the\nefficacy of our approach in learning policies on table-top manipulation and\nnavigation tasks. Additionally, we contribute an open-source framework for\nefficient video modeling, enabling the training of high-fidelity policy models\nwith four GPUs within a single day.",
        "translated": ""
    },
    {
        "title": "HairCLIPv2: Unifying Hair Editing via Proxy Feature Blending",
        "url": "http://arxiv.org/abs/2310.10651v1",
        "pub_date": "2023-10-16",
        "summary": "Hair editing has made tremendous progress in recent years. Early hair editing\nmethods use well-drawn sketches or masks to specify the editing conditions.\nEven though they can enable very fine-grained local control, such interaction\nmodes are inefficient for the editing conditions that can be easily specified\nby language descriptions or reference images. Thanks to the recent breakthrough\nof cross-modal models (e.g., CLIP), HairCLIP is the first work that enables\nhair editing based on text descriptions or reference images. However, such\ntext-driven and reference-driven interaction modes make HairCLIP unable to\nsupport fine-grained controls specified by sketch or mask. In this paper, we\npropose HairCLIPv2, aiming to support all the aforementioned interactions with\none unified framework. Simultaneously, it improves upon HairCLIP with better\nirrelevant attributes (e.g., identity, background) preservation and unseen text\ndescriptions support. The key idea is to convert all the hair editing tasks\ninto hair transfer tasks, with editing conditions converted into different\nproxies accordingly. The editing effects are added upon the input image by\nblending the corresponding proxy features within the hairstyle or hair color\nfeature spaces. Besides the unprecedented user interaction mode support,\nquantitative and qualitative experiments demonstrate the superiority of\nHairCLIPv2 in terms of editing effects, irrelevant attribute preservation and\nvisual naturalness. Our code is available at\n\\url{https://github.com/wty-ustc/HairCLIPv2}.",
        "translated": ""
    },
    {
        "title": "TraM-NeRF: Tracing Mirror and Near-Perfect Specular Reflections through\n  Neural Radiance Fields",
        "url": "http://arxiv.org/abs/2310.10650v1",
        "pub_date": "2023-10-16",
        "summary": "Implicit representations like Neural Radiance Fields (NeRF) showed impressive\nresults for photorealistic rendering of complex scenes with fine details.\nHowever, ideal or near-perfectly specular reflecting objects such as mirrors,\nwhich are often encountered in various indoor scenes, impose ambiguities and\ninconsistencies in the representation of the reconstructed scene leading to\nsevere artifacts in the synthesized renderings. In this paper, we present a\nnovel reflection tracing method tailored for the involved volume rendering\nwithin NeRF that takes these mirror-like objects into account while avoiding\nthe cost of straightforward but expensive extensions through standard path\ntracing. By explicitly modeling the reflection behavior using physically\nplausible materials and estimating the reflected radiance with Monte-Carlo\nmethods within the volume rendering formulation, we derive efficient strategies\nfor importance sampling and the transmittance computation along rays from only\nfew samples. We show that our novel method enables the training of consistent\nrepresentations of such challenging scenes and achieves superior results in\ncomparison to previous state-of-the-art approaches.",
        "translated": ""
    },
    {
        "title": "A Survey on Video Diffusion Models",
        "url": "http://arxiv.org/abs/2310.10647v1",
        "pub_date": "2023-10-16",
        "summary": "The recent wave of AI-generated content (AIGC) has witnessed substantial\nsuccess in computer vision, with the diffusion model playing a crucial role in\nthis achievement. Due to their impressive generative capabilities, diffusion\nmodels are gradually superseding methods based on GANs and auto-regressive\nTransformers, demonstrating exceptional performance not only in image\ngeneration and editing, but also in the realm of video-related research.\nHowever, existing surveys mainly focus on diffusion models in the context of\nimage generation, with few up-to-date reviews on their application in the video\ndomain. To address this gap, this paper presents a comprehensive review of\nvideo diffusion models in the AIGC era. Specifically, we begin with a concise\nintroduction to the fundamentals and evolution of diffusion models.\nSubsequently, we present an overview of research on diffusion models in the\nvideo domain, categorizing the work into three key areas: video generation,\nvideo editing, and other video understanding tasks. We conduct a thorough\nreview of the literature in these three key areas, including further\ncategorization and practical contributions in the field. Finally, we discuss\nthe challenges faced by research in this domain and outline potential future\ndevelopmental trends. A comprehensive list of video diffusion models studied in\nthis survey is available at\nhttps://github.com/ChenHsing/Awesome-Video-Diffusion-Models.",
        "translated": ""
    },
    {
        "title": "TOSS:High-quality Text-guided Novel View Synthesis from a Single Image",
        "url": "http://arxiv.org/abs/2310.10644v1",
        "pub_date": "2023-10-16",
        "summary": "In this paper, we present TOSS, which introduces text to the task of novel\nview synthesis (NVS) from just a single RGB image. While Zero-1-to-3 has\ndemonstrated impressive zero-shot open-set NVS capability, it treats NVS as a\npure image-to-image translation problem. This approach suffers from the\nchallengingly under-constrained nature of single-view NVS: the process lacks\nmeans of explicit user control and often results in implausible NVS\ngenerations. To address this limitation, TOSS uses text as high-level semantic\ninformation to constrain the NVS solution space. TOSS fine-tunes text-to-image\nStable Diffusion pre-trained on large-scale text-image pairs and introduces\nmodules specifically tailored to image and camera pose conditioning, as well as\ndedicated training for pose correctness and preservation of fine details.\nComprehensive experiments are conducted with results showing that our proposed\nTOSS outperforms Zero-1-to-3 with more plausible, controllable and\nmultiview-consistent NVS results. We further support these results with\ncomprehensive ablations that underscore the effectiveness and potential of the\nintroduced semantic guidance and architecture design.",
        "translated": ""
    },
    {
        "title": "Real-time Photorealistic Dynamic Scene Representation and Rendering with\n  4D Gaussian Splatting",
        "url": "http://arxiv.org/abs/2310.10642v1",
        "pub_date": "2023-10-16",
        "summary": "Reconstructing dynamic 3D scenes from 2D images and generating diverse views\nover time is challenging due to scene complexity and temporal dynamics. Despite\nadvancements in neural implicit models, limitations persist: (i) Inadequate\nScene Structure: Existing methods struggle to reveal the spatial and temporal\nstructure of dynamic scenes from directly learning the complex 6D plenoptic\nfunction. (ii) Scaling Deformation Modeling: Explicitly modeling scene element\ndeformation becomes impractical for complex dynamics. To address these issues,\nwe consider the spacetime as an entirety and propose to approximate the\nunderlying spatio-temporal 4D volume of a dynamic scene by optimizing a\ncollection of 4D primitives, with explicit geometry and appearance modeling.\nLearning to optimize the 4D primitives enables us to synthesize novel views at\nany desired time with our tailored rendering routine. Our model is conceptually\nsimple, consisting of a 4D Gaussian parameterized by anisotropic ellipses that\ncan rotate arbitrarily in space and time, as well as view-dependent and\ntime-evolved appearance represented by the coefficient of 4D spherindrical\nharmonics. This approach offers simplicity, flexibility for variable-length\nvideo and end-to-end training, and efficient real-time rendering, making it\nsuitable for capturing complex dynamic scene motions. Experiments across\nvarious benchmarks, including monocular and multi-view scenarios, demonstrate\nour 4DGS model's superior visual quality and efficiency.",
        "translated": ""
    },
    {
        "title": "LLM Blueprint: Enabling Text-to-Image Generation with Complex and\n  Detailed Prompts",
        "url": "http://arxiv.org/abs/2310.10640v1",
        "pub_date": "2023-10-16",
        "summary": "Diffusion-based generative models have significantly advanced text-to-image\ngeneration but encounter challenges when processing lengthy and intricate text\nprompts describing complex scenes with multiple objects. While excelling in\ngenerating images from short, single-object descriptions, these models often\nstruggle to faithfully capture all the nuanced details within longer and more\nelaborate textual inputs. In response, we present a novel approach leveraging\nLarge Language Models (LLMs) to extract critical components from text prompts,\nincluding bounding box coordinates for foreground objects, detailed textual\ndescriptions for individual objects, and a succinct background context. These\ncomponents form the foundation of our layout-to-image generation model, which\noperates in two phases. The initial Global Scene Generation utilizes object\nlayouts and background context to create an initial scene but often falls short\nin faithfully representing object characteristics as specified in the prompts.\nTo address this limitation, we introduce an Iterative Refinement Scheme that\niteratively evaluates and refines box-level content to align them with their\ntextual descriptions, recomposing objects as needed to ensure consistency. Our\nevaluation on complex prompts featuring multiple objects demonstrates a\nsubstantial improvement in recall compared to baseline diffusion models. This\nis further validated by a user study, underscoring the efficacy of our approach\nin generating coherent and detailed scenes from intricate textual inputs.",
        "translated": ""
    },
    {
        "title": "Towards Scenario-based Safety Validation for Autonomous Trains with Deep\n  Generative Models",
        "url": "http://arxiv.org/abs/2310.10635v1",
        "pub_date": "2023-10-16",
        "summary": "Modern AI techniques open up ever-increasing possibilities for autonomous\nvehicles, but how to appropriately verify the reliability of such systems\nremains unclear. A common approach is to conduct safety validation based on a\npredefined Operational Design Domain (ODD) describing specific conditions under\nwhich a system under test is required to operate properly. However, collecting\nsufficient realistic test cases to ensure comprehensive ODD coverage is\nchallenging. In this paper, we report our practical experiences regarding the\nutility of data simulation with deep generative models for scenario-based ODD\nvalidation. We consider the specific use case of a camera-based rail-scene\nsegmentation system designed to support autonomous train operation. We\ndemonstrate the capabilities of semantically editing railway scenes with deep\ngenerative models to make a limited amount of test data more representative. We\nalso show how our approach helps to analyze the degree to which a system\ncomplies with typical ODD requirements. Specifically, we focus on evaluating\nproper operation under different lighting and weather conditions as well as\nwhile transitioning between them.",
        "translated": ""
    },
    {
        "title": "Video Language Planning",
        "url": "http://arxiv.org/abs/2310.10625v1",
        "pub_date": "2023-10-16",
        "summary": "We are interested in enabling visual planning for complex long-horizon tasks\nin the space of generated videos and language, leveraging recent advances in\nlarge generative models pretrained on Internet-scale data. To this end, we\npresent video language planning (VLP), an algorithm that consists of a tree\nsearch procedure, where we train (i) vision-language models to serve as both\npolicies and value functions, and (ii) text-to-video models as dynamics models.\nVLP takes as input a long-horizon task instruction and current image\nobservation, and outputs a long video plan that provides detailed multimodal\n(video and language) specifications that describe how to complete the final\ntask. VLP scales with increasing computation budget where more computation time\nresults in improved video plans, and is able to synthesize long-horizon video\nplans across different robotics domains: from multi-object rearrangement, to\nmulti-camera bi-arm dexterous manipulation. Generated video plans can be\ntranslated into real robot actions via goal-conditioned policies, conditioned\non each intermediate frame of the generated video. Experiments show that VLP\nsubstantially improves long-horizon task success rates compared to prior\nmethods on both simulated and real robots (across 3 hardware platforms).",
        "translated": ""
    },
    {
        "title": "DynVideo-E: Harnessing Dynamic NeRF for Large-Scale Motion- and\n  View-Change Human-Centric Video Editing",
        "url": "http://arxiv.org/abs/2310.10624v1",
        "pub_date": "2023-10-16",
        "summary": "Despite remarkable research advances in diffusion-based video editing,\nexisting methods are limited to short-length videos due to the contradiction\nbetween long-range consistency and frame-wise editing. Recent approaches\nattempt to tackle this challenge by introducing video-2D representations to\ndegrade video editing to image editing. However, they encounter significant\ndifficulties in handling large-scale motion- and view-change videos especially\nfor human-centric videos. This motivates us to introduce the dynamic Neural\nRadiance Fields (NeRF) as the human-centric video representation to ease the\nvideo editing problem to a 3D space editing task. As such, editing can be\nperformed in the 3D spaces and propagated to the entire video via the\ndeformation field. To provide finer and direct controllable editing, we propose\nthe image-based 3D space editing pipeline with a set of effective designs.\nThese include multi-view multi-pose Score Distillation Sampling (SDS) from both\n2D personalized diffusion priors and 3D diffusion priors, reconstruction losses\non the reference image, text-guided local parts super-resolution, and style\ntransfer for 3D background space. Extensive experiments demonstrate that our\nmethod, dubbed as DynVideo-E, significantly outperforms SOTA approaches on two\nchallenging datasets by a large margin of 50% ~ 95% in terms of human\npreference. Compelling video comparisons are provided in the project page\nhttps://showlab.github.io/DynVideo-E/. Our code and data will be released to\nthe community.",
        "translated": ""
    },
    {
        "title": "Motion2Language, Unsupervised learning of synchronized semantic motion\n  segmentation",
        "url": "http://arxiv.org/abs/2310.10594v1",
        "pub_date": "2023-10-16",
        "summary": "In this paper, we investigate building a sequence to sequence architecture\nfor motion to language translation and synchronization. The aim is to translate\nmotion capture inputs into English natural-language descriptions, such that the\ndescriptions are generated synchronously with the actions performed, enabling\nsemantic segmentation as a byproduct, but without requiring synchronized\ntraining data. We propose a new recurrent formulation of local attention that\nis suited for synchronous/live text generation, as well as an improved motion\nencoder architecture better suited to smaller data and for synchronous\ngeneration. We evaluate both contributions in individual experiments, using the\nstandard BLEU4 metric, as well as a simple semantic equivalence measure, on the\nKIT motion language dataset. In a follow-up experiment, we assess the quality\nof the synchronization of generated text in our proposed approaches through\nmultiple evaluation metrics. We find that both contributions to the attention\nmechanism and the encoder architecture additively improve the quality of\ngenerated text (BLEU and semantic equivalence), but also of synchronization.\nOur code will be made available at\n\\url{https://github.com/rd20karim/M2T-Segmentation/tree/main}",
        "translated": ""
    },
    {
        "title": "DELIFFAS: Deformable Light Fields for Fast Avatar Synthesis",
        "url": "http://arxiv.org/abs/2310.11449v1",
        "pub_date": "2023-10-17",
        "summary": "Generating controllable and photorealistic digital human avatars is a\nlong-standing and important problem in Vision and Graphics. Recent methods have\nshown great progress in terms of either photorealism or inference speed while\nthe combination of the two desired properties still remains unsolved. To this\nend, we propose a novel method, called DELIFFAS, which parameterizes the\nappearance of the human as a surface light field that is attached to a\ncontrollable and deforming human mesh model. At the core, we represent the\nlight field around the human with a deformable two-surface parameterization,\nwhich enables fast and accurate inference of the human appearance. This allows\nperceptual supervision on the full image compared to previous approaches that\ncould only supervise individual pixels or small patches due to their slow\nruntime. Our carefully designed human representation and supervision strategy\nleads to state-of-the-art synthesis results and inference time. The video\nresults and code are available at\nhttps://vcai.mpi-inf.mpg.de/projects/DELIFFAS.",
        "translated": ""
    },
    {
        "title": "4K4D: Real-Time 4D View Synthesis at 4K Resolution",
        "url": "http://arxiv.org/abs/2310.11448v2",
        "pub_date": "2023-10-17",
        "summary": "This paper targets high-fidelity and real-time view synthesis of dynamic 3D\nscenes at 4K resolution. Recently, some methods on dynamic view synthesis have\nshown impressive rendering quality. However, their speed is still limited when\nrendering high-resolution images. To overcome this problem, we propose 4K4D, a\n4D point cloud representation that supports hardware rasterization and enables\nunprecedented rendering speed. Our representation is built on a 4D feature grid\nso that the points are naturally regularized and can be robustly optimized. In\naddition, we design a novel hybrid appearance model that significantly boosts\nthe rendering quality while preserving efficiency. Moreover, we develop a\ndifferentiable depth peeling algorithm to effectively learn the proposed model\nfrom RGB videos. Experiments show that our representation can be rendered at\nover 400 FPS on the DNA-Rendering dataset at 1080p resolution and 80 FPS on the\nENeRF-Outdoor dataset at 4K resolution using an RTX 4090 GPU, which is 30x\nfaster than previous methods and achieves the state-of-the-art rendering\nquality. Our project page is available at https://zju3dv.github.io/4k4d/.",
        "translated": ""
    },
    {
        "title": "Set-of-Mark Prompting Unleashes Extraordinary Visual Grounding in GPT-4V",
        "url": "http://arxiv.org/abs/2310.11441v1",
        "pub_date": "2023-10-17",
        "summary": "We present Set-of-Mark (SoM), a new visual prompting method, to unleash the\nvisual grounding abilities of large multimodal models (LMMs), such as GPT-4V.\nAs illustrated in Fig. 1 (right), we employ off-the-shelf interactive\nsegmentation models, such as SAM, to partition an image into regions at\ndifferent levels of granularity, and overlay these regions with a set of marks\ne.g., alphanumerics, masks, boxes. Using the marked image as input, GPT-4V can\nanswer the questions that require visual grounding. We perform a comprehensive\nempirical study to validate the effectiveness of SoM on a wide range of\nfine-grained vision and multimodal tasks. For example, our experiments show\nthat GPT-4V with SoM outperforms the state-of-the-art fully-finetuned referring\nsegmentation model on RefCOCOg in a zero-shot setting.",
        "translated": ""
    },
    {
        "title": "EvalCrafter: Benchmarking and Evaluating Large Video Generation Models",
        "url": "http://arxiv.org/abs/2310.11440v2",
        "pub_date": "2023-10-17",
        "summary": "The vision and language generative models have been overgrown in recent\nyears. For video generation, various open-sourced models and public-available\nservices are released for generating high-visual quality videos. However, these\nmethods often use a few academic metrics, for example, FVD or IS, to evaluate\nthe performance. We argue that it is hard to judge the large conditional\ngenerative models from the simple metrics since these models are often trained\non very large datasets with multi-aspect abilities. Thus, we propose a new\nframework and pipeline to exhaustively evaluate the performance of the\ngenerated videos. To achieve this, we first conduct a new prompt list for\ntext-to-video generation by analyzing the real-world prompt list with the help\nof the large language model. Then, we evaluate the state-of-the-art video\ngenerative models on our carefully designed benchmarks, in terms of visual\nqualities, content qualities, motion qualities, and text-caption alignment with\naround 18 objective metrics. To obtain the final leaderboard of the models, we\nalso fit a series of coefficients to align the objective metrics to the users'\nopinions. Based on the proposed opinion alignment method, our final score shows\na higher correlation than simply averaging the metrics, showing the\neffectiveness of the proposed evaluation method.",
        "translated": ""
    },
    {
        "title": "Revisiting Map Relations for Unsupervised Non-Rigid Shape Matching",
        "url": "http://arxiv.org/abs/2310.11420v1",
        "pub_date": "2023-10-17",
        "summary": "We propose a novel unsupervised learning approach for non-rigid 3D shape\nmatching. Our approach improves upon recent state-of-the art deep functional\nmap methods and can be applied to a broad range of different challenging\nscenarios. Previous deep functional map methods mainly focus on feature\nextraction and aim exclusively at obtaining more expressive features for\nfunctional map computation. However, the importance of the functional map\ncomputation itself is often neglected and the relationship between the\nfunctional map and point-wise map is underexplored. In this paper, we\nsystematically investigate the coupling relationship between the functional map\nfrom the functional map solver and the point-wise map based on feature\nsimilarity. To this end, we propose a self-adaptive functional map solver to\nadjust the functional map regularisation for different shape matching\nscenarios, together with a vertex-wise contrastive loss to obtain more\ndiscriminative features. Using different challenging datasets (including\nnon-isometry, topological noise and partiality), we demonstrate that our method\nsubstantially outperforms previous state-of-the-art methods.",
        "translated": ""
    },
    {
        "title": "VcT: Visual change Transformer for Remote Sensing Image Change Detection",
        "url": "http://arxiv.org/abs/2310.11417v1",
        "pub_date": "2023-10-17",
        "summary": "Existing visual change detectors usually adopt CNNs or Transformers for\nfeature representation learning and focus on learning effective representation\nfor the changed regions between images. Although good performance can be\nobtained by enhancing the features of the change regions, however, these works\nare still limited mainly due to the ignorance of mining the unchanged\nbackground context information. It is known that one main challenge for change\ndetection is how to obtain the consistent representations for two images\ninvolving different variations, such as spatial variation, sunlight intensity,\netc. In this work, we demonstrate that carefully mining the common background\ninformation provides an important cue to learn the consistent representations\nfor the two images which thus obviously facilitates the visual change detection\nproblem. Based on this observation, we propose a novel Visual change\nTransformer (VcT) model for visual change detection problem. To be specific, a\nshared backbone network is first used to extract the feature maps for the given\nimage pair. Then, each pixel of feature map is regarded as a graph node and the\ngraph neural network is proposed to model the structured information for coarse\nchange map prediction. Top-K reliable tokens can be mined from the map and\nrefined by using the clustering algorithm. Then, these reliable tokens are\nenhanced by first utilizing self/cross-attention schemes and then interacting\nwith original features via an anchor-primary attention learning module.\nFinally, the prediction head is proposed to get a more accurate change map.\nExtensive experiments on multiple benchmark datasets validated the\neffectiveness of our proposed VcT model.",
        "translated": ""
    },
    {
        "title": "Towards Automatic Satellite Images Captions Generation Using Large\n  Language Models",
        "url": "http://arxiv.org/abs/2310.11392v1",
        "pub_date": "2023-10-17",
        "summary": "Automatic image captioning is a promising technique for conveying visual\ninformation using natural language. It can benefit various tasks in satellite\nremote sensing, such as environmental monitoring, resource management, disaster\nmanagement, etc. However, one of the main challenges in this domain is the lack\nof large-scale image-caption datasets, as they require a lot of human expertise\nand effort to create. Recent research on large language models (LLMs) has\ndemonstrated their impressive performance in natural language understanding and\ngeneration tasks. Nonetheless, most of them cannot handle images (GPT-3.5,\nFalcon, Claude, etc.), while conventional captioning models pre-trained on\ngeneral ground-view images often fail to produce detailed and accurate captions\nfor aerial images (BLIP, GIT, CM3, CM3Leon, etc.). To address this problem, we\npropose a novel approach: Automatic Remote Sensing Image Captioning (ARSIC) to\nautomatically collect captions for remote sensing images by guiding LLMs to\ndescribe their object annotations. We also present a benchmark model that\nadapts the pre-trained generative image2text model (GIT) to generate\nhigh-quality captions for remote-sensing images. Our evaluation demonstrates\nthe effectiveness of our approach for collecting captions for remote sensing\nimages.",
        "translated": ""
    },
    {
        "title": "A voxel-level approach to brain age prediction: A method to assess\n  regional brain aging",
        "url": "http://arxiv.org/abs/2310.11385v1",
        "pub_date": "2023-10-17",
        "summary": "Brain aging is a regional phenomenon, a facet that remains relatively\nunder-explored within the realm of brain age prediction research using machine\nlearning methods. Voxel-level predictions can provide localized brain age\nestimates that can provide granular insights into the regional aging processes.\nThis is essential to understand the differences in aging trajectories in\nhealthy versus diseased subjects. In this work, a deep learning-based multitask\nmodel is proposed for voxel-level brain age prediction from T1-weighted\nmagnetic resonance images. The proposed model outperforms the models existing\nin the literature and yields valuable clinical insights when applied to both\nhealthy and diseased populations. Regional analysis is performed on the\nvoxel-level brain age predictions to understand aging trajectories of known\nanatomical regions in the brain and show that there exist disparities in\nregional aging trajectories of healthy subjects compared to ones with\nunderlying neurological disorders such as Dementia and more specifically,\nAlzheimer's disease. Our code is available at\nhttps://github.com/nehagianchandani/Voxel-level-brain-age-prediction.",
        "translated": ""
    },
    {
        "title": "Towards Generalizable Multi-Camera 3D Object Detection via Perspective\n  Debiasing",
        "url": "http://arxiv.org/abs/2310.11346v1",
        "pub_date": "2023-10-17",
        "summary": "Detecting objects in 3D space using multiple cameras, known as Multi-Camera\n3D Object Detection (MC3D-Det), has gained prominence with the advent of\nbird's-eye view (BEV) approaches. However, these methods often struggle when\nfaced with unfamiliar testing environments due to the lack of diverse training\ndata encompassing various viewpoints and environments. To address this, we\npropose a novel method that aligns 3D detection with 2D camera plane results,\nensuring consistent and accurate detections. Our framework, anchored in\nperspective debiasing, helps the learning of features resilient to domain\nshifts. In our approach, we render diverse view maps from BEV features and\nrectify the perspective bias of these maps, leveraging implicit foreground\nvolumes to bridge the camera and BEV planes. This two-step process promotes the\nlearning of perspective- and context-independent features, crucial for accurate\nobject detection across varying viewpoints, camera parameters and environment\nconditions. Notably, our model-agnostic approach preserves the original network\nstructure without incurring additional inference costs, facilitating seamless\nintegration across various models and simplifying deployment. Furthermore, we\nalso show our approach achieves satisfactory results in real data when trained\nonly with virtual datasets, eliminating the need for real scene annotations.\nExperimental results on both Domain Generalization (DG) and Unsupervised Domain\nAdaptation (UDA) clearly demonstrate its effectiveness. Our code will be\nreleased.",
        "translated": ""
    },
    {
        "title": "Dual Cognitive Architecture: Incorporating Biases and Multi-Memory\n  Systems for Lifelong Learning",
        "url": "http://arxiv.org/abs/2310.11341v1",
        "pub_date": "2023-10-17",
        "summary": "Artificial neural networks (ANNs) exhibit a narrow scope of expertise on\nstationary independent data. However, the data in the real world is continuous\nand dynamic, and ANNs must adapt to novel scenarios while also retaining the\nlearned knowledge to become lifelong learners. The ability of humans to excel\nat these tasks can be attributed to multiple factors ranging from cognitive\ncomputational structures, cognitive biases, and the multi-memory systems in the\nbrain. We incorporate key concepts from each of these to design a novel\nframework, Dual Cognitive Architecture (DUCA), which includes multiple\nsub-systems, implicit and explicit knowledge representation dichotomy,\ninductive bias, and a multi-memory system. The inductive bias learner within\nDUCA is instrumental in encoding shape information, effectively countering the\ntendency of ANNs to learn local textures. Simultaneously, the inclusion of a\nsemantic memory submodule facilitates the gradual consolidation of knowledge,\nreplicating the dynamics observed in fast and slow learning systems,\nreminiscent of the principles underpinning the complementary learning system in\nhuman cognition. DUCA shows improvement across different settings and datasets,\nand it also exhibits reduced task recency bias, without the need for extra\ninformation. To further test the versatility of lifelong learning methods on a\nchallenging distribution shift, we introduce a novel domain-incremental dataset\nDN4IL. In addition to improving performance on existing benchmarks, DUCA also\ndemonstrates superior performance on this complex dataset.",
        "translated": ""
    },
    {
        "title": "Probabilistic Sampling of Balanced K-Means using Adiabatic Quantum\n  Computing",
        "url": "http://arxiv.org/abs/2310.12153v1",
        "pub_date": "2023-10-18",
        "summary": "Adiabatic quantum computing (AQC) is a promising quantum computing approach\nfor discrete and often NP-hard optimization problems. Current AQCs allow to\nimplement problems of research interest, which has sparked the development of\nquantum representations for many machine learning and computer vision tasks.\nDespite requiring multiple measurements from the noisy AQC, current approaches\nonly utilize the best measurement, discarding information contained in the\nremaining ones. In this work, we explore the potential of using this\ninformation for probabilistic balanced k-means clustering. Instead of\ndiscarding non-optimal solutions, we propose to use them to compute calibrated\nposterior probabilities with little additional compute cost. This allows us to\nidentify ambiguous solutions and data points, which we demonstrate on a D-Wave\nAQC on synthetic and real data.",
        "translated": ""
    },
    {
        "title": "Learning from Rich Semantics and Coarse Locations for Long-tailed Object\n  Detection",
        "url": "http://arxiv.org/abs/2310.12152v1",
        "pub_date": "2023-10-18",
        "summary": "Long-tailed object detection (LTOD) aims to handle the extreme data imbalance\nin real-world datasets, where many tail classes have scarce instances. One\npopular strategy is to explore extra data with image-level labels, yet it\nproduces limited results due to (1) semantic ambiguity -- an image-level label\nonly captures a salient part of the image, ignoring the remaining rich\nsemantics within the image; and (2) location sensitivity -- the label highly\ndepends on the locations and crops of the original image, which may change\nafter data transformations like random cropping. To remedy this, we propose\nRichSem, a simple but effective method, which is robust to learn rich semantics\nfrom coarse locations without the need of accurate bounding boxes. RichSem\nleverages rich semantics from images, which are then served as additional soft\nsupervision for training detectors. Specifically, we add a semantic branch to\nour detector to learn these soft semantics and enhance feature representations\nfor long-tailed object detection. The semantic branch is only used for training\nand is removed during inference. RichSem achieves consistent improvements on\nboth overall and rare-category of LVIS under different backbones and detectors.\nOur method achieves state-of-the-art performance without requiring complex\ntraining and testing procedures. Moreover, we show the effectiveness of our\nmethod on other long-tailed datasets with additional experiments. Code is\navailable at \\url{https://github.com/MengLcool/RichSem}.",
        "translated": ""
    },
    {
        "title": "Object-aware Inversion and Reassembly for Image Editing",
        "url": "http://arxiv.org/abs/2310.12149v1",
        "pub_date": "2023-10-18",
        "summary": "By comparing the original and target prompts in editing task, we can obtain\nnumerous editing pairs, each comprising an object and its corresponding editing\ntarget. To allow editability while maintaining fidelity to the input image,\nexisting editing methods typically involve a fixed number of inversion steps\nthat project the whole input image to its noisier latent representation,\nfollowed by a denoising process guided by the target prompt. However, we find\nthat the optimal number of inversion steps for achieving ideal editing results\nvaries significantly among different editing pairs, owing to varying editing\ndifficulties. Therefore, the current literature, which relies on a fixed number\nof inversion steps, produces sub-optimal generation quality, especially when\nhandling multiple editing pairs in a natural image. To this end, we propose a\nnew image editing paradigm, dubbed Object-aware Inversion and Reassembly (OIR),\nto enable object-level fine-grained editing. Specifically, we design a new\nsearch metric, which determines the optimal inversion steps for each editing\npair, by jointly considering the editability of the target and the fidelity of\nthe non-editing region. We use our search metric to find the optimal inversion\nstep for each editing pair when editing an image. We then edit these editing\npairs separately to avoid concept mismatch. Subsequently, we propose an\nadditional reassembly step to seamlessly integrate the respective editing\nresults and the non-editing region to obtain the final edited image. To\nsystematically evaluate the effectiveness of our method, we collect two\ndatasets for benchmarking single- and multi-object editing, respectively.\nExperiments demonstrate that our method achieves superior performance in\nediting object shapes, colors, materials, categories, etc., especially in\nmulti-object editing scenarios.",
        "translated": ""
    },
    {
        "title": "InViG: Benchmarking Interactive Visual Grounding with 500K Human-Robot\n  Interactions",
        "url": "http://arxiv.org/abs/2310.12147v1",
        "pub_date": "2023-10-18",
        "summary": "Ambiguity is ubiquitous in human communication. Previous approaches in\nHuman-Robot Interaction (HRI) have often relied on predefined interaction\ntemplates, leading to reduced performance in realistic and open-ended\nscenarios. To address these issues, we present a large-scale dataset, \\invig,\nfor interactive visual grounding under language ambiguity. Our dataset\ncomprises over 520K images accompanied by open-ended goal-oriented\ndisambiguation dialogues, encompassing millions of object instances and\ncorresponding question-answer pairs. Leveraging the \\invig dataset, we conduct\nextensive studies and propose a set of baseline solutions for end-to-end\ninteractive visual disambiguation and grounding, achieving a 45.6\\% success\nrate during validation. To the best of our knowledge, the \\invig dataset is the\nfirst large-scale dataset for resolving open-ended interactive visual\ngrounding, presenting a practical yet highly challenging benchmark for\nambiguity-aware HRI. Codes and datasets are available at:\n\\href{https://openivg.github.io}{https://openivg.github.io}.",
        "translated": ""
    },
    {
        "title": "DiagrammerGPT: Generating Open-Domain, Open-Platform Diagrams via LLM\n  Planning",
        "url": "http://arxiv.org/abs/2310.12128v1",
        "pub_date": "2023-10-18",
        "summary": "Text-to-image (T2I) generation has seen significant growth over the past few\nyears. Despite this, there has been little work on generating diagrams with T2I\nmodels. A diagram is a symbolic/schematic representation that explains\ninformation using structurally rich and spatially complex visualizations (e.g.,\na dense combination of related objects, text labels, directional arrows,\nconnection lines, etc.). Existing state-of-the-art T2I models often fail at\ndiagram generation because they lack fine-grained object layout control when\nmany objects are densely connected via complex relations such as arrows/lines\nand also often fail to render comprehensible text labels. To address this gap,\nwe present DiagrammerGPT, a novel two-stage text-to-diagram generation\nframework that leverages the layout guidance capabilities of LLMs (e.g., GPT-4)\nto generate more accurate open-domain, open-platform diagrams. In the first\nstage, we use LLMs to generate and iteratively refine 'diagram plans' (in a\nplanner-auditor feedback loop) which describe all the entities (objects and\ntext labels), their relationships (arrows or lines), and their bounding box\nlayouts. In the second stage, we use a diagram generator, DiagramGLIGEN, and a\ntext label rendering module to generate diagrams following the diagram plans.\nTo benchmark the text-to-diagram generation task, we introduce AI2D-Caption, a\ndensely annotated diagram dataset built on top of the AI2D dataset. We show\nquantitatively and qualitatively that our DiagrammerGPT framework produces more\naccurate diagrams, outperforming existing T2I models. We also provide\ncomprehensive analysis including open-domain diagram generation, vector graphic\ndiagram generation in different platforms, human-in-the-loop diagram plan\nediting, and multimodal planner/auditor LLMs (e.g., GPT-4Vision). We hope our\nwork can inspire further research on diagram generation via T2I models and\nLLMs.",
        "translated": ""
    },
    {
        "title": "Non-Intrusive Adaptation: Input-Centric Parameter-efficient Fine-Tuning\n  for Versatile Multimodal Modeling",
        "url": "http://arxiv.org/abs/2310.12100v1",
        "pub_date": "2023-10-18",
        "summary": "Large language models (LLMs) and vision language models (VLMs) demonstrate\nexcellent performance on a wide range of tasks by scaling up parameter counts\nfrom O(10^9) to O(10^{12}) levels and further beyond. These large scales make\nit impossible to adapt and deploy fully specialized models given a task of\ninterest. Parameter-efficient fine-tuning (PEFT) emerges as a promising\ndirection to tackle the adaptation and serving challenges for such large\nmodels. We categorize PEFT techniques into two types: intrusive and\nnon-intrusive. Intrusive PEFT techniques directly change a model's internal\narchitecture. Though more flexible, they introduce significant complexities for\ntraining and serving. Non-intrusive PEFT techniques leave the internal\narchitecture unchanged and only adapt model-external parameters, such as\nembeddings for input. In this work, we describe AdaLink as a non-intrusive PEFT\ntechnique that achieves competitive performance compared to SoTA intrusive PEFT\n(LoRA) and full model fine-tuning (FT) on various tasks. We evaluate using both\ntext-only and multimodal tasks, with experiments that account for both\nparameter-count scaling and training regime (with and without instruction\ntuning).",
        "translated": ""
    },
    {
        "title": "HSTR-Net: Reference Based Video Super-resolution for Aerial Surveillance\n  with Dual Cameras",
        "url": "http://arxiv.org/abs/2310.12092v1",
        "pub_date": "2023-10-18",
        "summary": "Aerial surveillance requires high spatio-temporal resolution (HSTR) video for\nmore accurate detection and tracking of objects. This is especially true for\nwide-area surveillance (WAS), where the surveyed region is large and the\nobjects of interest are small. This paper proposes a dual camera system for the\ngeneration of HSTR video using reference-based super-resolution (RefSR). One\ncamera captures high spatial resolution low frame rate (HSLF) video while the\nother captures low spatial resolution high frame rate (LSHF) video\nsimultaneously for the same scene. A novel deep learning architecture is\nproposed to fuse HSLF and LSHF video feeds and synthesize HSTR video frames at\nthe output. The proposed model combines optical flow estimation and\n(channel-wise and spatial) attention mechanisms to capture the fine motion and\nintricate dependencies between frames of the two video feeds. Simulations show\nthat the proposed model provides significant improvement over existing\nreference-based SR techniques in terms of PSNR and SSIM metrics. The method\nalso exhibits sufficient frames per second (FPS) for WAS when deployed on a\npower-constrained drone equipped with dual cameras.",
        "translated": ""
    },
    {
        "title": "Unveiling the Siren's Song: Towards Reliable Fact-Conflicting\n  Hallucination Detection",
        "url": "http://arxiv.org/abs/2310.12086v1",
        "pub_date": "2023-10-18",
        "summary": "Large Language Models (LLMs), such as ChatGPT/GPT-4, have garnered widespread\nattention owing to their myriad of practical applications, yet their adoption\nhas been constrained by issues of fact-conflicting hallucinations across web\nplatforms. The assessment of factuality in text, produced by LLMs, remains\ninadequately explored, extending not only to the judgment of vanilla facts but\nalso encompassing the evaluation of factual errors emerging in complex\ninferential tasks like multi-hop, and etc. In response, we introduce FactCHD, a\nfact-conflicting hallucination detection benchmark meticulously designed for\nLLMs. Functioning as a pivotal tool in evaluating factuality within\n\"Query-Respons\" contexts, our benchmark assimilates a large-scale dataset,\nencapsulating a broad spectrum of factuality patterns, such as vanilla,\nmulti-hops, comparison, and set-operation patterns. A distinctive feature of\nour benchmark is its incorporation of fact-based chains of evidence, thereby\nfacilitating comprehensive and conducive factual reasoning throughout the\nassessment process. We evaluate multiple LLMs, demonstrating the effectiveness\nof the benchmark and current methods fall short of faithfully detecting factual\nerrors. Furthermore, we present TRUTH-TRIANGULATOR that synthesizes reflective\nconsiderations by tool-enhanced ChatGPT and LoRA-tuning based on Llama2, aiming\nto yield more credible detection through the amalgamation of predictive results\nand evidence. The benchmark dataset and source code will be made available in\nhttps://github.com/zjunlp/FactCHD.",
        "translated": ""
    },
    {
        "title": "On the Benefit of Generative Foundation Models for Human Activity\n  Recognition",
        "url": "http://arxiv.org/abs/2310.12085v1",
        "pub_date": "2023-10-18",
        "summary": "In human activity recognition (HAR), the limited availability of annotated\ndata presents a significant challenge. Drawing inspiration from the latest\nadvancements in generative AI, including Large Language Models (LLMs) and\nmotion synthesis models, we believe that generative AI can address this data\nscarcity by autonomously generating virtual IMU data from text descriptions.\nBeyond this, we spotlight several promising research pathways that could\nbenefit from generative AI for the community, including the generating\nbenchmark datasets, the development of foundational models specific to HAR, the\nexploration of hierarchical structures within HAR, breaking down complex\nactivities, and applications in health sensing and activity summarization.",
        "translated": ""
    },
    {
        "title": "One-Shot Imitation Learning: A Pose Estimation Perspective",
        "url": "http://arxiv.org/abs/2310.12077v1",
        "pub_date": "2023-10-18",
        "summary": "In this paper, we study imitation learning under the challenging setting of:\n(1) only a single demonstration, (2) no further data collection, and (3) no\nprior task or object knowledge. We show how, with these constraints, imitation\nlearning can be formulated as a combination of trajectory transfer and unseen\nobject pose estimation. To explore this idea, we provide an in-depth study on\nhow state-of-the-art unseen object pose estimators perform for one-shot\nimitation learning on ten real-world tasks, and we take a deep dive into the\neffects that camera calibration, pose estimation error, and spatial\ngeneralisation have on task success rates. For videos, please visit\nhttps://www.robot-learning.uk/pose-estimation-perspective.",
        "translated": ""
    },
    {
        "title": "Putting the Object Back into Video Object Segmentation",
        "url": "http://arxiv.org/abs/2310.12982v1",
        "pub_date": "2023-10-19",
        "summary": "We present Cutie, a video object segmentation (VOS) network with object-level\nmemory reading, which puts the object representation from memory back into the\nvideo object segmentation result. Recent works on VOS employ bottom-up\npixel-level memory reading which struggles due to matching noise, especially in\nthe presence of distractors, resulting in lower performance in more challenging\ndata. In contrast, Cutie performs top-down object-level memory reading by\nadapting a small set of object queries for restructuring and interacting with\nthe bottom-up pixel features iteratively with a query-based object transformer\n(qt, hence Cutie). The object queries act as a high-level summary of the target\nobject, while high-resolution feature maps are retained for accurate\nsegmentation. Together with foreground-background masked attention, Cutie\ncleanly separates the semantics of the foreground object from the background.\nOn the challenging MOSE dataset, Cutie improves by 8.7 J&amp;F over XMem with a\nsimilar running time and improves by 4.2 J&amp;F over DeAOT while running three\ntimes as fast. Code is available at: https://hkchengrex.github.io/Cutie",
        "translated": ""
    },
    {
        "title": "HumanTOMATO: Text-aligned Whole-body Motion Generation",
        "url": "http://arxiv.org/abs/2310.12978v1",
        "pub_date": "2023-10-19",
        "summary": "This work targets a novel text-driven whole-body motion generation task,\nwhich takes a given textual description as input and aims at generating\nhigh-quality, diverse, and coherent facial expressions, hand gestures, and body\nmotions simultaneously. Previous works on text-driven motion generation tasks\nmainly have two limitations: they ignore the key role of fine-grained hand and\nface controlling in vivid whole-body motion generation, and lack a good\nalignment between text and motion. To address such limitations, we propose a\nText-aligned whOle-body Motion generATiOn framework, named HumanTOMATO, which\nis the first attempt to our knowledge towards applicable holistic motion\ngeneration in this research area. To tackle this challenging task, our solution\nincludes two key designs: (1) a Holistic Hierarchical VQ-VAE (aka H$^2$VQ) and\na Hierarchical-GPT for fine-grained body and hand motion reconstruction and\ngeneration with two structured codebooks; and (2) a pre-trained\ntext-motion-alignment model to help generated motion align with the input\ntextual description explicitly. Comprehensive experiments verify that our model\nhas significant advantages in both the quality of generated motions and their\nalignment with text.",
        "translated": ""
    },
    {
        "title": "On the Hidden Waves of Image",
        "url": "http://arxiv.org/abs/2310.12976v1",
        "pub_date": "2023-10-19",
        "summary": "In this paper, we introduce an intriguing phenomenon-the successful\nreconstruction of images using a set of one-way wave equations with hidden and\nlearnable speeds. Each individual image corresponds to a solution with a unique\ninitial condition, which can be computed from the original image using a visual\nencoder (e.g., a convolutional neural network). Furthermore, the solution for\neach image exhibits two noteworthy mathematical properties: (a) it can be\ndecomposed into a collection of special solutions of the same one-way wave\nequations that are first-order autoregressive, with shared coefficient matrices\nfor autoregression, and (b) the product of these coefficient matrices forms a\ndiagonal matrix with the speeds of the wave equations as its diagonal elements.\nWe term this phenomenon hidden waves, as it reveals that, although the speeds\nof the set of wave equations and autoregressive coefficient matrices are\nlatent, they are both learnable and shared across images. This represents a\nmathematical invariance across images, providing a new mathematical perspective\nto understand images.",
        "translated": ""
    },
    {
        "title": "Variational Inference for SDEs Driven by Fractional Noise",
        "url": "http://arxiv.org/abs/2310.12975v1",
        "pub_date": "2023-10-19",
        "summary": "We present a novel variational framework for performing inference in (neural)\nstochastic differential equations (SDEs) driven by Markov-approximate\nfractional Brownian motion (fBM). SDEs offer a versatile tool for modeling\nreal-world continuous-time dynamic systems with inherent noise and randomness.\nCombining SDEs with the powerful inference capabilities of variational methods,\nenables the learning of representative function distributions through\nstochastic gradient descent. However, conventional SDEs typically assume the\nunderlying noise to follow a Brownian motion (BM), which hinders their ability\nto capture long-term dependencies. In contrast, fractional Brownian motion\n(fBM) extends BM to encompass non-Markovian dynamics, but existing methods for\ninferring fBM parameters are either computationally demanding or statistically\ninefficient. In this paper, building upon the Markov approximation of fBM, we\nderive the evidence lower bound essential for efficient variational inference\nof posterior path measures, drawing from the well-established field of\nstochastic analysis. Additionally, we provide a closed-form expression to\ndetermine optimal approximation coefficients. Furthermore, we propose the use\nof neural networks to learn the drift, diffusion and control terms within our\nvariational posterior, leading to the variational training of neural-SDEs. In\nthis framework, we also optimize the Hurst index, governing the nature of our\nfractional noise. Beyond validation on synthetic data, we contribute a novel\narchitecture for variational latent video prediction,-an approach that, to the\nbest of our knowledge, enables the first variational neural-SDE application to\nvideo perception.",
        "translated": ""
    },
    {
        "title": "FSD: Fast Self-Supervised Single RGB-D to Categorical 3D Objects",
        "url": "http://arxiv.org/abs/2310.12974v1",
        "pub_date": "2023-10-19",
        "summary": "In this work, we address the challenging task of 3D object recognition\nwithout the reliance on real-world 3D labeled data. Our goal is to predict the\n3D shape, size, and 6D pose of objects within a single RGB-D image, operating\nat the category level and eliminating the need for CAD models during inference.\nWhile existing self-supervised methods have made strides in this field, they\noften suffer from inefficiencies arising from non-end-to-end processing,\nreliance on separate models for different object categories, and slow surface\nextraction during the training of implicit reconstruction models; thus\nhindering both the speed and real-world applicability of the 3D recognition\nprocess. Our proposed method leverages a multi-stage training pipeline,\ndesigned to efficiently transfer synthetic performance to the real-world\ndomain. This approach is achieved through a combination of 2D and 3D supervised\nlosses during the synthetic domain training, followed by the incorporation of\n2D supervised and 3D self-supervised losses on real-world data in two\nadditional learning stages. By adopting this comprehensive strategy, our method\nsuccessfully overcomes the aforementioned limitations and outperforms existing\nself-supervised 6D pose and size estimation baselines on the NOCS test-set with\na 16.4% absolute improvement in mAP for 6D pose estimation while running in\nnear real-time at 5 Hz.",
        "translated": ""
    },
    {
        "title": "Frozen Transformers in Language Models Are Effective Visual Encoder\n  Layers",
        "url": "http://arxiv.org/abs/2310.12973v1",
        "pub_date": "2023-10-19",
        "summary": "This paper reveals that large language models (LLMs), despite being trained\nsolely on textual data, are surprisingly strong encoders for purely visual\ntasks in the absence of language. Even more intriguingly, this can be achieved\nby a simple yet previously overlooked strategy -- employing a frozen\ntransformer block from pre-trained LLMs as a constituent encoder layer to\ndirectly process visual tokens. Our work pushes the boundaries of leveraging\nLLMs for computer vision tasks, significantly departing from conventional\npractices that typically necessitate a multi-modal vision-language setup with\nassociated language prompts, inputs, or outputs. We demonstrate that our\napproach consistently enhances performance across a diverse range of tasks,\nencompassing pure 2D and 3D visual recognition tasks (e.g., image and point\ncloud classification), temporal modeling tasks (e.g., action recognition),\nnon-semantic tasks (e.g., motion forecasting), and multi-modal tasks (e.g.,\n2D/3D visual question answering and image-text retrieval). Such improvements\nare a general phenomenon, applicable to various types of LLMs (e.g., LLaMA and\nOPT) and different LLM transformer blocks. We additionally propose the\ninformation filtering hypothesis to explain the effectiveness of pre-trained\nLLMs in visual encoding -- the pre-trained LLM transformer blocks discern\ninformative visual tokens and further amplify their effect. This hypothesis is\nempirically supported by the observation that the feature activation, after\ntraining with LLM transformer blocks, exhibits a stronger focus on relevant\nregions. We hope that our work inspires new perspectives on utilizing LLMs and\ndeepening our understanding of their underlying mechanisms. Code is available\nat https://github.com/ziqipang/LM4VisualEncoding.",
        "translated": ""
    },
    {
        "title": "Real-Time Motion Prediction via Heterogeneous Polyline Transformer with\n  Relative Pose Encoding",
        "url": "http://arxiv.org/abs/2310.12970v1",
        "pub_date": "2023-10-19",
        "summary": "The real-world deployment of an autonomous driving system requires its\ncomponents to run on-board and in real-time, including the motion prediction\nmodule that predicts the future trajectories of surrounding traffic\nparticipants. Existing agent-centric methods have demonstrated outstanding\nperformance on public benchmarks. However, they suffer from high computational\noverhead and poor scalability as the number of agents to be predicted\nincreases. To address this problem, we introduce the K-nearest neighbor\nattention with relative pose encoding (KNARPE), a novel attention mechanism\nallowing the pairwise-relative representation to be used by Transformers. Then,\nbased on KNARPE we present the Heterogeneous Polyline Transformer with Relative\npose encoding (HPTR), a hierarchical framework enabling asynchronous token\nupdate during the online inference. By sharing contexts among agents and\nreusing the unchanged contexts, our approach is as efficient as scene-centric\nmethods, while performing on par with state-of-the-art agent-centric methods.\nExperiments on Waymo and Argoverse-2 datasets show that HPTR achieves superior\nperformance among end-to-end methods that do not apply expensive\npost-processing or model ensembling. The code is available at\nhttps://github.com/zhejz/HPTR.",
        "translated": ""
    },
    {
        "title": "CLAIR: Evaluating Image Captions with Large Language Models",
        "url": "http://arxiv.org/abs/2310.12971v1",
        "pub_date": "2023-10-19",
        "summary": "The evaluation of machine-generated image captions poses an interesting yet\npersistent challenge. Effective evaluation measures must consider numerous\ndimensions of similarity, including semantic relevance, visual structure,\nobject interactions, caption diversity, and specificity. Existing\nhighly-engineered measures attempt to capture specific aspects, but fall short\nin providing a holistic score that aligns closely with human judgments. Here,\nwe propose CLAIR, a novel method that leverages the zero-shot language modeling\ncapabilities of large language models (LLMs) to evaluate candidate captions. In\nour evaluations, CLAIR demonstrates a stronger correlation with human judgments\nof caption quality compared to existing measures. Notably, on Flickr8K-Expert,\nCLAIR achieves relative correlation improvements over SPICE of 39.6% and over\nimage-augmented methods such as RefCLIP-S of 18.3%. Moreover, CLAIR provides\nnoisily interpretable results by allowing the language model to identify the\nunderlying reasoning behind its assigned score. Code is available at\nhttps://davidmchan.github.io/clair/",
        "translated": ""
    },
    {
        "title": "Eureka-Moments in Transformers: Multi-Step Tasks Reveal Softmax Induced\n  Optimization Problems",
        "url": "http://arxiv.org/abs/2310.12956v1",
        "pub_date": "2023-10-19",
        "summary": "In this work, we study rapid, step-wise improvements of the loss in\ntransformers when being confronted with multi-step decision tasks. We found\nthat transformers struggle to learn the intermediate tasks, whereas CNNs have\nno such issue on the tasks we studied. When transformers learn the intermediate\ntask, they do this rapidly and unexpectedly after both training and validation\nloss saturated for hundreds of epochs. We call these rapid improvements\nEureka-moments, since the transformer appears to suddenly learn a previously\nincomprehensible task. Similar leaps in performance have become known as\nGrokking. In contrast to Grokking, for Eureka-moments, both the validation and\nthe training loss saturate before rapidly improving. We trace the problem back\nto the Softmax function in the self-attention block of transformers and show\nways to alleviate the problem. These fixes improve training speed. The improved\nmodels reach 95% of the baseline model in just 20% of training steps while\nhaving a much higher likelihood to learn the intermediate task, lead to higher\nfinal accuracy and are more robust to hyper-parameters.",
        "translated": ""
    },
    {
        "title": "3D-GPT: Procedural 3D Modeling with Large Language Models",
        "url": "http://arxiv.org/abs/2310.12945v1",
        "pub_date": "2023-10-19",
        "summary": "In the pursuit of efficient automated content creation, procedural\ngeneration, leveraging modifiable parameters and rule-based systems, emerges as\na promising approach. Nonetheless, it could be a demanding endeavor, given its\nintricate nature necessitating a deep understanding of rules, algorithms, and\nparameters. To reduce workload, we introduce 3D-GPT, a framework utilizing\nlarge language models~(LLMs) for instruction-driven 3D modeling. 3D-GPT\npositions LLMs as proficient problem solvers, dissecting the procedural 3D\nmodeling tasks into accessible segments and appointing the apt agent for each\ntask. 3D-GPT integrates three core agents: the task dispatch agent, the\nconceptualization agent, and the modeling agent. They collaboratively achieve\ntwo objectives. First, it enhances concise initial scene descriptions, evolving\nthem into detailed forms while dynamically adapting the text based on\nsubsequent instructions. Second, it integrates procedural generation,\nextracting parameter values from enriched text to effortlessly interface with\n3D software for asset creation. Our empirical investigations confirm that\n3D-GPT not only interprets and executes instructions, delivering reliable\nresults but also collaborates effectively with human designers. Furthermore, it\nseamlessly integrates with Blender, unlocking expanded manipulation\npossibilities. Our work highlights the potential of LLMs in 3D modeling,\noffering a basic framework for future advancements in scene generation and\nanimation.",
        "translated": ""
    },
    {
        "title": "Using Human-like Mechanism to Weaken Effect of Pre-training Weight Bias\n  in Face-Recognition Convolutional Neural Network",
        "url": "http://arxiv.org/abs/2310.13674v1",
        "pub_date": "2023-10-20",
        "summary": "Convolutional neural network (CNN), as an important model in artificial\nintelligence, has been widely used and studied in different disciplines. The\ncomputational mechanisms of CNNs are still not fully revealed due to the their\ncomplex nature. In this study, we focused on 4 extensively studied CNNs\n(AlexNet, VGG11, VGG13, and VGG16) which has been analyzed as human-like models\nby neuroscientists with ample evidence. We trained these CNNs to emotion\nvalence classification task by transfer learning. Comparing their performance\nwith human data, the data unveiled that these CNNs would partly perform as\nhuman does. We then update the object-based AlexNet using self-attention\nmechanism based on neuroscience and behavioral data. The updated FE-AlexNet\noutperformed all the other tested CNNs and closely resembles human perception.\nThe results further unveil the computational mechanisms of these CNNs.\nMoreover, this study offers a new paradigm to better understand and improve CNN\nperformance via human data.",
        "translated": ""
    },
    {
        "title": "ManifoldNeRF: View-dependent Image Feature Supervision for Few-shot\n  Neural Radiance Fields",
        "url": "http://arxiv.org/abs/2310.13670v1",
        "pub_date": "2023-10-20",
        "summary": "Novel view synthesis has recently made significant progress with the advent\nof Neural Radiance Fields (NeRF). DietNeRF is an extension of NeRF that aims to\nachieve this task from only a few images by introducing a new loss function for\nunknown viewpoints with no input images. The loss function assumes that a\npre-trained feature extractor should output the same feature even if input\nimages are captured at different viewpoints since the images contain the same\nobject. However, while that assumption is ideal, in reality, it is known that\nas viewpoints continuously change, also feature vectors continuously change.\nThus, the assumption can harm training. To avoid this harmful training, we\npropose ManifoldNeRF, a method for supervising feature vectors at unknown\nviewpoints using interpolated features from neighboring known viewpoints. Since\nthe method provides appropriate supervision for each unknown viewpoint by the\ninterpolated features, the volume representation is learned better than\nDietNeRF. Experimental results show that the proposed method performs better\nthan others in a complex scene. We also experimented with several subsets of\nviewpoints from a set of viewpoints and identified an effective set of\nviewpoints for real environments. This provided a basic policy of viewpoint\npatterns for real-world application. The code is available at\nhttps://github.com/haganelego/ManifoldNeRF_BMVC2023",
        "translated": ""
    },
    {
        "title": "Deep-Learning-based Change Detection with Spaceborne Hyperspectral\n  PRISMA data",
        "url": "http://arxiv.org/abs/2310.13627v1",
        "pub_date": "2023-10-20",
        "summary": "Change detection (CD) methods have been applied to optical data for decades,\nwhile the use of hyperspectral data with a fine spectral resolution has been\nrarely explored. CD is applied in several sectors, such as environmental\nmonitoring and disaster management. Thanks to the PRecursore IperSpettrale\ndella Missione operativA (PRISMA), hyperspectral-from-space CD is now possible.\nIn this work, we apply standard and deep-learning (DL) CD methods to different\ntargets, from natural to urban areas. We propose a pipeline starting from\ncoregistration, followed by CD with a full-spectrum algorithm and by a DL\nnetwork developed for optical data. We find that changes in vegetation and\nbuilt environments are well captured. The spectral information is valuable to\nidentify subtle changes and the DL methods are less affected by noise compared\nto the statistical method, but atmospheric effects and the lack of reliable\nground truth represent a major challenge to hyperspectral CD.",
        "translated": ""
    },
    {
        "title": "What you see is what you get: Experience ranking with deep neural\n  dataset-to-dataset similarity for topological localisation",
        "url": "http://arxiv.org/abs/2310.13622v1",
        "pub_date": "2023-10-20",
        "summary": "Recalling the most relevant visual memories for localisation or understanding\na priori the likely outcome of localisation effort against a particular visual\nmemory is useful for efficient and robust visual navigation. Solutions to this\nproblem should be divorced from performance appraisal against ground truth - as\nthis is not available at run-time - and should ideally be based on\ngeneralisable environmental observations. For this, we propose applying the\nrecently developed Visual DNA as a highly scalable tool for comparing datasets\nof images - in this work, sequences of map and live experiences. In the case of\nlocalisation, important dataset differences impacting performance are modes of\nappearance change, including weather, lighting, and season. Specifically, for\nany deep architecture which is used for place recognition by matching feature\nvolumes at a particular layer, we use distribution measures to compare\nneuron-wise activation statistics between live images and multiple previously\nrecorded past experiences, with a potentially large seasonal (winter/summer) or\ntime of day (day/night) shift. We find that differences in these statistics\ncorrelate to performance when localising using a past experience with the same\nappearance gap. We validate our approach over the Nordland cross-season dataset\nas well as data from Oxford's University Parks with lighting and mild seasonal\nchange, showing excellent ability of our system to rank actual localisation\nperformance across candidate experiences.",
        "translated": ""
    },
    {
        "title": "Semi-supervised multimodal coreference resolution in image narrations",
        "url": "http://arxiv.org/abs/2310.13619v1",
        "pub_date": "2023-10-20",
        "summary": "In this paper, we study multimodal coreference resolution, specifically where\na longer descriptive text, i.e., a narration is paired with an image. This\nposes significant challenges due to fine-grained image-text alignment, inherent\nambiguity present in narrative language, and unavailability of large annotated\ntraining sets. To tackle these challenges, we present a data efficient\nsemi-supervised approach that utilizes image-narration pairs to resolve\ncoreferences and narrative grounding in a multimodal context. Our approach\nincorporates losses for both labeled and unlabeled data within a cross-modal\nframework. Our evaluation shows that the proposed approach outperforms strong\nbaselines both quantitatively and qualitatively, for the tasks of coreference\nresolution and narrative grounding.",
        "translated": ""
    },
    {
        "title": "FMRT: Learning Accurate Feature Matching with Reconciliatory Transformer",
        "url": "http://arxiv.org/abs/2310.13605v1",
        "pub_date": "2023-10-20",
        "summary": "Local Feature Matching, an essential component of several computer vision\ntasks (e.g., structure from motion and visual localization), has been\neffectively settled by Transformer-based methods. However, these methods only\nintegrate long-range context information among keypoints with a fixed receptive\nfield, which constrains the network from reconciling the importance of features\nwith different receptive fields to realize complete image perception, hence\nlimiting the matching accuracy. In addition, these methods utilize a\nconventional handcrafted encoding approach to integrate the positional\ninformation of keypoints into the visual descriptors, which limits the\ncapability of the network to extract reliable positional encoding message. In\nthis study, we propose Feature Matching with Reconciliatory Transformer (FMRT),\na novel Transformer-based detector-free method that reconciles different\nfeatures with multiple receptive fields adaptively and utilizes parallel\nnetworks to realize reliable positional encoding. Specifically, FMRT proposes a\ndedicated Reconciliatory Transformer (RecFormer) that consists of a Global\nPerception Attention Layer (GPAL) to extract visual descriptors with different\nreceptive fields and integrate global context information under various scales,\nPerception Weight Layer (PWL) to measure the importance of various receptive\nfields adaptively, and Local Perception Feed-forward Network (LPFFN) to extract\ndeep aggregated multi-scale local feature representation. Extensive experiments\ndemonstrate that FMRT yields extraordinary performance on multiple benchmarks,\nincluding pose estimation, visual localization, homography estimation, and\nimage matching.",
        "translated": ""
    },
    {
        "title": "Skin Lesion Segmentation Improved by Transformer-based Networks with\n  Inter-scale Dependency Modeling",
        "url": "http://arxiv.org/abs/2310.13604v1",
        "pub_date": "2023-10-20",
        "summary": "Melanoma, a dangerous type of skin cancer resulting from abnormal skin cell\ngrowth, can be treated if detected early. Various approaches using Fully\nConvolutional Networks (FCNs) have been proposed, with the U-Net architecture\nbeing prominent To aid in its diagnosis through automatic skin lesion\nsegmentation. However, the symmetrical U-Net model's reliance on convolutional\noperations hinders its ability to capture long-range dependencies crucial for\naccurate medical image segmentation. Several Transformer-based U-Net topologies\nhave recently been created to overcome this limitation by replacing CNN blocks\nwith different Transformer modules to capture local and global representations.\nFurthermore, the U-shaped structure is hampered by semantic gaps between the\nencoder and decoder. This study intends to increase the network's feature\nre-usability by carefully building the skip connection path. Integrating an\nalready calculated attention affinity within the skip connection path improves\nthe typical concatenation process utilized in the conventional skip connection\npath. As a result, we propose a U-shaped hierarchical Transformer-based\nstructure for skin lesion segmentation and an Inter-scale Context Fusion (ISCF)\nmethod that uses attention correlations in each stage of the encoder to\nadaptively combine the contexts from each stage to mitigate semantic gaps. The\nfindings from two skin lesion segmentation benchmarks support the ISCF module's\napplicability and effectiveness. The code is publicly available at\n\\url{https://github.com/saniaesk/skin-lesion-segmentation}",
        "translated": ""
    },
    {
        "title": "Longer-range Contextualized Masked Autoencoder",
        "url": "http://arxiv.org/abs/2310.13593v1",
        "pub_date": "2023-10-20",
        "summary": "Masked image modeling (MIM) has emerged as a promising self-supervised\nlearning (SSL) strategy. The MIM pre-training facilitates learning powerful\nrepresentations using an encoder-decoder framework by randomly masking some\ninput pixels and reconstructing the masked pixels from the remaining ones.\nHowever, as the encoder is trained with partial pixels, the MIM pre-training\ncan suffer from a low capability of understanding long-range dependency. This\nlimitation may hinder its capability to fully understand multiple-range\ndependencies, resulting in narrow highlighted regions in the attention map that\nmay incur accuracy drops. To mitigate the limitation, We propose a\nself-supervised learning framework, named Longer-range Contextualized Masked\nAutoencoder (LC-MAE). LC-MAE effectively leverages a global context\nunderstanding of visual representations while simultaneously reducing the\nspatial redundancy of input at the same time. Our method steers the encoder to\nlearn from entire pixels in multiple views while also learning local\nrepresentation from sparse pixels. As a result, LC-MAE learns more\ndiscriminative representations, leading to a performance improvement of\nachieving 84.2% top-1 accuracy with ViT-B on ImageNet-1K with 0.6%p gain. We\nattribute the success to the enhanced pre-training method, as evidenced by the\nsingular value spectrum and attention analyses. Finally, LC-MAE achieves\nsignificant performance gains at the downstream semantic segmentation and\nfine-grained visual classification tasks; and on diverse robust evaluation\nmetrics. Our code will be publicly available.",
        "translated": ""
    },
    {
        "title": "POTLoc: Pseudo-Label Oriented Transformer for Point-Supervised Temporal\n  Action Localization",
        "url": "http://arxiv.org/abs/2310.13585v1",
        "pub_date": "2023-10-20",
        "summary": "This paper tackles the challenge of point-supervised temporal action\ndetection, wherein only a single frame is annotated for each action instance in\nthe training set. Most of the current methods, hindered by the sparse nature of\nannotated points, struggle to effectively represent the continuous structure of\nactions or the inherent temporal and semantic dependencies within action\ninstances. Consequently, these methods frequently learn merely the most\ndistinctive segments of actions, leading to the creation of incomplete action\nproposals. This paper proposes POTLoc, a Pseudo-label Oriented Transformer for\nweakly-supervised Action Localization utilizing only point-level annotation.\nPOTLoc is designed to identify and track continuous action structures via a\nself-training strategy. The base model begins by generating action proposals\nsolely with point-level supervision. These proposals undergo refinement and\nregression to enhance the precision of the estimated action boundaries, which\nsubsequently results in the production of `pseudo-labels' to serve as\nsupplementary supervisory signals. The architecture of the model integrates a\ntransformer with a temporal feature pyramid to capture video snippet\ndependencies and model actions of varying duration. The pseudo-labels,\nproviding information about the coarse locations and boundaries of actions,\nassist in guiding the transformer for enhanced learning of action dynamics.\nPOTLoc outperforms the state-of-the-art point-supervised methods on THUMOS'14\nand ActivityNet-v1.2 datasets, showing a significant improvement of 5% average\nmAP on the former.",
        "translated": ""
    },
    {
        "title": "Progressive Dual Priori Network for Generalized Breast Tumor\n  Segmentation",
        "url": "http://arxiv.org/abs/2310.13574v1",
        "pub_date": "2023-10-20",
        "summary": "To promote the generalization ability of breast tumor segmentation models, as\nwell as to improve the segmentation performance for breast tumors with smaller\nsize, low-contrast amd irregular shape, we propose a progressive dual priori\nnetwork (PDPNet) to segment breast tumors from dynamic enhanced magnetic\nresonance images (DCE-MRI) acquired at different sites. The PDPNet first\ncropped tumor regions with a coarse-segmentation based localization module,\nthen the breast tumor mask was progressively refined by using the weak semantic\npriori and cross-scale correlation prior knowledge. To validate the\neffectiveness of PDPNet, we compared it with several state-of-the-art methods\non multi-center datasets. The results showed that, comparing against the\nsuboptimal method, the DSC, SEN, KAPPA and HD95 of PDPNet were improved 3.63\\%,\n8.19\\%, 5.52\\%, and 3.66\\% respectively. In addition, through ablations, we\ndemonstrated that the proposed localization module can decrease the influence\nof normal tissues and therefore improve the generalization ability of the\nmodel. The weak semantic priors allow focusing on tumor regions to avoid\nmissing small tumors and low-contrast tumors. The cross-scale correlation\npriors are beneficial for promoting the shape-aware ability for irregual\ntumors. Thus integrating them in a unified framework improved the multi-center\nbreast tumor segmentation performance.",
        "translated": ""
    },
    {
        "title": "RoboDepth: Robust Out-of-Distribution Depth Estimation under Corruptions",
        "url": "http://arxiv.org/abs/2310.15171v1",
        "pub_date": "2023-10-23",
        "summary": "Depth estimation from monocular images is pivotal for real-world visual\nperception systems. While current learning-based depth estimation models train\nand test on meticulously curated data, they often overlook out-of-distribution\n(OoD) situations. Yet, in practical settings -- especially safety-critical ones\nlike autonomous driving -- common corruptions can arise. Addressing this\noversight, we introduce a comprehensive robustness test suite, RoboDepth,\nencompassing 18 corruptions spanning three categories: i) weather and lighting\nconditions; ii) sensor failures and movement; and iii) data processing\nanomalies. We subsequently benchmark 42 depth estimation models across indoor\nand outdoor scenes to assess their resilience to these corruptions. Our\nfindings underscore that, in the absence of a dedicated robustness evaluation\nframework, many leading depth estimation models may be susceptible to typical\ncorruptions. We delve into design considerations for crafting more robust depth\nestimation models, touching upon pre-training, augmentation, modality, model\ncapacity, and learning paradigms. We anticipate our benchmark will establish a\nfoundational platform for advancing robust OoD depth estimation.",
        "translated": ""
    },
    {
        "title": "FreeNoise: Tuning-Free Longer Video Diffusion Via Noise Rescheduling",
        "url": "http://arxiv.org/abs/2310.15169v1",
        "pub_date": "2023-10-23",
        "summary": "With the availability of large-scale video datasets and the advances of\ndiffusion models, text-driven video generation has achieved substantial\nprogress. However, existing video generation models are typically trained on a\nlimited number of frames, resulting in the inability to generate high-fidelity\nlong videos during inference. Furthermore, these models only support\nsingle-text conditions, whereas real-life scenarios often require multi-text\nconditions as the video content changes over time. To tackle these challenges,\nthis study explores the potential of extending the text-driven capability to\ngenerate longer videos conditioned on multiple texts. 1) We first analyze the\nimpact of initial noise in video diffusion models. Then building upon the\nobservation of noise, we propose FreeNoise, a tuning-free and time-efficient\nparadigm to enhance the generative capabilities of pretrained video diffusion\nmodels while preserving content consistency. Specifically, instead of\ninitializing noises for all frames, we reschedule a sequence of noises for\nlong-range correlation and perform temporal attention over them by window-based\nfunction. 2) Additionally, we design a novel motion injection method to support\nthe generation of videos conditioned on multiple text prompts. Extensive\nexperiments validate the superiority of our paradigm in extending the\ngenerative capabilities of video diffusion models. It is noteworthy that\ncompared with the previous best-performing method which brought about 255%\nextra time cost, our method incurs only negligible time cost of approximately\n17%. Generated video samples are available at our website:\nhttp://haonanqiu.com/projects/FreeNoise.html.",
        "translated": ""
    },
    {
        "title": "Ghost on the Shell: An Expressive Representation of General 3D Shapes",
        "url": "http://arxiv.org/abs/2310.15168v2",
        "pub_date": "2023-10-23",
        "summary": "The creation of photorealistic virtual worlds requires the accurate modeling\nof 3D surface geometry for a wide range of objects. For this, meshes are\nappealing since they 1) enable fast physics-based rendering with realistic\nmaterial and lighting, 2) support physical simulation, and 3) are\nmemory-efficient for modern graphics pipelines. Recent work on reconstructing\nand statistically modeling 3D shape, however, has critiqued meshes as being\ntopologically inflexible. To capture a wide range of object shapes, any 3D\nrepresentation must be able to model solid, watertight, shapes as well as thin,\nopen, surfaces. Recent work has focused on the former, and methods for\nreconstructing open surfaces do not support fast reconstruction with material\nand lighting or unconditional generative modelling. Inspired by the observation\nthat open surfaces can be seen as islands floating on watertight surfaces, we\nparameterize open surfaces by defining a manifold signed distance field on\nwatertight templates. With this parameterization, we further develop a\ngrid-based and differentiable representation that parameterizes both watertight\nand non-watertight meshes of arbitrary topology. Our new representation, called\nGhost-on-the-Shell (G-Shell), enables two important applications:\ndifferentiable rasterization-based reconstruction from multiview images and\ngenerative modelling of non-watertight meshes. We empirically demonstrate that\nG-Shell achieves state-of-the-art performance on non-watertight mesh\nreconstruction and generation tasks, while also performing effectively for\nwatertight meshes.",
        "translated": ""
    },
    {
        "title": "Large Language Models are Visual Reasoning Coordinators",
        "url": "http://arxiv.org/abs/2310.15166v1",
        "pub_date": "2023-10-23",
        "summary": "Visual reasoning requires multimodal perception and commonsense cognition of\nthe world. Recently, multiple vision-language models (VLMs) have been proposed\nwith excellent commonsense reasoning ability in various domains. However, how\nto harness the collective power of these complementary VLMs is rarely explored.\nExisting methods like ensemble still struggle to aggregate these models with\nthe desired higher-order communications. In this work, we propose Cola, a novel\nparadigm that coordinates multiple VLMs for visual reasoning. Our key insight\nis that a large language model (LLM) can efficiently coordinate multiple VLMs\nby facilitating natural language communication that leverages their distinct\nand complementary capabilities. Extensive experiments demonstrate that our\ninstruction tuning variant, Cola-FT, achieves state-of-the-art performance on\nvisual question answering (VQA), outside knowledge VQA, visual entailment, and\nvisual spatial reasoning tasks. Moreover, we show that our in-context learning\nvariant, Cola-Zero, exhibits competitive performance in zero and few-shot\nsettings, without finetuning. Through systematic ablation studies and\nvisualizations, we validate that a coordinator LLM indeed comprehends the\ninstruction prompts as well as the separate functionalities of VLMs; it then\ncoordinates them to enable impressive visual reasoning capabilities.",
        "translated": ""
    },
    {
        "title": "Handling Data Heterogeneity via Architectural Design for Federated\n  Visual Recognition",
        "url": "http://arxiv.org/abs/2310.15165v1",
        "pub_date": "2023-10-23",
        "summary": "Federated Learning (FL) is a promising research paradigm that enables the\ncollaborative training of machine learning models among various parties without\nthe need for sensitive information exchange. Nonetheless, retaining data in\nindividual clients introduces fundamental challenges to achieving performance\non par with centrally trained models. Our study provides an extensive review of\nfederated learning applied to visual recognition. It underscores the critical\nrole of thoughtful architectural design choices in achieving optimal\nperformance, a factor often neglected in the FL literature. Many existing FL\nsolutions are tested on shallow or simple networks, which may not accurately\nreflect real-world applications. This practice restricts the transferability of\nresearch findings to large-scale visual recognition models. Through an in-depth\nanalysis of diverse cutting-edge architectures such as convolutional neural\nnetworks, transformers, and MLP-mixers, we experimentally demonstrate that\narchitectural choices can substantially enhance FL systems' performance,\nparticularly when handling heterogeneous data. We study 19 visual recognition\nmodels from five different architectural families on four challenging FL\ndatasets. We also re-investigate the inferior performance of convolution-based\narchitectures in the FL setting and analyze the influence of normalization\nlayers on the FL performance. Our findings emphasize the importance of\narchitectural design for computer vision tasks in practical scenarios,\neffectively narrowing the performance gap between federated and centralized\nlearning. Our source code is available at\nhttps://github.com/sarapieri/fed_het.git.",
        "translated": ""
    },
    {
        "title": "SAM-Med3D",
        "url": "http://arxiv.org/abs/2310.15161v1",
        "pub_date": "2023-10-23",
        "summary": "Although the Segment Anything Model (SAM) has demonstrated impressive\nperformance in 2D natural image segmentation, its application to 3D volumetric\nmedical images reveals significant shortcomings, namely suboptimal performance\nand unstable prediction, necessitating an excessive number of prompt points to\nattain the desired outcomes. These issues can hardly be addressed by\nfine-tuning SAM on medical data because the original 2D structure of SAM\nneglects 3D spatial information. In this paper, we introduce SAM-Med3D, the\nmost comprehensive study to modify SAM for 3D medical images. Our approach is\ncharacterized by its comprehensiveness in two primary aspects: firstly, by\ncomprehensively reformulating SAM to a thorough 3D architecture trained on a\ncomprehensively processed large-scale volumetric medical dataset; and secondly,\nby providing a comprehensive evaluation of its performance. Specifically, we\ntrain SAM-Med3D with over 131K 3D masks and 247 categories. Our SAM-Med3D\nexcels at capturing 3D spatial information, exhibiting competitive performance\nwith significantly fewer prompt points than the top-performing fine-tuned SAM\nin the medical domain. We then evaluate its capabilities across 15 datasets and\nanalyze it from multiple perspectives, including anatomical structures,\nmodalities, targets, and generalization abilities. Our approach, compared with\nSAM, showcases pronouncedly enhanced efficiency and broad segmentation\ncapabilities for 3D volumetric medical images. Our code is released at\nhttps://github.com/uni-medical/SAM-Med3D.",
        "translated": ""
    },
    {
        "title": "FreeMask: Synthetic Images with Dense Annotations Make Stronger\n  Segmentation Models",
        "url": "http://arxiv.org/abs/2310.15160v1",
        "pub_date": "2023-10-23",
        "summary": "Semantic segmentation has witnessed tremendous progress due to the proposal\nof various advanced network architectures. However, they are extremely hungry\nfor delicate annotations to train, and the acquisition is laborious and\nunaffordable. Therefore, we present FreeMask in this work, which resorts to\nsynthetic images from generative models to ease the burden of both data\ncollection and annotation procedures. Concretely, we first synthesize abundant\ntraining images conditioned on the semantic masks provided by realistic\ndatasets. This yields extra well-aligned image-mask training pairs for semantic\nsegmentation models. We surprisingly observe that, solely trained with\nsynthetic images, we already achieve comparable performance with real ones\n(e.g., 48.3 vs. 48.5 mIoU on ADE20K, and 49.3 vs. 50.5 on COCO-Stuff). Then, we\ninvestigate the role of synthetic images by joint training with real images, or\npre-training for real images. Meantime, we design a robust filtering principle\nto suppress incorrectly synthesized regions. In addition, we propose to\ninequally treat different semantic masks to prioritize those harder ones and\nsample more corresponding synthetic images for them. As a result, either\njointly trained or pre-trained with our filtered and re-sampled synthesized\nimages, segmentation models can be greatly enhanced, e.g., from 48.7 to 52.0 on\nADE20K. Code is available at https://github.com/LiheYoung/FreeMask.",
        "translated": ""
    },
    {
        "title": "Online Detection of AI-Generated Images",
        "url": "http://arxiv.org/abs/2310.15150v1",
        "pub_date": "2023-10-23",
        "summary": "With advancements in AI-generated images coming on a continuous basis, it is\nincreasingly difficult to distinguish traditionally-sourced images (e.g.,\nphotos, artwork) from AI-generated ones. Previous detection methods study the\ngeneralization from a single generator to another in isolation. However, in\nreality, new generators are released on a streaming basis. We study\ngeneralization in this setting, training on N models and testing on the next\n(N+k), following the historical release dates of well-known generation methods.\nFurthermore, images increasingly consist of both real and generated components,\nfor example through image inpainting. Thus, we extend this approach to pixel\nprediction, demonstrating strong performance using automatically-generated\ninpainted data. In addition, for settings where commercial models are not\npublicly available for automatic data generation, we evaluate if pixel\ndetectors can be trained solely on whole synthetic images.",
        "translated": ""
    },
    {
        "title": "DEsignBench: Exploring and Benchmarking DALL-E 3 for Imagining Visual\n  Design",
        "url": "http://arxiv.org/abs/2310.15144v1",
        "pub_date": "2023-10-23",
        "summary": "We introduce DEsignBench, a text-to-image (T2I) generation benchmark tailored\nfor visual design scenarios. Recent T2I models like DALL-E 3 and others, have\ndemonstrated remarkable capabilities in generating photorealistic images that\nalign closely with textual inputs. While the allure of creating visually\ncaptivating images is undeniable, our emphasis extends beyond mere aesthetic\npleasure. We aim to investigate the potential of using these powerful models in\nauthentic design contexts. In pursuit of this goal, we develop DEsignBench,\nwhich incorporates test samples designed to assess T2I models on both \"design\ntechnical capability\" and \"design application scenario.\" Each of these two\ndimensions is supported by a diverse set of specific design categories. We\nexplore DALL-E 3 together with other leading T2I models on DEsignBench,\nresulting in a comprehensive visual gallery for side-by-side comparisons. For\nDEsignBench benchmarking, we perform human evaluations on generated images in\nDEsignBench gallery, against the criteria of image-text alignment, visual\naesthetic, and design creativity. Our evaluation also considers other\nspecialized design capabilities, including text rendering, layout composition,\ncolor harmony, 3D design, and medium style. In addition to human evaluations,\nwe introduce the first automatic image generation evaluator powered by GPT-4V.\nThis evaluator provides ratings that align well with human judgments, while\nbeing easily replicable and cost-efficient. A high-resolution version is\navailable at\nhttps://github.com/design-bench/design-bench.github.io/raw/main/designbench.pdf?download=",
        "translated": ""
    },
    {
        "title": "Fusion-Driven Tree Reconstruction and Fruit Localization: Advancing\n  Precision in Agriculture",
        "url": "http://arxiv.org/abs/2310.15138v1",
        "pub_date": "2023-10-23",
        "summary": "Fruit distribution is pivotal in shaping the future of both agriculture and\nagricultural robotics, paving the way for a streamlined supply chain. This\nstudy introduces an innovative methodology that harnesses the synergy of RGB\nimagery, LiDAR, and IMU data, to achieve intricate tree reconstructions and the\npinpoint localization of fruits. Such integration not only offers insights into\nthe fruit distribution, which enhances the precision of guidance for\nagricultural robotics and automation systems, but also sets the stage for\nsimulating synthetic fruit patterns across varied tree architectures. To\nvalidate this approach, experiments have been carried out in both a controlled\nenvironment and an actual peach orchard. The results underscore the robustness\nand efficacy of this fusion-driven methodology, highlighting its potential as a\ntransformative tool for future agricultural robotics and precision farming.",
        "translated": ""
    },
    {
        "title": "Synthetic Data as Validation",
        "url": "http://arxiv.org/abs/2310.16052v1",
        "pub_date": "2023-10-24",
        "summary": "This study leverages synthetic data as a validation set to reduce overfitting\nand ease the selection of the best model in AI development. While synthetic\ndata have been used for augmenting the training set, we find that synthetic\ndata can also significantly diversify the validation set, offering marked\nadvantages in domains like healthcare, where data are typically limited,\nsensitive, and from out-domain sources (i.e., hospitals). In this study, we\nillustrate the effectiveness of synthetic data for early cancer detection in\ncomputed tomography (CT) volumes, where synthetic tumors are generated and\nsuperimposed onto healthy organs, thereby creating an extensive dataset for\nrigorous validation. Using synthetic data as validation can improve AI\nrobustness in both in-domain and out-domain test sets. Furthermore, we\nestablish a new continual learning framework that continuously trains AI models\non a stream of out-domain data with synthetic tumors. The AI model trained and\nvalidated in dynamically expanding synthetic data can consistently outperform\nmodels trained and validated exclusively on real-world data. Specifically, the\nDSC score for liver tumor segmentation improves from 26.7% (95% CI:\n22.6%-30.9%) to 34.5% (30.8%-38.2%) when evaluated on an in-domain dataset and\nfrom 31.1% (26.0%-36.2%) to 35.4% (32.1%-38.7%) on an out-domain dataset.\nImportantly, the performance gain is particularly significant in identifying\nvery tiny liver tumors (radius &lt; 5mm) in CT volumes, with Sensitivity improving\nfrom 33.1% to 55.4% on an in-domain dataset and 33.9% to 52.3% on an out-domain\ndataset, justifying the efficacy in early detection of cancer. The application\nof synthetic data, from both training and validation perspectives, underlines a\npromising avenue to enhance AI robustness when dealing with data from varying\ndomains.",
        "translated": ""
    },
    {
        "title": "From Posterior Sampling to Meaningful Diversity in Image Restoration",
        "url": "http://arxiv.org/abs/2310.16047v1",
        "pub_date": "2023-10-24",
        "summary": "Image restoration problems are typically ill-posed in the sense that each\ndegraded image can be restored in infinitely many valid ways. To accommodate\nthis, many works generate a diverse set of outputs by attempting to randomly\nsample from the posterior distribution of natural images given the degraded\ninput. Here we argue that this strategy is commonly of limited practical value\nbecause of the heavy tail of the posterior distribution. Consider for example\ninpainting a missing region of the sky in an image. Since there is a high\nprobability that the missing region contains no object but clouds, any set of\nsamples from the posterior would be entirely dominated by (practically\nidentical) completions of sky. However, arguably, presenting users with only\none clear sky completion, along with several alternative solutions such as\nairships, birds, and balloons, would better outline the set of possibilities.\nIn this paper, we initiate the study of meaningfully diverse image restoration.\nWe explore several post-processing approaches that can be combined with any\ndiverse image restoration method to yield semantically meaningful diversity.\nMoreover, we propose a practical approach for allowing diffusion based image\nrestoration methods to generate meaningfully diverse outputs, while incurring\nonly negligent computational overhead. We conduct extensive user studies to\nanalyze the proposed techniques, and find the strategy of reducing similarity\nbetween outputs to be significantly favorable over posterior sampling. Code and\nexamples are available in https://noa-cohen.github.io/MeaningfulDiversityInIR",
        "translated": ""
    },
    {
        "title": "Woodpecker: Hallucination Correction for Multimodal Large Language\n  Models",
        "url": "http://arxiv.org/abs/2310.16045v1",
        "pub_date": "2023-10-24",
        "summary": "Hallucination is a big shadow hanging over the rapidly evolving Multimodal\nLarge Language Models (MLLMs), referring to the phenomenon that the generated\ntext is inconsistent with the image content. In order to mitigate\nhallucinations, existing studies mainly resort to an instruction-tuning manner\nthat requires retraining the models with specific data. In this paper, we pave\na different way, introducing a training-free method named Woodpecker. Like a\nwoodpecker heals trees, it picks out and corrects hallucinations from the\ngenerated text. Concretely, Woodpecker consists of five stages: key concept\nextraction, question formulation, visual knowledge validation, visual claim\ngeneration, and hallucination correction. Implemented in a post-remedy manner,\nWoodpecker can easily serve different MLLMs, while being interpretable by\naccessing intermediate outputs of the five stages. We evaluate Woodpecker both\nquantitatively and qualitatively and show the huge potential of this new\nparadigm. On the POPE benchmark, our method obtains a 30.66%/24.33% improvement\nin accuracy over the baseline MiniGPT-4/mPLUG-Owl. The source code is released\nat https://github.com/BradyFU/Woodpecker.",
        "translated": ""
    },
    {
        "title": "Stanford-ORB: A Real-World 3D Object Inverse Rendering Benchmark",
        "url": "http://arxiv.org/abs/2310.16044v1",
        "pub_date": "2023-10-24",
        "summary": "We introduce Stanford-ORB, a new real-world 3D Object inverse Rendering\nBenchmark. Recent advances in inverse rendering have enabled a wide range of\nreal-world applications in 3D content generation, moving rapidly from research\nand commercial use cases to consumer devices. While the results continue to\nimprove, there is no real-world benchmark that can quantitatively assess and\ncompare the performance of various inverse rendering methods. Existing\nreal-world datasets typically only consist of the shape and multi-view images\nof objects, which are not sufficient for evaluating the quality of material\nrecovery and object relighting. Methods capable of recovering material and\nlighting often resort to synthetic data for quantitative evaluation, which on\nthe other hand does not guarantee generalization to complex real-world\nenvironments. We introduce a new dataset of real-world objects captured under a\nvariety of natural scenes with ground-truth 3D scans, multi-view images, and\nenvironment lighting. Using this dataset, we establish the first comprehensive\nreal-world evaluation benchmark for object inverse rendering tasks from\nin-the-wild scenes, and compare the performance of various existing methods.\nAll data, code, and models can be accessed at https://stanfordorb.github.io/.",
        "translated": ""
    },
    {
        "title": "What's Left? Concept Grounding with Logic-Enhanced Foundation Models",
        "url": "http://arxiv.org/abs/2310.16035v1",
        "pub_date": "2023-10-24",
        "summary": "Recent works such as VisProg and ViperGPT have smartly composed foundation\nmodels for visual reasoning-using large language models (LLMs) to produce\nprograms that can be executed by pre-trained vision-language models. However,\nthey operate in limited domains, such as 2D images, not fully exploiting the\ngeneralization of language: abstract concepts like \"left\" can also be grounded\nin 3D, temporal, and action data, as in moving to your left. This limited\ngeneralization stems from these inference-only methods' inability to learn or\nadapt pre-trained models to a new domain. We propose the Logic-Enhanced\nFoundation Model (LEFT), a unified framework that learns to ground and reason\nwith concepts across domains with a differentiable, domain-independent,\nfirst-order logic-based program executor. LEFT has an LLM interpreter that\noutputs a program represented in a general, logic-based reasoning language,\nwhich is shared across all domains and tasks. LEFT's executor then executes the\nprogram with trainable domain-specific grounding modules. We show that LEFT\nflexibly learns concepts in four domains: 2D images, 3D scenes, human motions,\nand robotic manipulation. It exhibits strong reasoning ability in a wide\nvariety of tasks, including those that are complex and not seen during\ntraining, and can be easily applied to new domains.",
        "translated": ""
    },
    {
        "title": "Visual Cropping Improves Zero-Shot Question Answering of Multimodal\n  Large Language Models",
        "url": "http://arxiv.org/abs/2310.16033v1",
        "pub_date": "2023-10-24",
        "summary": "Multimodal Large Language Models (LLMs) have recently achieved promising\nzero-shot accuracy on visual question answering (VQA) -- a fundamental task\naffecting various downstream applications and domains. Given the great\npotential for the broad use of these models, it is important to investigate\ntheir limitations in dealing with different image and question properties. In\nthis work, we investigate whether multimodal LLMs can perceive small details as\nwell as large details in images. In particular, we show that their zero-shot\naccuracy in answering visual questions is very sensitive to the size of the\nvisual subject of the question, declining up to $46\\%$ with size. Furthermore,\nwe show that this effect is causal by observing that human visual cropping can\nsignificantly mitigate their sensitivity to size. Inspired by the usefulness of\nhuman cropping, we then propose three automatic visual cropping methods as\ninference time mechanisms to improve the zero-shot performance of multimodal\nLLMs. We study their effectiveness on four popular VQA datasets, and a subset\nof the VQAv2 dataset tailored towards fine visual details. Our findings suggest\nthat multimodal LLMs should be used with caution in detail-sensitive VQA\napplications, and that visual cropping is a promising direction to improve\ntheir zero-shot performance. Our code and data are publicly available.",
        "translated": ""
    },
    {
        "title": "Finetuning Offline World Models in the Real World",
        "url": "http://arxiv.org/abs/2310.16029v1",
        "pub_date": "2023-10-24",
        "summary": "Reinforcement Learning (RL) is notoriously data-inefficient, which makes\ntraining on a real robot difficult. While model-based RL algorithms (world\nmodels) improve data-efficiency to some extent, they still require hours or\ndays of interaction to learn skills. Recently, offline RL has been proposed as\na framework for training RL policies on pre-existing datasets without any\nonline interaction. However, constraining an algorithm to a fixed dataset\ninduces a state-action distribution shift between training and inference, and\nlimits its applicability to new tasks. In this work, we seek to get the best of\nboth worlds: we consider the problem of pretraining a world model with offline\ndata collected on a real robot, and then finetuning the model on online data\ncollected by planning with the learned model. To mitigate extrapolation errors\nduring online interaction, we propose to regularize the planner at test-time by\nbalancing estimated returns and (epistemic) model uncertainty. We evaluate our\nmethod on a variety of visuo-motor control tasks in simulation and on a real\nrobot, and find that our method enables few-shot finetuning to seen and unseen\ntasks even when offline data is limited. Videos, code, and data are available\nat https://yunhaifeng.com/FOWM .",
        "translated": ""
    },
    {
        "title": "ConvBKI: Real-Time Probabilistic Semantic Mapping Network with\n  Quantifiable Uncertainty",
        "url": "http://arxiv.org/abs/2310.16020v1",
        "pub_date": "2023-10-24",
        "summary": "In this paper, we develop a modular neural network for real-time semantic\nmapping in uncertain environments, which explicitly updates per-voxel\nprobabilistic distributions within a neural network layer. Our approach\ncombines the reliability of classical probabilistic algorithms with the\nperformance and efficiency of modern neural networks. Although robotic\nperception is often divided between modern differentiable methods and classical\nexplicit methods, a union of both is necessary for real-time and trustworthy\nperformance. We introduce a novel Convolutional Bayesian Kernel Inference\n(ConvBKI) layer which incorporates semantic segmentation predictions online\ninto a 3D map through a depthwise convolution layer by leveraging conjugate\npriors. We compare ConvBKI against state-of-the-art deep learning approaches\nand probabilistic algorithms for mapping to evaluate reliability and\nperformance. We also create a Robot Operating System (ROS) package of ConvBKI\nand test it on real-world perceptually challenging off-road driving data.",
        "translated": ""
    },
    {
        "title": "Human-in-the-Loop Task and Motion Planning for Imitation Learning",
        "url": "http://arxiv.org/abs/2310.16014v1",
        "pub_date": "2023-10-24",
        "summary": "Imitation learning from human demonstrations can teach robots complex\nmanipulation skills, but is time-consuming and labor intensive. In contrast,\nTask and Motion Planning (TAMP) systems are automated and excel at solving\nlong-horizon tasks, but they are difficult to apply to contact-rich tasks. In\nthis paper, we present Human-in-the-Loop Task and Motion Planning (HITL-TAMP),\na novel system that leverages the benefits of both approaches. The system\nemploys a TAMP-gated control mechanism, which selectively gives and takes\ncontrol to and from a human teleoperator. This enables the human teleoperator\nto manage a fleet of robots, maximizing data collection efficiency. The\ncollected human data is then combined with an imitation learning framework to\ntrain a TAMP-gated policy, leading to superior performance compared to training\non full task demonstrations. We compared HITL-TAMP to a conventional\nteleoperation system -- users gathered more than 3x the number of demos given\nthe same time budget. Furthermore, proficient agents (75\\%+ success) could be\ntrained from just 10 minutes of non-expert teleoperation data. Finally, we\ncollected 2.1K demos with HITL-TAMP across 12 contact-rich, long-horizon tasks\nand show that the system often produces near-perfect agents. Videos and\nadditional results at https://hitltamp.github.io .",
        "translated": ""
    },
    {
        "title": "CVPR 2023 Text Guided Video Editing Competition",
        "url": "http://arxiv.org/abs/2310.16003v1",
        "pub_date": "2023-10-24",
        "summary": "Humans watch more than a billion hours of video per day. Most of this video\nwas edited manually, which is a tedious process. However, AI-enabled\nvideo-generation and video-editing is on the rise. Building on text-to-image\nmodels like Stable Diffusion and Imagen, generative AI has improved\ndramatically on video tasks. But it's hard to evaluate progress in these video\ntasks because there is no standard benchmark. So, we propose a new dataset for\ntext-guided video editing (TGVE), and we run a competition at CVPR to evaluate\nmodels on our TGVE dataset. In this paper we present a retrospective on the\ncompetition and describe the winning method. The competition dataset is\navailable at https://sites.google.com/view/loveucvpr23/track4.",
        "translated": ""
    },
    {
        "title": "SparseDFF: Sparse-View Feature Distillation for One-Shot Dexterous\n  Manipulation",
        "url": "http://arxiv.org/abs/2310.16838v1",
        "pub_date": "2023-10-25",
        "summary": "Humans excel at transferring manipulation skills across diverse object\nshapes, poses, and appearances due to their understanding of semantic\ncorrespondences between different instances. To endow robots with a similar\nhigh-level understanding, we develop a Distilled Feature Field (DFF) for 3D\nscenes, leveraging large 2D vision models to distill semantic features from\nmultiview images. While current research demonstrates advanced performance in\nreconstructing DFFs from dense views, the development of learning a DFF from\nsparse views is relatively nascent, despite its prevalence in numerous\nmanipulation tasks with fixed cameras. In this work, we introduce SparseDFF, a\nnovel method for acquiring view-consistent 3D DFFs from sparse RGBD\nobservations, enabling one-shot learning of dexterous manipulations that are\ntransferable to novel scenes. Specifically, we map the image features to the 3D\npoint cloud, allowing for propagation across the 3D space to establish a dense\nfeature field. At the core of SparseDFF is a lightweight feature refinement\nnetwork, optimized with a contrastive loss between pairwise views after\nback-projecting the image features onto the 3D point cloud. Additionally, we\nimplement a point-pruning mechanism to augment feature continuity within each\nlocal neighborhood. By establishing coherent feature fields on both source and\ntarget scenes, we devise an energy function that facilitates the minimization\nof feature discrepancies w.r.t. the end-effector parameters between the\ndemonstration and the target manipulation. We evaluate our approach using a\ndexterous hand, mastering real-world manipulations on both rigid and deformable\nobjects, and showcase robust generalization in the face of object and\nscene-context variations.",
        "translated": ""
    },
    {
        "title": "LLM-FP4: 4-Bit Floating-Point Quantized Transformers",
        "url": "http://arxiv.org/abs/2310.16836v1",
        "pub_date": "2023-10-25",
        "summary": "We propose LLM-FP4 for quantizing both weights and activations in large\nlanguage models (LLMs) down to 4-bit floating-point values, in a post-training\nmanner. Existing post-training quantization (PTQ) solutions are primarily\ninteger-based and struggle with bit widths below 8 bits. Compared to integer\nquantization, floating-point (FP) quantization is more flexible and can better\nhandle long-tail or bell-shaped distributions, and it has emerged as a default\nchoice in many hardware platforms. One characteristic of FP quantization is\nthat its performance largely depends on the choice of exponent bits and\nclipping range. In this regard, we construct a strong FP-PTQ baseline by\nsearching for the optimal quantization parameters. Furthermore, we observe a\nhigh inter-channel variance and low intra-channel variance pattern in\nactivation distributions, which adds activation quantization difficulty. We\nrecognize this pattern to be consistent across a spectrum of transformer models\ndesigned for diverse tasks, such as LLMs, BERT, and Vision Transformer models.\nTo tackle this, we propose per-channel activation quantization and show that\nthese additional scaling factors can be reparameterized as exponential biases\nof weights, incurring a negligible cost. Our method, for the first time, can\nquantize both weights and activations in the LLaMA-13B to only 4-bit and\nachieves an average score of 63.1 on the common sense zero-shot reasoning\ntasks, which is only 5.8 lower than the full-precision model, significantly\noutperforming the previous state-of-the-art by 12.7 points. Code is available\nat: https://github.com/nbasyl/LLM-FP4.",
        "translated": ""
    },
    {
        "title": "Proposal-Contrastive Pretraining for Object Detection from Fewer Data",
        "url": "http://arxiv.org/abs/2310.16835v1",
        "pub_date": "2023-10-25",
        "summary": "The use of pretrained deep neural networks represents an attractive way to\nachieve strong results with few data available. When specialized in dense\nproblems such as object detection, learning local rather than global\ninformation in images has proven to be more efficient. However, for\nunsupervised pretraining, the popular contrastive learning requires a large\nbatch size and, therefore, a lot of resources. To address this problem, we are\ninterested in transformer-based object detectors that have recently gained\ntraction in the community with good performance and with the particularity of\ngenerating many diverse object proposals.\n  In this work, we present Proposal Selection Contrast (ProSeCo), a novel\nunsupervised overall pretraining approach that leverages this property. ProSeCo\nuses the large number of object proposals generated by the detector for\ncontrastive learning, which allows the use of a smaller batch size, combined\nwith object-level features to learn local information in the images. To improve\nthe effectiveness of the contrastive loss, we introduce the object location\ninformation in the selection of positive examples to take into account multiple\noverlapping object proposals. When reusing pretrained backbone, we advocate for\nconsistency in learning local information between the backbone and the\ndetection head.\n  We show that our method outperforms state of the art in unsupervised\npretraining for object detection on standard and novel benchmarks in learning\nwith fewer data.",
        "translated": ""
    },
    {
        "title": "LightSpeed: Light and Fast Neural Light Fields on Mobile Devices",
        "url": "http://arxiv.org/abs/2310.16832v1",
        "pub_date": "2023-10-25",
        "summary": "Real-time novel-view image synthesis on mobile devices is prohibitive due to\nthe limited computational power and storage. Using volumetric rendering\nmethods, such as NeRF and its derivatives, on mobile devices is not suitable\ndue to the high computational cost of volumetric rendering. On the other hand,\nrecent advances in neural light field representations have shown promising\nreal-time view synthesis results on mobile devices. Neural light field methods\nlearn a direct mapping from a ray representation to the pixel color. The\ncurrent choice of ray representation is either stratified ray sampling or\nPl\\\"{u}cker coordinates, overlooking the classic light slab (two-plane)\nrepresentation, the preferred representation to interpolate between light field\nviews. In this work, we find that using the light slab representation is an\nefficient representation for learning a neural light field. More importantly,\nit is a lower-dimensional ray representation enabling us to learn the 4D ray\nspace using feature grids which are significantly faster to train and render.\nAlthough mostly designed for frontal views, we show that the light-slab\nrepresentation can be further extended to non-frontal scenes using a\ndivide-and-conquer strategy. Our method offers superior rendering quality\ncompared to previous light field methods and achieves a significantly improved\ntrade-off between rendering quality and speed.",
        "translated": ""
    },
    {
        "title": "PERF: Panoramic Neural Radiance Field from a Single Panorama",
        "url": "http://arxiv.org/abs/2310.16831v1",
        "pub_date": "2023-10-25",
        "summary": "Neural Radiance Field (NeRF) has achieved substantial progress in novel view\nsynthesis given multi-view images. Recently, some works have attempted to train\na NeRF from a single image with 3D priors. They mainly focus on a limited field\nof view and there are few invisible occlusions, which greatly limits their\nscalability to real-world 360-degree panoramic scenarios with large-size\nocclusions. In this paper, we present PERF, a 360-degree novel view synthesis\nframework that trains a panoramic neural radiance field from a single panorama.\nNotably, PERF allows 3D roaming in a complex scene without expensive and\ntedious image collection. To achieve this goal, we propose a novel\ncollaborative RGBD inpainting method and a progressive inpainting-and-erasing\nmethod to lift up a 360-degree 2D scene to a 3D scene. Specifically, we first\npredict a panoramic depth map as initialization given a single panorama, and\nreconstruct visible 3D regions with volume rendering. Then we introduce a\ncollaborative RGBD inpainting approach into a NeRF for completing RGB images\nand depth maps from random views, which is derived from an RGB Stable Diffusion\nmodel and a monocular depth estimator. Finally, we introduce an\ninpainting-and-erasing strategy to avoid inconsistent geometry between a\nnewly-sampled view and reference views. The two components are integrated into\nthe learning of NeRFs in a unified optimization framework and achieve promising\nresults. Extensive experiments on Replica and a new dataset PERF-in-the-wild\ndemonstrate the superiority of our PERF over state-of-the-art methods. Our PERF\ncan be widely used for real-world applications, such as panorama-to-3D,\ntext-to-3D, and 3D scene stylization applications. Project page and code are\navailable at https://perf-project.github.io/.",
        "translated": ""
    },
    {
        "title": "TD-MPC2: Scalable, Robust World Models for Continuous Control",
        "url": "http://arxiv.org/abs/2310.16828v1",
        "pub_date": "2023-10-25",
        "summary": "TD-MPC is a model-based reinforcement learning (RL) algorithm that performs\nlocal trajectory optimization in the latent space of a learned implicit\n(decoder-free) world model. In this work, we present TD-MPC2: a series of\nimprovements upon the TD-MPC algorithm. We demonstrate that TD-MPC2 improves\nsignificantly over baselines across 104 online RL tasks spanning 4 diverse task\ndomains, achieving consistently strong results with a single set of\nhyperparameters. We further show that agent capabilities increase with model\nand data size, and successfully train a single 317M parameter agent to perform\n80 tasks across multiple task domains, embodiments, and action spaces. We\nconclude with an account of lessons, opportunities, and risks associated with\nlarge TD-MPC2 agents. Explore videos, models, data, code, and more at\nhttps://nicklashansen.github.io/td-mpc2",
        "translated": ""
    },
    {
        "title": "CommonCanvas: An Open Diffusion Model Trained with Creative-Commons\n  Images",
        "url": "http://arxiv.org/abs/2310.16825v1",
        "pub_date": "2023-10-25",
        "summary": "We assemble a dataset of Creative-Commons-licensed (CC) images, which we use\nto train a set of open diffusion models that are qualitatively competitive with\nStable Diffusion 2 (SD2). This task presents two challenges: (1)\nhigh-resolution CC images lack the captions necessary to train text-to-image\ngenerative models; (2) CC images are relatively scarce. In turn, to address\nthese challenges, we use an intuitive transfer learning technique to produce a\nset of high-quality synthetic captions paired with curated CC images. We then\ndevelop a data- and compute-efficient training recipe that requires as little\nas 3% of the LAION-2B data needed to train existing SD2 models, but obtains\ncomparable quality. These results indicate that we have a sufficient number of\nCC images (~70 million) for training high-quality models. Our training recipe\nalso implements a variety of optimizations that achieve ~3X training speed-ups,\nenabling rapid model iteration. We leverage this recipe to train several\nhigh-quality text-to-image models, which we dub the CommonCanvas family. Our\nlargest model achieves comparable performance to SD2 on a human evaluation,\ndespite being trained on our CC dataset that is significantly smaller than\nLAION and using synthetic captions for training. We release our models, data,\nand code at\nhttps://github.com/mosaicml/diffusion/blob/main/assets/common-canvas.md",
        "translated": ""
    },
    {
        "title": "DreamCraft3D: Hierarchical 3D Generation with Bootstrapped Diffusion\n  Prior",
        "url": "http://arxiv.org/abs/2310.16818v2",
        "pub_date": "2023-10-25",
        "summary": "We present DreamCraft3D, a hierarchical 3D content generation method that\nproduces high-fidelity and coherent 3D objects. We tackle the problem by\nleveraging a 2D reference image to guide the stages of geometry sculpting and\ntexture boosting. A central focus of this work is to address the consistency\nissue that existing works encounter. To sculpt geometries that render\ncoherently, we perform score distillation sampling via a view-dependent\ndiffusion model. This 3D prior, alongside several training strategies,\nprioritizes the geometry consistency but compromises the texture fidelity. We\nfurther propose Bootstrapped Score Distillation to specifically boost the\ntexture. We train a personalized diffusion model, Dreambooth, on the augmented\nrenderings of the scene, imbuing it with 3D knowledge of the scene being\noptimized. The score distillation from this 3D-aware diffusion prior provides\nview-consistent guidance for the scene. Notably, through an alternating\noptimization of the diffusion prior and 3D scene representation, we achieve\nmutually reinforcing improvements: the optimized 3D scene aids in training the\nscene-specific diffusion model, which offers increasingly view-consistent\nguidance for 3D optimization. The optimization is thus bootstrapped and leads\nto substantial texture boosting. With tailored 3D priors throughout the\nhierarchical generation, DreamCraft3D generates coherent 3D objects with\nphotorealistic renderings, advancing the state-of-the-art in 3D content\ngeneration. Code available at https://github.com/deepseek-ai/DreamCraft3D.",
        "translated": ""
    },
    {
        "title": "Exploring OCR Capabilities of GPT-4V(ision) : A Quantitative and\n  In-depth Evaluation",
        "url": "http://arxiv.org/abs/2310.16809v1",
        "pub_date": "2023-10-25",
        "summary": "This paper presents a comprehensive evaluation of the Optical Character\nRecognition (OCR) capabilities of the recently released GPT-4V(ision), a Large\nMultimodal Model (LMM). We assess the model's performance across a range of OCR\ntasks, including scene text recognition, handwritten text recognition,\nhandwritten mathematical expression recognition, table structure recognition,\nand information extraction from visually-rich document. The evaluation reveals\nthat GPT-4V performs well in recognizing and understanding Latin contents, but\nstruggles with multilingual scenarios and complex tasks. Based on these\nobservations, we delve deeper into the necessity of specialized OCR models and\ndeliberate on the strategies to fully harness the pretrained general LMMs like\nGPT-4V for OCR downstream tasks. The study offers a critical reference for\nfuture research in OCR with LMMs. Evaluation pipeline and results are available\nat https://github.com/SCUT-DLVCLab/GPT-4V_OCR.",
        "translated": ""
    },
    {
        "title": "Fingervein Verification using Convolutional Multi-Head Attention Network",
        "url": "http://arxiv.org/abs/2310.16808v1",
        "pub_date": "2023-10-25",
        "summary": "Biometric verification systems are deployed in various security-based\naccess-control applications that require user-friendly and reliable person\nverification. Among the different biometric characteristics, fingervein\nbiometrics have been extensively studied owing to their reliable verification\nperformance. Furthermore, fingervein patterns reside inside the skin and are\nnot visible outside; therefore, they possess inherent resistance to\npresentation attacks and degradation due to external factors. In this paper, we\nintroduce a novel fingervein verification technique using a convolutional\nmultihead attention network called VeinAtnNet. The proposed VeinAtnNet is\ndesigned to achieve light weight with a smaller number of learnable parameters\nwhile extracting discriminant information from both normal and enhanced\nfingervein images. The proposed VeinAtnNet was trained on the newly constructed\nfingervein dataset with 300 unique fingervein patterns that were captured in\nmultiple sessions to obtain 92 samples per unique fingervein. Extensive\nexperiments were performed on the newly collected dataset FV-300 and the\npublicly available FV-USM and FV-PolyU fingervein dataset. The performance of\nthe proposed method was compared with five state-of-the-art fingervein\nverification systems, indicating the efficacy of the proposed VeinAtnNet.",
        "translated": ""
    },
    {
        "title": "Fantastic Gains and Where to Find Them: On the Existence and Prospect of\n  General Knowledge Transfer between Any Pretrained Model",
        "url": "http://arxiv.org/abs/2310.17653v1",
        "pub_date": "2023-10-26",
        "summary": "Training deep networks requires various design decisions regarding for\ninstance their architecture, data augmentation, or optimization. In this work,\nwe find these training variations to result in networks learning unique feature\nsets from the data. Using public model libraries comprising thousands of models\ntrained on canonical datasets like ImageNet, we observe that for arbitrary\npairings of pretrained models, one model extracts significant data context\nunavailable in the other -- independent of overall performance. Given any\narbitrary pairing of pretrained models and no external rankings (such as\nseparate test sets, e.g. due to data privacy), we investigate if it is possible\nto transfer such \"complementary\" knowledge from one model to another without\nperformance degradation -- a task made particularly difficult as additional\nknowledge can be contained in stronger, equiperformant or weaker models. Yet\nfacilitating robust transfer in scenarios agnostic to pretrained model pairings\nwould unlock auxiliary gains and knowledge fusion from any model repository\nwithout restrictions on model and problem specifics - including from weaker,\nlower-performance models. This work therefore provides an initial, in-depth\nexploration on the viability of such general-purpose knowledge transfer. Across\nlarge-scale experiments, we first reveal the shortcomings of standard knowledge\ndistillation techniques, and then propose a much more general extension through\ndata partitioning for successful transfer between nearly all pretrained models,\nwhich we show can also be done unsupervised. Finally, we assess both the\nscalability and impact of fundamental model properties on successful\nmodel-agnostic knowledge transfer.",
        "translated": ""
    },
    {
        "title": "A Coarse-to-Fine Pseudo-Labeling (C2FPL) Framework for Unsupervised\n  Video Anomaly Detection",
        "url": "http://arxiv.org/abs/2310.17650v1",
        "pub_date": "2023-10-26",
        "summary": "Detection of anomalous events in videos is an important problem in\napplications such as surveillance. Video anomaly detection (VAD) is\nwell-studied in the one-class classification (OCC) and weakly supervised (WS)\nsettings. However, fully unsupervised (US) video anomaly detection methods,\nwhich learn a complete system without any annotation or human supervision, have\nnot been explored in depth. This is because the lack of any ground truth\nannotations significantly increases the magnitude of the VAD challenge. To\naddress this challenge, we propose a simple-but-effective two-stage\npseudo-label generation framework that produces segment-level (normal/anomaly)\npseudo-labels, which can be further used to train a segment-level anomaly\ndetector in a supervised manner. The proposed coarse-to-fine pseudo-label\n(C2FPL) generator employs carefully-designed hierarchical divisive clustering\nand statistical hypothesis testing to identify anomalous video segments from a\nset of completely unlabeled videos. The trained anomaly detector can be\ndirectly applied on segments of an unseen test video to obtain segment-level,\nand subsequently, frame-level anomaly predictions. Extensive studies on two\nlarge-scale public-domain datasets, UCF-Crime and XD-Violence, demonstrate that\nthe proposed unsupervised approach achieves superior performance compared to\nall existing OCC and US methods , while yielding comparable performance to the\nstate-of-the-art WS methods.",
        "translated": ""
    },
    {
        "title": "6-DoF Stability Field via Diffusion Models",
        "url": "http://arxiv.org/abs/2310.17649v1",
        "pub_date": "2023-10-26",
        "summary": "A core capability for robot manipulation is reasoning over where and how to\nstably place objects in cluttered environments. Traditionally, robots have\nrelied on object-specific, hand-crafted heuristics in order to perform such\nreasoning, with limited generalizability beyond a small number of object\ninstances and object interaction patterns. Recent approaches instead learn\nnotions of physical interaction, namely motion prediction, but require\nsupervision in the form of labeled object information or come at the cost of\nhigh sample complexity, and do not directly reason over stability or object\nplacement. We present 6-DoFusion, a generative model capable of generating 3D\nposes of an object that produces a stable configuration of a given scene.\nUnderlying 6-DoFusion is a diffusion model that incrementally refines a\nrandomly initialized SE(3) pose to generate a sample from a learned,\ncontext-dependent distribution over stable poses. We evaluate our model on\ndifferent object placement and stacking tasks, demonstrating its ability to\nconstruct stable scenes that involve novel object classes as well as to improve\nthe accuracy of state-of-the-art 3D pose estimation methods.",
        "translated": ""
    },
    {
        "title": "Defending Against Transfer Attacks From Public Models",
        "url": "http://arxiv.org/abs/2310.17645v1",
        "pub_date": "2023-10-26",
        "summary": "Adversarial attacks have been a looming and unaddressed threat in the\nindustry. However, through a decade-long history of the robustness evaluation\nliterature, we have learned that mounting a strong or optimal attack is\nchallenging. It requires both machine learning and domain expertise. In other\nwords, the white-box threat model, religiously assumed by a large majority of\nthe past literature, is unrealistic. In this paper, we propose a new practical\nthreat model where the adversary relies on transfer attacks through publicly\navailable surrogate models. We argue that this setting will become the most\nprevalent for security-sensitive applications in the future. We evaluate the\ntransfer attacks in this setting and propose a specialized defense method based\non a game-theoretic perspective. The defenses are evaluated under 24 public\nmodels and 11 attack algorithms across three datasets (CIFAR-10, CIFAR-100, and\nImageNet). Under this threat model, our defense, PubDef, outperforms the\nstate-of-the-art white-box adversarial training by a large margin with almost\nno loss in the normal accuracy. For instance, on ImageNet, our defense achieves\n62% accuracy under the strongest transfer attack vs only 36% of the best\nadversarially trained model. Its accuracy when not under attack is only 2%\nlower than that of an undefended model (78% vs 80%). We release our code at\nhttps://github.com/wagner-group/pubdef.",
        "translated": ""
    },
    {
        "title": "torchdistill Meets Hugging Face Libraries for Reproducible, Coding-Free\n  Deep Learning Studies: A Case Study on NLP",
        "url": "http://arxiv.org/abs/2310.17644v1",
        "pub_date": "2023-10-26",
        "summary": "Reproducibility in scientific work has been becoming increasingly important\nin research communities such as machine learning, natural language processing,\nand computer vision communities due to the rapid development of the research\ndomains supported by recent advances in deep learning. In this work, we present\na significantly upgraded version of torchdistill, a modular-driven coding-free\ndeep learning framework significantly upgraded from the initial release, which\nsupports only image classification and object detection tasks for reproducible\nknowledge distillation experiments. To demonstrate that the upgraded framework\ncan support more tasks with third-party libraries, we reproduce the GLUE\nbenchmark results of BERT models using a script based on the upgraded\ntorchdistill, harmonizing with various Hugging Face libraries. All the 27\nfine-tuned BERT models and configurations to reproduce the results are\npublished at Hugging Face, and the model weights have already been widely used\nin research communities. We also reimplement popular small-sized models and new\nknowledge distillation methods and perform additional experiments for computer\nvision tasks.",
        "translated": ""
    },
    {
        "title": "Drive Anywhere: Generalizable End-to-end Autonomous Driving with\n  Multi-modal Foundation Models",
        "url": "http://arxiv.org/abs/2310.17642v1",
        "pub_date": "2023-10-26",
        "summary": "As autonomous driving technology matures, end-to-end methodologies have\nemerged as a leading strategy, promising seamless integration from perception\nto control via deep learning. However, existing systems grapple with challenges\nsuch as unexpected open set environments and the complexity of black-box\nmodels. At the same time, the evolution of deep learning introduces larger,\nmultimodal foundational models, offering multi-modal visual and textual\nunderstanding. In this paper, we harness these multimodal foundation models to\nenhance the robustness and adaptability of autonomous driving systems, enabling\nout-of-distribution, end-to-end, multimodal, and more explainable autonomy.\nSpecifically, we present an approach to apply end-to-end open-set (any\nenvironment/scene) autonomous driving that is capable of providing driving\ndecisions from representations queryable by image and text. To do so, we\nintroduce a method to extract nuanced spatial (pixel/patch-aligned) features\nfrom transformers to enable the encapsulation of both spatial and semantic\nfeatures. Our approach (i) demonstrates unparalleled results in diverse tests\nwhile achieving significantly greater robustness in out-of-distribution\nsituations, and (ii) allows the incorporation of latent space simulation (via\ntext) for improved training (data augmentation via text) and policy debugging.\nWe encourage the reader to check our explainer video at\nhttps://www.youtube.com/watch?v=4n-DJf8vXxo&amp;feature=youtu.be and to view the\ncode and demos on our project webpage at https://drive-anywhere.github.io/.",
        "translated": ""
    },
    {
        "title": "DeepShaRM: Multi-View Shape and Reflectance Map Recovery Under Unknown\n  Lighting",
        "url": "http://arxiv.org/abs/2310.17632v1",
        "pub_date": "2023-10-26",
        "summary": "Geometry reconstruction of textureless, non-Lambertian objects under unknown\nnatural illumination (i.e., in the wild) remains challenging as correspondences\ncannot be established and the reflectance cannot be expressed in simple\nanalytical forms. We derive a novel multi-view method, DeepShaRM, that achieves\nstate-of-the-art accuracy on this challenging task. Unlike past methods that\nformulate this as inverse-rendering, i.e., estimation of reflectance,\nillumination, and geometry from images, our key idea is to realize that\nreflectance and illumination need not be disentangled and instead estimated as\na compound reflectance map. We introduce a novel deep reflectance map\nestimation network that recovers the camera-view reflectance maps from the\nsurface normals of the current geometry estimate and the input multi-view\nimages. The network also explicitly estimates per-pixel confidence scores to\nhandle global light transport effects. A deep shape-from-shading network then\nupdates the geometry estimate expressed with a signed distance function using\nthe recovered reflectance maps. By alternating between these two, and, most\nimportant, by bypassing the ill-posed problem of reflectance and illumination\ndecomposition, the method accurately recovers object geometry in these\nchallenging settings. Extensive experiments on both synthetic and real-world\ndata clearly demonstrate its state-of-the-art accuracy.",
        "translated": ""
    },
    {
        "title": "A Survey on Transferability of Adversarial Examples across Deep Neural\n  Networks",
        "url": "http://arxiv.org/abs/2310.17626v1",
        "pub_date": "2023-10-26",
        "summary": "The emergence of Deep Neural Networks (DNNs) has revolutionized various\ndomains, enabling the resolution of complex tasks spanning image recognition,\nnatural language processing, and scientific problem-solving. However, this\nprogress has also exposed a concerning vulnerability: adversarial examples.\nThese crafted inputs, imperceptible to humans, can manipulate machine learning\nmodels into making erroneous predictions, raising concerns for safety-critical\napplications. An intriguing property of this phenomenon is the transferability\nof adversarial examples, where perturbations crafted for one model can deceive\nanother, often with a different architecture. This intriguing property enables\n\"black-box\" attacks, circumventing the need for detailed knowledge of the\ntarget model. This survey explores the landscape of the adversarial\ntransferability of adversarial examples. We categorize existing methodologies\nto enhance adversarial transferability and discuss the fundamental principles\nguiding each approach. While the predominant body of research primarily\nconcentrates on image classification, we also extend our discussion to\nencompass other vision tasks and beyond. Challenges and future prospects are\ndiscussed, highlighting the importance of fortifying DNNs against adversarial\nvulnerabilities in an evolving landscape.",
        "translated": ""
    },
    {
        "title": "MimicGen: A Data Generation System for Scalable Robot Learning using\n  Human Demonstrations",
        "url": "http://arxiv.org/abs/2310.17596v1",
        "pub_date": "2023-10-26",
        "summary": "Imitation learning from a large set of human demonstrations has proved to be\nan effective paradigm for building capable robot agents. However, the\ndemonstrations can be extremely costly and time-consuming to collect. We\nintroduce MimicGen, a system for automatically synthesizing large-scale, rich\ndatasets from only a small number of human demonstrations by adapting them to\nnew contexts. We use MimicGen to generate over 50K demonstrations across 18\ntasks with diverse scene configurations, object instances, and robot arms from\njust ~200 human demonstrations. We show that robot agents can be effectively\ntrained on this generated dataset by imitation learning to achieve strong\nperformance in long-horizon and high-precision tasks, such as multi-part\nassembly and coffee preparation, across broad initial state distributions. We\nfurther demonstrate that the effectiveness and utility of MimicGen data compare\nfavorably to collecting additional human demonstrations, making it a powerful\nand economical approach towards scaling up robot learning. Datasets, simulation\nenvironments, videos, and more at https://mimicgen.github.io .",
        "translated": ""
    },
    {
        "title": "SPA: A Graph Spectral Alignment Perspective for Domain Adaptation",
        "url": "http://arxiv.org/abs/2310.17594v1",
        "pub_date": "2023-10-26",
        "summary": "Unsupervised domain adaptation (UDA) is a pivotal form in machine learning to\nextend the in-domain model to the distinctive target domains where the data\ndistributions differ. Most prior works focus on capturing the inter-domain\ntransferability but largely overlook rich intra-domain structures, which\nempirically results in even worse discriminability. In this work, we introduce\na novel graph SPectral Alignment (SPA) framework to tackle the tradeoff. The\ncore of our method is briefly condensed as follows: (i)-by casting the DA\nproblem to graph primitives, SPA composes a coarse graph alignment mechanism\nwith a novel spectral regularizer towards aligning the domain graphs in\neigenspaces; (ii)-we further develop a fine-grained message propagation module\n-- upon a novel neighbor-aware self-training mechanism -- in order for enhanced\ndiscriminability in the target domain. On standardized benchmarks, the\nextensive experiments of SPA demonstrate that its performance has surpassed the\nexisting cutting-edge DA methods. Coupled with dense model analysis, we\nconclude that our approach indeed possesses superior efficacy, robustness,\ndiscriminability, and transferability. Code and data are available at:\nhttps://github.com/CrownX/SPA.",
        "translated": ""
    },
    {
        "title": "Image Clustering Conditioned on Text Criteria",
        "url": "http://arxiv.org/abs/2310.18297v1",
        "pub_date": "2023-10-27",
        "summary": "Classical clustering methods do not provide users with direct control of the\nclustering results, and the clustering results may not be consistent with the\nrelevant criterion that a user has in mind. In this work, we present a new\nmethodology for performing image clustering based on user-specified text\ncriteria by leveraging modern vision-language models and large language models.\nWe call our method Image Clustering Conditioned on Text Criteria (IC$|$TC), and\nit represents a different paradigm of image clustering. IC$|$TC requires a\nminimal and practical degree of human intervention and grants the user\nsignificant control over the clustering results in return. Our experiments show\nthat IC$|$TC can effectively cluster images with various criteria, such as\nhuman action, physical location, or the person's mood, while significantly\noutperforming baselines.",
        "translated": ""
    },
    {
        "title": "Always Clear Days: Degradation Type and Severity Aware All-In-One\n  Adverse Weather Removal",
        "url": "http://arxiv.org/abs/2310.18293v1",
        "pub_date": "2023-10-27",
        "summary": "All-in-one adverse weather removal is an emerging topic on image restoration,\nwhich aims to restore multiple weather degradation in an unified model, and the\nchallenging are twofold. First, discovering and handling the property of\nmulti-domain in target distribution formed by multiple weather conditions.\nSecond, design efficient and effective operations for different degradation\ntypes. To address this problem, most prior works focus on the multi-domain\ncaused by weather type. Inspired by inter\\&amp;intra-domain adaptation literature,\nwe observed that not only weather type but also weather severity introduce\nmulti-domain within each weather type domain, which is ignored by previous\nmethods, and further limit their performance. To this end, we proposed a\ndegradation type and severity aware model, called \\textbf{UtilityIR}, for blind\nall-in-one bad weather image restoration. To extract weather information from\nsingle image, we proposed a novel Marginal Quality Ranking Loss (MQRL) and\nutilized Contrastive Loss (CL) to guide weather severity and type extraction,\nand leverage a bag of novel techniques such as Multi-Head Cross Attention\n(MHCA) and Local-Global Adaptive Instance Normalization (LG-AdaIN) to\nefficiently restore spatial varying weather degradation. The proposed method\ncan significantly outperform the SOTA methods subjectively and objectively on\ndifferent weather restoration tasks with a large margin, and enjoy less model\nparameters. Proposed method even can restore \\textbf{unseen} domain combined\nmultiple degradation images, and modulating restoration level. Implementation\ncode will be available at\n{https://github.com/fordevoted/UtilityIR}{\\textit{this repository}}",
        "translated": ""
    },
    {
        "title": "Heterogeneous Federated Learning with Group-Aware Prompt Tuning",
        "url": "http://arxiv.org/abs/2310.18285v1",
        "pub_date": "2023-10-27",
        "summary": "Transformers have achieved remarkable success in various machine-learning\ntasks, prompting their widespread adoption. In this paper, we explore their\napplication in the context of federated learning (FL), with a particular focus\non heterogeneous scenarios where individual clients possess diverse local\ndatasets. To meet the computational and communication demands of FL, we\nleverage pre-trained Transformers and use an efficient prompt-tuning strategy.\nOur strategy introduces the concept of learning both shared and group prompts,\nenabling the acquisition of universal knowledge and group-specific knowledge\nsimultaneously. Additionally, a prompt selection module assigns personalized\ngroup prompts to each input, aligning the global model with the data\ndistribution of each client. This approach allows us to train a single global\nmodel that can automatically adapt to various local client data distributions\nwithout requiring local fine-tuning. In this way, our proposed method\neffectively bridges the gap between global and personalized local models in\nFederated Learning and surpasses alternative approaches that lack the\ncapability to adapt to previously unseen clients. The effectiveness of our\napproach is rigorously validated through extensive experimentation and ablation\nstudies.",
        "translated": ""
    },
    {
        "title": "FOUND: Foot Optimization with Uncertain Normals for Surface Deformation\n  Using Synthetic Data",
        "url": "http://arxiv.org/abs/2310.18279v1",
        "pub_date": "2023-10-27",
        "summary": "Surface reconstruction from multi-view images is a challenging task, with\nsolutions often requiring a large number of sampled images with high overlap.\nWe seek to develop a method for few-view reconstruction, for the case of the\nhuman foot. To solve this task, we must extract rich geometric cues from RGB\nimages, before carefully fusing them into a final 3D object. Our FOUND approach\ntackles this, with 4 main contributions: (i) SynFoot, a synthetic dataset of\n50,000 photorealistic foot images, paired with ground truth surface normals and\nkeypoints; (ii) an uncertainty-aware surface normal predictor trained on our\nsynthetic dataset; (iii) an optimization scheme for fitting a generative foot\nmodel to a series of images; and (iv) a benchmark dataset of calibrated images\nand high resolution ground truth geometry. We show that our normal predictor\noutperforms all off-the-shelf equivalents significantly on real images, and our\noptimization scheme outperforms state-of-the-art photogrammetry pipelines,\nespecially for a few-view setting. We release our synthetic dataset and\nbaseline 3D scans to the research community.",
        "translated": ""
    },
    {
        "title": "LipSim: A Provably Robust Perceptual Similarity Metric",
        "url": "http://arxiv.org/abs/2310.18274v1",
        "pub_date": "2023-10-27",
        "summary": "Recent years have seen growing interest in developing and applying perceptual\nsimilarity metrics. Research has shown the superiority of perceptual metrics\nover pixel-wise metrics in aligning with human perception and serving as a\nproxy for the human visual system. On the other hand, as perceptual metrics\nrely on neural networks, there is a growing concern regarding their resilience,\ngiven the established vulnerability of neural networks to adversarial attacks.\nIt is indeed logical to infer that perceptual metrics may inherit both the\nstrengths and shortcomings of neural networks. In this work, we demonstrate the\nvulnerability of state-of-the-art perceptual similarity metrics based on an\nensemble of ViT-based feature extractors to adversarial attacks. We then\npropose a framework to train a robust perceptual similarity metric called\nLipSim (Lipschitz Similarity Metric) with provable guarantees. By leveraging\n1-Lipschitz neural networks as the backbone, LipSim provides guarded areas\naround each data point and certificates for all perturbations within an\n$\\ell_2$ ball. Finally, a comprehensive set of experiments shows the\nperformance of LipSim in terms of natural and certified scores and on the image\nretrieval application. The code is available at\nhttps://github.com/SaraGhazanfari/LipSim.",
        "translated": ""
    },
    {
        "title": "PlantPlotGAN: A Physics-Informed Generative Adversarial Network for\n  Plant Disease Prediction",
        "url": "http://arxiv.org/abs/2310.18268v1",
        "pub_date": "2023-10-27",
        "summary": "Monitoring plantations is crucial for crop management and producing healthy\nharvests. Unmanned Aerial Vehicles (UAVs) have been used to collect\nmultispectral images that aid in this monitoring. However, given the number of\nhectares to be monitored and the limitations of flight, plant disease signals\nbecome visually clear only in the later stages of plant growth and only if the\ndisease has spread throughout a significant portion of the plantation. This\nlimited amount of relevant data hampers the prediction models, as the\nalgorithms struggle to generalize patterns with unbalanced or unrealistic\naugmented datasets effectively. To address this issue, we propose PlantPlotGAN,\na physics-informed generative model capable of creating synthetic multispectral\nplot images with realistic vegetation indices. These indices served as a proxy\nfor disease detection and were used to evaluate if our model could help\nincrease the accuracy of prediction models. The results demonstrate that the\nsynthetic imagery generated from PlantPlotGAN outperforms state-of-the-art\nmethods regarding the Fr\\'echet inception distance. Moreover, prediction models\nachieve higher accuracy metrics when trained with synthetic and original\nimagery for earlier plant disease detection compared to the training processes\nbased solely on real imagery.",
        "translated": ""
    },
    {
        "title": "A Self-Supervised Approach to Land Cover Segmentation",
        "url": "http://arxiv.org/abs/2310.18251v1",
        "pub_date": "2023-10-27",
        "summary": "Land use/land cover change (LULC) maps are integral resources in earth\nscience and agricultural research. Due to the nature of such maps, the creation\nof LULC maps is often constrained by the time and human resources necessary to\naccurately annotate satellite imagery and remote sensing data. While computer\nvision models that perform semantic segmentation to create detailed labels from\nsuch data are not uncommon, litle research has been done on self-supervised and\nunsupervised approaches to labelling LULC maps without the use of ground-truth\nmasks. Here, we demonstrate a self-supervised method of land cover segmentation\nthat has no need for high-quality ground truth labels. The proposed deep\nlearning employs a frozen pre-trained ViT backbone transferred from DINO in a\nSTEGO architecture and is fine-tuned using a custom dataset consisting of very\nhigh resolution (VHR) sattelite imagery. After only 10 epochs of fine-tuning,\nan accuracy of roughly 52% was observed across 5 samples, signifying the\nfeasibility of self-supervised models for the automated labelling of VHR LULC\nmaps.",
        "translated": ""
    },
    {
        "title": "Generative AI Model for Artistic Style Transfer Using Convolutional\n  Neural Networks",
        "url": "http://arxiv.org/abs/2310.18237v1",
        "pub_date": "2023-10-27",
        "summary": "Artistic style transfer, a captivating application of generative artificial\nintelligence, involves fusing the content of one image with the artistic style\nof another to create unique visual compositions. This paper presents a\ncomprehensive overview of a novel technique for style transfer using\nConvolutional Neural Networks (CNNs). By leveraging deep image representations\nlearned by CNNs, we demonstrate how to separate and manipulate image content\nand style, enabling the synthesis of high-quality images that combine content\nand style in a harmonious manner. We describe the methodology, including\ncontent and style representations, loss computation, and optimization, and\nshowcase experimental results highlighting the effectiveness and versatility of\nthe approach across different styles and content",
        "translated": ""
    },
    {
        "title": "How Re-sampling Helps for Long-Tail Learning?",
        "url": "http://arxiv.org/abs/2310.18236v1",
        "pub_date": "2023-10-27",
        "summary": "Long-tail learning has received significant attention in recent years due to\nthe challenge it poses with extremely imbalanced datasets. In these datasets,\nonly a few classes (known as the head classes) have an adequate number of\ntraining samples, while the rest of the classes (known as the tail classes) are\ninfrequent in the training data. Re-sampling is a classical and widely used\napproach for addressing class imbalance issues. Unfortunately, recent studies\nclaim that re-sampling brings negligible performance improvements in modern\nlong-tail learning tasks. This paper aims to investigate this phenomenon\nsystematically. Our research shows that re-sampling can considerably improve\ngeneralization when the training images do not contain semantically irrelevant\ncontexts. In other scenarios, however, it can learn unexpected spurious\ncorrelations between irrelevant contexts and target labels. We design\nexperiments on two homogeneous datasets, one containing irrelevant context and\nthe other not, to confirm our findings. To prevent the learning of spurious\ncorrelations, we propose a new context shift augmentation module that generates\ndiverse training images for the tail class by maintaining a context bank\nextracted from the head-class images. Experiments demonstrate that our proposed\nmodule can boost the generalization and outperform other approaches, including\nclass-balanced re-sampling, decoupled classifier re-training, and data\naugmentation methods. The source code is available at\nhttps://www.lamda.nju.edu.cn/code_CSA.ashx.",
        "translated": ""
    },
    {
        "title": "Davidsonian Scene Graph: Improving Reliability in Fine-grained\n  Evaluation for Text-Image Generation",
        "url": "http://arxiv.org/abs/2310.18235v1",
        "pub_date": "2023-10-27",
        "summary": "Evaluating text-to-image models is notoriously difficult. A strong recent\napproach for assessing text-image faithfulness is based on QG/A (question\ngeneration and answering), which uses pre-trained foundational models to\nautomatically generate a set of questions and answers from the prompt, and\noutput images are scored based on whether these answers extracted with a visual\nquestion answering model are consistent with the prompt-based answers. This\nkind of evaluation is naturally dependent on the quality of the underlying QG\nand QA models. We identify and address several reliability challenges in\nexisting QG/A work: (a) QG questions should respect the prompt (avoiding\nhallucinations, duplications, and omissions) and (b) VQA answers should be\nconsistent (not asserting that there is no motorcycle in an image while also\nclaiming the motorcycle is blue). We address these issues with Davidsonian\nScene Graph (DSG), an empirically grounded evaluation framework inspired by\nformal semantics. DSG is an automatic, graph-based QG/A that is modularly\nimplemented to be adaptable to any QG/A module. DSG produces atomic and unique\nquestions organized in dependency graphs, which (i) ensure appropriate semantic\ncoverage and (ii) sidestep inconsistent answers. With extensive experimentation\nand human evaluation on a range of model configurations (LLM, VQA, and T2I), we\nempirically demonstrate that DSG addresses the challenges noted above. Finally,\nwe present DSG-1k, an open-sourced evaluation benchmark that includes 1,060\nprompts, covering a wide range of fine-grained semantic categories with a\nbalanced distribution. We will release the DSG-1k prompts and the corresponding\nDSG questions.",
        "translated": ""
    },
    {
        "title": "There Are No Data Like More Data- Datasets for Deep Learning in Earth\n  Observation",
        "url": "http://arxiv.org/abs/2310.19231v1",
        "pub_date": "2023-10-30",
        "summary": "Carefully curated and annotated datasets are the foundation of machine\nlearning, with particularly data-hungry deep neural networks forming the core\nof what is often called Artificial Intelligence (AI). Due to the massive\nsuccess of deep learning applied to Earth Observation (EO) problems, the focus\nof the community has been largely on the development of ever-more sophisticated\ndeep neural network architectures and training strategies largely ignoring the\noverall importance of datasets. For that purpose, numerous task-specific\ndatasets have been created that were largely ignored by previously published\nreview articles on AI for Earth observation. With this article, we want to\nchange the perspective and put machine learning datasets dedicated to Earth\nobservation data and applications into the spotlight. Based on a review of the\nhistorical developments, currently available resources are described and a\nperspective for future developments is formed. We hope to contribute to an\nunderstanding that the nature of our data is what distinguishes the Earth\nobservation community from many other communities that apply deep learning\ntechniques to image data, and that a detailed understanding of EO data\npeculiarities is among the core competencies of our discipline.",
        "translated": ""
    },
    {
        "title": "CHAMMI: A benchmark for channel-adaptive models in microscopy imaging",
        "url": "http://arxiv.org/abs/2310.19224v1",
        "pub_date": "2023-10-30",
        "summary": "Most neural networks assume that input images have a fixed number of channels\n(three for RGB images). However, there are many settings where the number of\nchannels may vary, such as microscopy images where the number of channels\nchanges depending on instruments and experimental goals. Yet, there has not\nbeen a systemic attempt to create and evaluate neural networks that are\ninvariant to the number and type of channels. As a result, trained models\nremain specific to individual studies and are hardly reusable for other\nmicroscopy settings. In this paper, we present a benchmark for investigating\nchannel-adaptive models in microscopy imaging, which consists of 1) a dataset\nof varied-channel single-cell images, and 2) a biologically relevant evaluation\nframework. In addition, we adapted several existing techniques to create\nchannel-adaptive models and compared their performance on this benchmark to\nfixed-channel, baseline models. We find that channel-adaptive models can\ngeneralize better to out-of-domain tasks and can be computationally efficient.\nWe contribute a curated dataset (https://doi.org/10.5281/zenodo.7988357) and an\nevaluation API (https://github.com/broadinstitute/MorphEm.git) to facilitate\nobjective comparisons in future research and applications.",
        "translated": ""
    },
    {
        "title": "Modular Anti-noise Deep Learning Network for Robotic Grasp Detection\n  Based on RGB Images",
        "url": "http://arxiv.org/abs/2310.19223v1",
        "pub_date": "2023-10-30",
        "summary": "While traditional methods relies on depth sensors, the current trend leans\ntowards utilizing cost-effective RGB images, despite their absence of depth\ncues. This paper introduces an interesting approach to detect grasping pose\nfrom a single RGB image. To this end, we propose a modular learning network\naugmented with grasp detection and semantic segmentation, tailored for robots\nequipped with parallel-plate grippers. Our network not only identifies\ngraspable objects but also fuses prior grasp analyses with semantic\nsegmentation, thereby boosting grasp detection precision. Significantly, our\ndesign exhibits resilience, adeptly handling blurred and noisy visuals. Key\ncontributions encompass a trainable network for grasp detection from RGB\nimages, a modular design facilitating feasible grasp implementation, and an\narchitecture robust against common image distortions. We demonstrate the\nfeasibility and accuracy of our proposed approach through practical experiments\nand evaluations.",
        "translated": ""
    },
    {
        "title": "Generalized Category Discovery with Clustering Assignment Consistency",
        "url": "http://arxiv.org/abs/2310.19210v1",
        "pub_date": "2023-10-30",
        "summary": "Generalized category discovery (GCD) is a recently proposed open-world task.\nGiven a set of images consisting of labeled and unlabeled instances, the goal\nof GCD is to automatically cluster the unlabeled samples using information\ntransferred from the labeled dataset. The unlabeled dataset comprises both\nknown and novel classes. The main challenge is that unlabeled novel class\nsamples and unlabeled known class samples are mixed together in the unlabeled\ndataset. To address the GCD without knowing the class number of unlabeled\ndataset, we propose a co-training-based framework that encourages clustering\nconsistency. Specifically, we first introduce weak and strong augmentation\ntransformations to generate two sufficiently different views for the same\nsample. Then, based on the co-training assumption, we propose a consistency\nrepresentation learning strategy, which encourages consistency between\nfeature-prototype similarity and clustering assignment. Finally, we use the\ndiscriminative embeddings learned from the semi-supervised representation\nlearning process to construct an original sparse network and use a community\ndetection method to obtain the clustering results and the number of categories\nsimultaneously. Extensive experiments show that our method achieves\nstate-of-the-art performance on three generic benchmarks and three fine-grained\nvisual recognition datasets. Especially in the ImageNet-100 data set, our\nmethod significantly exceeds the best baseline by 15.5\\% and 7.0\\% on the\n\\texttt{Novel} and \\texttt{All} classes, respectively.",
        "translated": ""
    },
    {
        "title": "3DMiner: Discovering Shapes from Large-Scale Unannotated Image Datasets",
        "url": "http://arxiv.org/abs/2310.19188v1",
        "pub_date": "2023-10-29",
        "summary": "We present 3DMiner -- a pipeline for mining 3D shapes from challenging\nlarge-scale unannotated image datasets. Unlike other unsupervised 3D\nreconstruction methods, we assume that, within a large-enough dataset, there\nmust exist images of objects with similar shapes but varying backgrounds,\ntextures, and viewpoints. Our approach leverages the recent advances in\nlearning self-supervised image representations to cluster images with\ngeometrically similar shapes and find common image correspondences between\nthem. We then exploit these correspondences to obtain rough camera estimates as\ninitialization for bundle-adjustment. Finally, for every image cluster, we\napply a progressive bundle-adjusting reconstruction method to learn a neural\noccupancy field representing the underlying shape. We show that this procedure\nis robust to several types of errors introduced in previous steps (e.g., wrong\ncamera poses, images containing dissimilar shapes, etc.), allowing us to obtain\nshape and pose annotations for images in-the-wild. When using images from Pix3D\nchairs, our method is capable of producing significantly better results than\nstate-of-the-art unsupervised 3D reconstruction techniques, both quantitatively\nand qualitatively. Furthermore, we show how 3DMiner can be applied to\nin-the-wild data by reconstructing shapes present in images from the LAION-5B\ndataset. Project Page: https://ttchengab.github.io/3dminerOfficial",
        "translated": ""
    },
    {
        "title": "Fast Trainable Projection for Robust Fine-Tuning",
        "url": "http://arxiv.org/abs/2310.19182v1",
        "pub_date": "2023-10-29",
        "summary": "Robust fine-tuning aims to achieve competitive in-distribution (ID)\nperformance while maintaining the out-of-distribution (OOD) robustness of a\npre-trained model when transferring it to a downstream task. Recently,\nprojected gradient descent has been successfully used in robust fine-tuning by\nconstraining the deviation from the initialization of the fine-tuned model\nexplicitly through projection. However, algorithmically, two limitations\nprevent this method from being adopted more widely, scalability and efficiency.\nIn this paper, we propose a new projection-based fine-tuning algorithm, Fast\nTrainable Projection (FTP) for computationally efficient learning of per-layer\nprojection constraints, resulting in an average $35\\%$ speedup on our\nbenchmarks compared to prior works. FTP can be combined with existing\noptimizers such as AdamW, and be used in a plug-and-play fashion. Finally, we\nshow that FTP is a special instance of hyper-optimizers that tune the\nhyper-parameters of optimizers in a learnable manner through nested\ndifferentiation. Empirically, we show superior robustness on OOD datasets,\nincluding domain shifts and natural corruptions, across four different vision\ntasks with five different pre-trained models. Additionally, we demonstrate that\nFTP is broadly applicable and beneficial to other learning scenarios such as\nlow-label and continual learning settings thanks to its easy adaptability. The\ncode will be available at https://github.com/GT-RIPL/FTP.git.",
        "translated": ""
    },
    {
        "title": "JEN-1 Composer: A Unified Framework for High-Fidelity Multi-Track Music\n  Generation",
        "url": "http://arxiv.org/abs/2310.19180v1",
        "pub_date": "2023-10-29",
        "summary": "With rapid advances in generative artificial intelligence, the text-to-music\nsynthesis task has emerged as a promising direction for music generation from\nscratch. However, finer-grained control over multi-track generation remains an\nopen challenge. Existing models exhibit strong raw generation capability but\nlack the flexibility to compose separate tracks and combine them in a\ncontrollable manner, differing from typical workflows of human composers. To\naddress this issue, we propose JEN-1 Composer, a unified framework to\nefficiently model marginal, conditional, and joint distributions over\nmulti-track music via a single model. JEN-1 Composer framework exhibits the\ncapacity to seamlessly incorporate any diffusion-based music generation system,\n\\textit{e.g.} Jen-1, enhancing its capacity for versatile multi-track music\ngeneration. We introduce a curriculum training strategy aimed at incrementally\ninstructing the model in the transition from single-track generation to the\nflexible generation of multi-track combinations. During the inference, users\nhave the ability to iteratively produce and choose music tracks that meet their\npreferences, subsequently creating an entire musical composition incrementally\nfollowing the proposed Human-AI co-composition workflow. Quantitative and\nqualitative assessments demonstrate state-of-the-art performance in\ncontrollable and high-fidelity multi-track music synthesis. The proposed JEN-1\nComposer represents a significant advance toward interactive AI-facilitated\nmusic creation and composition. Demos will be available at\nhttps://jenmusic.ai/audio-demos.",
        "translated": ""
    },
    {
        "title": "BirdSAT: Cross-View Contrastive Masked Autoencoders for Bird Species\n  Classification and Mapping",
        "url": "http://arxiv.org/abs/2310.19168v1",
        "pub_date": "2023-10-29",
        "summary": "We propose a metadata-aware self-supervised learning~(SSL)~framework useful\nfor fine-grained classification and ecological mapping of bird species around\nthe world. Our framework unifies two SSL strategies: Contrastive Learning~(CL)\nand Masked Image Modeling~(MIM), while also enriching the embedding space with\nmetadata available with ground-level imagery of birds. We separately train\nuni-modal and cross-modal ViT on a novel cross-view global bird species dataset\ncontaining ground-level imagery, metadata (location, time), and corresponding\nsatellite imagery. We demonstrate that our models learn fine-grained and\ngeographically conditioned features of birds, by evaluating on two downstream\ntasks: fine-grained visual classification~(FGVC) and cross-modal retrieval.\nPre-trained models learned using our framework achieve SotA performance on FGVC\nof iNAT-2021 birds and in transfer learning settings for CUB-200-2011 and\nNABirds datasets. Moreover, the impressive cross-modal retrieval performance of\nour model enables the creation of species distribution maps across any\ngeographic region. The dataset and source code will be released at\nhttps://github.com/mvrl/BirdSAT}.",
        "translated": ""
    },
    {
        "title": "Learning to Follow Object-Centric Image Editing Instructions Faithfully",
        "url": "http://arxiv.org/abs/2310.19145v1",
        "pub_date": "2023-10-29",
        "summary": "Natural language instructions are a powerful interface for editing the\noutputs of text-to-image diffusion models. However, several challenges need to\nbe addressed: 1) underspecification (the need to model the implicit meaning of\ninstructions) 2) grounding (the need to localize where the edit has to be\nperformed), 3) faithfulness (the need to preserve the elements of the image not\naffected by the edit instruction). Current approaches focusing on image editing\nwith natural language instructions rely on automatically generated paired data,\nwhich, as shown in our investigation, is noisy and sometimes nonsensical,\nexacerbating the above issues. Building on recent advances in segmentation,\nChain-of-Thought prompting, and visual question answering, we significantly\nimprove the quality of the paired data. In addition, we enhance the supervision\nsignal by highlighting parts of the image that need to be changed by the\ninstruction. The model fine-tuned on the improved data is capable of performing\nfine-grained object-centric edits better than state-of-the-art baselines,\nmitigating the problems outlined above, as shown by automatic and human\nevaluations. Moreover, our model is capable of generalizing to domains unseen\nduring training, such as visual metaphors.",
        "translated": ""
    },
    {
        "title": "Women Wearing Lipstick: Measuring the Bias Between an Object and Its\n  Related Gender",
        "url": "http://arxiv.org/abs/2310.19130v1",
        "pub_date": "2023-10-29",
        "summary": "In this paper, we investigate the impact of objects on gender bias in image\ncaptioning systems. Our results show that only gender-specific objects have a\nstrong gender bias (e.g., women-lipstick). In addition, we propose a visual\nsemantic-based gender score that measures the degree of bias and can be used as\na plug-in for any image captioning system. Our experiments demonstrate the\nutility of the gender score, since we observe that our score can measure the\nbias relation between a caption and its related gender; therefore, our score\ncan be used as an additional metric to the existing Object Gender Co-Occ\napproach. Code and data are publicly available at\n\\url{https://github.com/ahmedssabir/GenderScore}.",
        "translated": ""
    },
    {
        "title": "FPO++: Efficient Encoding and Rendering of Dynamic Neural Radiance\n  Fields by Analyzing and Enhancing Fourier PlenOctrees",
        "url": "http://arxiv.org/abs/2310.20710v1",
        "pub_date": "2023-10-31",
        "summary": "Fourier PlenOctrees have shown to be an efficient representation for\nreal-time rendering of dynamic Neural Radiance Fields (NeRF). Despite its many\nadvantages, this method suffers from artifacts introduced by the involved\ncompression when combining it with recent state-of-the-art techniques for\ntraining the static per-frame NeRF models. In this paper, we perform an\nin-depth analysis of these artifacts and leverage the resulting insights to\npropose an improved representation. In particular, we present a novel density\nencoding that adapts the Fourier-based compression to the characteristics of\nthe transfer function used by the underlying volume rendering procedure and\nleads to a substantial reduction of artifacts in the dynamic model.\nFurthermore, we show an augmentation of the training data that relaxes the\nperiodicity assumption of the compression. We demonstrate the effectiveness of\nour enhanced Fourier PlenOctrees in the scope of quantitative and qualitative\nevaluations on synthetic and real-world scenes.",
        "translated": ""
    },
    {
        "title": "DDAM-PS: Diligent Domain Adaptive Mixer for Person Search",
        "url": "http://arxiv.org/abs/2310.20706v1",
        "pub_date": "2023-10-31",
        "summary": "Person search (PS) is a challenging computer vision problem where the\nobjective is to achieve joint optimization for pedestrian detection and\nre-identification (ReID). Although previous advancements have shown promising\nperformance in the field under fully and weakly supervised learning fashion,\nthere exists a major gap in investigating the domain adaptation ability of PS\nmodels. In this paper, we propose a diligent domain adaptive mixer (DDAM) for\nperson search (DDAP-PS) framework that aims to bridge a gap to improve\nknowledge transfer from the labeled source domain to the unlabeled target\ndomain. Specifically, we introduce a novel DDAM module that generates moderate\nmixed-domain representations by combining source and target domain\nrepresentations. The proposed DDAM module encourages domain mixing to minimize\nthe distance between the two extreme domains, thereby enhancing the ReID task.\nTo achieve this, we introduce two bridge losses and a disparity loss. The\nobjective of the two bridge losses is to guide the moderate mixed-domain\nrepresentations to maintain an appropriate distance from both the source and\ntarget domain representations. The disparity loss aims to prevent the moderate\nmixed-domain representations from being biased towards either the source or\ntarget domains, thereby avoiding overfitting. Furthermore, we address the\nconflict between the two subtasks, localization and ReID, during domain\nadaptation. To handle this cross-task conflict, we forcefully decouple the\nnorm-aware embedding, which aids in better learning of the moderate\nmixed-domain representation. We conduct experiments to validate the\neffectiveness of our proposed method. Our approach demonstrates favorable\nperformance on the challenging PRW and CUHK-SYSU datasets. Our source code is\npublicly available at \\url{https://github.com/mustansarfiaz/DDAM-PS}.",
        "translated": ""
    },
    {
        "title": "Limited Data, Unlimited Potential: A Study on ViTs Augmented by Masked\n  Autoencoders",
        "url": "http://arxiv.org/abs/2310.20704v1",
        "pub_date": "2023-10-31",
        "summary": "Vision Transformers (ViTs) have become ubiquitous in computer vision. Despite\ntheir success, ViTs lack inductive biases, which can make it difficult to train\nthem with limited data. To address this challenge, prior studies suggest\ntraining ViTs with self-supervised learning (SSL) and fine-tuning sequentially.\nHowever, we observe that jointly optimizing ViTs for the primary task and a\nSelf-Supervised Auxiliary Task (SSAT) is surprisingly beneficial when the\namount of training data is limited. We explore the appropriate SSL tasks that\ncan be optimized alongside the primary task, the training schemes for these\ntasks, and the data scale at which they can be most effective. Our findings\nreveal that SSAT is a powerful technique that enables ViTs to leverage the\nunique characteristics of both the self-supervised and primary tasks, achieving\nbetter performance than typical ViTs pre-training with SSL and fine-tuning\nsequentially. Our experiments, conducted on 10 datasets, demonstrate that SSAT\nsignificantly improves ViT performance while reducing carbon footprint. We also\nconfirm the effectiveness of SSAT in the video domain for deepfake detection,\nshowcasing its generalizability. Our code is available at\nhttps://github.com/dominickrei/Limited-data-vits.",
        "translated": ""
    },
    {
        "title": "SEINE: Short-to-Long Video Diffusion Model for Generative Transition and\n  Prediction",
        "url": "http://arxiv.org/abs/2310.20700v1",
        "pub_date": "2023-10-31",
        "summary": "Recently video generation has achieved substantial progress with realistic\nresults. Nevertheless, existing AI-generated videos are usually very short\nclips (\"shot-level\") depicting a single scene. To deliver a coherent long video\n(\"story-level\"), it is desirable to have creative transition and prediction\neffects across different clips. This paper presents a short-to-long video\ndiffusion model, SEINE, that focuses on generative transition and prediction.\nThe goal is to generate high-quality long videos with smooth and creative\ntransitions between scenes and varying lengths of shot-level videos.\nSpecifically, we propose a random-mask video diffusion model to automatically\ngenerate transitions based on textual descriptions. By providing the images of\ndifferent scenes as inputs, combined with text-based control, our model\ngenerates transition videos that ensure coherence and visual quality.\nFurthermore, the model can be readily extended to various tasks such as\nimage-to-video animation and autoregressive video prediction. To conduct a\ncomprehensive evaluation of this new generative task, we propose three\nassessing criteria for smooth and creative transition: temporal consistency,\nsemantic similarity, and video-text semantic alignment. Extensive experiments\nvalidate the effectiveness of our approach over existing methods for generative\ntransition and prediction, enabling the creation of story-level long videos.\nProject page: https://vchitect.github.io/SEINE-project/ .",
        "translated": ""
    },
    {
        "title": "HAP: Structure-Aware Masked Image Modeling for Human-Centric Perception",
        "url": "http://arxiv.org/abs/2310.20695v1",
        "pub_date": "2023-10-31",
        "summary": "Model pre-training is essential in human-centric perception. In this paper,\nwe first introduce masked image modeling (MIM) as a pre-training approach for\nthis task. Upon revisiting the MIM training strategy, we reveal that human\nstructure priors offer significant potential. Motivated by this insight, we\nfurther incorporate an intuitive human structure prior - human parts - into\npre-training. Specifically, we employ this prior to guide the mask sampling\nprocess. Image patches, corresponding to human part regions, have high priority\nto be masked out. This encourages the model to concentrate more on body\nstructure information during pre-training, yielding substantial benefits across\na range of human-centric perception tasks. To further capture human\ncharacteristics, we propose a structure-invariant alignment loss that enforces\ndifferent masked views, guided by the human part prior, to be closely aligned\nfor the same image. We term the entire method as HAP. HAP simply uses a plain\nViT as the encoder yet establishes new state-of-the-art performance on 11\nhuman-centric benchmarks, and on-par result on one dataset. For example, HAP\nachieves 78.1% mAP on MSMT17 for person re-identification, 86.54% mA on PA-100K\nfor pedestrian attribute recognition, 78.2% AP on MS COCO for 2D pose\nestimation, and 56.0 PA-MPJPE on 3DPW for 3D pose and shape estimation.",
        "translated": ""
    },
    {
        "title": "NeRF Revisited: Fixing Quadrature Instability in Volume Rendering",
        "url": "http://arxiv.org/abs/2310.20685v1",
        "pub_date": "2023-10-31",
        "summary": "Neural radiance fields (NeRF) rely on volume rendering to synthesize novel\nviews. Volume rendering requires evaluating an integral along each ray, which\nis numerically approximated with a finite sum that corresponds to the exact\nintegral along the ray under piecewise constant volume density. As a\nconsequence, the rendered result is unstable w.r.t. the choice of samples along\nthe ray, a phenomenon that we dub quadrature instability. We propose a\nmathematically principled solution by reformulating the sample-based rendering\nequation so that it corresponds to the exact integral under piecewise linear\nvolume density. This simultaneously resolves multiple issues: conflicts between\nsamples along different rays, imprecise hierarchical sampling, and\nnon-differentiability of quantiles of ray termination distances w.r.t. model\nparameters. We demonstrate several benefits over the classical sample-based\nrendering equation, such as sharper textures, better geometric reconstruction,\nand stronger depth supervision. Our proposed formulation can be also be used as\na drop-in replacement to the volume rendering equation of existing NeRF-based\nmethods. Our project page can be found at pl-nerf.github.io.",
        "translated": ""
    },
    {
        "title": "StairNet: Visual Recognition of Stairs for Human-Robot Locomotion",
        "url": "http://arxiv.org/abs/2310.20666v1",
        "pub_date": "2023-10-31",
        "summary": "Human-robot walking with prosthetic legs and exoskeletons, especially over\ncomplex terrains such as stairs, remains a significant challenge. Egocentric\nvision has the unique potential to detect the walking environment prior to\nphysical interactions, which can improve transitions to and from stairs. This\nmotivated us to create the StairNet initiative to support the development of\nnew deep learning models for visual sensing and recognition of stairs, with an\nemphasis on lightweight and efficient neural networks for onboard real-time\ninference. In this study, we present an overview of the development of our\nlarge-scale dataset with over 515,000 manually labeled images, as well as our\ndevelopment of different deep learning models (e.g., 2D and 3D CNN, hybrid CNN\nand LSTM, and ViT networks) and training methods (e.g., supervised learning\nwith temporal data and semi-supervised learning with unlabeled images) using\nour new dataset. We consistently achieved high classification accuracy (i.e.,\nup to 98.8%) with different designs, offering trade-offs between model accuracy\nand size. When deployed on mobile devices with GPU and NPU accelerators, our\ndeep learning models achieved inference speeds up to 2.8 ms. We also deployed\nour models on custom-designed CPU-powered smart glasses. However, limitations\nin the embedded hardware yielded slower inference speeds of 1.5 seconds,\npresenting a trade-off between human-centered design and performance. Overall,\nwe showed that StairNet can be an effective platform to develop and study new\nvisual perception systems for human-robot locomotion with applications in\nexoskeleton and prosthetic leg control.",
        "translated": ""
    },
    {
        "title": "Addressing Limitations of State-Aware Imitation Learning for Autonomous\n  Driving",
        "url": "http://arxiv.org/abs/2310.20650v1",
        "pub_date": "2023-10-31",
        "summary": "Conditional Imitation learning is a common and effective approach to train\nautonomous driving agents. However, two issues limit the full potential of this\napproach: (i) the inertia problem, a special case of causal confusion where the\nagent mistakenly correlates low speed with no acceleration, and (ii) low\ncorrelation between offline and online performance due to the accumulation of\nsmall errors that brings the agent in a previously unseen state. Both issues\nare critical for state-aware models, yet informing the driving agent of its\ninternal state as well as the state of the environment is of crucial\nimportance. In this paper we propose a multi-task learning agent based on a\nmulti-stage vision transformer with state token propagation. We feed the state\nof the vehicle along with the representation of the environment as a special\ntoken of the transformer and propagate it throughout the network. This allows\nus to tackle the aforementioned issues from different angles: guiding the\ndriving policy with learned stop/go information, performing data augmentation\ndirectly on the state of the vehicle and visually explaining the model's\ndecisions. We report a drastic decrease in inertia and a high correlation\nbetween offline and online metrics.",
        "translated": ""
    },
    {
        "title": "Dynamic Batch Norm Statistics Update for Natural Robustness",
        "url": "http://arxiv.org/abs/2310.20649v1",
        "pub_date": "2023-10-31",
        "summary": "DNNs trained on natural clean samples have been shown to perform poorly on\ncorrupted samples, such as noisy or blurry images. Various data augmentation\nmethods have been recently proposed to improve DNN's robustness against common\ncorruptions. Despite their success, they require computationally expensive\ntraining and cannot be applied to off-the-shelf trained models. Recently, it\nhas been shown that updating BatchNorm (BN) statistics of an off-the-shelf\nmodel on a single corruption improves its accuracy on that corruption\nsignificantly. However, adopting the idea at inference time when the type of\ncorruption is unknown and changing decreases the effectiveness of this method.\nIn this paper, we harness the Fourier domain to detect the corruption type, a\nchallenging task in the image domain. We propose a unified framework consisting\nof a corruption-detection model and BN statistics update that improves the\ncorruption accuracy of any off-the-shelf trained model. We benchmark our\nframework on different models and datasets. Our results demonstrate about 8%\nand 4% accuracy improvement on CIFAR10-C and ImageNet-C, respectively.\nFurthermore, our framework can further improve the accuracy of state-of-the-art\nrobust models, such as AugMix and DeepAug.",
        "translated": ""
    },
    {
        "title": "Histopathological Image Analysis with Style-Augmented Feature Domain\n  Mixing for Improved Generalization",
        "url": "http://arxiv.org/abs/2310.20638v1",
        "pub_date": "2023-10-31",
        "summary": "Histopathological images are essential for medical diagnosis and treatment\nplanning, but interpreting them accurately using machine learning can be\nchallenging due to variations in tissue preparation, staining and imaging\nprotocols. Domain generalization aims to address such limitations by enabling\nthe learning models to generalize to new datasets or populations. Style\ntransfer-based data augmentation is an emerging technique that can be used to\nimprove the generalizability of machine learning models for histopathological\nimages. However, existing style transfer-based methods can be computationally\nexpensive, and they rely on artistic styles, which can negatively impact model\naccuracy. In this study, we propose a feature domain style mixing technique\nthat uses adaptive instance normalization to generate style-augmented versions\nof images. We compare our proposed method with existing style transfer-based\ndata augmentation methods and found that it performs similarly or better,\ndespite requiring less computation and time. Our results demonstrate the\npotential of feature domain statistics mixing in the generalization of learning\nmodels for histopathological image analysis.",
        "translated": ""
    },
    {
        "title": "What User Behaviors Make the Differences During the Process of Visual\n  Analytics?",
        "url": "http://arxiv.org/abs/2311.00690v1",
        "pub_date": "2023-11-01",
        "summary": "The understanding of visual analytics process can benefit visualization\nresearchers from multiple aspects, including improving visual designs and\ndeveloping advanced interaction functions. However, the log files of user\nbehaviors are still hard to analyze due to the complexity of sensemaking and\nour lack of knowledge on the related user behaviors. This work presents a study\non a comprehensive data collection of user behaviors, and our analysis approach\nwith time-series classification methods. We have chosen a classical\nvisualization application, Covid-19 data analysis, with common analysis tasks\ncovering geo-spatial, time-series and multi-attributes. Our user study collects\nuser behaviors on a diverse set of visualization tasks with two comparable\nsystems, desktop and immersive visualizations. We summarize the classification\nresults with three time-series machine learning algorithms at two scales, and\nexplore the influences of behavior features. Our results reveal that user\nbehaviors can be distinguished during the process of visual analytics and there\nis a potentially strong association between the physical behaviors of users and\nthe visualization tasks they perform. We also demonstrate the usage of our\nmodels by interpreting open sessions of visual analytics, which provides an\nautomatic way to study sensemaking without tedious manual annotations.",
        "translated": ""
    },
    {
        "title": "Collaboration in Immersive Environments: Challenges and Solutions",
        "url": "http://arxiv.org/abs/2311.00689v1",
        "pub_date": "2023-11-01",
        "summary": "Virtual Reality (VR) and Augmented Reality (AR) tools have been applied in\nall engineering fields in order to avoid the use of physical prototypes, to\ntrain in high-risk situations, and to interpret real or simulated results. In\norder to complete a shared task or assign tasks to the agents in such immersive\nenvironments, collaboration or Shared Cooperative Activities are a necessity.\nCollaboration in immersive environments is an emerging field of research that\naims to study and enhance the ways in which people interact and work together\nin Virtual and Augmented Reality settings. Collaboration in immersive\nenvironments is a complex process that involves different factors such as\ncommunication, coordination, and social presence. This paper provides an\noverview of the current state of research on collaboration in immersive\nenvironments. It discusses the different types of immersive environments,\nincluding VR and AR, and the different forms of collaboration that can occur in\nthese environments. The paper also highlights the challenges and limitations of\ncollaboration in immersive environments, such as the lack of physical cues,\ncost and usability and the need for further research in this area. Overall,\ncollaboration in immersive environments is a promising field with a wide range\nof potential applications, from education to industry, and it can benefit both\nindividuals and groups by enhancing their ability to work together effectively.",
        "translated": ""
    },
    {
        "title": "ProcSim: Proxy-based Confidence for Robust Similarity Learning",
        "url": "http://arxiv.org/abs/2311.00668v1",
        "pub_date": "2023-11-01",
        "summary": "Deep Metric Learning (DML) methods aim at learning an embedding space in\nwhich distances are closely related to the inherent semantic similarity of the\ninputs. Previous studies have shown that popular benchmark datasets often\ncontain numerous wrong labels, and DML methods are susceptible to them.\nIntending to study the effect of realistic noise, we create an ontology of the\nclasses in a dataset and use it to simulate semantically coherent labeling\nmistakes. To train robust DML models, we propose ProcSim, a simple framework\nthat assigns a confidence score to each sample using the normalized distance to\nits class representative. The experimental results show that the proposed\nmethod achieves state-of-the-art performance on the DML benchmark datasets\ninjected with uniform and the proposed semantically coherent noise.",
        "translated": ""
    },
    {
        "title": "TPSeNCE: Towards Artifact-Free Realistic Rain Generation for Deraining\n  and Object Detection in Rain",
        "url": "http://arxiv.org/abs/2311.00660v1",
        "pub_date": "2023-11-01",
        "summary": "Rain generation algorithms have the potential to improve the generalization\nof deraining methods and scene understanding in rainy conditions. However, in\npractice, they produce artifacts and distortions and struggle to control the\namount of rain generated due to a lack of proper constraints. In this paper, we\npropose an unpaired image-to-image translation framework for generating\nrealistic rainy images. We first introduce a Triangular Probability Similarity\n(TPS) constraint to guide the generated images toward clear and rainy images in\nthe discriminator manifold, thereby minimizing artifacts and distortions during\nrain generation. Unlike conventional contrastive learning approaches, which\nindiscriminately push negative samples away from the anchors, we propose a\nSemantic Noise Contrastive Estimation (SeNCE) strategy and reassess the pushing\nforce of negative samples based on the semantic similarity between the clear\nand the rainy images and the feature similarity between the anchor and the\nnegative samples. Experiments demonstrate realistic rain generation with\nminimal artifacts and distortions, which benefits image deraining and object\ndetection in rain. Furthermore, the method can be used to generate realistic\nsnowy and night images, underscoring its potential for broader applicability.\nCode is available at https://github.com/ShenZheng2000/TPSeNCE.",
        "translated": ""
    },
    {
        "title": "De-Diffusion Makes Text a Strong Cross-Modal Interface",
        "url": "http://arxiv.org/abs/2311.00618v1",
        "pub_date": "2023-11-01",
        "summary": "We demonstrate text as a strong cross-modal interface. Rather than relying on\ndeep embeddings to connect image and language as the interface representation,\nour approach represents an image as text, from which we enjoy the\ninterpretability and flexibility inherent to natural language. We employ an\nautoencoder that uses a pre-trained text-to-image diffusion model for decoding.\nThe encoder is trained to transform an input image into text, which is then fed\ninto the fixed text-to-image diffusion decoder to reconstruct the original\ninput -- a process we term De-Diffusion. Experiments validate both the\nprecision and comprehensiveness of De-Diffusion text representing images, such\nthat it can be readily ingested by off-the-shelf text-to-image tools and LLMs\nfor diverse multi-modal tasks. For example, a single De-Diffusion model can\ngeneralize to provide transferable prompts for different text-to-image tools,\nand also achieves a new state of the art on open-ended vision-language tasks by\nsimply prompting large language models with few-shot examples.",
        "translated": ""
    },
    {
        "title": "Occluded Person Re-Identification with Deep Learning: A Survey and\n  Perspectives",
        "url": "http://arxiv.org/abs/2311.00603v1",
        "pub_date": "2023-11-01",
        "summary": "Person re-identification (Re-ID) technology plays an increasingly crucial\nrole in intelligent surveillance systems. Widespread occlusion significantly\nimpacts the performance of person Re-ID. Occluded person Re-ID refers to a\npedestrian matching method that deals with challenges such as pedestrian\ninformation loss, noise interference, and perspective misalignment. It has\ngarnered extensive attention from researchers. Over the past few years, several\nocclusion-solving person Re-ID methods have been proposed, tackling various\nsub-problems arising from occlusion. However, there is a lack of comprehensive\nstudies that compare, summarize, and evaluate the potential of occluded person\nRe-ID methods in detail. In this review, we start by providing a detailed\noverview of the datasets and evaluation scheme used for occluded person Re-ID.\nNext, we scientifically classify and analyze existing deep learning-based\noccluded person Re-ID methods from various perspectives, summarizing them\nconcisely. Furthermore, we conduct a systematic comparison among these methods,\nidentify the state-of-the-art approaches, and present an outlook on the future\ndevelopment of occluded person Re-ID.",
        "translated": ""
    },
    {
        "title": "PAUMER: Patch Pausing Transformer for Semantic Segmentation",
        "url": "http://arxiv.org/abs/2311.00586v1",
        "pub_date": "2023-11-01",
        "summary": "We study the problem of improving the efficiency of segmentation transformers\nby using disparate amounts of computation for different parts of the image. Our\nmethod, PAUMER, accomplishes this by pausing computation for patches that are\ndeemed to not need any more computation before the final decoder. We use the\nentropy of predictions computed from intermediate activations as the pausing\ncriterion, and find this aligns well with semantics of the image. Our method\nhas a unique advantage that a single network trained with the proposed strategy\ncan be effortlessly adapted at inference to various run-time requirements by\nmodulating its pausing parameters. On two standard segmentation datasets,\nCityscapes and ADE20K, we show that our method operates with about a $50\\%$\nhigher throughput with an mIoU drop of about $0.65\\%$ and $4.6\\%$ respectively.",
        "translated": ""
    },
    {
        "title": "LLaVA-Interactive: An All-in-One Demo for Image Chat, Segmentation,\n  Generation and Editing",
        "url": "http://arxiv.org/abs/2311.00571v1",
        "pub_date": "2023-11-01",
        "summary": "LLaVA-Interactive is a research prototype for multimodal human-AI\ninteraction. The system can have multi-turn dialogues with human users by\ntaking multimodal user inputs and generating multimodal responses. Importantly,\nLLaVA-Interactive goes beyond language prompt, where visual prompt is enabled\nto align human intents in the interaction. The development of LLaVA-Interactive\nis extremely cost-efficient as the system combines three multimodal skills of\npre-built AI models without additional model training: visual chat of LLaVA,\nimage segmentation from SEEM, as well as image generation and editing from\nGLIGEN. A diverse set of application scenarios is presented to demonstrate the\npromises of LLaVA-Interactive and to inspire future research in multimodal\ninteractive systems.",
        "translated": ""
    },
    {
        "title": "A Robust Deep Learning Method with Uncertainty Estimation for the\n  Pathological Classification of Renal Cell Carcinoma based on CT Images",
        "url": "http://arxiv.org/abs/2311.00567v1",
        "pub_date": "2023-11-01",
        "summary": "Objectives To develop and validate a deep learning-based diagnostic model\nincorporating uncertainty estimation so as to facilitate radiologists in the\npreoperative differentiation of the pathological subtypes of renal cell\ncarcinoma (RCC) based on CT images. Methods Data from 668 consecutive patients,\npathologically proven RCC, were retrospectively collected from Center 1. By\nusing five-fold cross-validation, a deep learning model incorporating\nuncertainty estimation was developed to classify RCC subtypes into clear cell\nRCC (ccRCC), papillary RCC (pRCC), and chromophobe RCC (chRCC). An external\nvalidation set of 78 patients from Center 2 further evaluated the model's\nperformance. Results In the five-fold cross-validation, the model's area under\nthe receiver operating characteristic curve (AUC) for the classification of\nccRCC, pRCC, and chRCC was 0.868 (95% CI: 0.826-0.923), 0.846 (95% CI:\n0.812-0.886), and 0.839 (95% CI: 0.802-0.88), respectively. In the external\nvalidation set, the AUCs were 0.856 (95% CI: 0.838-0.882), 0.787 (95% CI:\n0.757-0.818), and 0.793 (95% CI: 0.758-0.831) for ccRCC, pRCC, and chRCC,\nrespectively. Conclusions The developed deep learning model demonstrated robust\nperformance in predicting the pathological subtypes of RCC, while the\nincorporated uncertainty emphasized the importance of understanding model\nconfidence, which is crucial for assisting clinical decision-making for\npatients with renal tumors. Clinical relevance statement Our deep learning\napproach, integrated with uncertainty estimation, offers clinicians a dual\nadvantage: accurate RCC subtype predictions complemented by diagnostic\nconfidence references, promoting informed decision-making for patients with\nRCC.",
        "translated": ""
    },
    {
        "title": "CROMA: Remote Sensing Representations with Contrastive Radar-Optical\n  Masked Autoencoders",
        "url": "http://arxiv.org/abs/2311.00566v1",
        "pub_date": "2023-11-01",
        "summary": "A vital and rapidly growing application, remote sensing offers vast yet\nsparsely labeled, spatially aligned multimodal data; this makes self-supervised\nlearning algorithms invaluable. We present CROMA: a framework that combines\ncontrastive and reconstruction self-supervised objectives to learn rich\nunimodal and multimodal representations. Our method separately encodes\nmasked-out multispectral optical and synthetic aperture radar samples --\naligned in space and time -- and performs cross-modal contrastive learning.\nAnother encoder fuses these sensors, producing joint multimodal encodings that\nare used to predict the masked patches via a lightweight decoder. We show that\nthese objectives are complementary when leveraged on spatially aligned\nmultimodal data. We also introduce X- and 2D-ALiBi, which spatially biases our\ncross- and self-attention matrices. These strategies improve representations\nand allow our models to effectively extrapolate to images up to 17.6x larger at\ntest-time. CROMA outperforms the current SoTA multispectral model, evaluated\non: four classification benchmarks -- finetuning (avg. 1.8%), linear (avg.\n2.4%) and nonlinear (avg. 1.4%) probing, kNN classification (avg. 3.5%), and\nK-means clustering (avg. 8.4%); and three segmentation benchmarks (avg. 6.4%).\nCROMA's rich, optionally multimodal representations can be widely leveraged\nacross remote sensing applications.",
        "translated": ""
    },
    {
        "title": "Idempotent Generative Network",
        "url": "http://arxiv.org/abs/2311.01462v1",
        "pub_date": "2023-11-02",
        "summary": "We propose a new approach for generative modeling based on training a neural\nnetwork to be idempotent. An idempotent operator is one that can be applied\nsequentially without changing the result beyond the initial application, namely\n$f(f(z))=f(z)$. The proposed model $f$ is trained to map a source distribution\n(e.g, Gaussian noise) to a target distribution (e.g. realistic images) using\nthe following objectives: (1) Instances from the target distribution should map\nto themselves, namely $f(x)=x$. We define the target manifold as the set of all\ninstances that $f$ maps to themselves. (2) Instances that form the source\ndistribution should map onto the defined target manifold. This is achieved by\noptimizing the idempotence term, $f(f(z))=f(z)$ which encourages the range of\n$f(z)$ to be on the target manifold. Under ideal assumptions such a process\nprovably converges to the target distribution. This strategy results in a model\ncapable of generating an output in one step, maintaining a consistent latent\nspace, while also allowing sequential applications for refinement.\nAdditionally, we find that by processing inputs from both target and source\ndistributions, the model adeptly projects corrupted or modified data back to\nthe target manifold. This work is a first step towards a ``global projector''\nthat enables projecting any input into a target data distribution.",
        "translated": ""
    },
    {
        "title": "Align Your Prompts: Test-Time Prompting with Distribution Alignment for\n  Zero-Shot Generalization",
        "url": "http://arxiv.org/abs/2311.01459v1",
        "pub_date": "2023-11-02",
        "summary": "The promising zero-shot generalization of vision-language models such as CLIP\nhas led to their adoption using prompt learning for numerous downstream tasks.\nPrevious works have shown test-time prompt tuning using entropy minimization to\nadapt text prompts for unseen domains. While effective, this overlooks the key\ncause for performance degradation to unseen domains -- distribution shift. In\nthis work, we explicitly handle this problem by aligning the\nout-of-distribution (OOD) test sample statistics to those of the source data\nusing prompt tuning. We use a single test sample to adapt multi-modal prompts\nat test time by minimizing the feature distribution shift to bridge the gap in\nthe test domain. Evaluating against the domain generalization benchmark, our\nmethod improves zero-shot top- 1 accuracy beyond existing prompt-learning\ntechniques, with a 3.08% improvement over the baseline MaPLe. In cross-dataset\ngeneralization with unseen categories across 10 datasets, our method improves\nconsistently across all datasets compared to the existing state-of-the-art. Our\nsource code and models are available at\nhttps://jameelhassan.github.io/promptalign.",
        "translated": ""
    },
    {
        "title": "Detecting Deepfakes Without Seeing Any",
        "url": "http://arxiv.org/abs/2311.01458v1",
        "pub_date": "2023-11-02",
        "summary": "Deepfake attacks, malicious manipulation of media containing people, are a\nserious concern for society. Conventional deepfake detection methods train\nsupervised classifiers to distinguish real media from previously encountered\ndeepfakes. Such techniques can only detect deepfakes similar to those\npreviously seen, but not zero-day (previously unseen) attack types. As current\ndeepfake generation techniques are changing at a breathtaking pace, new attack\ntypes are proposed frequently, making this a major issue. Our main observations\nare that: i) in many effective deepfake attacks, the fake media must be\naccompanied by false facts i.e. claims about the identity, speech, motion, or\nappearance of the person. For instance, when impersonating Obama, the attacker\nexplicitly or implicitly claims that the fake media show Obama; ii) current\ngenerative techniques cannot perfectly synthesize the false facts claimed by\nthe attacker. We therefore introduce the concept of \"fact checking\", adapted\nfrom fake news detection, for detecting zero-day deepfake attacks. Fact\nchecking verifies that the claimed facts (e.g. identity is Obama), agree with\nthe observed media (e.g. is the face really Obama's?), and thus can\ndifferentiate between real and fake media. Consequently, we introduce FACTOR, a\npractical recipe for deepfake fact checking and demonstrate its power in\ncritical attack settings: face swapping and audio-visual synthesis. Although it\nis training-free, relies exclusively on off-the-shelf features, is very easy to\nimplement, and does not see any deepfakes, it achieves better than\nstate-of-the-art accuracy.",
        "translated": ""
    },
    {
        "title": "RoboGen: Towards Unleashing Infinite Data for Automated Robot Learning\n  via Generative Simulation",
        "url": "http://arxiv.org/abs/2311.01455v1",
        "pub_date": "2023-11-02",
        "summary": "We present RoboGen, a generative robotic agent that automatically learns\ndiverse robotic skills at scale via generative simulation. RoboGen leverages\nthe latest advancements in foundation and generative models. Instead of\ndirectly using or adapting these models to produce policies or low-level\nactions, we advocate for a generative scheme, which uses these models to\nautomatically generate diversified tasks, scenes, and training supervisions,\nthereby scaling up robotic skill learning with minimal human supervision. Our\napproach equips a robotic agent with a self-guided propose-generate-learn\ncycle: the agent first proposes interesting tasks and skills to develop, and\nthen generates corresponding simulation environments by populating pertinent\nobjects and assets with proper spatial configurations. Afterwards, the agent\ndecomposes the proposed high-level task into sub-tasks, selects the optimal\nlearning approach (reinforcement learning, motion planning, or trajectory\noptimization), generates required training supervision, and then learns\npolicies to acquire the proposed skill. Our work attempts to extract the\nextensive and versatile knowledge embedded in large-scale models and transfer\nthem to the field of robotics. Our fully generative pipeline can be queried\nrepeatedly, producing an endless stream of skill demonstrations associated with\ndiverse tasks and environments.",
        "translated": ""
    },
    {
        "title": "UltraLiDAR: Learning Compact Representations for LiDAR Completion and\n  Generation",
        "url": "http://arxiv.org/abs/2311.01448v1",
        "pub_date": "2023-11-02",
        "summary": "LiDAR provides accurate geometric measurements of the 3D world.\nUnfortunately, dense LiDARs are very expensive and the point clouds captured by\nlow-beam LiDAR are often sparse. To address these issues, we present\nUltraLiDAR, a data-driven framework for scene-level LiDAR completion, LiDAR\ngeneration, and LiDAR manipulation. The crux of UltraLiDAR is a compact,\ndiscrete representation that encodes the point cloud's geometric structure, is\nrobust to noise, and is easy to manipulate. We show that by aligning the\nrepresentation of a sparse point cloud to that of a dense point cloud, we can\ndensify the sparse point clouds as if they were captured by a real high-density\nLiDAR, drastically reducing the cost. Furthermore, by learning a prior over the\ndiscrete codebook, we can generate diverse, realistic LiDAR point clouds for\nself-driving. We evaluate the effectiveness of UltraLiDAR on sparse-to-dense\nLiDAR completion and LiDAR generation. Experiments show that densifying\nreal-world point clouds with our approach can significantly improve the\nperformance of downstream perception systems. Compared to prior art on LiDAR\ngeneration, our approach generates much more realistic point clouds. According\nto A/B test, over 98.5\\% of the time human participants prefer our results over\nthose of previous methods.",
        "translated": ""
    },
    {
        "title": "CADSim: Robust and Scalable in-the-wild 3D Reconstruction for\n  Controllable Sensor Simulation",
        "url": "http://arxiv.org/abs/2311.01447v1",
        "pub_date": "2023-11-02",
        "summary": "Realistic simulation is key to enabling safe and scalable development of %\nself-driving vehicles. A core component is simulating the sensors so that the\nentire autonomy system can be tested in simulation. Sensor simulation involves\nmodeling traffic participants, such as vehicles, with high quality appearance\nand articulated geometry, and rendering them in real time. The self-driving\nindustry has typically employed artists to build these assets. However, this is\nexpensive, slow, and may not reflect reality. Instead, reconstructing assets\nautomatically from sensor data collected in the wild would provide a better\npath to generating a diverse and large set with good real-world coverage.\nNevertheless, current reconstruction approaches struggle on in-the-wild sensor\ndata, due to its sparsity and noise. To tackle these issues, we present CADSim,\nwhich combines part-aware object-class priors via a small set of CAD models\nwith differentiable rendering to automatically reconstruct vehicle geometry,\nincluding articulated wheels, with high-quality appearance. Our experiments\nshow our method recovers more accurate shapes from sparse data compared to\nexisting approaches. Importantly, it also trains and renders efficiently. We\ndemonstrate our reconstructed vehicles in several applications, including\naccurate testing of autonomy perception systems.",
        "translated": ""
    },
    {
        "title": "Adv3D: Generating Safety-Critical 3D Objects through Closed-Loop\n  Simulation",
        "url": "http://arxiv.org/abs/2311.01446v1",
        "pub_date": "2023-11-02",
        "summary": "Self-driving vehicles (SDVs) must be rigorously tested on a wide range of\nscenarios to ensure safe deployment. The industry typically relies on\nclosed-loop simulation to evaluate how the SDV interacts on a corpus of\nsynthetic and real scenarios and verify it performs properly. However, they\nprimarily only test the system's motion planning module, and only consider\nbehavior variations. It is key to evaluate the full autonomy system in\nclosed-loop, and to understand how variations in sensor data based on scene\nappearance, such as the shape of actors, affect system performance. In this\npaper, we propose a framework, Adv3D, that takes real world scenarios and\nperforms closed-loop sensor simulation to evaluate autonomy performance, and\nfinds vehicle shapes that make the scenario more challenging, resulting in\nautonomy failures and uncomfortable SDV maneuvers. Unlike prior works that add\ncontrived adversarial shapes to vehicle roof-tops or roadside to harm\nperception only, we optimize a low-dimensional shape representation to modify\nthe vehicle shape itself in a realistic manner to degrade autonomy performance\n(e.g., perception, prediction, and motion planning). Moreover, we find that the\nshape variations found with Adv3D optimized in closed-loop are much more\neffective than those in open-loop, demonstrating the importance of finding\nscene appearance variations that affect autonomy in the interactive setting.",
        "translated": ""
    },
    {
        "title": "LabelFormer: Object Trajectory Refinement for Offboard Perception from\n  LiDAR Point Clouds",
        "url": "http://arxiv.org/abs/2311.01444v1",
        "pub_date": "2023-11-02",
        "summary": "A major bottleneck to scaling-up training of self-driving perception systems\nare the human annotations required for supervision. A promising alternative is\nto leverage \"auto-labelling\" offboard perception models that are trained to\nautomatically generate annotations from raw LiDAR point clouds at a fraction of\nthe cost. Auto-labels are most commonly generated via a two-stage approach --\nfirst objects are detected and tracked over time, and then each object\ntrajectory is passed to a learned refinement model to improve accuracy. Since\nexisting refinement models are overly complex and lack advanced temporal\nreasoning capabilities, in this work we propose LabelFormer, a simple,\nefficient, and effective trajectory-level refinement approach. Our approach\nfirst encodes each frame's observations separately, then exploits\nself-attention to reason about the trajectory with full temporal context, and\nfinally decodes the refined object size and per-frame poses. Evaluation on both\nurban and highway datasets demonstrates that LabelFormer outperforms existing\nworks by a large margin. Finally, we show that training on a dataset augmented\nwith auto-labels generated by our method leads to improved downstream detection\nperformance compared to existing methods. Please visit the project website for\ndetails https://waabi.ai/labelformer",
        "translated": ""
    },
    {
        "title": "Distilling Out-of-Distribution Robustness from Vision-Language\n  Foundation Models",
        "url": "http://arxiv.org/abs/2311.01441v1",
        "pub_date": "2023-11-02",
        "summary": "We propose a conceptually simple and lightweight framework for improving the\nrobustness of vision models through the combination of knowledge distillation\nand data augmentation. We address the conjecture that larger models do not make\nfor better teachers by showing strong gains in out-of-distribution robustness\nwhen distilling from pretrained foundation models. Following this finding, we\npropose Discrete Adversarial Distillation (DAD), which leverages a robust\nteacher to generate adversarial examples and a VQGAN to discretize them,\ncreating more informative samples than standard data augmentation techniques.\nWe provide a theoretical framework for the use of a robust teacher in the\nknowledge distillation with data augmentation setting and demonstrate strong\ngains in out-of-distribution robustness and clean accuracy across different\nstudent architectures. Notably, our method adds minor computational overhead\ncompared to similar techniques and can be easily combined with other data\naugmentations for further improvements.",
        "translated": ""
    },
    {
        "title": "Transformation Decoupling Strategy based on Screw Theory for\n  Deterministic Point Cloud Registration with Gravity Prior",
        "url": "http://arxiv.org/abs/2311.01432v1",
        "pub_date": "2023-11-02",
        "summary": "Point cloud registration is challenging in the presence of heavy outlier\ncorrespondences. This paper focuses on addressing the robust\ncorrespondence-based registration problem with gravity prior that often arises\nin practice. The gravity directions are typically obtained by inertial\nmeasurement units (IMUs) and can reduce the degree of freedom (DOF) of rotation\nfrom 3 to 1. We propose a novel transformation decoupling strategy by\nleveraging screw theory. This strategy decomposes the original 4-DOF problem\ninto three sub-problems with 1-DOF, 2-DOF, and 1-DOF, respectively, thereby\nenhancing the computation efficiency. Specifically, the first 1-DOF represents\nthe translation along the rotation axis and we propose an interval\nstabbing-based method to solve it. The second 2-DOF represents the pole which\nis an auxiliary variable in screw theory and we utilize a branch-and-bound\nmethod to solve it. The last 1-DOF represents the rotation angle and we propose\na global voting method for its estimation. The proposed method sequentially\nsolves three consensus maximization sub-problems, leading to efficient and\ndeterministic registration. In particular, it can even handle the\ncorrespondence-free registration problem due to its significant robustness.\nExtensive experiments on both synthetic and real-world datasets demonstrate\nthat our method is more efficient and robust than state-of-the-art methods,\neven when dealing with outlier rates exceeding 99%.",
        "translated": ""
    },
    {
        "title": "EmerNeRF: Emergent Spatial-Temporal Scene Decomposition via\n  Self-Supervision",
        "url": "http://arxiv.org/abs/2311.02077v1",
        "pub_date": "2023-11-03",
        "summary": "We present EmerNeRF, a simple yet powerful approach for learning\nspatial-temporal representations of dynamic driving scenes. Grounded in neural\nfields, EmerNeRF simultaneously captures scene geometry, appearance, motion,\nand semantics via self-bootstrapping. EmerNeRF hinges upon two core components:\nFirst, it stratifies scenes into static and dynamic fields. This decomposition\nemerges purely from self-supervision, enabling our model to learn from general,\nin-the-wild data sources. Second, EmerNeRF parameterizes an induced flow field\nfrom the dynamic field and uses this flow field to further aggregate\nmulti-frame features, amplifying the rendering precision of dynamic objects.\nCoupling these three fields (static, dynamic, and flow) enables EmerNeRF to\nrepresent highly-dynamic scenes self-sufficiently, without relying on ground\ntruth object annotations or pre-trained models for dynamic object segmentation\nor optical flow estimation. Our method achieves state-of-the-art performance in\nsensor simulation, significantly outperforming previous methods when\nreconstructing static (+2.93 PSNR) and dynamic (+3.70 PSNR) scenes. In\naddition, to bolster EmerNeRF's semantic generalization, we lift 2D visual\nfoundation model features into 4D space-time and address a general positional\nbias in modern Transformers, significantly boosting 3D perception performance\n(e.g., 37.50% relative improvement in occupancy prediction accuracy on\naverage). Finally, we construct a diverse and challenging 120-sequence dataset\nto benchmark neural fields under extreme and highly-dynamic settings.",
        "translated": ""
    },
    {
        "title": "Learning Historical Status Prompt for Accurate and Robust Visual\n  Tracking",
        "url": "http://arxiv.org/abs/2311.02072v1",
        "pub_date": "2023-11-03",
        "summary": "Most trackers perform template and search region similarity matching to find\nthe most similar object to the template during tracking. However, they struggle\nto make prediction when the target appearance changes due to the limited\nhistorical information introduced by roughly cropping the current search region\nbased on the predicted result of previous frame. In this paper, we identify\nthat the central impediment to improving the performance of existing trackers\nis the incapacity to integrate abundant and effective historical information.\nTo address this issue, we propose a Historical Information Prompter (HIP) to\nenhance the provision of historical information. We also build HIPTrack upon\nHIP module. HIP is a plug-and-play module that make full use of search region\nfeatures to introduce historical appearance information. It also incorporates\nhistorical position information by constructing refined mask of the target. HIP\nis a lightweight module to generate historical information prompts. By\nintegrating historical information prompts, HIPTrack significantly enhances the\ntracking performance without the need to retrain the backbone. Experimental\nresults demonstrate that our method outperforms all state-of-the-art approaches\non LaSOT, LaSOT ext, GOT10k and NfS. Futhermore, HIP module exhibits strong\ngenerality and can be seamlessly integrated into trackers to improve tracking\nperformance. The source code and models will be released for further research.",
        "translated": ""
    },
    {
        "title": "LOTUS: Continual Imitation Learning for Robot Manipulation Through\n  Unsupervised Skill Discovery",
        "url": "http://arxiv.org/abs/2311.02058v1",
        "pub_date": "2023-11-03",
        "summary": "We introduce LOTUS, a continual imitation learning algorithm that empowers a\nphysical robot to continuously and efficiently learn to solve new manipulation\ntasks throughout its lifespan. The core idea behind LOTUS is constructing an\never-growing skill library from a sequence of new tasks with a small number of\nhuman demonstrations. LOTUS starts with a continual skill discovery process\nusing an open-vocabulary vision model, which extracts skills as recurring\npatterns presented in unsegmented demonstrations. Continual skill discovery\nupdates existing skills to avoid catastrophic forgetting of previous tasks and\nadds new skills to solve novel tasks. LOTUS trains a meta-controller that\nflexibly composes various skills to tackle vision-based manipulation tasks in\nthe lifelong learning process. Our comprehensive experiments show that LOTUS\noutperforms state-of-the-art baselines by over 11% in success rate, showing its\nsuperior knowledge transfer ability compared to prior methods. More results and\nvideos can be found on the project website:\nhttps://ut-austin-rpl.github.io/Lotus/.",
        "translated": ""
    },
    {
        "title": "Occlusion-Aware 2D and 3D Centerline Detection for Urban Driving via\n  Automatic Label Generation",
        "url": "http://arxiv.org/abs/2311.02044v1",
        "pub_date": "2023-11-03",
        "summary": "This research work seeks to explore and identify strategies that can\ndetermine road topology information in 2D and 3D under highly dynamic urban\ndriving scenarios. To facilitate this exploration, we introduce a substantial\ndataset comprising nearly one million automatically labeled data frames. A key\ncontribution of our research lies in developing an automatic label-generation\nprocess and an occlusion handling strategy. This strategy is designed to model\na wide range of occlusion scenarios, from mild disruptions to severe blockages.\nFurthermore, we present a comprehensive ablation study wherein multiple\ncenterline detection methods are developed and evaluated. This analysis not\nonly benchmarks the performance of various approaches but also provides\nvaluable insights into the interpretability of these methods. Finally, we\ndemonstrate the practicality of our methods and assess their adaptability\nacross different sensor configurations, highlighting their versatility and\nrelevance in real-world scenarios. Our dataset and experimental models are\npublicly available.",
        "translated": ""
    },
    {
        "title": "VQPy: An Object-Oriented Approach to Modern Video Analytics",
        "url": "http://arxiv.org/abs/2311.01623v1",
        "pub_date": "2023-11-03",
        "summary": "Video analytics is widely used in contemporary systems and services. At the\nforefront of video analytics are video queries that users develop to find\nobjects of particular interest. Building upon the insight that video objects\n(e.g., human, animals, cars, etc.), the center of video analytics, are similar\nin spirit to objects modeled by traditional object-oriented languages, we\npropose to develop an object-oriented approach to video analytics. This\napproach, named VQPy, consists of a frontend$\\unicode{x2015}$a Python variant\nwith constructs that make it easy for users to express video objects and their\ninteractions$\\unicode{x2015}$as well as an extensible backend that can\nautomatically construct and optimize pipelines based on video objects. We have\nimplemented and open-sourced VQPy, which has been productized in Cisco as part\nof its DeepVision framework.",
        "translated": ""
    },
    {
        "title": "Active Reasoning in an Open-World Environment",
        "url": "http://arxiv.org/abs/2311.02018v1",
        "pub_date": "2023-11-03",
        "summary": "Recent advances in vision-language learning have achieved notable success on\ncomplete-information question-answering datasets through the integration of\nextensive world knowledge. Yet, most models operate passively, responding to\nquestions based on pre-stored knowledge. In stark contrast, humans possess the\nability to actively explore, accumulate, and reason using both newfound and\nexisting information to tackle incomplete-information questions. In response to\nthis gap, we introduce $Conan$, an interactive open-world environment devised\nfor the assessment of active reasoning. $Conan$ facilitates active exploration\nand promotes multi-round abductive inference, reminiscent of rich, open-world\nsettings like Minecraft. Diverging from previous works that lean primarily on\nsingle-round deduction via instruction following, $Conan$ compels agents to\nactively interact with their surroundings, amalgamating new evidence with prior\nknowledge to elucidate events from incomplete observations. Our analysis on\n$Conan$ underscores the shortcomings of contemporary state-of-the-art models in\nactive exploration and understanding complex scenarios. Additionally, we\nexplore Abduction from Deduction, where agents harness Bayesian rules to recast\nthe challenge of abduction as a deductive process. Through $Conan$, we aim to\ngalvanize advancements in active reasoning and set the stage for the next\ngeneration of artificial intelligence agents adept at dynamically engaging in\nenvironments.",
        "translated": ""
    },
    {
        "title": "Towards Unsupervised Object Detection From LiDAR Point Clouds",
        "url": "http://arxiv.org/abs/2311.02007v1",
        "pub_date": "2023-11-03",
        "summary": "In this paper, we study the problem of unsupervised object detection from 3D\npoint clouds in self-driving scenes. We present a simple yet effective method\nthat exploits (i) point clustering in near-range areas where the point clouds\nare dense, (ii) temporal consistency to filter out noisy unsupervised\ndetections, (iii) translation equivariance of CNNs to extend the auto-labels to\nlong range, and (iv) self-supervision for improving on its own. Our approach,\nOYSTER (Object Discovery via Spatio-Temporal Refinement), does not impose\nconstraints on data collection (such as repeated traversals of the same\nlocation), is able to detect objects in a zero-shot manner without supervised\nfinetuning (even in sparse, distant regions), and continues to self-improve\ngiven more rounds of iterative self-training. To better measure model\nperformance in self-driving scenarios, we propose a new planning-centric\nperception metric based on distance-to-collision. We demonstrate that our\nunsupervised object detector significantly outperforms unsupervised baselines\non PandaSet and Argoverse 2 Sensor dataset, showing promise that\nself-supervision combined with object priors can enable object discovery in the\nwild. For more information, visit the project website:\nhttps://waabi.ai/research/oyster",
        "translated": ""
    },
    {
        "title": "A Structured Pruning Algorithm for Model-based Deep Learning",
        "url": "http://arxiv.org/abs/2311.02003v1",
        "pub_date": "2023-11-03",
        "summary": "There is a growing interest in model-based deep learning (MBDL) for solving\nimaging inverse problems. MBDL networks can be seen as iterative algorithms\nthat estimate the desired image using a physical measurement model and a\nlearned image prior specified using a convolutional neural net (CNNs). The\niterative nature of MBDL networks increases the test-time computational\ncomplexity, which limits their applicability in certain large-scale\napplications. We address this issue by presenting structured pruning algorithm\nfor model-based deep learning (SPADE) as the first structured pruning algorithm\nfor MBDL networks. SPADE reduces the computational complexity of CNNs used\nwithin MBDL networks by pruning its non-essential weights. We propose three\ndistinct strategies to fine-tune the pruned MBDL networks to minimize the\nperformance loss. Each fine-tuning strategy has a unique benefit that depends\non the presence of a pre-trained model and a high-quality ground truth. We\nvalidate SPADE on two distinct inverse problems, namely compressed sensing MRI\nand image super-resolution. Our results highlight that MBDL models pruned by\nSPADE can achieve substantial speed up in testing time while maintaining\ncompetitive performance.",
        "translated": ""
    },
    {
        "title": "Detection of keratoconus Diseases using deep Learning",
        "url": "http://arxiv.org/abs/2311.01996v1",
        "pub_date": "2023-11-03",
        "summary": "One of the most serious corneal disorders, keratoconus is difficult to\ndiagnose in its early stages and can result in blindness. This illness, which\noften appears in the second decade of life, affects people of all sexes and\nraces. Convolutional neural networks (CNNs), one of the deep learning\napproaches, have recently come to light as particularly promising tools for the\naccurate and timely diagnosis of keratoconus. The purpose of this study was to\nevaluate how well different D-CNN models identified keratoconus-related\ndiseases. To be more precise, we compared five different CNN-based deep\nlearning architectures (DenseNet201, InceptionV3, MobileNetV2, VGG19,\nXception). In our comprehensive experimental analysis, the DenseNet201-based\nmodel performed very well in keratoconus disease identification in our\nextensive experimental research. This model outperformed its D-CNN equivalents,\nwith an astounding accuracy rate of 89.14% in three crucial classes:\nKeratoconus, Normal, and Suspect. The results demonstrate not only the\nstability and robustness of the model but also its practical usefulness in\nreal-world applications for accurate and dependable keratoconus identification.\nIn addition, D-CNN DenseNet201 performs extraordinarily well in terms of\nprecision, recall rates, and F1 scores in addition to accuracy. These measures\nvalidate the model's usefulness as an effective diagnostic tool by highlighting\nits capacity to reliably detect instances of keratoconus and to reduce false\npositives and negatives.",
        "translated": ""
    },
    {
        "title": "Leveraging Large-Scale Pretrained Vision Foundation Models for\n  Label-Efficient 3D Point Cloud Segmentation",
        "url": "http://arxiv.org/abs/2311.01989v1",
        "pub_date": "2023-11-03",
        "summary": "Recently, large-scale pre-trained models such as Segment-Anything Model (SAM)\nand Contrastive Language-Image Pre-training (CLIP) have demonstrated remarkable\nsuccess and revolutionized the field of computer vision. These foundation\nvision models effectively capture knowledge from a large-scale broad data with\ntheir vast model parameters, enabling them to perform zero-shot segmentation on\npreviously unseen data without additional training. While they showcase\ncompetence in 2D tasks, their potential for enhancing 3D scene understanding\nremains relatively unexplored. To this end, we present a novel framework that\nadapts various foundational models for the 3D point cloud segmentation task.\nOur approach involves making initial predictions of 2D semantic masks using\ndifferent large vision models. We then project these mask predictions from\nvarious frames of RGB-D video sequences into 3D space. To generate robust 3D\nsemantic pseudo labels, we introduce a semantic label fusion strategy that\neffectively combines all the results via voting. We examine diverse scenarios,\nlike zero-shot learning and limited guidance from sparse 2D point labels, to\nassess the pros and cons of different vision foundation models. Our approach is\nexperimented on ScanNet dataset for 3D indoor scenes, and the results\ndemonstrate the effectiveness of adopting general 2D foundation models on\nsolving 3D point cloud segmentation tasks.",
        "translated": ""
    },
    {
        "title": "Exploitation-Guided Exploration for Semantic Embodied Navigation",
        "url": "http://arxiv.org/abs/2311.03357v1",
        "pub_date": "2023-11-06",
        "summary": "In the recent progress in embodied navigation and sim-to-robot transfer,\nmodular policies have emerged as a de facto framework. However, there is more\nto compositionality beyond the decomposition of the learning load into modular\ncomponents. In this work, we investigate a principled way to syntactically\ncombine these components. Particularly, we propose Exploitation-Guided\nExploration (XGX) where separate modules for exploration and exploitation come\ntogether in a novel and intuitive manner. We configure the exploitation module\nto take over in the deterministic final steps of navigation i.e. when the goal\nbecomes visible. Crucially, an exploitation module teacher-forces the\nexploration module and continues driving an overridden policy optimization.\nXGX, with effective decomposition and novel guidance, improves the\nstate-of-the-art performance on the challenging object navigation task from 70%\nto 73%. Along with better accuracy, through targeted analysis, we show that XGX\nis also more efficient at goal-conditioned exploration. Finally, we show\nsim-to-real transfer to robot hardware and XGX performs over two-fold better\nthan the best baseline from simulation benchmarking. Project page:\nxgxvisnav.github.io",
        "translated": ""
    },
    {
        "title": "SegGen: Supercharging Segmentation Models with Text2Mask and Mask2Img\n  Synthesis",
        "url": "http://arxiv.org/abs/2311.03355v1",
        "pub_date": "2023-11-06",
        "summary": "We propose SegGen, a highly-effective training data generation method for\nimage segmentation, which pushes the performance limits of state-of-the-art\nsegmentation models to a significant extent. SegGen designs and integrates two\ndata generation strategies: MaskSyn and ImgSyn. (i) MaskSyn synthesizes new\nmask-image pairs via our proposed text-to-mask generation model and\nmask-to-image generation model, greatly improving the diversity in segmentation\nmasks for model supervision; (ii) ImgSyn synthesizes new images based on\nexisting masks using the mask-to-image generation model, strongly improving\nimage diversity for model inputs. On the highly competitive ADE20K and COCO\nbenchmarks, our data generation method markedly improves the performance of\nstate-of-the-art segmentation models in semantic segmentation, panoptic\nsegmentation, and instance segmentation. Notably, in terms of the ADE20K mIoU,\nMask2Former R50 is largely boosted from 47.2 to 49.9 (+2.7); Mask2Former Swin-L\nis also significantly increased from 56.1 to 57.4 (+1.3). These promising\nresults strongly suggest the effectiveness of our SegGen even when abundant\nhuman-annotated training data is utilized. Moreover, training with our\nsynthetic data makes the segmentation models more robust towards unseen\ndomains. Project website: https://seggenerator.github.io",
        "translated": ""
    },
    {
        "title": "GLaMM: Pixel Grounding Large Multimodal Model",
        "url": "http://arxiv.org/abs/2311.03356v1",
        "pub_date": "2023-11-06",
        "summary": "Large Multimodal Models (LMMs) extend Large Language Models to the vision\ndomain. Initial efforts towards LMMs used holistic images and text prompts to\ngenerate ungrounded textual responses. Very recently, region-level LMMs have\nbeen used to generate visually grounded responses. However, they are limited to\nonly referring a single object category at a time, require users to specify the\nregions in inputs, or cannot offer dense pixel-wise object grounding. In this\nwork, we present Grounding LMM (GLaMM), the first model that can generate\nnatural language responses seamlessly intertwined with corresponding object\nsegmentation masks. GLaMM not only grounds objects appearing in the\nconversations but is flexible enough to accept both textual and optional visual\nprompts (region of interest) as input. This empowers users to interact with the\nmodel at various levels of granularity, both in textual and visual domains. Due\nto the lack of standard benchmarks for the novel setting of generating visually\ngrounded detailed conversations, we introduce a comprehensive evaluation\nprotocol with our curated grounded conversations. Our proposed Grounded\nConversation Generation (GCG) task requires densely grounded concepts in\nnatural scenes at a large-scale. To this end, we propose a densely annotated\nGrounding-anything Dataset (GranD) using our proposed automated annotation\npipeline that encompasses 7.5M unique concepts grounded in a total of 810M\nregions available with segmentation masks. Besides GCG, GLaMM also performs\neffectively on several downstream tasks e.g., referring expression\nsegmentation, image and region-level captioning and vision-language\nconversations. Project Page: https://mbzuai-oryx.github.io/groundingLMM.",
        "translated": ""
    },
    {
        "title": "CoVLM: Composing Visual Entities and Relationships in Large Language\n  Models Via Communicative Decoding",
        "url": "http://arxiv.org/abs/2311.03354v1",
        "pub_date": "2023-11-06",
        "summary": "A remarkable ability of human beings resides in compositional reasoning,\ni.e., the capacity to make \"infinite use of finite means\". However, current\nlarge vision-language foundation models (VLMs) fall short of such compositional\nabilities due to their \"bag-of-words\" behaviors and inability to construct\nwords that correctly represent visual entities and the relations among the\nentities. To this end, we propose CoVLM, which can guide the LLM to explicitly\ncompose visual entities and relationships among the text and dynamically\ncommunicate with the vision encoder and detection network to achieve\nvision-language communicative decoding. Specifically, we first devise a set of\nnovel communication tokens for the LLM, for dynamic communication between the\nvisual detection system and the language system. A communication token is\ngenerated by the LLM following a visual entity or a relation, to inform the\ndetection network to propose regions that are relevant to the sentence\ngenerated so far. The proposed regions-of-interests (ROIs) are then fed back\ninto the LLM for better language generation contingent on the relevant regions.\nThe LLM is thus able to compose the visual entities and relationships through\nthe communication tokens. The vision-to-language and language-to-vision\ncommunication are iteratively performed until the entire sentence is generated.\nOur framework seamlessly bridges the gap between visual perception and LLMs and\noutperforms previous VLMs by a large margin on compositional reasoning\nbenchmarks (e.g., ~20% in HICO-DET mAP, ~14% in Cola top-1 accuracy, and ~3% on\nARO top-1 accuracy). We also achieve state-of-the-art performances on\ntraditional vision-language tasks such as referring expression comprehension\nand visual question answering.",
        "translated": ""
    },
    {
        "title": "Rethinking Evaluation Metrics of Open-Vocabulary Segmentaion",
        "url": "http://arxiv.org/abs/2311.03352v1",
        "pub_date": "2023-11-06",
        "summary": "In this paper, we highlight a problem of evaluation metrics adopted in the\nopen-vocabulary segmentation. That is, the evaluation process still heavily\nrelies on closed-set metrics on zero-shot or cross-dataset pipelines without\nconsidering the similarity between predicted and ground truth categories. To\ntackle this issue, we first survey eleven similarity measurements between two\ncategorical words using WordNet linguistics statistics, text embedding, and\nlanguage models by comprehensive quantitative analysis and user study. Built\nupon those explored measurements, we designed novel evaluation metrics, namely\nOpen mIoU, Open AP, and Open PQ, tailored for three open-vocabulary\nsegmentation tasks. We benchmarked the proposed evaluation metrics on 12\nopen-vocabulary methods of three segmentation tasks. Even though the relative\nsubjectivity of similarity distance, we demonstrate that our metrics can still\nwell evaluate the open ability of the existing open-vocabulary segmentation\nmethods. We hope that our work can bring with the community new thinking about\nhow to evaluate the open ability of models. The evaluation code is released in\ngithub.",
        "translated": ""
    },
    {
        "title": "Long-Term Invariant Local Features via Implicit Cross-Domain\n  Correspondences",
        "url": "http://arxiv.org/abs/2311.03345v1",
        "pub_date": "2023-11-06",
        "summary": "Modern learning-based visual feature extraction networks perform well in\nintra-domain localization, however, their performance significantly declines\nwhen image pairs are captured across long-term visual domain variations, such\nas different seasonal and daytime variations. In this paper, our first\ncontribution is a benchmark to investigate the performance impact of long-term\nvariations on visual localization. We conduct a thorough analysis of the\nperformance of current state-of-the-art feature extraction networks under\nvarious domain changes and find a significant performance gap between intra-\nand cross-domain localization. We investigate different methods to close this\ngap by improving the supervision of modern feature extractor networks. We\npropose a novel data-centric method, Implicit Cross-Domain Correspondences\n(iCDC). iCDC represents the same environment with multiple Neural Radiance\nFields, each fitting the scene under individual visual domains. It utilizes the\nunderlying 3D representations to generate accurate correspondences across\ndifferent long-term visual conditions. Our proposed method enhances\ncross-domain localization performance, significantly reducing the performance\ngap. When evaluated on popular long-term localization benchmarks, our trained\nnetworks consistently outperform existing methods. This work serves as a\nsubstantial stride toward more robust visual localization pipelines for\nlong-term deployments, and opens up research avenues in the development of\nlong-term invariant descriptors.",
        "translated": ""
    },
    {
        "title": "FLOGA: A machine learning ready dataset, a benchmark and a novel deep\n  learning model for burnt area mapping with Sentinel-2",
        "url": "http://arxiv.org/abs/2311.03339v1",
        "pub_date": "2023-11-06",
        "summary": "Over the last decade there has been an increasing frequency and intensity of\nwildfires across the globe, posing significant threats to human and animal\nlives, ecosystems, and socio-economic stability. Therefore urgent action is\nrequired to mitigate their devastating impact and safeguard Earth's natural\nresources. Robust Machine Learning methods combined with the abundance of\nhigh-resolution satellite imagery can provide accurate and timely mappings of\nthe affected area in order to assess the scale of the event, identify the\nimpacted assets and prioritize and allocate resources effectively for the\nproper restoration of the damaged region. In this work, we create and introduce\na machine-learning ready dataset we name FLOGA (Forest wiLdfire Observations\nfor the Greek Area). This dataset is unique as it comprises of satellite\nimagery acquired before and after a wildfire event, it contains information\nfrom Sentinel-2 and MODIS modalities with variable spatial and spectral\nresolution, and contains a large number of events where the corresponding burnt\narea ground truth has been annotated by domain experts. FLOGA covers the wider\nregion of Greece, which is characterized by a Mediterranean landscape and\nclimatic conditions. We use FLOGA to provide a thorough comparison of multiple\nMachine Learning and Deep Learning algorithms for the automatic extraction of\nburnt areas, approached as a change detection task. We also compare the results\nto those obtained using standard specialized spectral indices for burnt area\nmapping. Finally, we propose a novel Deep Learning model, namely BAM-CD. Our\nbenchmark results demonstrate the efficacy of the proposed technique in the\nautomatic extraction of burnt areas, outperforming all other methods in terms\nof accuracy and robustness. Our dataset and code are publicly available at:\nhttps://github.com/Orion-AI-Lab/FLOGA.",
        "translated": ""
    },
    {
        "title": "Cross-Image Attention for Zero-Shot Appearance Transfer",
        "url": "http://arxiv.org/abs/2311.03335v1",
        "pub_date": "2023-11-06",
        "summary": "Recent advancements in text-to-image generative models have demonstrated a\nremarkable ability to capture a deep semantic understanding of images. In this\nwork, we leverage this semantic knowledge to transfer the visual appearance\nbetween objects that share similar semantics but may differ significantly in\nshape. To achieve this, we build upon the self-attention layers of these\ngenerative models and introduce a cross-image attention mechanism that\nimplicitly establishes semantic correspondences across images. Specifically,\ngiven a pair of images -- one depicting the target structure and the other\nspecifying the desired appearance -- our cross-image attention combines the\nqueries corresponding to the structure image with the keys and values of the\nappearance image. This operation, when applied during the denoising process,\nleverages the established semantic correspondences to generate an image\ncombining the desired structure and appearance. In addition, to improve the\noutput image quality, we harness three mechanisms that either manipulate the\nnoisy latent codes or the model's internal representations throughout the\ndenoising process. Importantly, our approach is zero-shot, requiring no\noptimization or training. Experiments show that our method is effective across\na wide range of object categories and is robust to variations in shape, size,\nand viewpoint between the two input images.",
        "translated": ""
    },
    {
        "title": "A Robust Bi-Directional Algorithm For People Count In Crowded Areas",
        "url": "http://arxiv.org/abs/2311.03323v1",
        "pub_date": "2023-11-06",
        "summary": "People counting system in crowded places has become a very useful practical\napplication that can be accomplished in various ways which include many\ntraditional methods using sensors. Examining the case of real time scenarios,\nthe algorithm espoused should be steadfast and accurate. People counting\nalgorithm presented in this paper, is centered on blob assessment, devoted to\nyield the count of the people through a path along with the direction of\ntraversal. The system depicted is often ensconced at the entrance of a building\nso that the unmitigated frequency of visitors can be recorded. The core premise\nof this work is to extricate count of people inflow and outflow pertaining to a\nparticular area. The tot-up achieved can be exploited for purpose of statistics\nin the circumstances of any calamity occurrence in that zone. Relying upon the\ncount totaled, the population in that vicinity can be assimilated in order to\ntake on relevant measures to rescue the people.",
        "translated": ""
    },
    {
        "title": "FATE: Feature-Agnostic Transformer-based Encoder for learning\n  generalized embedding spaces in flow cytometry data",
        "url": "http://arxiv.org/abs/2311.03314v1",
        "pub_date": "2023-11-06",
        "summary": "While model architectures and training strategies have become more generic\nand flexible with respect to different data modalities over the past years, a\npersistent limitation lies in the assumption of fixed quantities and\narrangements of input features. This limitation becomes particularly relevant\nin scenarios where the attributes captured during data acquisition vary across\ndifferent samples. In this work, we aim at effectively leveraging data with\nvarying features, without the need to constrain the input space to the\nintersection of potential feature sets or to expand it to their union. We\npropose a novel architecture that can directly process data without the\nnecessity of aligned feature modalities by learning a general embedding space\nthat captures the relationship between features across data samples with\nvarying sets of features. This is achieved via a set-transformer architecture\naugmented by feature-encoder layers, thereby enabling the learning of a shared\nlatent feature space from data originating from heterogeneous feature spaces.\nThe advantages of the model are demonstrated for automatic cancer cell\ndetection in acute myeloid leukemia in flow cytometry data, where the features\nmeasured during acquisition often vary between samples. Our proposed\narchitecture's capacity to operate seamlessly across incongruent feature spaces\nis particularly relevant in this context, where data scarcity arises from the\nlow prevalence of the disease. The code is available for research purposes at\nhttps://github.com/lisaweijler/FATE.",
        "translated": ""
    },
    {
        "title": "OtterHD: A High-Resolution Multi-modality Model",
        "url": "http://arxiv.org/abs/2311.04219v1",
        "pub_date": "2023-11-07",
        "summary": "In this paper, we present OtterHD-8B, an innovative multimodal model evolved\nfrom Fuyu-8B, specifically engineered to interpret high-resolution visual\ninputs with granular precision. Unlike conventional models that are constrained\nby fixed-size vision encoders, OtterHD-8B boasts the ability to handle flexible\ninput dimensions, ensuring its versatility across various inference\nrequirements. Alongside this model, we introduce MagnifierBench, an evaluation\nframework designed to scrutinize models' ability to discern minute details and\nspatial relationships of small objects. Our comparative analysis reveals that\nwhile current leading models falter on this benchmark, OtterHD-8B, particularly\nwhen directly processing high-resolution inputs, outperforms its counterparts\nby a substantial margin. The findings illuminate the structural variances in\nvisual information processing among different models and the influence that the\nvision encoders' pre-training resolution disparities have on model\neffectiveness within such benchmarks. Our study highlights the critical role of\nflexibility and high-resolution input capabilities in large multimodal models\nand also exemplifies the potential inherent in the Fuyu architecture's\nsimplicity for handling complex visual data.",
        "translated": ""
    },
    {
        "title": "Towards Garment Sewing Pattern Reconstruction from a Single Image",
        "url": "http://arxiv.org/abs/2311.04218v1",
        "pub_date": "2023-11-07",
        "summary": "Garment sewing pattern represents the intrinsic rest shape of a garment, and\nis the core for many applications like fashion design, virtual try-on, and\ndigital avatars. In this work, we explore the challenging problem of recovering\ngarment sewing patterns from daily photos for augmenting these applications. To\nsolve the problem, we first synthesize a versatile dataset, named SewFactory,\nwhich consists of around 1M images and ground-truth sewing patterns for model\ntraining and quantitative evaluation. SewFactory covers a wide range of human\nposes, body shapes, and sewing patterns, and possesses realistic appearances\nthanks to the proposed human texture synthesis network. Then, we propose a\ntwo-level Transformer network called Sewformer, which significantly improves\nthe sewing pattern prediction performance. Extensive experiments demonstrate\nthat the proposed framework is effective in recovering sewing patterns and well\ngeneralizes to casually-taken human photos. Code, dataset, and pre-trained\nmodels are available at: https://sewformer.github.io.",
        "translated": ""
    },
    {
        "title": "Video Instance Matting",
        "url": "http://arxiv.org/abs/2311.04212v1",
        "pub_date": "2023-11-07",
        "summary": "Conventional video matting outputs one alpha matte for all instances\nappearing in a video frame so that individual instances are not distinguished.\nWhile video instance segmentation provides time-consistent instance masks,\nresults are unsatisfactory for matting applications, especially due to applied\nbinarization. To remedy this deficiency, we propose Video Instance\nMatting~(VIM), that is, estimating alpha mattes of each instance at each frame\nof a video sequence. To tackle this challenging problem, we present MSG-VIM, a\nMask Sequence Guided Video Instance Matting neural network, as a novel baseline\nmodel for VIM. MSG-VIM leverages a mixture of mask augmentations to make\npredictions robust to inaccurate and inconsistent mask guidance. It\nincorporates temporal mask and temporal feature guidance to improve the\ntemporal consistency of alpha matte predictions. Furthermore, we build a new\nbenchmark for VIM, called VIM50, which comprises 50 video clips with multiple\nhuman instances as foreground objects. To evaluate performances on the VIM\ntask, we introduce a suitable metric called Video Instance-aware Matting\nQuality~(VIMQ). Our proposed model MSG-VIM sets a strong baseline on the VIM50\nbenchmark and outperforms existing methods by a large margin. The project is\nopen-sourced at https://github.com/SHI-Labs/VIM.",
        "translated": ""
    },
    {
        "title": "Deep Hashing via Householder Quantization",
        "url": "http://arxiv.org/abs/2311.04207v1",
        "pub_date": "2023-11-07",
        "summary": "Hashing is at the heart of large-scale image similarity search, and recent\nmethods have been substantially improved through deep learning techniques. Such\nalgorithms typically learn continuous embeddings of the data. To avoid a\nsubsequent costly binarization step, a common solution is to employ loss\nfunctions that combine a similarity learning term (to ensure similar images are\ngrouped to nearby embeddings) and a quantization penalty term (to ensure that\nthe embedding entries are close to binarized entries, e.g., -1 or 1). Still,\nthe interaction between these two terms can make learning harder and the\nembeddings worse. We propose an alternative quantization strategy that\ndecomposes the learning problem in two stages: first, perform similarity\nlearning over the embedding space with no quantization; second, find an optimal\northogonal transformation of the embeddings so each coordinate of the embedding\nis close to its sign, and then quantize the transformed embedding through the\nsign function. In the second step, we parametrize orthogonal transformations\nusing Householder matrices to efficiently leverage stochastic gradient descent.\nSince similarity measures are usually invariant under orthogonal\ntransformations, this quantization strategy comes at no cost in terms of\nperformance. The resulting algorithm is unsupervised, fast, hyperparameter-free\nand can be run on top of any existing deep hashing or metric learning\nalgorithm. We provide extensive experimental results showing that this approach\nleads to state-of-the-art performance on widely used image datasets, and,\nunlike other quantization strategies, brings consistent improvements in\nperformance to existing deep hashing algorithms.",
        "translated": ""
    },
    {
        "title": "Selective Visual Representations Improve Convergence and Generalization\n  for Embodied AI",
        "url": "http://arxiv.org/abs/2311.04193v1",
        "pub_date": "2023-11-07",
        "summary": "Embodied AI models often employ off the shelf vision backbones like CLIP to\nencode their visual observations. Although such general purpose representations\nencode rich syntactic and semantic information about the scene, much of this\ninformation is often irrelevant to the specific task at hand. This introduces\nnoise within the learning process and distracts the agent's focus from\ntask-relevant visual cues. Inspired by selective attention in humans-the\nprocess through which people filter their perception based on their\nexperiences, knowledge, and the task at hand-we introduce a parameter-efficient\napproach to filter visual stimuli for embodied AI. Our approach induces a\ntask-conditioned bottleneck using a small learnable codebook module. This\ncodebook is trained jointly to optimize task reward and acts as a\ntask-conditioned selective filter over the visual observation. Our experiments\nshowcase state-of-the-art performance for object goal navigation and object\ndisplacement across 5 benchmarks, ProcTHOR, ArchitecTHOR, RoboTHOR, AI2-iTHOR,\nand ManipulaTHOR. The filtered representations produced by the codebook are\nalso able generalize better and converge faster when adapted to other\nsimulation environments such as Habitat. Our qualitative analyses show that\nagents explore their environments more effectively and their representations\nretain task-relevant information like target object recognition while ignoring\nsuperfluous information about other objects. Code and pretrained models are\navailable at our project website: https://embodied-codebook.github.io.",
        "translated": ""
    },
    {
        "title": "JaSPICE: Automatic Evaluation Metric Using Predicate-Argument Structures\n  for Image Captioning Models",
        "url": "http://arxiv.org/abs/2311.04192v1",
        "pub_date": "2023-11-07",
        "summary": "Image captioning studies heavily rely on automatic evaluation metrics such as\nBLEU and METEOR. However, such n-gram-based metrics have been shown to\ncorrelate poorly with human evaluation, leading to the proposal of alternative\nmetrics such as SPICE for English; however, no equivalent metrics have been\nestablished for other languages. Therefore, in this study, we propose an\nautomatic evaluation metric called JaSPICE, which evaluates Japanese captions\nbased on scene graphs. The proposed method generates a scene graph from\ndependencies and the predicate-argument structure, and extends the graph using\nsynonyms. We conducted experiments employing 10 image captioning models trained\non STAIR Captions and PFN-PIC and constructed the Shichimi dataset, which\ncontains 103,170 human evaluations. The results showed that our metric\noutperformed the baseline metrics for the correlation coefficient with the\nhuman evaluation.",
        "translated": ""
    },
    {
        "title": "Outliers with Opposing Signals Have an Outsized Effect on Neural Network\n  Optimization",
        "url": "http://arxiv.org/abs/2311.04163v1",
        "pub_date": "2023-11-07",
        "summary": "We identify a new phenomenon in neural network optimization which arises from\nthe interaction of depth and a particular heavy-tailed structure in natural\ndata. Our result offers intuitive explanations for several previously reported\nobservations about network training dynamics. In particular, it implies a\nconceptually new cause for progressive sharpening and the edge of stability; we\nalso highlight connections to other concepts in optimization and generalization\nincluding grokking, simplicity bias, and Sharpness-Aware Minimization.\n  Experimentally, we demonstrate the significant influence of paired groups of\noutliers in the training data with strong opposing signals: consistent, large\nmagnitude features which dominate the network output throughout training and\nprovide gradients which point in opposite directions. Due to these outliers,\nearly optimization enters a narrow valley which carefully balances the opposing\ngroups; subsequent sharpening causes their loss to rise rapidly, oscillating\nbetween high on one group and then the other, until the overall loss spikes. We\ndescribe how to identify these groups, explore what sets them apart, and\ncarefully study their effect on the network's optimization and behavior. We\ncomplement these experiments with a mechanistic explanation on a toy example of\nopposing signals and a theoretical analysis of a two-layer linear network on a\nsimple model. Our finding enables new qualitative predictions of training\nbehavior which we confirm experimentally. It also provides a new lens through\nwhich to study and improve modern training practices for stochastic\noptimization, which we highlight via a case study of Adam versus SGD.",
        "translated": ""
    },
    {
        "title": "A Simple Interpretable Transformer for Fine-Grained Image Classification\n  and Analysis",
        "url": "http://arxiv.org/abs/2311.04157v1",
        "pub_date": "2023-11-07",
        "summary": "We present a novel usage of Transformers to make image classification\ninterpretable. Unlike mainstream classifiers that wait until the last\nfully-connected layer to incorporate class information to make predictions, we\ninvestigate a proactive approach, asking each class to search for itself in an\nimage. We realize this idea via a Transformer encoder-decoder inspired by\nDEtection TRansformer (DETR). We learn ``class-specific'' queries (one for each\nclass) as input to the decoder, enabling each class to localize its patterns in\nan image via cross-attention. We name our approach INterpretable TRansformer\n(INTR), which is fairly easy to implement and exhibits several compelling\nproperties. We show that INTR intrinsically encourages each class to attend\ndistinctively; the cross-attention weights thus provide a faithful\ninterpretation of the prediction. Interestingly, via ``multi-head''\ncross-attention, INTR could identify different ``attributes'' of a class,\nmaking it particularly suitable for fine-grained classification and analysis,\nwhich we demonstrate on eight datasets. Our code and pre-trained model are\npublicly accessible at https://github.com/Imageomics/INTR.",
        "translated": ""
    },
    {
        "title": "High-fidelity 3D Reconstruction of Plants using Neural Radiance Field",
        "url": "http://arxiv.org/abs/2311.04154v1",
        "pub_date": "2023-11-07",
        "summary": "Accurate reconstruction of plant phenotypes plays a key role in optimising\nsustainable farming practices in the field of Precision Agriculture (PA).\nCurrently, optical sensor-based approaches dominate the field, but the need for\nhigh-fidelity 3D reconstruction of crops and plants in unstructured\nagricultural environments remains challenging. Recently, a promising\ndevelopment has emerged in the form of Neural Radiance Field (NeRF), a novel\nmethod that utilises neural density fields. This technique has shown impressive\nperformance in various novel vision synthesis tasks, but has remained\nrelatively unexplored in the agricultural context. In our study, we focus on\ntwo fundamental tasks within plant phenotyping: (1) the synthesis of 2D\nnovel-view images and (2) the 3D reconstruction of crop and plant models. We\nexplore the world of neural radiance fields, in particular two SOTA methods:\nInstant-NGP, which excels in generating high-quality images with impressive\ntraining and inference speed, and Instant-NSR, which improves the reconstructed\ngeometry by incorporating the Signed Distance Function (SDF) during training.\nIn particular, we present a novel plant phenotype dataset comprising real plant\nimages from production environments. This dataset is a first-of-its-kind\ninitiative aimed at comprehensively exploring the advantages and limitations of\nNeRF in agricultural contexts. Our experimental results show that NeRF\ndemonstrates commendable performance in the synthesis of novel-view images and\nis able to achieve reconstruction results that are competitive with Reality\nCapture, a leading commercial software for 3D Multi-View Stereo (MVS)-based\nreconstruction. However, our study also highlights certain drawbacks of NeRF,\nincluding relatively slow training speeds, performance limitations in cases of\ninsufficient sampling, and challenges in obtaining geometry quality in complex\nsetups.",
        "translated": ""
    },
    {
        "title": "Contactless Fingerprint Biometric Anti-Spoofing: An Unsupervised Deep\n  Learning Approach",
        "url": "http://arxiv.org/abs/2311.04148v1",
        "pub_date": "2023-11-07",
        "summary": "Contactless fingerprint recognition offers a higher level of user comfort and\naddresses hygiene concerns more effectively. However, it is also more\nvulnerable to presentation attacks such as photo paper, paper-printout, and\nvarious display attacks, which makes it more challenging to implement in\nbiometric systems compared to contact-based modalities. Limited research has\nbeen conducted on presentation attacks in contactless fingerprint systems, and\nthese studies have encountered challenges in terms of generalization and\nscalability since both bonafide samples and presentation attacks are utilized\nduring training model. Although this approach appears promising, it lacks the\nability to handle unseen attacks, which is a crucial factor for developing PAD\nmethods that can generalize effectively. We introduced an innovative\nanti-spoofing approach that combines an unsupervised autoencoder with a\nconvolutional block attention module to address the limitations of existing\nmethods. Our model is exclusively trained on bonafide images without exposure\nto any spoofed samples during the training phase. It is then evaluated against\nvarious types of presentation attack images in the testing phase. The scheme we\nproposed has achieved an average BPCER of 0.96\\% with an APCER of 1.6\\% for\npresentation attacks involving various types of spoofed samples.",
        "translated": ""
    },
    {
        "title": "GENOME: GenerativE Neuro-symbOlic visual reasoning by growing and\n  reusing ModulEs",
        "url": "http://arxiv.org/abs/2311.04901v1",
        "pub_date": "2023-11-08",
        "summary": "Recent works have shown that Large Language Models (LLMs) could empower\ntraditional neuro-symbolic models via programming capabilities to translate\nlanguage into module descriptions, thus achieving strong visual reasoning\nresults while maintaining the model's transparency and efficiency. However,\nthese models usually exhaustively generate the entire code snippet given each\nnew instance of a task, which is extremely ineffective. We propose generative\nneuro-symbolic visual reasoning by growing and reusing modules. Specifically,\nour model consists of three unique stages, module initialization, module\ngeneration, and module execution. First, given a vision-language task, we adopt\nLLMs to examine whether we could reuse and grow over established modules to\nhandle this new task. If not, we initialize a new module needed by the task and\nspecify the inputs and outputs of this new module. After that, the new module\nis created by querying LLMs to generate corresponding code snippets that match\nthe requirements. In order to get a better sense of the new module's ability,\nwe treat few-shot training examples as test cases to see if our new module\ncould pass these cases. If yes, the new module is added to the module library\nfor future reuse. Finally, we evaluate the performance of our model on the\ntesting set by executing the parsed programs with the newly made visual modules\nto get the results. We find the proposed model possesses several advantages.\nFirst, it performs competitively on standard tasks like visual question\nanswering and referring expression comprehension; Second, the modules learned\nfrom one task can be seamlessly transferred to new tasks; Last but not least,\nit is able to adapt to new visual reasoning tasks by observing a few training\nexamples and reusing modules.",
        "translated": ""
    },
    {
        "title": "Two Complementary Perspectives to Continual Learning: Ask Not Only What\n  to Optimize, But Also How",
        "url": "http://arxiv.org/abs/2311.04898v1",
        "pub_date": "2023-11-08",
        "summary": "Recent years have seen considerable progress in the continual training of\ndeep neural networks, predominantly thanks to approaches that add replay or\nregularization terms to the loss function to approximate the joint loss over\nall tasks so far. However, we show that even with a perfect approximation to\nthe joint loss, these approaches still suffer from temporary but substantial\nforgetting when starting to train on a new task. Motivated by this 'stability\ngap', we propose that continual learning strategies should focus not only on\nthe optimization objective, but also on the way this objective is optimized.\nWhile there is some continual learning work that alters the optimization\ntrajectory (e.g., using gradient projection techniques), this line of research\nis positioned as alternative to improving the optimization objective, while we\nargue it should be complementary. To evaluate the merits of our proposition, we\nplan to combine replay-approximated joint objectives with gradient\nprojection-based optimization routines to test whether the addition of the\nlatter provides benefits in terms of (1) alleviating the stability gap, (2)\nincreasing the learning efficiency and (3) improving the final learning\noutcome.",
        "translated": ""
    },
    {
        "title": "DAMEX: Dataset-aware Mixture-of-Experts for visual understanding of\n  mixture-of-datasets",
        "url": "http://arxiv.org/abs/2311.04894v1",
        "pub_date": "2023-11-08",
        "summary": "Construction of a universal detector poses a crucial question: How can we\nmost effectively train a model on a large mixture of datasets? The answer lies\nin learning dataset-specific features and ensembling their knowledge but do all\nthis in a single model. Previous methods achieve this by having separate\ndetection heads on a common backbone but that results in a significant increase\nin parameters. In this work, we present Mixture-of-Experts as a solution,\nhighlighting that MoEs are much more than a scalability tool. We propose\nDataset-Aware Mixture-of-Experts, DAMEX where we train the experts to become an\n`expert' of a dataset by learning to route each dataset tokens to its mapped\nexpert. Experiments on Universal Object-Detection Benchmark show that we\noutperform the existing state-of-the-art by average +10.2 AP score and improve\nover our non-MoE baseline by average +2.0 AP score. We also observe consistent\ngains while mixing datasets with (1) limited availability, (2) disparate\ndomains and (3) divergent label sets. Further, we qualitatively show that DAMEX\nis robust against expert representation collapse.",
        "translated": ""
    },
    {
        "title": "Towards Few-Annotation Learning in Computer Vision: Application to Image\n  Classification and Object Detection tasks",
        "url": "http://arxiv.org/abs/2311.04888v1",
        "pub_date": "2023-11-08",
        "summary": "In this thesis, we develop theoretical, algorithmic and experimental\ncontributions for Machine Learning with limited labels, and more specifically\nfor the tasks of Image Classification and Object Detection in Computer Vision.\nIn a first contribution, we are interested in bridging the gap between theory\nand practice for popular Meta-Learning algorithms used in Few-Shot\nClassification. We make connections to Multi-Task Representation Learning,\nwhich benefits from solid theoretical foundations, to verify the best\nconditions for a more efficient meta-learning. Then, to leverage unlabeled data\nwhen training object detectors based on the Transformer architecture, we\npropose both an unsupervised pretraining and a semi-supervised learning method\nin two other separate contributions. For pretraining, we improve Contrastive\nLearning for object detectors by introducing the localization information.\nFinally, our semi-supervised method is the first tailored to transformer-based\ndetectors.",
        "translated": ""
    },
    {
        "title": "Are foundation models efficient for medical image segmentation?",
        "url": "http://arxiv.org/abs/2311.04847v1",
        "pub_date": "2023-11-08",
        "summary": "Foundation models are experiencing a surge in popularity. The Segment\nAnything model (SAM) asserts an ability to segment a wide spectrum of objects\nbut required supervised training at unprecedented scale. We compared SAM's\nperformance (against clinical ground truth) and resources (labeling time,\ncompute) to a modality-specific, label-free self-supervised learning (SSL)\nmethod on 25 measurements for 100 cardiac ultrasounds. SAM performed poorly and\nrequired significantly more labeling and computing resources, demonstrating\nworse efficiency than SSL.",
        "translated": ""
    },
    {
        "title": "Self-Supervised Learning for Visual Relationship Detection through\n  Masked Bounding Box Reconstruction",
        "url": "http://arxiv.org/abs/2311.04834v1",
        "pub_date": "2023-11-08",
        "summary": "We present a novel self-supervised approach for representation learning,\nparticularly for the task of Visual Relationship Detection (VRD). Motivated by\nthe effectiveness of Masked Image Modeling (MIM), we propose Masked Bounding\nBox Reconstruction (MBBR), a variation of MIM where a percentage of the\nentities/objects within a scene are masked and subsequently reconstructed based\non the unmasked objects. The core idea is that, through object-level masked\nmodeling, the network learns context-aware representations that capture the\ninteraction of objects within a scene and thus are highly predictive of visual\nobject relationships. We extensively evaluate learned representations, both\nqualitatively and quantitatively, in a few-shot setting and demonstrate the\nefficacy of MBBR for learning robust visual representations, particularly\ntailored for VRD. The proposed method is able to surpass state-of-the-art VRD\nmethods on the Predicate Detection (PredDet) evaluation setting, using only a\nfew annotated samples. We make our code available at\nhttps://github.com/deeplab-ai/SelfSupervisedVRD.",
        "translated": ""
    },
    {
        "title": "Anonymizing medical case-based explanations through disentanglement",
        "url": "http://arxiv.org/abs/2311.04833v1",
        "pub_date": "2023-11-08",
        "summary": "Case-based explanations are an intuitive method to gain insight into the\ndecision-making process of deep learning models in clinical contexts. However,\nmedical images cannot be shared as explanations due to privacy concerns. To\naddress this problem, we propose a novel method for disentangling identity and\nmedical characteristics of images and apply it to anonymize medical images. The\ndisentanglement mechanism replaces some feature vectors in an image while\nensuring that the remaining features are preserved, obtaining independent\nfeature vectors that encode the images' identity and medical characteristics.\nWe also propose a model to manufacture synthetic privacy-preserving identities\nto replace the original image's identity and achieve anonymization. The models\nare applied to medical and biometric datasets, demonstrating their capacity to\ngenerate realistic-looking anonymized images that preserve their original\nmedical content. Additionally, the experiments show the network's inherent\ncapacity to generate counterfactual images through the replacement of medical\nfeatures.",
        "translated": ""
    },
    {
        "title": "SODAWideNet -- Salient Object Detection with an Attention augmented Wide\n  Encoder Decoder network without ImageNet pre-training",
        "url": "http://arxiv.org/abs/2311.04828v1",
        "pub_date": "2023-11-08",
        "summary": "Developing a new Salient Object Detection (SOD) model involves selecting an\nImageNet pre-trained backbone and creating novel feature refinement modules to\nuse backbone features. However, adding new components to a pre-trained backbone\nneeds retraining the whole network on the ImageNet dataset, which requires\nsignificant time. Hence, we explore developing a neural network from scratch\ndirectly trained on SOD without ImageNet pre-training. Such a formulation\noffers full autonomy to design task-specific components. To that end, we\npropose SODAWideNet, an encoder-decoder-style network for Salient Object\nDetection. We deviate from the commonly practiced paradigm of narrow and deep\nconvolutional models to a wide and shallow architecture, resulting in a\nparameter-efficient deep neural network. To achieve a shallower network, we\nincrease the receptive field from the beginning of the network using a\ncombination of dilated convolutions and self-attention. Therefore, we propose\nMulti Receptive Field Feature Aggregation Module (MRFFAM) that efficiently\nobtains discriminative features from farther regions at higher resolutions\nusing dilated convolutions. Next, we propose Multi-Scale Attention (MSA), which\ncreates a feature pyramid and efficiently computes attention across multiple\nresolutions to extract global features from larger feature maps. Finally, we\npropose two variants, SODAWideNet-S (3.03M) and SODAWideNet (9.03M), that\nachieve competitive performance against state-of-the-art models on five\ndatasets.",
        "translated": ""
    },
    {
        "title": "Cross-Silo Federated Learning Across Divergent Domains with Iterative\n  Parameter Alignment",
        "url": "http://arxiv.org/abs/2311.04818v1",
        "pub_date": "2023-11-08",
        "summary": "Learning from the collective knowledge of data dispersed across private\nsources can provide neural networks with enhanced generalization capabilities.\nFederated learning, a method for collaboratively training a machine learning\nmodel across remote clients, achieves this by combining client models via the\norchestration of a central server. However, current approaches face two\ncritical limitations: i) they struggle to converge when client domains are\nsufficiently different, and ii) current aggregation techniques produce an\nidentical global model for each client. In this work, we address these issues\nby reformulating the typical federated learning setup: rather than learning a\nsingle global model, we learn N models each optimized for a common objective.\nTo achieve this, we apply a weighted distance minimization to model parameters\nshared in a peer-to-peer topology. The resulting framework, Iterative Parameter\nAlignment, applies naturally to the cross-silo setting, and has the following\nproperties: (i) a unique solution for each participant, with the option to\nglobally converge each model in the federation, and (ii) an optional\nearly-stopping mechanism to elicit fairness among peers in collaborative\nlearning settings. These characteristics jointly provide a flexible new\nframework for iteratively learning from peer models trained on disparate\ndatasets. We find that the technique achieves competitive results on a variety\nof data partitions compared to state-of-the-art approaches. Further, we show\nthat the method is robust to divergent domains (i.e. disjoint classes across\npeers) where existing approaches struggle.",
        "translated": ""
    },
    {
        "title": "Domain Adaptive Object Detection via Balancing Between Self-Training and\n  Adversarial Learning",
        "url": "http://arxiv.org/abs/2311.04815v1",
        "pub_date": "2023-11-08",
        "summary": "Deep learning based object detectors struggle generalizing to a new target\ndomain bearing significant variations in object and background. Most current\nmethods align domains by using image or instance-level adversarial feature\nalignment. This often suffers due to unwanted background and lacks\nclass-specific alignment. A straightforward approach to promote class-level\nalignment is to use high confidence predictions on unlabeled domain as\npseudo-labels. These predictions are often noisy since model is poorly\ncalibrated under domain shift. In this paper, we propose to leverage model's\npredictive uncertainty to strike the right balance between adversarial feature\nalignment and class-level alignment. We develop a technique to quantify\npredictive uncertainty on class assignments and bounding-box predictions. Model\npredictions with low uncertainty are used to generate pseudo-labels for\nself-training, whereas the ones with higher uncertainty are used to generate\ntiles for adversarial feature alignment. This synergy between tiling around\nuncertain object regions and generating pseudo-labels from highly certain\nobject regions allows capturing both image and instance-level context during\nthe model adaptation. We report thorough ablation study to reveal the impact of\ndifferent components in our approach. Results on five diverse and challenging\nadaptation scenarios show that our approach outperforms existing\nstate-of-the-art methods with noticeable margins.",
        "translated": ""
    },
    {
        "title": "Window Attention is Bugged: How not to Interpolate Position Embeddings",
        "url": "http://arxiv.org/abs/2311.05613v1",
        "pub_date": "2023-11-09",
        "summary": "Window attention, position embeddings, and high resolution finetuning are\ncore concepts in the modern transformer era of computer vision. However, we\nfind that naively combining these near ubiquitous components can have a\ndetrimental effect on performance. The issue is simple: interpolating position\nembeddings while using window attention is wrong. We study two state-of-the-art\nmethods that have these three components, namely Hiera and ViTDet, and find\nthat both do indeed suffer from this bug. To fix it, we introduce a simple\nabsolute window position embedding strategy, which solves the bug outright in\nHiera and allows us to increase both speed and performance of the model in\nViTDet. We finally combine the two to obtain HieraDet, which achieves 61.7 box\nmAP on COCO, making it state-of-the-art for models that only use ImageNet-1k\npretraining. This all stems from what is essentially a 3 line bug fix, which we\nname \"absolute win\".",
        "translated": ""
    },
    {
        "title": "What Do I Hear? Generating Sounds for Visuals with ChatGPT",
        "url": "http://arxiv.org/abs/2311.05609v1",
        "pub_date": "2023-11-09",
        "summary": "This short paper introduces a workflow for generating realistic soundscapes\nfor visual media. In contrast to prior work, which primarily focus on matching\nsounds for on-screen visuals, our approach extends to suggesting sounds that\nmay not be immediately visible but are essential to crafting a convincing and\nimmersive auditory environment. Our key insight is leveraging the reasoning\ncapabilities of language models, such as ChatGPT. In this paper, we describe\nour workflow, which includes creating a scene context, brainstorming sounds,\nand generating the sounds.",
        "translated": ""
    },
    {
        "title": "Real-Time Neural Rasterization for Large Scenes",
        "url": "http://arxiv.org/abs/2311.05607v1",
        "pub_date": "2023-11-09",
        "summary": "We propose a new method for realistic real-time novel-view synthesis (NVS) of\nlarge scenes. Existing neural rendering methods generate realistic results, but\nprimarily work for small scale scenes (&lt;50 square meters) and have difficulty\nat large scale (&gt;10000 square meters). Traditional graphics-based rasterization\nrendering is fast for large scenes but lacks realism and requires expensive\nmanually created assets. Our approach combines the best of both worlds by\ntaking a moderate-quality scaffold mesh as input and learning a neural texture\nfield and shader to model view-dependant effects to enhance realism, while\nstill using the standard graphics pipeline for real-time rendering. Our method\noutperforms existing neural rendering methods, providing at least 30x faster\nrendering with comparable or better realism for large self-driving and drone\nscenes. Our work is the first to enable real-time rendering of large real-world\nscenes.",
        "translated": ""
    },
    {
        "title": "3D-QAE: Fully Quantum Auto-Encoding of 3D Point Clouds",
        "url": "http://arxiv.org/abs/2311.05604v1",
        "pub_date": "2023-11-09",
        "summary": "Existing methods for learning 3D representations are deep neural networks\ntrained and tested on classical hardware. Quantum machine learning\narchitectures, despite their theoretically predicted advantages in terms of\nspeed and the representational capacity, have so far not been considered for\nthis problem nor for tasks involving 3D data in general. This paper thus\nintroduces the first quantum auto-encoder for 3D point clouds. Our 3D-QAE\napproach is fully quantum, i.e. all its data processing components are designed\nfor quantum hardware. It is trained on collections of 3D point clouds to\nproduce their compressed representations. Along with finding a suitable\narchitecture, the core challenges in designing such a fully quantum model\ninclude 3D data normalisation and parameter optimisation, and we propose\nsolutions for both these tasks. Experiments on simulated gate-based quantum\nhardware demonstrate that our method outperforms simple classical baselines,\npaving the way for a new research direction in 3D computer vision. The source\ncode is available at https://4dqv.mpi-inf.mpg.de/QAE3D/.",
        "translated": ""
    },
    {
        "title": "Reconstructing Objects in-the-wild for Realistic Sensor Simulation",
        "url": "http://arxiv.org/abs/2311.05602v1",
        "pub_date": "2023-11-09",
        "summary": "Reconstructing objects from real world data and rendering them at novel views\nis critical to bringing realism, diversity and scale to simulation for robotics\ntraining and testing. In this work, we present NeuSim, a novel approach that\nestimates accurate geometry and realistic appearance from sparse in-the-wild\ndata captured at distance and at limited viewpoints. Towards this goal, we\nrepresent the object surface as a neural signed distance function and leverage\nboth LiDAR and camera sensor data to reconstruct smooth and accurate geometry\nand normals. We model the object appearance with a robust physics-inspired\nreflectance representation effective for in-the-wild data. Our experiments show\nthat NeuSim has strong view synthesis performance on challenging scenarios with\nsparse training views. Furthermore, we showcase composing NeuSim assets into a\nvirtual world and generating realistic multi-sensor data for evaluating\nself-driving perception models.",
        "translated": ""
    },
    {
        "title": "Accuracy of a Vision-Language Model on Challenging Medical Cases",
        "url": "http://arxiv.org/abs/2311.05591v1",
        "pub_date": "2023-11-09",
        "summary": "Background: General-purpose large language models that utilize both text and\nimages have not been evaluated on a diverse array of challenging medical cases.\n  Methods: Using 934 cases from the NEJM Image Challenge published between 2005\nand 2023, we evaluated the accuracy of the recently released Generative\nPre-trained Transformer 4 with Vision model (GPT-4V) compared to human\nrespondents overall and stratified by question difficulty, image type, and skin\ntone. We further conducted a physician evaluation of GPT-4V on 69 NEJM\nclinicopathological conferences (CPCs). Analyses were conducted for models\nutilizing text alone, images alone, and both text and images.\n  Results: GPT-4V achieved an overall accuracy of 61% (95% CI, 58 to 64%)\ncompared to 49% (95% CI, 49 to 50%) for humans. GPT-4V outperformed humans at\nall levels of difficulty and disagreement, skin tones, and image types; the\nexception was radiographic images, where performance was equivalent between\nGPT-4V and human respondents. Longer, more informative captions were associated\nwith improved performance for GPT-4V but similar performance for human\nrespondents. GPT-4V included the correct diagnosis in its differential for 80%\n(95% CI, 68 to 88%) of CPCs when using text alone, compared to 58% (95% CI, 45\nto 70%) of CPCs when using both images and text.\n  Conclusions: GPT-4V outperformed human respondents on challenging medical\ncases and was able to synthesize information from both images and text, but\nperformance deteriorated when images were added to highly informative text.\nOverall, our results suggest that multimodal AI models may be useful in medical\ndiagnostic reasoning but that their accuracy may depend heavily on context.",
        "translated": ""
    },
    {
        "title": "SigScatNet: A Siamese + Scattering based Deep Learning Approach for\n  Signature Forgery Detection and Similarity Assessment",
        "url": "http://arxiv.org/abs/2311.05579v1",
        "pub_date": "2023-11-09",
        "summary": "The surge in counterfeit signatures has inflicted widespread inconveniences\nand formidable challenges for both individuals and organizations. This\ngroundbreaking research paper introduces SigScatNet, an innovative solution to\ncombat this issue by harnessing the potential of a Siamese deep learning\nnetwork, bolstered by Scattering wavelets, to detect signature forgery and\nassess signature similarity. The Siamese Network empowers us to ascertain the\nauthenticity of signatures through a comprehensive similarity index, enabling\nprecise validation and comparison. Remarkably, the integration of Scattering\nwavelets endows our model with exceptional efficiency, rendering it light\nenough to operate seamlessly on cost-effective hardware systems. To validate\nthe efficacy of our approach, extensive experimentation was conducted on two\nopen-sourced datasets: the ICDAR SigComp Dutch dataset and the CEDAR dataset.\nThe experimental results demonstrate the practicality and resounding success of\nour proposed SigScatNet, yielding an unparalleled Equal Error Rate of 3.689%\nwith the ICDAR SigComp Dutch dataset and an astonishing 0.0578% with the CEDAR\ndataset. Through the implementation of SigScatNet, our research spearheads a\nnew state-of-the-art in signature analysis in terms of EER scores and\ncomputational efficiency, offering an advanced and accessible solution for\ndetecting forgery and quantifying signature similarities. By employing\ncutting-edge Siamese deep learning and Scattering wavelets, we provide a robust\nframework that paves the way for secure and efficient signature verification\nsystems.",
        "translated": ""
    },
    {
        "title": "Exploring Emotion Expression Recognition in Older Adults Interacting\n  with a Virtual Coach",
        "url": "http://arxiv.org/abs/2311.05567v1",
        "pub_date": "2023-11-09",
        "summary": "The EMPATHIC project aimed to design an emotionally expressive virtual coach\ncapable of engaging healthy seniors to improve well-being and promote\nindependent aging. One of the core aspects of the system is its human sensing\ncapabilities, allowing for the perception of emotional states to provide a\npersonalized experience. This paper outlines the development of the emotion\nexpression recognition module of the virtual coach, encompassing data\ncollection, annotation design, and a first methodological approach, all\ntailored to the project requirements. With the latter, we investigate the role\nof various modalities, individually and combined, for discrete emotion\nexpression recognition in this context: speech from audio, and facial\nexpressions, gaze, and head dynamics from video. The collected corpus includes\nusers from Spain, France, and Norway, and was annotated separately for the\naudio and video channels with distinct emotional labels, allowing for a\nperformance comparison across cultures and label types. Results confirm the\ninformative power of the modalities studied for the emotional categories\nconsidered, with multimodal methods generally outperforming others (around 68%\naccuracy with audio labels and 72-74% with video labels). The findings are\nexpected to contribute to the limited literature on emotion recognition applied\nto older adults in conversational human-machine interaction.",
        "translated": ""
    },
    {
        "title": "High-Performance Transformers for Table Structure Recognition Need Early\n  Convolutions",
        "url": "http://arxiv.org/abs/2311.05565v1",
        "pub_date": "2023-11-09",
        "summary": "Table structure recognition (TSR) aims to convert tabular images into a\nmachine-readable format, where a visual encoder extracts image features and a\ntextual decoder generates table-representing tokens. Existing approaches use\nclassic convolutional neural network (CNN) backbones for the visual encoder and\ntransformers for the textual decoder. However, this hybrid CNN-Transformer\narchitecture introduces a complex visual encoder that accounts for nearly half\nof the total model parameters, markedly reduces both training and inference\nspeed, and hinders the potential for self-supervised learning in TSR. In this\nwork, we design a lightweight visual encoder for TSR without sacrificing\nexpressive power. We discover that a convolutional stem can match classic CNN\nbackbone performance, with a much simpler model. The convolutional stem strikes\nan optimal balance between two crucial factors for high-performance TSR: a\nhigher receptive field (RF) ratio and a longer sequence length. This allows it\nto \"see\" an appropriate portion of the table and \"store\" the complex table\nstructure within sufficient context length for the subsequent transformer. We\nconducted reproducible ablation studies and open-sourced our code at\nhttps://github.com/poloclub/tsr-convstem to enhance transparency, inspire\ninnovations, and facilitate fair comparisons in our domain as tables are a\npromising modality for representation learning.",
        "translated": ""
    },
    {
        "title": "Disentangling Quantum and Classical Contributions in Hybrid Quantum\n  Machine Learning Architectures",
        "url": "http://arxiv.org/abs/2311.05559v1",
        "pub_date": "2023-11-09",
        "summary": "Quantum computing offers the potential for superior computational\ncapabilities, particularly for data-intensive tasks. However, the current state\nof quantum hardware puts heavy restrictions on input size. To address this,\nhybrid transfer learning solutions have been developed, merging pre-trained\nclassical models, capable of handling extensive inputs, with variational\nquantum circuits. Yet, it remains unclear how much each component - classical\nand quantum - contributes to the model's results. We propose a novel hybrid\narchitecture: instead of utilizing a pre-trained network for compression, we\nemploy an autoencoder to derive a compressed version of the input data. This\ncompressed data is then channeled through the encoder part of the autoencoder\nto the quantum component. We assess our model's classification capabilities\nagainst two state-of-the-art hybrid transfer learning architectures, two purely\nclassical architectures and one quantum architecture. Their accuracy is\ncompared across four datasets: Banknote Authentication, Breast Cancer\nWisconsin, MNIST digits, and AudioMNIST. Our research suggests that classical\ncomponents significantly influence classification in hybrid transfer learning,\na contribution often mistakenly ascribed to the quantum element. The\nperformance of our model aligns with that of a variational quantum circuit\nusing amplitude embedding, positioning it as a feasible alternative.",
        "translated": ""
    },
    {
        "title": "Parameter-Efficient Orthogonal Finetuning via Butterfly Factorization",
        "url": "http://arxiv.org/abs/2311.06243v1",
        "pub_date": "2023-11-10",
        "summary": "Large foundation models are becoming ubiquitous, but training them from\nscratch is prohibitively expensive. Thus, efficiently adapting these powerful\nmodels to downstream tasks is increasingly important. In this paper, we study a\nprincipled finetuning paradigm -- Orthogonal Finetuning (OFT) -- for downstream\ntask adaptation. Despite demonstrating good generalizability, OFT still uses a\nfairly large number of trainable parameters due to the high dimensionality of\northogonal matrices. To address this, we start by examining OFT from an\ninformation transmission perspective, and then identify a few key desiderata\nthat enable better parameter-efficiency. Inspired by how the Cooley-Tukey fast\nFourier transform algorithm enables efficient information transmission, we\npropose an efficient orthogonal parameterization using butterfly structures. We\napply this parameterization to OFT, creating a novel parameter-efficient\nfinetuning method, called Orthogonal Butterfly (BOFT). By subsuming OFT as a\nspecial case, BOFT introduces a generalized orthogonal finetuning framework.\nFinally, we conduct an extensive empirical study of adapting large vision\ntransformers, large language models, and text-to-image diffusion models to\nvarious downstream tasks in vision and language.",
        "translated": ""
    },
    {
        "title": "Florence-2: Advancing a Unified Representation for a Variety of Vision\n  Tasks",
        "url": "http://arxiv.org/abs/2311.06242v1",
        "pub_date": "2023-11-10",
        "summary": "We introduce Florence-2, a novel vision foundation model with a unified,\nprompt-based representation for a variety of computer vision and\nvision-language tasks. While existing large vision models excel in transfer\nlearning, they struggle to perform a diversity of tasks with simple\ninstructions, a capability that implies handling the complexity of various\nspatial hierarchy and semantic granularity. Florence-2 was designed to take\ntext-prompt as task instructions and generate desirable results in text forms,\nwhether it be captioning, object detection, grounding or segmentation. This\nmulti-task learning setup demands large-scale, high-quality annotated data. To\nthis end, we co-developed FLD-5B that consists of 5.4 billion comprehensive\nvisual annotations on 126 million images, using an iterative strategy of\nautomated image annotation and model refinement. We adopted a\nsequence-to-sequence structure to train Florence-2 to perform versatile and\ncomprehensive vision tasks. Extensive evaluations on numerous tasks\ndemonstrated Florence-2 to be a strong vision foundation model contender with\nunprecedented zero-shot and fine-tuning capabilities.",
        "translated": ""
    },
    {
        "title": "Learning Human Action Recognition Representations Without Real Humans",
        "url": "http://arxiv.org/abs/2311.06231v1",
        "pub_date": "2023-11-10",
        "summary": "Pre-training on massive video datasets has become essential to achieve high\naction recognition performance on smaller downstream datasets. However, most\nlarge-scale video datasets contain images of people and hence are accompanied\nwith issues related to privacy, ethics, and data protection, often preventing\nthem from being publicly shared for reproducible research. Existing work has\nattempted to alleviate these problems by blurring faces, downsampling videos,\nor training on synthetic data. On the other hand, analysis on the\ntransferability of privacy-preserving pre-trained models to downstream tasks\nhas been limited. In this work, we study this problem by first asking the\nquestion: can we pre-train models for human action recognition with data that\ndoes not include real humans? To this end, we present, for the first time, a\nbenchmark that leverages real-world videos with humans removed and synthetic\ndata containing virtual humans to pre-train a model. We then evaluate the\ntransferability of the representation learned on this data to a diverse set of\ndownstream action recognition benchmarks. Furthermore, we propose a novel\npre-training strategy, called Privacy-Preserving MAE-Align, to effectively\ncombine synthetic data and human-removed real data. Our approach outperforms\nprevious baselines by up to 5% and closes the performance gap between human and\nno-human action recognition representations on downstream tasks, for both\nlinear probing and fine-tuning. Our benchmark, code, and models are available\nat https://github.com/howardzh01/PPMA .",
        "translated": ""
    },
    {
        "title": "Harnessing Synthetic Datasets: The Role of Shape Bias in Deep Neural\n  Network Generalization",
        "url": "http://arxiv.org/abs/2311.06224v1",
        "pub_date": "2023-11-10",
        "summary": "Recent advancements in deep learning have been primarily driven by the use of\nlarge models trained on increasingly vast datasets. While neural scaling laws\nhave emerged to predict network performance given a specific level of\ncomputational resources, the growing demand for expansive datasets raises\nconcerns. To address this, a new research direction has emerged, focusing on\nthe creation of synthetic data as a substitute. In this study, we investigate\nhow neural networks exhibit shape bias during training on synthetic datasets,\nserving as an indicator of the synthetic data quality. Specifically, our\nfindings indicate three key points: (1) Shape bias varies across network\narchitectures and types of supervision, casting doubt on its reliability as a\npredictor for generalization and its ability to explain differences in model\nrecognition compared to human capabilities. (2) Relying solely on shape bias to\nestimate generalization is unreliable, as it is entangled with diversity and\nnaturalism. (3) We propose a novel interpretation of shape bias as a tool for\nestimating the diversity of samples within a dataset. Our research aims to\nclarify the implications of using synthetic data and its associated shape bias\nin deep learning, addressing concerns regarding generalization and dataset\nquality.",
        "translated": ""
    },
    {
        "title": "Diffusion Models for Earth Observation Use-cases: from cloud removal to\n  urban change detection",
        "url": "http://arxiv.org/abs/2311.06222v1",
        "pub_date": "2023-11-10",
        "summary": "The advancements in the state of the art of generative Artificial\nIntelligence (AI) brought by diffusion models can be highly beneficial in novel\ncontexts involving Earth observation data. After introducing this new family of\ngenerative models, this work proposes and analyses three use cases which\ndemonstrate the potential of diffusion-based approaches for satellite image\ndata. Namely, we tackle cloud removal and inpainting, dataset generation for\nchange-detection tasks, and urban replanning.",
        "translated": ""
    },
    {
        "title": "Semantic-aware Video Representation for Few-shot Action Recognition",
        "url": "http://arxiv.org/abs/2311.06218v1",
        "pub_date": "2023-11-10",
        "summary": "Recent work on action recognition leverages 3D features and textual\ninformation to achieve state-of-the-art performance. However, most of the\ncurrent few-shot action recognition methods still rely on 2D frame-level\nrepresentations, often require additional components to model temporal\nrelations, and employ complex distance functions to achieve accurate alignment\nof these representations. In addition, existing methods struggle to effectively\nintegrate textual semantics, some resorting to concatenation or addition of\ntextual and visual features, and some using text merely as an additional\nsupervision without truly achieving feature fusion and information transfer\nfrom different modalities. In this work, we propose a simple yet effective\nSemantic-Aware Few-Shot Action Recognition (SAFSAR) model to address these\nissues. We show that directly leveraging a 3D feature extractor combined with\nan effective feature-fusion scheme, and a simple cosine similarity for\nclassification can yield better performance without the need of extra\ncomponents for temporal modeling or complex distance functions. We introduce an\ninnovative scheme to encode the textual semantics into the video representation\nwhich adaptively fuses features from text and video, and encourages the visual\nencoder to extract more semantically consistent features. In this scheme,\nSAFSAR achieves alignment and fusion in a compact way. Experiments on five\nchallenging few-shot action recognition benchmarks under various settings\ndemonstrate that the proposed SAFSAR model significantly improves the\nstate-of-the-art performance.",
        "translated": ""
    },
    {
        "title": "MultiIoT: Towards Large-scale Multisensory Learning for the Internet of\n  Things",
        "url": "http://arxiv.org/abs/2311.06217v1",
        "pub_date": "2023-11-10",
        "summary": "The Internet of Things (IoT), the network integrating billions of smart\nphysical devices embedded with sensors, software, and communication\ntechnologies for the purpose of connecting and exchanging data with other\ndevices and systems, is a critical and rapidly expanding component of our\nmodern world. The IoT ecosystem provides a rich source of real-world modalities\nsuch as motion, thermal, geolocation, imaging, depth, sensors, video, and audio\nfor prediction tasks involving the pose, gaze, activities, and gestures of\nhumans as well as the touch, contact, pose, 3D of physical objects. Machine\nlearning presents a rich opportunity to automatically process IoT data at\nscale, enabling efficient inference for impact in understanding human\nwellbeing, controlling physical devices, and interconnecting smart cities. To\ndevelop machine learning technologies for IoT, this paper proposes MultiIoT,\nthe most expansive IoT benchmark to date, encompassing over 1.15 million\nsamples from 12 modalities and 8 tasks. MultiIoT introduces unique challenges\ninvolving (1) learning from many sensory modalities, (2) fine-grained\ninteractions across long temporal ranges, and (3) extreme heterogeneity due to\nunique structure and noise topologies in real-world sensors. We also release a\nset of strong modeling baselines, spanning modality and task-specific methods\nto multisensory and multitask models to encourage future research in\nmultisensory representation learning for IoT.",
        "translated": ""
    },
    {
        "title": "Instant3D: Fast Text-to-3D with Sparse-View Generation and Large\n  Reconstruction Model",
        "url": "http://arxiv.org/abs/2311.06214v1",
        "pub_date": "2023-11-10",
        "summary": "Text-to-3D with diffusion models have achieved remarkable progress in recent\nyears. However, existing methods either rely on score distillation-based\noptimization which suffer from slow inference, low diversity and Janus\nproblems, or are feed-forward methods that generate low quality results due to\nthe scarcity of 3D training data. In this paper, we propose Instant3D, a novel\nmethod that generates high-quality and diverse 3D assets from text prompts in a\nfeed-forward manner. We adopt a two-stage paradigm, which first generates a\nsparse set of four structured and consistent views from text in one shot with a\nfine-tuned 2D text-to-image diffusion model, and then directly regresses the\nNeRF from the generated images with a novel transformer-based sparse-view\nreconstructor. Through extensive experiments, we demonstrate that our method\ncan generate high-quality, diverse and Janus-free 3D assets within 20 seconds,\nwhich is two order of magnitude faster than previous optimization-based methods\nthat can take 1 to 10 hours. Our project webpage: https://jiahao.ai/instant3d/.",
        "translated": ""
    },
    {
        "title": "ASSIST: Interactive Scene Nodes for Scalable and Realistic Indoor\n  Simulation",
        "url": "http://arxiv.org/abs/2311.06211v1",
        "pub_date": "2023-11-10",
        "summary": "We present ASSIST, an object-wise neural radiance field as a panoptic\nrepresentation for compositional and realistic simulation. Central to our\napproach is a novel scene node data structure that stores the information of\neach object in a unified fashion, allowing online interaction in both intra-\nand cross-scene settings. By incorporating a differentiable neural network\nalong with the associated bounding box and semantic features, the proposed\nstructure guarantees user-friendly interaction on independent objects to scale\nup novel view simulation. Objects in the scene can be queried, added,\nduplicated, deleted, transformed, or swapped simply through mouse/keyboard\ncontrols or language instructions. Experiments demonstrate the efficacy of the\nproposed method, where scaled realistic simulation can be achieved through\ninteractive editing and compositional rendering, with color images, depth\nimages, and panoptic segmentation masks generated in a 3D consistent manner.",
        "translated": ""
    },
    {
        "title": "An Automated Pipeline for Tumour-Infiltrating Lymphocyte Scoring in\n  Breast Cancer",
        "url": "http://arxiv.org/abs/2311.06185v1",
        "pub_date": "2023-11-10",
        "summary": "Tumour-infiltrating lymphocytes (TILs) are considered as a valuable\nprognostic markers in both triple-negative and human epidermal growth factor\nreceptor 2 (HER2) breast cancer. In this study, we introduce an innovative deep\nlearning pipeline based on the Efficient-UNet architecture to compute a TILs\nscore for breast cancer whole slide images. Our pipeline first segments\ntumour-stroma regions and generates a tumour bulk mask. Subsequently, it\ndetects TILs within the tumour-associated stroma, generating a TILs score by\nclosely mirroring the pathologist's workflow. Our method exhibits\nstate-of-the-art performance in segmenting tumour/stroma areas and TILs\ndetection, as demonstrated by internal cross-validation on the TiGER Challenge\ntraining dataset and evaluation on the final leaderboards. Additionally, our\nTILs score proves competitive in predicting survival outcomes within the same\nchallenge, underscoring the clinical relevance and potential of our automated\nTILs scoring system as a breast cancer prognostic tool.",
        "translated": ""
    },
    {
        "title": "SPHINX: The Joint Mixing of Weights, Tasks, and Visual Embeddings for\n  Multi-modal Large Language Models",
        "url": "http://arxiv.org/abs/2311.07575v1",
        "pub_date": "2023-11-13",
        "summary": "We present SPHINX, a versatile multi-modal large language model (MLLM) with a\njoint mixing of model weights, tuning tasks, and visual embeddings. First, for\nstronger vision-language alignment, we unfreeze the large language model (LLM)\nduring pre-training, and introduce a weight mix strategy between LLMs trained\nby real-world and synthetic data. By directly integrating the weights from two\ndomains, the mixed LLM can efficiently incorporate diverse semantics with\nfavorable robustness. Then, to enable multi-purpose capabilities, we mix a\nvariety of tasks for joint visual instruction tuning, and design task-specific\ninstructions to avoid inter-task conflict. In addition to the basic visual\nquestion answering, we include more challenging tasks such as region-level\nunderstanding, caption grounding, document layout detection, and human pose\nestimation, contributing to mutual enhancement over different scenarios.\nAdditionally, we propose to extract comprehensive visual embeddings from\nvarious network architectures, pre-training paradigms, and information\ngranularity, providing language models with more robust image representations.\nBased on our proposed joint mixing, SPHINX exhibits superior multi-modal\nunderstanding capabilities on a wide range of applications. On top of this, we\nfurther propose an efficient strategy aiming to better capture fine-grained\nappearances of high-resolution images. With a mixing of different scales and\nhigh-resolution sub-images, SPHINX attains exceptional visual parsing and\nreasoning performance on existing evaluation benchmarks. We hope our work may\ncast a light on the exploration of joint mixing in future MLLM research. Code\nis released at https://github.com/Alpha-VLLM/LLaMA2-Accessory.",
        "translated": ""
    },
    {
        "title": "To See is to Believe: Prompting GPT-4V for Better Visual Instruction\n  Tuning",
        "url": "http://arxiv.org/abs/2311.07574v1",
        "pub_date": "2023-11-13",
        "summary": "Existing visual instruction tuning methods typically prompt large language\nmodels with textual descriptions to generate instruction-following data.\nDespite the promising performance achieved, these descriptions are derived from\nimage annotations, which are oftentimes coarse-grained. Furthermore, the\ninstructions might even contradict the visual content without observing the\nentire visual context. To address this challenge, we introduce a fine-grained\nvisual instruction dataset, LVIS-Instruct4V, which contains 220K visually\naligned and context-aware instructions produced by prompting the powerful\nGPT-4V with images from LVIS. Through experimental validation and case studies,\nwe demonstrate that high-quality visual instructional data could improve the\nperformance of LLaVA-1.5, a state-of-the-art large multimodal model, across a\nwide spectrum of benchmarks by clear margins. Notably, by simply replacing the\nLLaVA-Instruct with our LVIS-Instruct4V, we achieve better results than LLaVA\non most challenging LMM benchmarks, e.g., LLaVA$^w$ (76.7 vs. 70.7) and MM-Vet\n(40.2 vs. 35.4). We release our data and model at\nhttps://github.com/X2FD/LVIS-INSTRUCT4V.",
        "translated": ""
    },
    {
        "title": "GPT-4V in Wonderland: Large Multimodal Models for Zero-Shot Smartphone\n  GUI Navigation",
        "url": "http://arxiv.org/abs/2311.07562v1",
        "pub_date": "2023-11-13",
        "summary": "We present MM-Navigator, a GPT-4V-based agent for the smartphone graphical\nuser interface (GUI) navigation task. MM-Navigator can interact with a\nsmartphone screen as human users, and determine subsequent actions to fulfill\ngiven instructions. Our findings demonstrate that large multimodal models\n(LMMs), specifically GPT-4V, excel in zero-shot GUI navigation through its\nadvanced screen interpretation, action reasoning, and precise action\nlocalization capabilities. We first benchmark MM-Navigator on our collected iOS\nscreen dataset. According to human assessments, the system exhibited a 91\\%\naccuracy rate in generating reasonable action descriptions and a 75\\% accuracy\nrate in executing the correct actions for single-step instructions on iOS.\nAdditionally, we evaluate the model on a subset of an Android screen navigation\ndataset, where the model outperforms previous GUI navigators in a zero-shot\nfashion. Our benchmark and detailed analyses aim to lay a robust groundwork for\nfuture research into the GUI navigation task. The project page is at\nhttps://github.com/zzxslp/MM-Navigator.",
        "translated": ""
    },
    {
        "title": "Fast Normalized Cross-Correlation for Template Matching with Rotations",
        "url": "http://arxiv.org/abs/2311.07561v1",
        "pub_date": "2023-11-13",
        "summary": "Normalized cross-correlation is the reference approach to carry out template\nmatching on images. When it is computed in Fourier space, it can handle\nefficiently template translations but it cannot do so with template rotations.\nIncluding rotations requires sampling the whole space of rotations, repeating\nthe computation of the correlation each time.\n  This article develops an alternative mathematical theory to handle\nefficiently, at the same time, rotations and translations. Our proposal has a\nreduced computational complexity because it does not require to repeatedly\nsample the space of rotations. To do so, we integrate the information relative\nto all rotated versions of the template into a unique symmetric tensor template\n-which is computed only once per template-. Afterward, we demonstrate that the\ncorrelation between the image to be processed with the independent tensor\ncomponents of the tensorial template contains enough information to recover\ntemplate instance positions and rotations.\n  Our proposed method has the potential to speed up conventional template\nmatching computations by a factor of several magnitude orders for the case of\n3D images.",
        "translated": ""
    },
    {
        "title": "GPT-4V(ision) as A Social Media Analysis Engine",
        "url": "http://arxiv.org/abs/2311.07547v1",
        "pub_date": "2023-11-13",
        "summary": "Recent research has offered insights into the extraordinary capabilities of\nLarge Multimodal Models (LMMs) in various general vision and language tasks.\nThere is growing interest in how LMMs perform in more specialized domains.\nSocial media content, inherently multimodal, blends text, images, videos, and\nsometimes audio. Understanding social multimedia content remains a challenging\nproblem for contemporary machine learning frameworks. In this paper, we explore\nGPT-4V(ision)'s capabilities for social multimedia analysis. We select five\nrepresentative tasks, including sentiment analysis, hate speech detection, fake\nnews identification, demographic inference, and political ideology detection,\nto evaluate GPT-4V. Our investigation begins with a preliminary quantitative\nanalysis for each task using existing benchmark datasets, followed by a careful\nreview of the results and a selection of qualitative samples that illustrate\nGPT-4V's potential in understanding multimodal social media content. GPT-4V\ndemonstrates remarkable efficacy in these tasks, showcasing strengths such as\njoint understanding of image-text pairs, contextual and cultural awareness, and\nextensive commonsense knowledge. Despite the overall impressive capacity of\nGPT-4V in the social media domain, there remain notable challenges. GPT-4V\nstruggles with tasks involving multilingual social multimedia comprehension and\nhas difficulties in generalizing to the latest trends in social media.\nAdditionally, it exhibits a tendency to generate erroneous information in the\ncontext of evolving celebrity and politician knowledge, reflecting the known\nhallucination problem. The insights gleaned from our findings underscore a\npromising future for LMMs in enhancing our comprehension of social media\ncontent and its users through the analysis of multimodal information.",
        "translated": ""
    },
    {
        "title": "VGSG: Vision-Guided Semantic-Group Network for Text-based Person Search",
        "url": "http://arxiv.org/abs/2311.07514v1",
        "pub_date": "2023-11-13",
        "summary": "Text-based Person Search (TBPS) aims to retrieve images of target pedestrian\nindicated by textual descriptions. It is essential for TBPS to extract\nfine-grained local features and align them crossing modality. Existing methods\nutilize external tools or heavy cross-modal interaction to achieve explicit\nalignment of cross-modal fine-grained features, which is inefficient and\ntime-consuming. In this work, we propose a Vision-Guided Semantic-Group Network\n(VGSG) for text-based person search to extract well-aligned fine-grained visual\nand textual features. In the proposed VGSG, we develop a Semantic-Group Textual\nLearning (SGTL) module and a Vision-guided Knowledge Transfer (VGKT) module to\nextract textual local features under the guidance of visual local clues. In\nSGTL, in order to obtain the local textual representation, we group textual\nfeatures from the channel dimension based on the semantic cues of language\nexpression, which encourages similar semantic patterns to be grouped implicitly\nwithout external tools. In VGKT, a vision-guided attention is employed to\nextract visual-related textual features, which are inherently aligned with\nvisual cues and termed vision-guided textual features. Furthermore, we design a\nrelational knowledge transfer, including a vision-language similarity transfer\nand a class probability transfer, to adaptively propagate information of the\nvision-guided textual features to semantic-group textual features. With the\nhelp of relational knowledge transfer, VGKT is capable of aligning\nsemantic-group textual features with corresponding visual features without\nexternal tools and complex pairwise interaction. Experimental results on two\nchallenging benchmarks demonstrate its superiority over state-of-the-art\nmethods.",
        "translated": ""
    },
    {
        "title": "EvoFed: Leveraging Evolutionary Strategies for Communication-Efficient\n  Federated Learning",
        "url": "http://arxiv.org/abs/2311.07485v1",
        "pub_date": "2023-11-13",
        "summary": "Federated Learning (FL) is a decentralized machine learning paradigm that\nenables collaborative model training across dispersed nodes without having to\nforce individual nodes to share data. However, its broad adoption is hindered\nby the high communication costs of transmitting a large number of model\nparameters. This paper presents EvoFed, a novel approach that integrates\nEvolutionary Strategies (ES) with FL to address these challenges. EvoFed\nemploys a concept of 'fitness-based information sharing', deviating\nsignificantly from the conventional model-based FL. Rather than exchanging the\nactual updated model parameters, each node transmits a distance-based\nsimilarity measure between the locally updated model and each member of the\nnoise-perturbed model population. Each node, as well as the server, generates\nan identical population set of perturbed models in a completely synchronized\nfashion using the same random seeds. With properly chosen noise variance and\npopulation size, perturbed models can be combined to closely reflect the actual\nmodel updated using the local dataset, allowing the transmitted similarity\nmeasures (or fitness values) to carry nearly the complete information about the\nmodel parameters. As the population size is typically much smaller than the\nnumber of model parameters, the savings in communication load is large. The\nserver aggregates these fitness values and is able to update the global model.\nThis global fitness vector is then disseminated back to the nodes, each of\nwhich applies the same update to be synchronized to the global model. Our\nanalysis shows that EvoFed converges, and our experimental results validate\nthat at the cost of increased local processing loads, EvoFed achieves\nperformance comparable to FedAvg while reducing overall communication\nrequirements drastically in various practical settings.",
        "translated": ""
    },
    {
        "title": "Temporal Performance Prediction for Deep Convolutional Long Short-Term\n  Memory Networks",
        "url": "http://arxiv.org/abs/2311.07477v1",
        "pub_date": "2023-11-13",
        "summary": "Quantifying predictive uncertainty of deep semantic segmentation networks is\nessential in safety-critical tasks. In applications like autonomous driving,\nwhere video data is available, convolutional long short-term memory networks\nare capable of not only providing semantic segmentations but also predicting\nthe segmentations of the next timesteps. These models use cell states to\nbroadcast information from previous data by taking a time series of inputs to\npredict one or even further steps into the future. We present a temporal\npostprocessing method which estimates the prediction performance of\nconvolutional long short-term memory networks by either predicting the\nintersection over union of predicted and ground truth segments or classifying\nbetween intersection over union being equal to zero or greater than zero. To\nthis end, we create temporal cell state-based input metrics per segment and\ninvestigate different models for the estimation of the predictive quality based\non these metrics. We further study the influence of the number of considered\ncell states for the proposed metrics.",
        "translated": ""
    },
    {
        "title": "Masked Face Dataset Generation and Masked Face Recognition",
        "url": "http://arxiv.org/abs/2311.07475v1",
        "pub_date": "2023-11-13",
        "summary": "In the post-pandemic era, wearing face masks has posed great challenge to the\nordinary face recognition. In the previous study, researchers has applied\npretrained VGG16, and ResNet50 to extract features on the elaborate curated\nexisting masked face recognition (MFR) datasets, RMFRD and SMFRD. To make the\nmodel more adaptable to the real world situation where the sample size is\nsmaller and the camera environment has greater changes, we created a more\nchallenging masked face dataset ourselves, by selecting 50 identities with 1702\nimages from Labelled Faces in the Wild (LFW) Dataset, and simulated face masks\nthrough key point detection. The another part of our study is to solve the\nmasked face recognition problem, and we chose models by referring to the former\nstate of the art results, instead of directly using pretrained models, we fine\ntuned the model on our new dataset and use the last linear layer to do the\nclassification directly. Furthermore, we proposed using data augmentation\nstrategy to further increase the test accuracy, and fine tuned a new networks\nbeyond the former study, one of the most SOTA networks, Inception ResNet v1.\nThe best test accuracy on 50 identity MFR has achieved 95%.",
        "translated": ""
    },
    {
        "title": "ChartCheck: An Evidence-Based Fact-Checking Dataset over Real-World\n  Chart Images",
        "url": "http://arxiv.org/abs/2311.07453v1",
        "pub_date": "2023-11-13",
        "summary": "Data visualizations are common in the real-world. We often use them in data\nsources such as scientific documents, news articles, textbooks, and social\nmedia to summarize key information in a visual form. Charts can also mislead\nits audience by communicating false information or biasing them towards a\nspecific agenda. Verifying claims against charts is not a straightforward\nprocess. It requires analyzing both the text and visual components of the\nchart, considering characteristics such as colors, positions, and orientations.\nMoreover, to determine if a claim is supported by the chart content often\nrequires different types of reasoning. To address this challenge, we introduce\nChartCheck, a novel dataset for fact-checking against chart images. ChartCheck\nis the first large-scale dataset with 1.7k real-world charts and 10.5k\nhuman-written claims and explanations. We evaluated the dataset on\nstate-of-the-art models and achieved an accuracy of 73.9 in the finetuned\nsetting. Additionally, we identified chart characteristics and reasoning types\nthat challenge the models.",
        "translated": ""
    },
    {
        "title": "Instant3D: Instant Text-to-3D Generation",
        "url": "http://arxiv.org/abs/2311.08403v1",
        "pub_date": "2023-11-14",
        "summary": "Text-to-3D generation, which aims to synthesize vivid 3D objects from text\nprompts, has attracted much attention from the computer vision community. While\nseveral existing works have achieved impressive results for this task, they\nmainly rely on a time-consuming optimization paradigm. Specifically, these\nmethods optimize a neural field from scratch for each text prompt, taking\napproximately one hour or more to generate one object. This heavy and\nrepetitive training cost impedes their practical deployment. In this paper, we\npropose a novel framework for fast text-to-3D generation, dubbed Instant3D.\nOnce trained, Instant3D is able to create a 3D object for an unseen text prompt\nin less than one second with a single run of a feedforward network. We achieve\nthis remarkable speed by devising a new network that directly constructs a 3D\ntriplane from a text prompt. The core innovation of our Instant3D lies in our\nexploration of strategies to effectively inject text conditions into the\nnetwork. Furthermore, we propose a simple yet effective activation function,\nthe scaled-sigmoid, to replace the original sigmoid function, which speeds up\nthe training convergence by more than ten times. Finally, to address the Janus\n(multi-head) problem in 3D generation, we propose an adaptive Perp-Neg\nalgorithm that can dynamically adjust its concept negation scales according to\nthe severity of the Janus problem during training, effectively reducing the\nmulti-head effect. Extensive experiments on a wide variety of benchmark\ndatasets demonstrate that the proposed algorithm performs favorably against the\nstate-of-the-art methods both qualitatively and quantitatively, while achieving\nsignificantly better efficiency. The project page is at\nhttps://ming1993li.github.io/Instant3DProj.",
        "translated": ""
    },
    {
        "title": "Towards Open-Ended Visual Recognition with Large Language Model",
        "url": "http://arxiv.org/abs/2311.08400v1",
        "pub_date": "2023-11-14",
        "summary": "Localizing and recognizing objects in the open-ended physical world poses a\nlong-standing challenge within the domain of machine perception. Recent methods\nhave endeavored to address the issue by employing a class-agnostic mask (or\nbox) proposal model, complemented by an open-vocabulary classifier (e.g., CLIP)\nusing pre-extracted text embeddings. However, it is worth noting that these\nopen-vocabulary recognition models still exhibit limitations in practical\napplications. On one hand, they rely on the provision of class names during\ntesting, where the recognition performance heavily depends on this predefined\nset of semantic classes by users. On the other hand, when training with\nmultiple datasets, human intervention is required to alleviate the label\ndefinition conflict between them. In this paper, we introduce the OmniScient\nModel (OSM), a novel Large Language Model (LLM) based mask classifier, as a\nstraightforward and effective solution to the aforementioned challenges.\nSpecifically, OSM predicts class labels in a generative manner, thus removing\nthe supply of class names during both training and testing. It also enables\ncross-dataset training without any human interference, exhibiting robust\ngeneralization capabilities due to the world knowledge acquired from the LLM.\nBy combining OSM with an off-the-shelf mask proposal model, we present\npromising results on various benchmarks, and demonstrate its effectiveness in\nhandling novel concepts. Code/model are available at\nhttps://github.com/bytedance/OmniScient-Model.",
        "translated": ""
    },
    {
        "title": "MVSA-Net: Multi-View State-Action Recognition for Robust and Deployable\n  Trajectory Generation",
        "url": "http://arxiv.org/abs/2311.08393v1",
        "pub_date": "2023-11-14",
        "summary": "The learn-from-observation (LfO) paradigm is a human-inspired mode for a\nrobot to learn to perform a task simply by watching it being performed. LfO can\nfacilitate robot integration on factory floors by minimizing disruption and\nreducing tedious programming. A key component of the LfO pipeline is a\ntransformation of the depth camera frames to the corresponding task state and\naction pairs, which are then relayed to learning techniques such as imitation\nor inverse reinforcement learning for understanding the task parameters. While\nseveral existing computer vision models analyze videos for activity\nrecognition, SA-Net specifically targets robotic LfO from RGB-D data. However,\nSA-Net and many other models analyze frame data captured from a single\nviewpoint. Their analysis is therefore highly sensitive to occlusions of the\nobserved task, which are frequent in deployments. An obvious way of reducing\nocclusions is to simultaneously observe the task from multiple viewpoints and\nsynchronously fuse the multiple streams in the model. Toward this, we present\nmulti-view SA-Net, which generalizes the SA-Net model to allow the perception\nof multiple viewpoints of the task activity, integrate them, and better\nrecognize the state and action in each frame. Performance evaluations on two\ndistinct domains establish that MVSA-Net recognizes the state-action pairs\nunder occlusion more accurately compared to single-view MVSA-Net and other\nbaselines. Our ablation studies further evaluate its performance under\ndifferent ambient conditions and establish the contribution of the architecture\ncomponents. As such, MVSA-Net offers a significantly more robust and deployable\nstate-action trajectory generation compared to previous methods.",
        "translated": ""
    },
    {
        "title": "USLR: an open-source tool for unbiased and smooth longitudinal\n  registration of brain MR",
        "url": "http://arxiv.org/abs/2311.08371v1",
        "pub_date": "2023-11-14",
        "summary": "We present USLR, a computational framework for longitudinal registration of\nbrain MRI scans to estimate nonlinear image trajectories that are smooth across\ntime, unbiased to any timepoint, and robust to imaging artefacts. It operates\non the Lie algebra parameterisation of spatial transforms (which is compatible\nwith rigid transforms and stationary velocity fields for nonlinear deformation)\nand takes advantage of log-domain properties to solve the problem using\nBayesian inference. USRL estimates rigid and nonlinear registrations that: (i)\nbring all timepoints to an unbiased subject-specific space; and (i) compute a\nsmooth trajectory across the imaging time-series. We capitalise on\nlearning-based registration algorithms and closed-form expressions for fast\ninference. A use-case Alzheimer's disease study is used to showcase the\nbenefits of the pipeline in multiple fronts, such as time-consistent image\nsegmentation to reduce intra-subject variability, subject-specific prediction\nor population analysis using tensor-based morphometry. We demonstrate that such\napproach improves upon cross-sectional methods in identifying group\ndifferences, which can be helpful in detecting more subtle atrophy levels or in\nreducing sample sizes in clinical trials. The code is publicly available in\nhttps://github.com/acasamitjana/uslr",
        "translated": ""
    },
    {
        "title": "Rotation-Agnostic Image Representation Learning for Digital Pathology",
        "url": "http://arxiv.org/abs/2311.08359v1",
        "pub_date": "2023-11-14",
        "summary": "This paper addresses complex challenges in histopathological image analysis\nthrough three key contributions. Firstly, it introduces a fast patch selection\nmethod, FPS, for whole-slide image (WSI) analysis, significantly reducing\ncomputational cost while maintaining accuracy. Secondly, it presents PathDino,\na lightweight histopathology feature extractor with a minimal configuration of\nfive Transformer blocks and only 9 million parameters, markedly fewer than\nalternatives. Thirdly, it introduces a rotation-agnostic representation\nlearning paradigm using self-supervised learning, effectively mitigating\noverfitting. We also show that our compact model outperforms existing\nstate-of-the-art histopathology-specific vision transformers on 12 diverse\ndatasets, including both internal datasets spanning four sites (breast, liver,\nskin, and colorectal) and seven public datasets (PANDA, CAMELYON16, BRACS,\nDigestPath, Kather, PanNuke, and WSSS4LUAD). Notably, even with a training\ndataset of 6 million histopathology patches from The Cancer Genome Atlas\n(TCGA), our approach demonstrates an average 8.5% improvement in patch-level\nmajority vote performance. These contributions provide a robust framework for\nenhancing image analysis in digital pathology, rigorously validated through\nextensive evaluation. Project Page: https://rhazeslab.github.io/PathDino-Page/",
        "translated": ""
    },
    {
        "title": "Convolutional Neural Networks Exploiting Attributes of Biological\n  Neurons",
        "url": "http://arxiv.org/abs/2311.08314v1",
        "pub_date": "2023-11-14",
        "summary": "In this era of artificial intelligence, deep neural networks like\nConvolutional Neural Networks (CNNs) have emerged as front-runners, often\nsurpassing human capabilities. These deep networks are often perceived as the\npanacea for all challenges. Unfortunately, a common downside of these networks\nis their ''black-box'' character, which does not necessarily mirror the\noperation of biological neural systems. Some even have millions/billions of\nlearnable (tunable) parameters, and their training demands extensive data and\ntime.\n  Here, we integrate the principles of biological neurons in certain layer(s)\nof CNNs. Specifically, we explore the use of neuro-science-inspired\ncomputational models of the Lateral Geniculate Nucleus (LGN) and simple cells\nof the primary visual cortex. By leveraging such models, we aim to extract\nimage features to use as input to CNNs, hoping to enhance training efficiency\nand achieve better accuracy. We aspire to enable shallow networks with a\nPush-Pull Combination of Receptive Fields (PP-CORF) model of simple cells as\nthe foundation layer of CNNs to enhance their learning process and performance.\nTo achieve this, we propose a two-tower CNN, one shallow tower and the other as\nResNet 18. Rather than extracting the features blindly, it seeks to mimic how\nthe brain perceives and extracts features. The proposed system exhibits a\nnoticeable improvement in the performance (on an average of $5\\%-10\\%$) on\nCIFAR-10, CIFAR-100, and ImageNet-100 datasets compared to ResNet-18. We also\ncheck the efficiency of only the Push-Pull tower of the network.",
        "translated": ""
    },
    {
        "title": "The Heat is On: Thermal Facial Landmark Tracking",
        "url": "http://arxiv.org/abs/2311.08308v1",
        "pub_date": "2023-11-14",
        "summary": "Facial landmark tracking for thermal images requires tracking certain\nimportant regions of subjects' faces, using images from thermal images, which\nomit lighting and shading, but show the temperatures of their subjects. The\nfluctuations of heat in particular places reflect physiological changes like\nbloodflow and perspiration, which can be used to remotely gauge things like\nanxiety and excitement. Past work in this domain has been limited to only a\nvery limited set of architectures and techniques. This work goes further by\ntrying a comprehensive suit of various models with different components, such\nas residual connections, channel and feature-wise attention, as well as the\npractice of ensembling components of the network to work in parallel. The best\nmodel integrated convolutional and residual layers followed by a channel-wise\nself-attention layer, requiring less than 100K parameters.",
        "translated": ""
    },
    {
        "title": "Level Set KSVD",
        "url": "http://arxiv.org/abs/2311.08284v1",
        "pub_date": "2023-11-14",
        "summary": "We present a new algorithm for image segmentation - Level-set KSVD. Level-set\nKSVD merges the methods of sparse dictionary learning for feature extraction\nand variational level-set method for image segmentation. Specifically, we use a\ngeneralization of the Chan-Vese functional with features learned by KSVD. The\nmotivation for this model is agriculture based. Aerial images are taken in\norder to detect the spread of fungi in various crops. Our model is tested on\nsuch images of cotton fields. The results are compared to other methods.",
        "translated": ""
    },
    {
        "title": "ARTEMIS: Using GANs with Multiple Discriminators to Generate Art",
        "url": "http://arxiv.org/abs/2311.08278v1",
        "pub_date": "2023-11-14",
        "summary": "We propose a novel method for generating abstract art. First an autoencoder\nis trained to encode and decode the style representations of images, which are\nextracted from source images with a pretrained VGG network. Then, the decoder\ncomponent of the autoencoder is extracted and used as a generator in a GAN. The\ngenerator works with an ensemble of discriminators. Each discriminator takes\ndifferent style representations of the same images, and the generator is\ntrained to create images that create convincing style representations in order\nto deceive all of the generators. The generator is also trained to maximize a\ndiversity term. The resulting images had a surreal, geometric quality. We call\nour approach ARTEMIS (ARTistic Encoder- Multi- Discriminators Including\nSelf-Attention), as it uses the self-attention layers and an encoder-decoder\narchitecture.",
        "translated": ""
    },
    {
        "title": "Defining the boundaries: challenges and advances in identifying cells in\n  microscopy images",
        "url": "http://arxiv.org/abs/2311.08269v1",
        "pub_date": "2023-11-14",
        "summary": "Segmentation, or the outlining of objects within images, is a critical step\nin the measurement and analysis of cells within microscopy images. While\nimprovements continue to be made in tools that rely on classical methods for\nsegmentation, deep learning-based tools increasingly dominate advances in the\ntechnology. Specialist models such as Cellpose continue to improve in accuracy\nand user-friendliness, and segmentation challenges such as the Multi-Modality\nCell Segmentation Challenge continue to push innovation in accuracy across\nwidely-varying test data as well as efficiency and usability. Increased\nattention on documentation, sharing, and evaluation standards are leading to\nincreased user-friendliness and acceleration towards the goal of a truly\nuniversal method.",
        "translated": ""
    },
    {
        "title": "Single-Image 3D Human Digitization with Shape-Guided Diffusion",
        "url": "http://arxiv.org/abs/2311.09221v1",
        "pub_date": "2023-11-15",
        "summary": "We present an approach to generate a 360-degree view of a person with a\nconsistent, high-resolution appearance from a single input image. NeRF and its\nvariants typically require videos or images from different viewpoints. Most\nexisting approaches taking monocular input either rely on ground-truth 3D scans\nfor supervision or lack 3D consistency. While recent 3D generative models show\npromise of 3D consistent human digitization, these approaches do not generalize\nwell to diverse clothing appearances, and the results lack photorealism. Unlike\nexisting work, we utilize high-capacity 2D diffusion models pretrained for\ngeneral image synthesis tasks as an appearance prior of clothed humans. To\nachieve better 3D consistency while retaining the input identity, we\nprogressively synthesize multiple views of the human in the input image by\ninpainting missing regions with shape-guided diffusion conditioned on\nsilhouette and surface normal. We then fuse these synthesized multi-view images\nvia inverse rendering to obtain a fully textured high-resolution 3D mesh of the\ngiven person. Experiments show that our approach outperforms prior methods and\nachieves photorealistic 360-degree synthesis of a wide range of clothed humans\nwith complex textures from a single image.",
        "translated": ""
    },
    {
        "title": "DMV3D: Denoising Multi-View Diffusion using 3D Large Reconstruction\n  Model",
        "url": "http://arxiv.org/abs/2311.09217v1",
        "pub_date": "2023-11-15",
        "summary": "We propose \\textbf{DMV3D}, a novel 3D generation approach that uses a\ntransformer-based 3D large reconstruction model to denoise multi-view\ndiffusion. Our reconstruction model incorporates a triplane NeRF representation\nand can denoise noisy multi-view images via NeRF reconstruction and rendering,\nachieving single-stage 3D generation in $\\sim$30s on single A100 GPU. We train\n\\textbf{DMV3D} on large-scale multi-view image datasets of highly diverse\nobjects using only image reconstruction losses, without accessing 3D assets. We\ndemonstrate state-of-the-art results for the single-image reconstruction\nproblem where probabilistic modeling of unseen object parts is required for\ngenerating diverse reconstructions with sharp textures. We also show\nhigh-quality text-to-3D generation results outperforming previous 3D diffusion\nmodels. Our project website is at: https://justimyhxu.github.io/projects/dmv3d/ .",
        "translated": ""
    },
    {
        "title": "ConvNet vs Transformer, Supervised vs CLIP: Beyond ImageNet Accuracy",
        "url": "http://arxiv.org/abs/2311.09215v1",
        "pub_date": "2023-11-15",
        "summary": "Modern computer vision offers a great variety of models to practitioners, and\nselecting a model from multiple options for specific applications can be\nchallenging. Conventionally, competing model architectures and training\nprotocols are compared by their classification accuracy on ImageNet. However,\nthis single metric does not fully capture performance nuances critical for\nspecialized tasks. In this work, we conduct an in-depth comparative analysis of\nmodel behaviors beyond ImageNet accuracy, for both ConvNet and Vision\nTransformer architectures, each across supervised and CLIP training paradigms.\nAlthough our selected models have similar ImageNet accuracies and compute\nrequirements, we find that they differ in many other aspects: types of\nmistakes, output calibration, transferability, and feature invariance, among\nothers. This diversity in model characteristics, not captured by traditional\nmetrics, highlights the need for more nuanced analysis when choosing among\ndifferent models. Our code is available at\nhttps://github.com/kirill-vish/Beyond-INet.",
        "translated": ""
    },
    {
        "title": "The Role of Chain-of-Thought in Complex Vision-Language Reasoning Task",
        "url": "http://arxiv.org/abs/2311.09193v1",
        "pub_date": "2023-11-15",
        "summary": "The study explores the effectiveness of the Chain-of-Thought approach, known\nfor its proficiency in language tasks by breaking them down into sub-tasks and\nintermediate steps, in improving vision-language tasks that demand\nsophisticated perception and reasoning. We present the \"Description then\nDecision\" strategy, which is inspired by how humans process signals. This\nstrategy significantly improves probing task performance by 50%, establishing\nthe groundwork for future research on reasoning paradigms in complex\nvision-language tasks.",
        "translated": ""
    },
    {
        "title": "Domain Aligned CLIP for Few-shot Classification",
        "url": "http://arxiv.org/abs/2311.09191v1",
        "pub_date": "2023-11-15",
        "summary": "Large vision-language representation learning models like CLIP have\ndemonstrated impressive performance for zero-shot transfer to downstream tasks\nwhile largely benefiting from inter-modal (image-text) alignment via\ncontrastive objectives. This downstream performance can further be enhanced by\nfull-scale fine-tuning which is often compute intensive, requires large\nlabelled data, and can reduce out-of-distribution (OOD) robustness.\nFurthermore, sole reliance on inter-modal alignment might overlook the rich\ninformation embedded within each individual modality. In this work, we\nintroduce a sample-efficient domain adaptation strategy for CLIP, termed Domain\nAligned CLIP (DAC), which improves both intra-modal (image-image) and\ninter-modal alignment on target distributions without fine-tuning the main\nmodel. For intra-modal alignment, we introduce a lightweight adapter that is\nspecifically trained with an intra-modal contrastive objective. To improve\ninter-modal alignment, we introduce a simple framework to modulate the\nprecomputed class text embeddings. The proposed few-shot fine-tuning framework\nis computationally efficient, robust to distribution shifts, and does not alter\nCLIP's parameters. We study the effectiveness of DAC by benchmarking on 11\nwidely used image classification tasks with consistent improvements in 16-shot\nclassification upon strong baselines by about 2.3% and demonstrate competitive\nperformance on 4 OOD robustness benchmarks.",
        "translated": ""
    },
    {
        "title": "On the Computation of the Gaussian Rate-Distortion-Perception Function",
        "url": "http://arxiv.org/abs/2311.09190v1",
        "pub_date": "2023-11-15",
        "summary": "In this paper, we study the computation of the rate-distortion-perception\nfunction (RDPF) for a multivariate Gaussian source under mean squared error\n(MSE) distortion and, respectively, Kullback-Leibler divergence, geometric\nJensen-Shannon divergence, squared Hellinger distance, and squared\nWasserstein-2 distance perception metrics. To this end, we first characterize\nthe analytical bounds of the scalar Gaussian RDPF for the aforementioned\ndivergence functions, also providing the RDPF-achieving forward \"test-channel\"\nrealization. Focusing on the multivariate case, we establish that, for\ntensorizable distortion and perception metrics, the optimal solution resides on\nthe vector space spanned by the eigenvector of the source covariance matrix.\nConsequently, the multivariate optimization problem can be expressed as a\nfunction of the scalar Gaussian RDPFs of the source marginals, constrained by\nglobal distortion and perception levels. Leveraging this characterization, we\ndesign an alternating minimization scheme based on the block nonlinear\nGauss-Seidel method, which optimally solves the problem while identifying the\nGaussian RDPF-achieving realization. Furthermore, the associated algorithmic\nembodiment is provided, as well as the convergence and the rate of convergence\ncharacterization. Lastly, for the \"perfect realism\" regime, the analytical\nsolution for the multivariate Gaussian RDPF is obtained. We corroborate our\nresults with numerical simulations and draw connections to existing results.",
        "translated": ""
    },
    {
        "title": "RBPGAN: Recurrent Back-Projection GAN for Video Super Resolution",
        "url": "http://arxiv.org/abs/2311.09178v1",
        "pub_date": "2023-11-15",
        "summary": "Recently, video super resolution (VSR) has become a very impactful task in\nthe area of Computer Vision due to its various applications. In this paper, we\npropose Recurrent Back-Projection Generative Adversarial Network (RBPGAN) for\nVSR in an attempt to generate temporally coherent solutions while preserving\nspatial details. RBPGAN integrates two state-of-the-art models to get the best\nin both worlds without compromising the accuracy of produced video. The\ngenerator of the model is inspired by RBPN system, while the discriminator is\ninspired by TecoGAN. We also utilize Ping-Pong loss to increase temporal\nconsistency over time. Our contribution together results in a model that\noutperforms earlier work in terms of temporally consistent details, as we will\ndemonstrate qualitatively and quantitatively using different datasets.",
        "translated": ""
    },
    {
        "title": "WildlifeDatasets: An open-source toolkit for animal re-identification",
        "url": "http://arxiv.org/abs/2311.09118v1",
        "pub_date": "2023-11-15",
        "summary": "In this paper, we present WildlifeDatasets\n(https://github.com/WildlifeDatasets/wildlife-datasets) - an open-source\ntoolkit intended primarily for ecologists and computer-vision /\nmachine-learning researchers. The WildlifeDatasets is written in Python, allows\nstraightforward access to publicly available wildlife datasets, and provides a\nwide variety of methods for dataset pre-processing, performance analysis, and\nmodel fine-tuning. We showcase the toolkit in various scenarios and baseline\nexperiments, including, to the best of our knowledge, the most comprehensive\nexperimental comparison of datasets and methods for wildlife re-identification,\nincluding both local descriptors and deep learning approaches. Furthermore, we\nprovide the first-ever foundation model for individual re-identification within\na wide range of species - MegaDescriptor - that provides state-of-the-art\nperformance on animal re-identification datasets and outperforms other\npre-trained models such as CLIP and DINOv2 by a significant margin. To make the\nmodel available to the general public and to allow easy integration with any\nexisting wildlife monitoring applications, we provide multiple MegaDescriptor\nflavors (i.e., Small, Medium, and Large) through the HuggingFace hub\n(https://huggingface.co/BVRA).",
        "translated": ""
    },
    {
        "title": "Cross-view and Cross-pose Completion for 3D Human Understanding",
        "url": "http://arxiv.org/abs/2311.09104v1",
        "pub_date": "2023-11-15",
        "summary": "Human perception and understanding is a major domain of computer vision\nwhich, like many other vision subdomains recently, stands to gain from the use\nof large models pre-trained on large datasets. We hypothesize that the most\ncommon pre-training strategy of relying on general purpose, object-centric\nimage datasets such as ImageNet, is limited by an important domain shift. On\nthe other hand, collecting domain specific ground truth such as 2D or 3D labels\ndoes not scale well. Therefore, we propose a pre-training approach based on\nself-supervised learning that works on human-centric data using only images.\nOur method uses pairs of images of humans: the first is partially masked and\nthe model is trained to reconstruct the masked parts given the visible ones and\na second image. It relies on both stereoscopic (cross-view) pairs, and temporal\n(cross-pose) pairs taken from videos, in order to learn priors about 3D as well\nas human motion. We pre-train a model for body-centric tasks and one for\nhand-centric tasks. With a generic transformer architecture, these models\noutperform existing self-supervised pre-training methods on a wide set of\nhuman-centric downstream tasks, and obtain state-of-the-art performance for\ninstance when fine-tuning for model-based and model-free human mesh recovery.",
        "translated": ""
    },
    {
        "title": "Guided Scale Space Radon Transform for linear structures detection",
        "url": "http://arxiv.org/abs/2311.09103v1",
        "pub_date": "2023-11-15",
        "summary": "Using integral transforms to the end of lines detection in images with\ncomplex background, makes the detection a hard task needing additional\nprocessing to manage the detection. As an integral transform, the Scale Space\nRadon Transform (SSRT) suffers from such drawbacks, even with its great\nabilities for thick lines detection. In this work, we propose a method to\naddress this issue for automatic detection of thick linear structures in gray\nscale and binary images using the SSRT, whatever the image background content.\nThis method involves the calculated Hessian orientations of the investigated\nimage while computing its SSRT, in such a way that linear structures are\nemphasized in the SSRT space. As a consequence, the subsequent maxima detection\nin the SSRT space is done on a modified transform space freed from unwanted\nparts and, consequently, from irrelevant peaks that usually drown the peaks\nrepresenting lines. Besides, highlighting the linear structure in the SSRT\nspace permitting, thus, to efficiently detect lines of different thickness in\nsynthetic and real images, the experiments show also the method robustness\nagainst noise and complex background.",
        "translated": ""
    },
    {
        "title": "The Chosen One: Consistent Characters in Text-to-Image Diffusion Models",
        "url": "http://arxiv.org/abs/2311.10093v1",
        "pub_date": "2023-11-16",
        "summary": "Recent advances in text-to-image generation models have unlocked vast\npotential for visual creativity. However, these models struggle with generation\nof consistent characters, a crucial aspect for numerous real-world applications\nsuch as story visualization, game development asset design, advertising, and\nmore. Current methods typically rely on multiple pre-existing images of the\ntarget character or involve labor-intensive manual processes. In this work, we\npropose a fully automated solution for consistent character generation, with\nthe sole input being a text prompt. We introduce an iterative procedure that,\nat each stage, identifies a coherent set of images sharing a similar identity\nand extracts a more consistent identity from this set. Our quantitative\nanalysis demonstrates that our method strikes a better balance between prompt\nalignment and identity consistency compared to the baseline methods, and these\nfindings are reinforced by a user study. To conclude, we showcase several\npractical applications of our approach. Project page is available at\nhttps://omriavrahami.com/the-chosen-one",
        "translated": ""
    },
    {
        "title": "Traffic Video Object Detection using Motion Prior",
        "url": "http://arxiv.org/abs/2311.10092v1",
        "pub_date": "2023-11-16",
        "summary": "Traffic videos inherently differ from generic videos in their stationary\ncamera setup, thus providing a strong motion prior where objects often move in\na specific direction over a short time interval. Existing works predominantly\nemploy generic video object detection framework for traffic video object\ndetection, which yield certain advantages such as broad applicability and\nrobustness to diverse scenarios. However, they fail to harness the strength of\nmotion prior to enhance detection accuracy. In this work, we propose two\ninnovative methods to exploit the motion prior and boost the performance of\nboth fully-supervised and semi-supervised traffic video object detection.\nFirstly, we introduce a new self-attention module that leverages the motion\nprior to guide temporal information integration in the fully-supervised\nsetting. Secondly, we utilise the motion prior to develop a pseudo-labelling\nmechanism to eliminate noisy pseudo labels for the semi-supervised setting.\nBoth of our motion-prior-centred methods consistently demonstrates superior\nperformance, outperforming existing state-of-the-art approaches by a margin of\n2% in terms of mAP.",
        "translated": ""
    },
    {
        "title": "Adaptive Shells for Efficient Neural Radiance Field Rendering",
        "url": "http://arxiv.org/abs/2311.10091v1",
        "pub_date": "2023-11-16",
        "summary": "Neural radiance fields achieve unprecedented quality for novel view\nsynthesis, but their volumetric formulation remains expensive, requiring a huge\nnumber of samples to render high-resolution images. Volumetric encodings are\nessential to represent fuzzy geometry such as foliage and hair, and they are\nwell-suited for stochastic optimization. Yet, many scenes ultimately consist\nlargely of solid surfaces which can be accurately rendered by a single sample\nper pixel. Based on this insight, we propose a neural radiance formulation that\nsmoothly transitions between volumetric- and surface-based rendering, greatly\naccelerating rendering speed and even improving visual fidelity. Our method\nconstructs an explicit mesh envelope which spatially bounds a neural volumetric\nrepresentation. In solid regions, the envelope nearly converges to a surface\nand can often be rendered with a single sample. To this end, we generalize the\nNeuS formulation with a learned spatially-varying kernel size which encodes the\nspread of the density, fitting a wide kernel to volume-like regions and a tight\nkernel to surface-like regions. We then extract an explicit mesh of a narrow\nband around the surface, with width determined by the kernel size, and\nfine-tune the radiance field within this band. At inference time, we cast rays\nagainst the mesh and evaluate the radiance field only within the enclosed\nregion, greatly reducing the number of samples required. Experiments show that\nour approach enables efficient rendering at very high fidelity. We also\ndemonstrate that the extracted envelope enables downstream applications such as\nanimation and simulation.",
        "translated": ""
    },
    {
        "title": "Emu Edit: Precise Image Editing via Recognition and Generation Tasks",
        "url": "http://arxiv.org/abs/2311.10089v1",
        "pub_date": "2023-11-16",
        "summary": "Instruction-based image editing holds immense potential for a variety of\napplications, as it enables users to perform any editing operation using a\nnatural language instruction. However, current models in this domain often\nstruggle with accurately executing user instructions. We present Emu Edit, a\nmulti-task image editing model which sets state-of-the-art results in\ninstruction-based image editing. To develop Emu Edit we train it to multi-task\nacross an unprecedented range of tasks, such as region-based editing, free-form\nediting, and Computer Vision tasks, all of which are formulated as generative\ntasks. Additionally, to enhance Emu Edit's multi-task learning abilities, we\nprovide it with learned task embeddings which guide the generation process\ntowards the correct edit type. Both these elements are essential for Emu Edit's\noutstanding performance. Furthermore, we show that Emu Edit can generalize to\nnew tasks, such as image inpainting, super-resolution, and compositions of\nediting tasks, with just a few labeled examples. This capability offers a\nsignificant advantage in scenarios where high-quality samples are scarce.\nLastly, to facilitate a more rigorous and informed assessment of instructable\nimage editing models, we release a new challenging and versatile benchmark that\nincludes seven different image editing tasks.",
        "translated": ""
    },
    {
        "title": "DRESS: Instructing Large Vision-Language Models to Align and Interact\n  with Humans via Natural Language Feedback",
        "url": "http://arxiv.org/abs/2311.10081v1",
        "pub_date": "2023-11-16",
        "summary": "We present DRESS, a large vision language model (LVLM) that innovatively\nexploits Natural Language feedback (NLF) from Large Language Models to enhance\nits alignment and interactions by addressing two key limitations in the\nstate-of-the-art LVLMs. First, prior LVLMs generally rely only on the\ninstruction finetuning stage to enhance alignment with human preferences.\nWithout incorporating extra feedback, they are still prone to generate\nunhelpful, hallucinated, or harmful responses. Second, while the visual\ninstruction tuning data is generally structured in a multi-turn dialogue\nformat, the connections and dependencies among consecutive conversational turns\nare weak. This reduces the capacity for effective multi-turn interactions. To\ntackle these, we propose a novel categorization of the NLF into two key types:\ncritique and refinement. The critique NLF identifies the strengths and\nweaknesses of the responses and is used to align the LVLMs with human\npreferences. The refinement NLF offers concrete suggestions for improvement and\nis adopted to improve the interaction ability of the LVLMs-- which focuses on\nLVLMs' ability to refine responses by incorporating feedback in multi-turn\ninteractions. To address the non-differentiable nature of NLF, we generalize\nconditional reinforcement learning for training. Our experimental results\ndemonstrate that DRESS can generate more helpful (9.76%), honest (11.52%), and\nharmless (21.03%) responses, and more effectively learn from feedback during\nmulti-turn interactions compared to SOTA LVMLs.",
        "translated": ""
    },
    {
        "title": "Visual Environment Assessment for Safe Autonomous Quadrotor Landing",
        "url": "http://arxiv.org/abs/2311.10065v1",
        "pub_date": "2023-11-16",
        "summary": "Autonomous identification and evaluation of safe landing zones are of\nparamount importance for ensuring the safety and effectiveness of aerial robots\nin the event of system failures, low battery, or the successful completion of\nspecific tasks. In this paper, we present a novel approach for detection and\nassessment of potential landing sites for safe quadrotor landing. Our solution\nefficiently integrates 2D and 3D environmental information, eliminating the\nneed for external aids such as GPS and computationally intensive elevation\nmaps. The proposed pipeline combines semantic data derived from a Neural\nNetwork (NN), to extract environmental features, with geometric data obtained\nfrom a disparity map, to extract critical geometric attributes such as slope,\nflatness, and roughness. We define several cost metrics based on these\nattributes to evaluate safety, stability, and suitability of regions in the\nenvironments and identify the most suitable landing area. Our approach runs in\nreal-time on quadrotors equipped with limited computational capabilities.\nExperimental results conducted in diverse environments demonstrate that the\nproposed method can effectively assess and identify suitable landing areas,\nenabling the safe and autonomous landing of a quadrotor.",
        "translated": ""
    },
    {
        "title": "Analyzing Deviations of Dyadic Lines in Fast Hough Transform",
        "url": "http://arxiv.org/abs/2311.10064v1",
        "pub_date": "2023-11-16",
        "summary": "Fast Hough transform is a widely used algorithm in pattern recognition. The\nalgorithm relies on approximating lines using a specific discrete line model\ncalled dyadic lines. The worst-case deviation of a dyadic line from the ideal\nline it used to construct grows as $O(log(n))$, where $n$ is the linear size of\nthe image. But few lines actually reach the worst-case bound. The present paper\naddresses a statistical analysis of the deviation of a dyadic line from its\nideal counterpart. Specifically, our findings show that the mean deviation is\nzero, and the variance grows as $O(log(n))$. As $n$ increases, the distribution\nof these (suitably normalized) deviations converges towards a normal\ndistribution with zero mean and a small variance. This limiting result makes an\nessential use of ergodic theory.",
        "translated": ""
    },
    {
        "title": "Depth Insight -- Contribution of Different Features to Indoor\n  Single-image Depth Estimation",
        "url": "http://arxiv.org/abs/2311.10042v1",
        "pub_date": "2023-11-16",
        "summary": "Depth estimation from a single image is a challenging problem in computer\nvision because binocular disparity or motion information is absent. Whereas\nimpressive performances have been reported in this area recently using\nend-to-end trained deep neural architectures, as to what cues in the images\nthat are being exploited by these black box systems is hard to know. To this\nend, in this work, we quantify the relative contributions of the known cues of\ndepth in a monocular depth estimation setting using an indoor scene data set.\nOur work uses feature extraction techniques to relate the single features of\nshape, texture, colour and saturation, taken in isolation, to predict depth. We\nfind that the shape of objects extracted by edge detection substantially\ncontributes more than others in the indoor setting considered, while the other\nfeatures also have contributions in varying degrees. These insights will help\noptimise depth estimation models, boosting their accuracy and robustness. They\npromise to broaden the practical applications of vision-based depth estimation.\nThe project code is attached to the supplementary material and will be\npublished on GitHub.",
        "translated": ""
    },
    {
        "title": "Match and Locate: low-frequency monocular odometry based on deep feature\n  matching",
        "url": "http://arxiv.org/abs/2311.10034v1",
        "pub_date": "2023-11-16",
        "summary": "Accurate and robust pose estimation plays a crucial role in many robotic\nsystems. Popular algorithms for pose estimation typically rely on high-fidelity\nand high-frequency signals from various sensors. Inclusion of these sensors\nmakes the system less affordable and much more complicated. In this work we\nintroduce a novel approach for the robotic odometry which only requires a\nsingle camera and, importantly, can produce reliable estimates given even\nextremely low-frequency signal of around one frame per second. The approach is\nbased on matching image features between the consecutive frames of the video\nstream using deep feature matching models. The resulting coarse estimate is\nthen adjusted by a convolutional neural network, which is also responsible for\nestimating the scale of the transition, otherwise irretrievable using only the\nfeature matching information. We evaluate the performance of the approach in\nthe AISG-SLA Visual Localisation Challenge and find that while being\ncomputationally efficient and easy to implement our method shows competitive\nresults with only around $3^{\\circ}$ of orientation estimation error and $2m$\nof translation estimation error taking the third place in the challenge.",
        "translated": ""
    },
    {
        "title": "On the Overconfidence Problem in Semantic 3D Mapping",
        "url": "http://arxiv.org/abs/2311.10018v1",
        "pub_date": "2023-11-16",
        "summary": "Semantic 3D mapping, the process of fusing depth and image segmentation\ninformation between multiple views to build 3D maps annotated with object\nclasses in real-time, is a recent topic of interest. This paper highlights the\nfusion overconfidence problem, in which conventional mapping methods assign\nhigh confidence to the entire map even when they are incorrect, leading to\nmiscalibrated outputs. Several methods to improve uncertainty calibration at\ndifferent stages in the fusion pipeline are presented and compared on the\nScanNet dataset. We show that the most widely used Bayesian fusion strategy is\namong the worst calibrated, and propose a learned pipeline that combines fusion\nand calibration, GLFS, which achieves simultaneously higher accuracy and 3D map\ncalibration while retaining real-time capability. We further illustrate the\nimportance of map calibration on a downstream task by showing that\nincorporating proper semantic fusion on a modular ObjectNav agent improves its\nsuccess rates. Our code will be provided on Github for reproducibility upon\nacceptance.",
        "translated": ""
    },
    {
        "title": "Emu Video: Factorizing Text-to-Video Generation by Explicit Image\n  Conditioning",
        "url": "http://arxiv.org/abs/2311.10709v1",
        "pub_date": "2023-11-17",
        "summary": "We present Emu Video, a text-to-video generation model that factorizes the\ngeneration into two steps: first generating an image conditioned on the text,\nand then generating a video conditioned on the text and the generated image. We\nidentify critical design decisions--adjusted noise schedules for diffusion, and\nmulti-stage training--that enable us to directly generate high quality and high\nresolution videos, without requiring a deep cascade of models as in prior work.\nIn human evaluations, our generated videos are strongly preferred in quality\ncompared to all prior work--81% vs. Google's Imagen Video, 90% vs. Nvidia's\nPYOCO, and 96% vs. Meta's Make-A-Video. Our model outperforms commercial\nsolutions such as RunwayML's Gen2 and Pika Labs. Finally, our factorizing\napproach naturally lends itself to animating images based on a user's text\nprompt, where our generations are preferred 96% over prior work.",
        "translated": ""
    },
    {
        "title": "SelfEval: Leveraging the discriminative nature of generative models for\n  evaluation",
        "url": "http://arxiv.org/abs/2311.10708v1",
        "pub_date": "2023-11-17",
        "summary": "In this work, we show that text-to-image generative models can be 'inverted'\nto assess their own text-image understanding capabilities in a completely\nautomated manner.\n  Our method, called SelfEval, uses the generative model to compute the\nlikelihood of real images given text prompts, making the generative model\ndirectly applicable to discriminative tasks.\n  Using SelfEval, we repurpose standard datasets created for evaluating\nmultimodal text-image discriminative models to evaluate generative models in a\nfine-grained manner: assessing their performance on attribute binding, color\nrecognition, counting, shape recognition, spatial understanding.\n  To the best of our knowledge SelfEval is the first automated metric to show a\nhigh degree of agreement for measuring text-faithfulness with the gold-standard\nhuman evaluations across multiple models and benchmarks.\n  Moreover, SelfEval enables us to evaluate generative models on challenging\ntasks such as Winoground image-score where they demonstrate competitive\nperformance to discriminative models.\n  We also show severe drawbacks of standard automated metrics such as\nCLIP-score to measure text faithfulness on benchmarks such as DrawBench, and\nhow SelfEval sidesteps these issues.\n  We hope SelfEval enables easy and reliable automated evaluation for diffusion\nmodels.",
        "translated": ""
    },
    {
        "title": "Multimodal Representation Learning by Alternating Unimodal Adaptation",
        "url": "http://arxiv.org/abs/2311.10707v1",
        "pub_date": "2023-11-17",
        "summary": "Multimodal learning, which integrates data from diverse sensory modes, plays\na pivotal role in artificial intelligence. However, existing multimodal\nlearning methods often struggle with challenges where some modalities appear\nmore dominant than others during multimodal learning, resulting in suboptimal\nperformance. To address this challenge, we propose MLA (Multimodal Learning\nwith Alternating Unimodal Adaptation). MLA reframes the conventional joint\nmultimodal learning process by transforming it into an alternating unimodal\nlearning process, thereby minimizing interference between modalities.\nSimultaneously, it captures cross-modal interactions through a shared head,\nwhich undergoes continuous optimization across different modalities. This\noptimization process is controlled by a gradient modification mechanism to\nprevent the shared head from losing previously acquired information. During the\ninference phase, MLA utilizes a test-time uncertainty-based model fusion\nmechanism to integrate multimodal information. Extensive experiments are\nconducted on five diverse datasets, encompassing scenarios with complete\nmodalities and scenarios with missing modalities. These experiments demonstrate\nthe superiority of MLA over competing prior approaches.",
        "translated": ""
    },
    {
        "title": "SpACNN-LDVAE: Spatial Attention Convolutional Latent Dirichlet\n  Variational Autoencoder for Hyperspectral Pixel Unmixing",
        "url": "http://arxiv.org/abs/2311.10701v1",
        "pub_date": "2023-11-17",
        "summary": "The Hyperspectral Unxming problem is to find the pure spectral signal of the\nunderlying materials (endmembers) and their proportions (abundances). The\nproposed method builds upon the recently proposed method, Latent Dirichlet\nVariational Autoencoder (LDVAE). It assumes that abundances can be encoded as\nDirichlet Distributions while mixed pixels and endmembers are represented by\nMultivariate Normal Distributions. However, LDVAE does not leverage spatial\ninformation present in an HSI; we propose an Isotropic CNN encoder with spatial\nattention to solve the hyperspectral unmixing problem. We evaluated our model\non Samson, Hydice Urban, Cuprite, and OnTech-HSI-Syn-21 datasets. Our model\nalso leverages the transfer learning paradigm for Cuprite Dataset, where we\ntrain the model on synthetic data and evaluate it on real-world data. We are\nable to observe the improvement in the results for the endmember extraction and\nabundance estimation by incorporating the spatial information. Code can be\nfound at https://github.com/faisalqureshi/cnn-ldvae",
        "translated": ""
    },
    {
        "title": "Using linear initialisation to improve speed of convergence and\n  fully-trained error in Autoencoders",
        "url": "http://arxiv.org/abs/2311.10699v1",
        "pub_date": "2023-11-17",
        "summary": "Good weight initialisation is an important step in successful training of\nArtificial Neural Networks. Over time a number of improvements have been\nproposed to this process. In this paper we introduce a novel weight\ninitialisation technique called the Straddled Matrix Initialiser. This\ninitialisation technique is motivated by our assumption that major,\nglobal-scale relationships in data are linear with only smaller effects\nrequiring complex non-linearities. Combination of Straddled Matrix and ReLU\nactivation function initialises a Neural Network as a de facto linear model,\nwhich we postulate should be a better starting point for optimisation given our\nassumptions. We test this by training autoencoders on three datasets using\nStraddled Matrix and seven other state-of-the-art weight initialisation\ntechniques. In all our experiments the Straddeled Matrix Initialiser clearly\noutperforms all other methods.",
        "translated": ""
    },
    {
        "title": "Versatile Medical Image Segmentation Learned from Multi-Source Datasets\n  via Model Self-Disambiguation",
        "url": "http://arxiv.org/abs/2311.10696v1",
        "pub_date": "2023-11-17",
        "summary": "A versatile medical image segmentation model applicable to imaging data\ncollected with diverse equipment and protocols can facilitate model deployment\nand maintenance. However, building such a model typically requires a large,\ndiverse, and fully annotated dataset, which is rarely available due to the\nlabor-intensive and costly data curation. In this study, we develop a\ncost-efficient method by harnessing readily available data with partially or\neven sparsely annotated segmentation labels. We devise strategies for model\nself-disambiguation, prior knowledge incorporation, and imbalance mitigation to\naddress challenges associated with inconsistently labeled data from various\nsources, including label ambiguity and imbalances across modalities, datasets,\nand segmentation labels. Experimental results on a multi-modal dataset compiled\nfrom eight different sources for abdominal organ segmentation have demonstrated\nour method's effectiveness and superior performance over alternative\nstate-of-the-art methods, highlighting its potential for optimizing the use of\nexisting annotated data and reducing the annotation efforts for new data to\nfurther enhance model capability.",
        "translated": ""
    },
    {
        "title": "3D-TexSeg: Unsupervised Segmentation of 3D Texture using Mutual\n  Transformer Learning",
        "url": "http://arxiv.org/abs/2311.10651v1",
        "pub_date": "2023-11-17",
        "summary": "Analysis of the 3D Texture is indispensable for various tasks, such as\nretrieval, segmentation, classification, and inspection of sculptures, knitted\nfabrics, and biological tissues. A 3D texture is a locally repeated surface\nvariation independent of the surface's overall shape and can be determined\nusing the local neighborhood and its characteristics. Existing techniques\ntypically employ computer vision techniques that analyze a 3D mesh globally,\nderive features, and then utilize the obtained features for retrieval or\nclassification. Several traditional and learning-based methods exist in the\nliterature, however, only a few are on 3D texture, and nothing yet, to the best\nof our knowledge, on the unsupervised schemes. This paper presents an original\nframework for the unsupervised segmentation of the 3D texture on the mesh\nmanifold. We approach this problem as binary surface segmentation, partitioning\nthe mesh surface into textured and non-textured regions without prior\nannotation. We devise a mutual transformer-based system comprising a label\ngenerator and a cleaner. The two models take geometric image representations of\nthe surface mesh facets and label them as texture or non-texture across an\niterative mutual learning scheme. Extensive experiments on three publicly\navailable datasets with diverse texture patterns demonstrate that the proposed\nframework outperforms standard and SOTA unsupervised techniques and competes\nreasonably with supervised methods.",
        "translated": ""
    },
    {
        "title": "Self-trained Panoptic Segmentation",
        "url": "http://arxiv.org/abs/2311.10648v1",
        "pub_date": "2023-11-17",
        "summary": "Panoptic segmentation is an important computer vision task which combines\nsemantic and instance segmentation. It plays a crucial role in domains of\nmedical image analysis, self-driving vehicles, and robotics by providing a\ncomprehensive understanding of visual environments. Traditionally, deep\nlearning panoptic segmentation models have relied on dense and accurately\nannotated training data, which is expensive and time consuming to obtain.\nRecent advancements in self-supervised learning approaches have shown great\npotential in leveraging synthetic and unlabelled data to generate pseudo-labels\nusing self-training to improve the performance of instance and semantic\nsegmentation models. The three available methods for self-supervised panoptic\nsegmentation use proposal-based transformer architectures which are\ncomputationally expensive, complicated and engineered for specific tasks. The\naim of this work is to develop a framework to perform embedding-based\nself-supervised panoptic segmentation using self-training in a\nsynthetic-to-real domain adaptation problem setting.",
        "translated": ""
    },
    {
        "title": "Astronomical Images Quality Assessment with Automated Machine Learning",
        "url": "http://arxiv.org/abs/2311.10617v1",
        "pub_date": "2023-11-17",
        "summary": "Electronically Assisted Astronomy consists in capturing deep sky images with\na digital camera coupled to a telescope to display views of celestial objects\nthat would have been invisible through direct observation. This practice\ngenerates a large quantity of data, which may then be enhanced with dedicated\nimage editing software after observation sessions. In this study, we show how\nImage Quality Assessment can be useful for automatically rating astronomical\nimages, and we also develop a dedicated model by using Automated Machine\nLearning.",
        "translated": ""
    },
    {
        "title": "CA-Jaccard: Camera-aware Jaccard Distance for Person Re-identification",
        "url": "http://arxiv.org/abs/2311.10605v1",
        "pub_date": "2023-11-17",
        "summary": "Person re-identification (re-ID) is a challenging task that aims to learn\ndiscriminative features for person retrieval. In person re-ID, Jaccard distance\nis a widely used distance metric, especially in re-ranking and clustering\nscenarios. However, we discover that camera variation has a significant\nnegative impact on the reliability of Jaccard distance. In particular, Jaccard\ndistance calculates the distance based on the overlap of relevant neighbors.\nDue to camera variation, intra-camera samples dominate the relevant neighbors,\nwhich reduces the reliability of the neighbors by introducing intra-camera\nnegative samples and excluding inter-camera positive samples. To overcome this\nproblem, we propose a novel camera-aware Jaccard (CA-Jaccard) distance that\nleverages camera information to enhance the reliability of Jaccard distance.\nSpecifically, we introduce camera-aware k-reciprocal nearest neighbors (CKRNNs)\nto find k-reciprocal nearest neighbors on the intra-camera and inter-camera\nranking lists, which improves the reliability of relevant neighbors and\nguarantees the contribution of inter-camera samples in the overlap. Moreover,\nwe propose a camera-aware local query expansion (CLQE) to exploit camera\nvariation as a strong constraint to mine reliable samples in relevant neighbors\nand assign these samples higher weights in overlap to further improve the\nreliability. Our CA-Jaccard distance is simple yet effective and can serve as a\ngeneral distance metric for person re-ID methods with high reliability and low\ncomputational cost. Extensive experiments demonstrate the effectiveness of our\nmethod.",
        "translated": ""
    },
    {
        "title": "Hourglass Tokenizer for Efficient Transformer-Based 3D Human Pose\n  Estimation",
        "url": "http://arxiv.org/abs/2311.12028v1",
        "pub_date": "2023-11-20",
        "summary": "Transformers have been successfully applied in the field of video-based 3D\nhuman pose estimation. However, the high computational costs of these video\npose transformers (VPTs) make them impractical on resource-constrained devices.\nIn this paper, we present a plug-and-play pruning-and-recovering framework,\ncalled Hourglass Tokenizer (HoT), for efficient transformer-based 3D human pose\nestimation from videos. Our HoT begins with pruning pose tokens of redundant\nframes and ends with recovering full-length tokens, resulting in a few pose\ntokens in the intermediate transformer blocks and thus improving the model\nefficiency. To effectively achieve this, we propose a token pruning cluster\n(TPC) that dynamically selects a few representative tokens with high semantic\ndiversity while eliminating the redundancy of video frames. In addition, we\ndevelop a token recovering attention (TRA) to restore the detailed\nspatio-temporal information based on the selected tokens, thereby expanding the\nnetwork output to the original full-length temporal resolution for fast\ninference. Extensive experiments on two benchmark datasets (i.e., Human3.6M and\nMPI-INF-3DHP) demonstrate that our method can achieve both high efficiency and\nestimation accuracy compared to the original VPT models. For instance, applying\nto MotionBERT and MixSTE on Human3.6M, our HoT can save nearly 50% FLOPs\nwithout sacrificing accuracy and nearly 40% FLOPs with only 0.2% accuracy drop,\nrespectively. Our source code will be open-sourced.",
        "translated": ""
    },
    {
        "title": "PF-LRM: Pose-Free Large Reconstruction Model for Joint Pose and Shape\n  Prediction",
        "url": "http://arxiv.org/abs/2311.12024v1",
        "pub_date": "2023-11-20",
        "summary": "We propose a Pose-Free Large Reconstruction Model (PF-LRM) for reconstructing\na 3D object from a few unposed images even with little visual overlap, while\nsimultaneously estimating the relative camera poses in ~1.3 seconds on a single\nA100 GPU. PF-LRM is a highly scalable method utilizing the self-attention\nblocks to exchange information between 3D object tokens and 2D image tokens; we\npredict a coarse point cloud for each view, and then use a differentiable\nPerspective-n-Point (PnP) solver to obtain camera poses. When trained on a huge\namount of multi-view posed data of ~1M objects, PF-LRM shows strong\ncross-dataset generalization ability, and outperforms baseline methods by a\nlarge margin in terms of pose prediction accuracy and 3D reconstruction quality\non various unseen evaluation datasets. We also demonstrate our model's\napplicability in downstream text/image-to-3D task with fast feed-forward\ninference. Our project website is at: https://totoro97.github.io/pf-lrm .",
        "translated": ""
    },
    {
        "title": "GPT-4V(ision) for Robotics: Multimodal Task Planning from Human\n  Demonstration",
        "url": "http://arxiv.org/abs/2311.12015v1",
        "pub_date": "2023-11-20",
        "summary": "We introduce a pipeline that enhances a general-purpose Vision Language\nModel, GPT-4V(ision), by integrating observations of human actions to\nfacilitate robotic manipulation. This system analyzes videos of humans\nperforming tasks and creates executable robot programs that incorporate\naffordance insights. The computation starts by analyzing the videos with GPT-4V\nto convert environmental and action details into text, followed by a\nGPT-4-empowered task planner. In the following analyses, vision systems\nreanalyze the video with the task plan. Object names are grounded using an\nopen-vocabulary object detector, while focus on the hand-object relation helps\nto detect the moment of grasping and releasing. This spatiotemporal grounding\nallows the vision systems to further gather affordance data (e.g., grasp type,\nway points, and body postures). Experiments across various scenarios\ndemonstrate this method's efficacy in achieving real robots' operations from\nhuman demonstrations in a zero-shot manner. The prompts of GPT-4V/GPT-4 are\navailable at this project page:\nhttps://microsoft.github.io/GPT4Vision-Robot-Manipulation-Prompts/",
        "translated": ""
    },
    {
        "title": "Exploring Lip Segmentation Techniques in Computer Vision: A Comparative\n  Analysis",
        "url": "http://arxiv.org/abs/2311.11992v1",
        "pub_date": "2023-11-20",
        "summary": "Lip segmentation is crucial in computer vision, especially for lip reading.\nDespite extensive face segmentation research, lip segmentation has received\nlimited attention. The aim of this study is to compare state-of-the-art lip\nsegmentation models using a standardized setting and a publicly available\ndataset. Five techniques, namely EHANet, Mask2Former, BiSeNet V2, PIDNet, and\nSTDC1, are qualitatively selected based on their reported performance,\ninference time, code availability, recency, and popularity. The CelebAMask-HQ\ndataset, comprising manually annotated face images, is used to fairly assess\nthe lip segmentation performance of the selected models. Inference experiments\nare conducted on a Raspberry Pi4 to emulate limited computational resources.\nThe results show that Mask2Former and EHANet have the best performances in\nterms of mIoU score. BiSeNet V2 demonstrate competitive performance, while\nPIDNet excels in recall but has lower precision. Most models present inference\ntime ranging from 1000 to around 3000 milliseconds on a Raspberry Pi4, with\nPIDNet having the lowest mean inference time. This study provides a\ncomprehensive evaluation of lip segmentation models, highlighting their\nperformance and inference times. The findings contribute to the development of\nlightweight techniques and establish benchmarks for future advances in lip\nsegmentation, especially in IoT and edge computing scenarios.",
        "translated": ""
    },
    {
        "title": "Categorizing the Visual Environment and Analyzing the Visual Attention\n  of Dogs",
        "url": "http://arxiv.org/abs/2311.11988v1",
        "pub_date": "2023-11-20",
        "summary": "Dogs have a unique evolutionary relationship with humans and serve many\nimportant roles e.g. search and rescue, blind assistance, emotional support.\nHowever, few datasets exist to categorize visual features and objects available\nto dogs, as well as how dogs direct their visual attention within their\nenvironment. We collect and study a dataset with over 11,698 gazes to\ncategorize the objects available to be gazed at by 11 dogs in everyday outdoor\nenvironments i.e. a walk around a college campus and urban area. We explore the\navailability of these object categories and the visual attention of dogs over\nthese categories using a head mounted eye tracking apparatus. A small portion\n(approx. 600 images or &lt; 20% of total dataset) of the collected data is used to\nfine tune a MaskRCNN for the novel image domain to segment objects present in\nthe scene, enabling further statistical analysis on the visual gaze tendencies\nof dogs. The MaskRCNN, with eye tracking apparatus, serves as an end to end\nmodel for automatically classifying the visual fixations of dogs. The fine\ntuned MaskRCNN performs far better than chance. There are few individual\ndifferences between the 11 dogs and we observe greater visual fixations on\nbuses, plants, pavement, and construction equipment. This work takes a step\ntowards understanding visual behavior of dogs and their interaction with the\nphysical world.",
        "translated": ""
    },
    {
        "title": "Leveraging Previous Facial Action Units Knowledge for Emotion\n  Recognition on Faces",
        "url": "http://arxiv.org/abs/2311.11980v1",
        "pub_date": "2023-11-20",
        "summary": "People naturally understand emotions, thus permitting a machine to do the\nsame could open new paths for human-computer interaction. Facial expressions\ncan be very useful for emotion recognition techniques, as these are the biggest\ntransmitters of non-verbal cues capable of being correlated with emotions.\nSeveral techniques are based on Convolutional Neural Networks (CNNs) to extract\ninformation in a machine learning process. However, simple CNNs are not always\nsufficient to locate points of interest on the face that can be correlated with\nemotions. In this work, we intend to expand the capacity of emotion recognition\ntechniques by proposing the usage of Facial Action Units (AUs) recognition\ntechniques to recognize emotions. This recognition will be based on the Facial\nAction Coding System (FACS) and computed by a machine learning system. In\nparticular, our method expands over EmotiRAM, an approach for multi-cue emotion\nrecognition, in which we improve over their facial encoding module.",
        "translated": ""
    },
    {
        "title": "Evaluating Supervision Levels Trade-Offs for Infrared-Based People\n  Counting",
        "url": "http://arxiv.org/abs/2311.11974v1",
        "pub_date": "2023-11-20",
        "summary": "Object detection models are commonly used for people counting (and\nlocalization) in many applications but require a dataset with costly bounding\nbox annotations for training. Given the importance of privacy in people\ncounting, these models rely more and more on infrared images, making the task\neven harder. In this paper, we explore how weaker levels of supervision can\naffect the performance of deep person counting architectures for image\nclassification and point-level localization. Our experiments indicate that\ncounting people using a CNN Image-Level model achieves competitive results with\nYOLO detectors and point-level models, yet provides a higher frame rate and a\nsimilar amount of model parameters.",
        "translated": ""
    },
    {
        "title": "LiDAR-HMR: 3D Human Mesh Recovery from LiDAR",
        "url": "http://arxiv.org/abs/2311.11971v1",
        "pub_date": "2023-11-20",
        "summary": "In recent years, point cloud perception tasks have been garnering increasing\nattention. This paper presents the first attempt to estimate 3D human body mesh\nfrom sparse LiDAR point clouds. We found that the major challenge in estimating\nhuman pose and mesh from point clouds lies in the sparsity, noise, and\nincompletion of LiDAR point clouds. Facing these challenges, we propose an\neffective sparse-to-dense reconstruction scheme to reconstruct 3D human mesh.\nThis involves estimating a sparse representation of a human (3D human pose) and\ngradually reconstructing the body mesh. To better leverage the 3D structural\ninformation of point clouds, we employ a cascaded graph transformer\n(graphormer) to introduce point cloud features during sparse-to-dense\nreconstruction. Experimental results on three publicly available databases\ndemonstrate the effectiveness of the proposed approach. Code:\nhttps://github.com/soullessrobot/LiDAR-HMR/",
        "translated": ""
    },
    {
        "title": "SA-Med2D-20M Dataset: Segment Anything in 2D Medical Imaging with 20\n  Million masks",
        "url": "http://arxiv.org/abs/2311.11969v1",
        "pub_date": "2023-11-20",
        "summary": "Segment Anything Model (SAM) has achieved impressive results for natural\nimage segmentation with input prompts such as points and bounding boxes. Its\nsuccess largely owes to massive labeled training data. However, directly\napplying SAM to medical image segmentation cannot perform well because SAM\nlacks medical knowledge -- it does not use medical images for training. To\nincorporate medical knowledge into SAM, we introduce SA-Med2D-20M, a\nlarge-scale segmentation dataset of 2D medical images built upon numerous\npublic and private datasets. It consists of 4.6 million 2D medical images and\n19.7 million corresponding masks, covering almost the whole body and showing\nsignificant diversity. This paper describes all the datasets collected in\nSA-Med2D-20M and details how to process these datasets. Furthermore,\ncomprehensive statistics of SA-Med2D-20M are presented to facilitate the better\nuse of our dataset, which can help the researchers build medical vision\nfoundation models or apply their models to downstream medical applications. We\nhope that the large scale and diversity of SA-Med2D-20M can be leveraged to\ndevelop medical artificial intelligence for enhancing diagnosis, medical image\nanalysis, knowledge sharing, and education. The data with the redistribution\nlicense is publicly available at https://github.com/OpenGVLab/SAM-Med2D.",
        "translated": ""
    },
    {
        "title": "What Can AutoML Do For Continual Learning?",
        "url": "http://arxiv.org/abs/2311.11963v1",
        "pub_date": "2023-11-20",
        "summary": "This position paper outlines the potential of AutoML for incremental\n(continual) learning to encourage more research in this direction. Incremental\nlearning involves incorporating new data from a stream of tasks and\ndistributions to learn enhanced deep representations and adapt better to new\ntasks. However, a significant limitation of incremental learners is that most\ncurrent techniques freeze the backbone architecture, hyperparameters, and the\norder &amp; structure of the learning tasks throughout the learning and adaptation\nprocess. We strongly believe that AutoML offers promising solutions to address\nthese limitations, enabling incremental learning to adapt to more diverse\nreal-world tasks. Therefore, instead of directly proposing a new method, this\npaper takes a step back by posing the question: \"What can AutoML do for\nincremental learning?\" We outline three key areas of research that can\ncontribute to making incremental learners more dynamic, highlighting concrete\nopportunities to apply AutoML methods in novel ways as well as entirely new\nchallenges for AutoML research.",
        "translated": ""
    },
    {
        "title": "Physics-guided Shape-from-Template: Monocular Video Perception through\n  Neural Surrogate Models",
        "url": "http://arxiv.org/abs/2311.12796v1",
        "pub_date": "2023-11-21",
        "summary": "3D reconstruction of dynamic scenes is a long-standing problem in computer\ngraphics and increasingly difficult the less information is available.\nShape-from-Template (SfT) methods aim to reconstruct a template-based geometry\nfrom RGB images or video sequences, often leveraging just a single monocular\ncamera without depth information, such as regular smartphone recordings.\nUnfortunately, existing reconstruction methods are either unphysical and noisy\nor slow in optimization. To solve this problem, we propose a novel SfT\nreconstruction algorithm for cloth using a pre-trained neural surrogate model\nthat is fast to evaluate, stable, and produces smooth reconstructions due to a\nregularizing physics simulation. Differentiable rendering of the simulated mesh\nenables pixel-wise comparisons between the reconstruction and a target video\nsequence that can be used for a gradient-based optimization procedure to\nextract not only shape information but also physical parameters such as\nstretching, shearing, or bending stiffness of the cloth. This allows to retain\na precise, stable, and smooth reconstructed geometry while reducing the runtime\nby a factor of 400-500 compared to $\\phi$-SfT, a state-of-the-art physics-based\nSfT approach.",
        "translated": ""
    },
    {
        "title": "ShareGPT4V: Improving Large Multi-Modal Models with Better Captions",
        "url": "http://arxiv.org/abs/2311.12793v1",
        "pub_date": "2023-11-21",
        "summary": "In the realm of large multi-modal models (LMMs), efficient modality alignment\nis crucial yet often constrained by the scarcity of high-quality image-text\ndata. To address this bottleneck, we introduce the ShareGPT4V dataset, a\npioneering large-scale resource featuring 1.2 million highly descriptive\ncaptions, which surpasses existing datasets in diversity and information\ncontent, covering world knowledge, object properties, spatial relationships,\nand aesthetic evaluations. Specifically, ShareGPT4V originates from a curated\n100K high-quality captions collected from advanced GPT4-Vision and has been\nexpanded to 1.2M with a superb caption model trained on this subset. ShareGPT4V\nfirst demonstrates its effectiveness for the Supervised Fine-Tuning (SFT)\nphase, by substituting an equivalent quantity of detailed captions in existing\nSFT datasets with a subset of our high-quality captions, significantly\nenhancing the LMMs like LLaVA-7B, LLaVA-1.5-13B, and Qwen-VL-Chat-7B on the MME\nand MMBench benchmarks, with respective gains of 222.8/22.0/22.3 and\n2.7/1.3/1.5. We further incorporate ShareGPT4V data into both the pre-training\nand SFT phases, obtaining ShareGPT4V-7B, a superior LMM based on a simple\narchitecture that has remarkable performance across a majority of the\nmulti-modal benchmarks. This project is available at\nhttps://ShareGPT4V.github.io to serve as a pivotal resource for advancing the\nLMMs community.",
        "translated": ""
    },
    {
        "title": "Intrinsic Image Decomposition via Ordinal Shading",
        "url": "http://arxiv.org/abs/2311.12792v1",
        "pub_date": "2023-11-21",
        "summary": "Intrinsic decomposition is a fundamental mid-level vision problem that plays\na crucial role in various inverse rendering and computational photography\npipelines. Generating highly accurate intrinsic decompositions is an inherently\nunder-constrained task that requires precisely estimating continuous-valued\nshading and albedo. In this work, we achieve high-resolution intrinsic\ndecomposition by breaking the problem into two parts. First, we present a dense\nordinal shading formulation using a shift- and scale-invariant loss in order to\nestimate ordinal shading cues without restricting the predictions to obey the\nintrinsic model. We then combine low- and high-resolution ordinal estimations\nusing a second network to generate a shading estimate with both global\ncoherency and local details. We encourage the model to learn an accurate\ndecomposition by computing losses on the estimated shading as well as the\nalbedo implied by the intrinsic model. We develop a straightforward method for\ngenerating dense pseudo ground truth using our model's predictions and\nmulti-illumination data, enabling generalization to in-the-wild imagery. We\npresent an exhaustive qualitative and quantitative analysis of our predicted\nintrinsic components against state-of-the-art methods. Finally, we demonstrate\nthe real-world applicability of our estimations by performing otherwise\ndifficult editing tasks such as recoloring and relighting.",
        "translated": ""
    },
    {
        "title": "SuGaR: Surface-Aligned Gaussian Splatting for Efficient 3D Mesh\n  Reconstruction and High-Quality Mesh Rendering",
        "url": "http://arxiv.org/abs/2311.12775v1",
        "pub_date": "2023-11-21",
        "summary": "We propose a method to allow precise and extremely fast mesh extraction from\n3D Gaussian Splatting. Gaussian Splatting has recently become very popular as\nit yields realistic rendering while being significantly faster to train than\nNeRFs. It is however challenging to extract a mesh from the millions of tiny 3D\ngaussians as these gaussians tend to be unorganized after optimization and no\nmethod has been proposed so far. Our first key contribution is a regularization\nterm that encourages the gaussians to align well with the surface of the scene.\nWe then introduce a method that exploits this alignment to extract a mesh from\nthe Gaussians using Poisson reconstruction, which is fast, scalable, and\npreserves details, in contrast to the Marching Cubes algorithm usually applied\nto extract meshes from Neural SDFs. Finally, we introduce an optional\nrefinement strategy that binds gaussians to the surface of the mesh, and\njointly optimizes these Gaussians and the mesh through Gaussian splatting\nrendering. This enables easy editing, sculpting, rigging, animating,\ncompositing and relighting of the Gaussians using traditional softwares by\nmanipulating the mesh instead of the gaussians themselves. Retrieving such an\neditable mesh for realistic rendering is done within minutes with our method,\ncompared to hours with the state-of-the-art methods on neural SDFs, while\nproviding a better rendering quality.",
        "translated": ""
    },
    {
        "title": "Iris Presentation Attack: Assessing the Impact of Combining Vanadium\n  Dioxide Films with Artificial Eyes",
        "url": "http://arxiv.org/abs/2311.12773v1",
        "pub_date": "2023-11-21",
        "summary": "Iris recognition systems, operating in the near infrared spectrum (NIR), have\ndemonstrated vulnerability to presentation attacks, where an adversary uses\nartifacts such as cosmetic contact lenses, artificial eyes or printed iris\nimages in order to circumvent the system. At the same time, a number of\neffective presentation attack detection (PAD) methods have been developed.\nThese methods have demonstrated success in detecting artificial eyes (e.g.,\nfake Van Dyke eyes) as presentation attacks. In this work, we seek to alter the\noptical characteristics of artificial eyes by affixing Vanadium Dioxide (VO2)\nfilms on their surface in various spatial configurations. VO2 films can be used\nto selectively transmit NIR light and can, therefore, be used to regulate the\namount of NIR light from the object that is captured by the iris sensor. We\nstudy the impact of such images produced by the sensor on two state-of-the-art\niris PA detection methods. We observe that the addition of VO2 films on the\nsurface of artificial eyes can cause the PA detection methods to misclassify\nthem as bonafide eyes in some cases. This represents a vulnerability that must\nbe systematically analyzed and effectively addressed.",
        "translated": ""
    },
    {
        "title": "Swift Parameter-free Attention Network for Efficient Super-Resolution",
        "url": "http://arxiv.org/abs/2311.12770v1",
        "pub_date": "2023-11-21",
        "summary": "Single Image Super-Resolution (SISR) is a crucial task in low-level computer\nvision, aiming to reconstruct high-resolution images from low-resolution\ncounterparts. Conventional attention mechanisms have significantly improved\nSISR performance but often result in complex network structures and large\nnumber of parameters, leading to slow inference speed and large model size. To\naddress this issue, we propose the Swift Parameter-free Attention Network\n(SPAN), a highly efficient SISR model that balances parameter count, inference\nspeed, and image quality. SPAN employs a novel parameter-free attention\nmechanism, which leverages symmetric activation functions and residual\nconnections to enhance high-contribution information and suppress redundant\ninformation. Our theoretical analysis demonstrates the effectiveness of this\ndesign in achieving the attention mechanism's purpose. We evaluate SPAN on\nmultiple benchmarks, showing that it outperforms existing efficient\nsuper-resolution models in terms of both image quality and inference speed,\nachieving a significant quality-speed trade-off. This makes SPAN highly\nsuitable for real-world applications, particularly in resource-constrained\nscenarios. Notably, our model attains the best PSNR of 27.09 dB, and the test\nruntime of our team is reduced by 7.08ms in the NTIRE 2023 efficient\nsuper-resolution challenge. Our code and models are made publicly available at\n\\url{https://github.com/hongyuanyu/SPAN}.",
        "translated": ""
    },
    {
        "title": "Investigating Weight-Perturbed Deep Neural Networks With Application in\n  Iris Presentation Attack Detection",
        "url": "http://arxiv.org/abs/2311.12764v1",
        "pub_date": "2023-11-21",
        "summary": "Deep neural networks (DNNs) exhibit superior performance in various machine\nlearning tasks, e.g., image classification, speech recognition, biometric\nrecognition, object detection, etc. However, it is essential to analyze their\nsensitivity to parameter perturbations before deploying them in real-world\napplications. In this work, we assess the sensitivity of DNNs against\nperturbations to their weight and bias parameters. The sensitivity analysis\ninvolves three DNN architectures (VGG, ResNet, and DenseNet), three types of\nparameter perturbations (Gaussian noise, weight zeroing, and weight scaling),\nand two settings (entire network and layer-wise). We perform experiments in the\ncontext of iris presentation attack detection and evaluate on two publicly\navailable datasets: LivDet-Iris-2017 and LivDet-Iris-2020. Based on the\nsensitivity analysis, we propose improved models simply by perturbing\nparameters of the network without undergoing training. We further combine these\nperturbed models at the score-level and at the parameter-level to improve the\nperformance over the original model. The ensemble at the parameter-level shows\nan average improvement of 43.58% on the LivDet-Iris-2017 dataset and 9.25% on\nthe LivDet-Iris-2020 dataset. The source code is available at\n\\href{https://github.com/redwankarimsony/WeightPerturbation-MSU}{https://github.com/redwankarimsony/WeightPerturbation-MSU}.",
        "translated": ""
    },
    {
        "title": "High-resolution Image-based Malware Classification using Multiple\n  Instance Learning",
        "url": "http://arxiv.org/abs/2311.12760v1",
        "pub_date": "2023-11-21",
        "summary": "This paper proposes a novel method of classifying malware into families using\nhigh-resolution greyscale images and multiple instance learning to overcome\nadversarial binary enlargement. Current methods of visualisation-based malware\nclassification largely rely on lossy transformations of inputs such as resizing\nto handle the large, variable-sized images. Through empirical analysis and\nexperimentation, it is shown that these approaches cause crucial information\nloss that can be exploited. The proposed solution divides the images into\npatches and uses embedding-based multiple instance learning with a\nconvolutional neural network and an attention aggregation function for\nclassification. The implementation is evaluated on the Microsoft Malware\nClassification dataset and achieves accuracies of up to $96.6\\%$ on\nadversarially enlarged samples compared to the baseline of $22.8\\%$. The Python\ncode is available online at https://github.com/timppeters/MIL-Malware-Images .",
        "translated": ""
    },
    {
        "title": "SelfOcc: Self-Supervised Vision-Based 3D Occupancy Prediction",
        "url": "http://arxiv.org/abs/2311.12754v1",
        "pub_date": "2023-11-21",
        "summary": "3D occupancy prediction is an important task for the robustness of\nvision-centric autonomous driving, which aims to predict whether each point is\noccupied in the surrounding 3D space. Existing methods usually require 3D\noccupancy labels to produce meaningful results. However, it is very laborious\nto annotate the occupancy status of each voxel. In this paper, we propose\nSelfOcc to explore a self-supervised way to learn 3D occupancy using only video\nsequences. We first transform the images into the 3D space (e.g., bird's eye\nview) to obtain 3D representation of the scene. We directly impose constraints\non the 3D representations by treating them as signed distance fields. We can\nthen render 2D images of previous and future frames as self-supervision signals\nto learn the 3D representations. We propose an MVS-embedded strategy to\ndirectly optimize the SDF-induced weights with multiple depth proposals. Our\nSelfOcc outperforms the previous best method SceneRF by 58.7% using a single\nframe as input on SemanticKITTI and is the first self-supervised work that\nproduces reasonable 3D occupancy for surround cameras on Occ3D. SelfOcc\nproduces high-quality depth and achieves state-of-the-art results on novel\ndepth synthesis, monocular depth estimation, and surround-view depth estimation\non the SemanticKITTI, KITTI-2015, and nuScenes, respectively. Code:\nhttps://github.com/huang-yh/SelfOcc.",
        "translated": ""
    },
    {
        "title": "Towards Natural Language-Guided Drones: GeoText-1652 Benchmark with\n  Spatially Relation Matching",
        "url": "http://arxiv.org/abs/2311.12751v1",
        "pub_date": "2023-11-21",
        "summary": "Drone navigation through natural language commands remains a significant\nchallenge due to the lack of publicly available multi-modal datasets and the\nintricate demands of fine-grained visual-text alignment. In response to this\npressing need, we present a new human-computer interaction annotation benchmark\ncalled GeoText-1652, meticulously curated through a robust Large Language Model\n(LLM)-based data generation framework and the expertise of pre-trained vision\nmodels. This new dataset seamlessly extends the existing image dataset, \\ie,\nUniversity-1652, with spatial-aware text annotations, encompassing intricate\nimage-text-bounding box associations. Besides, we introduce a new optimization\nobjective to leverage fine-grained spatial associations, called blending\nspatial matching, for region-level spatial relation matching. Extensive\nexperiments reveal that our approach maintains an exceptional recall rate under\nvarying description complexities. This underscores the promising potential of\nour approach in elevating drone control and navigation through the seamless\nintegration of natural language commands in real-world scenarios.",
        "translated": ""
    },
    {
        "title": "Retrieval-Augmented Layout Transformer for Content-Aware Layout\n  Generation",
        "url": "http://arxiv.org/abs/2311.13602v1",
        "pub_date": "2023-11-22",
        "summary": "Content-aware graphic layout generation aims to automatically arrange visual\nelements along with a given content, such as an e-commerce product image. In\nthis paper, we argue that the current layout generation approaches suffer from\nthe limited training data for the high-dimensional layout structure. We show\nthat a simple retrieval augmentation can significantly improve the generation\nquality. Our model, which is named Retrieval-Augmented Layout Transformer\n(RALF), retrieves nearest neighbor layout examples based on an input image and\nfeeds these results into an autoregressive generator. Our model can apply\nretrieval augmentation to various controllable generation tasks and yield\nhigh-quality layouts within a unified architecture. Our extensive experiments\nshow that RALF successfully generates content-aware layouts in both constrained\nand unconstrained settings and significantly outperforms the baselines.",
        "translated": ""
    },
    {
        "title": "Visual In-Context Prompting",
        "url": "http://arxiv.org/abs/2311.13601v1",
        "pub_date": "2023-11-22",
        "summary": "In-context prompting in large language models (LLMs) has become a prevalent\napproach to improve zero-shot capabilities, but this idea is less explored in\nthe vision domain. Existing visual prompting methods focus on referring\nsegmentation to segment the most relevant object, falling short of addressing\nmany generic vision tasks like open-set segmentation and detection. In this\npaper, we introduce a universal visual in-context prompting framework for both\ntasks. In particular, we build on top of an encoder-decoder architecture, and\ndevelop a versatile prompt encoder to support a variety of prompts like\nstrokes, boxes, and points. We further enhance it to take an arbitrary number\nof reference image segments as the context. Our extensive explorations show\nthat the proposed visual in-context prompting elicits extraordinary referring\nand generic segmentation capabilities to refer and detect, yielding competitive\nperformance to close-set in-domain datasets and showing promising results on\nmany open-set segmentation datasets. By joint training on COCO and SA-1B, our\nmodel achieves $57.7$ PQ on COCO and $23.2$ PQ on ADE20K. Code will be\navailable at https://github.com/UX-Decoder/DINOv.",
        "translated": ""
    },
    {
        "title": "ZipLoRA: Any Subject in Any Style by Effectively Merging LoRAs",
        "url": "http://arxiv.org/abs/2311.13600v1",
        "pub_date": "2023-11-22",
        "summary": "Methods for finetuning generative models for concept-driven personalization\ngenerally achieve strong results for subject-driven or style-driven generation.\nRecently, low-rank adaptations (LoRA) have been proposed as a\nparameter-efficient way of achieving concept-driven personalization. While\nrecent work explores the combination of separate LoRAs to achieve joint\ngeneration of learned styles and subjects, existing techniques do not reliably\naddress the problem; they often compromise either subject fidelity or style\nfidelity. We propose ZipLoRA, a method to cheaply and effectively merge\nindependently trained style and subject LoRAs in order to achieve generation of\nany user-provided subject in any user-provided style. Experiments on a wide\nrange of subject and style combinations show that ZipLoRA can generate\ncompelling results with meaningful improvements over baselines in subject and\nstyle fidelity while preserving the ability to recontextualize. Project page:\nhttps://ziplora.github.io",
        "translated": ""
    },
    {
        "title": "T-Rex: Counting by Visual Prompting",
        "url": "http://arxiv.org/abs/2311.13596v1",
        "pub_date": "2023-11-22",
        "summary": "We introduce T-Rex, an interactive object counting model designed to first\ndetect and then count any objects. We formulate object counting as an open-set\nobject detection task with the integration of visual prompts. Users can specify\nthe objects of interest by marking points or boxes on a reference image, and\nT-Rex then detects all objects with a similar pattern. Guided by the visual\nfeedback from T-Rex, users can also interactively refine the counting results\nby prompting on missing or falsely-detected objects. T-Rex has achieved\nstate-of-the-art performance on several class-agnostic counting benchmarks. To\nfurther exploit its potential, we established a new counting benchmark\nencompassing diverse scenarios and challenges. Both quantitative and\nqualitative results show that T-Rex possesses exceptional zero-shot counting\ncapabilities. We also present various practical application scenarios for\nT-Rex, illustrating its potential in the realm of visual prompting.",
        "translated": ""
    },
    {
        "title": "XAGen: 3D Expressive Human Avatars Generation",
        "url": "http://arxiv.org/abs/2311.13574v1",
        "pub_date": "2023-11-22",
        "summary": "Recent advances in 3D-aware GAN models have enabled the generation of\nrealistic and controllable human body images. However, existing methods focus\non the control of major body joints, neglecting the manipulation of expressive\nattributes, such as facial expressions, jaw poses, hand poses, and so on. In\nthis work, we present XAGen, the first 3D generative model for human avatars\ncapable of expressive control over body, face, and hands. To enhance the\nfidelity of small-scale regions like face and hands, we devise a multi-scale\nand multi-part 3D representation that models fine details. Based on this\nrepresentation, we propose a multi-part rendering technique that disentangles\nthe synthesis of body, face, and hands to ease model training and enhance\ngeometric quality. Furthermore, we design multi-part discriminators that\nevaluate the quality of the generated avatars with respect to their appearance\nand fine-grained control capabilities. Experiments show that XAGen surpasses\nstate-of-the-art methods in terms of realism, diversity, and expressive control\nabilities. Code and data will be made available at\nhttps://showlab.github.io/xagen.",
        "translated": ""
    },
    {
        "title": "WildFusion: Learning 3D-Aware Latent Diffusion Models in View Space",
        "url": "http://arxiv.org/abs/2311.13570v1",
        "pub_date": "2023-11-22",
        "summary": "Modern learning-based approaches to 3D-aware image synthesis achieve high\nphotorealism and 3D-consistent viewpoint changes for the generated images.\nExisting approaches represent instances in a shared canonical space. However,\nfor in-the-wild datasets a shared canonical system can be difficult to define\nor might not even exist. In this work, we instead model instances in view\nspace, alleviating the need for posed images and learned camera distributions.\nWe find that in this setting, existing GAN-based methods are prone to\ngenerating flat geometry and struggle with distribution coverage. We hence\npropose WildFusion, a new approach to 3D-aware image synthesis based on latent\ndiffusion models (LDMs). We first train an autoencoder that infers a compressed\nlatent representation, which additionally captures the images' underlying 3D\nstructure and enables not only reconstruction but also novel view synthesis. To\nlearn a faithful 3D representation, we leverage cues from monocular depth\nprediction. Then, we train a diffusion model in the 3D-aware latent space,\nthereby enabling synthesis of high-quality 3D-consistent image samples,\noutperforming recent state-of-the-art GAN-based methods. Importantly, our\n3D-aware LDM is trained without any direct supervision from multiview images or\n3D geometry and does not require posed images or learned pose or camera\ndistributions. It directly learns a 3D representation without relying on\ncanonical camera coordinates. This opens up promising research avenues for\nscalable 3D-aware image synthesis and 3D content creation from in-the-wild\nimage data. See https://katjaschwarz.github.io/wildfusion for videos of our 3D\nresults.",
        "translated": ""
    },
    {
        "title": "Soulstyler: Using Large Language Model to Guide Image Style Transfer for\n  Target Object",
        "url": "http://arxiv.org/abs/2311.13562v1",
        "pub_date": "2023-11-22",
        "summary": "Image style transfer occupies an important place in both computer graphics\nand computer vision. However, most current methods require reference to\nstylized images and cannot individually stylize specific objects. To overcome\nthis limitation, we propose the \"Soulstyler\" framework, which allows users to\nguide the stylization of specific objects in an image through simple textual\ndescriptions. We introduce a large language model to parse the text and\nidentify stylization goals and specific styles. Combined with a CLIP-based\nsemantic visual embedding encoder, the model understands and matches text and\nimage content. We also introduce a novel localized text-image block matching\nloss that ensures that style transfer is performed only on specified target\nobjects, while non-target regions remain in their original style. Experimental\nresults demonstrate that our model is able to accurately perform style transfer\non target objects according to textual descriptions without affecting the style\nof background regions. Our code will be available at\nhttps://github.com/yisuanwang/Soulstyler.",
        "translated": ""
    },
    {
        "title": "Transfer Learning-based Real-time Handgun Detection",
        "url": "http://arxiv.org/abs/2311.13559v1",
        "pub_date": "2023-11-22",
        "summary": "Traditional surveillance systems rely on human attention, limiting their\neffectiveness. This study employs convolutional neural networks and transfer\nlearning to develop a real-time computer vision system for automatic handgun\ndetection. Comprehensive analysis of online handgun detection methods is\nconducted, emphasizing reducing false positives and learning time. Transfer\nlearning is demonstrated as an effective approach. Despite technical\nchallenges, the proposed system achieves a precision rate of 84.74%,\ndemonstrating promising performance comparable to related works, enabling\nfaster learning and accurate automatic handgun detection for enhanced security.\nThis research advances security measures by reducing human monitoring\ndependence, showcasing the potential of transfer learning-based approaches for\nefficient and reliable handgun detection.",
        "translated": ""
    },
    {
        "title": "ADriver-I: A General World Model for Autonomous Driving",
        "url": "http://arxiv.org/abs/2311.13549v1",
        "pub_date": "2023-11-22",
        "summary": "Typically, autonomous driving adopts a modular design, which divides the full\nstack into perception, prediction, planning and control parts. Though\ninterpretable, such modular design tends to introduce a substantial amount of\nredundancy. Recently, multimodal large language models (MLLM) and diffusion\ntechniques have demonstrated their superior performance on comprehension and\ngeneration ability. In this paper, we first introduce the concept of\ninterleaved vision-action pair, which unifies the format of visual features and\ncontrol signals. Based on the vision-action pairs, we construct a general world\nmodel based on MLLM and diffusion model for autonomous driving, termed\nADriver-I. It takes the vision-action pairs as inputs and autoregressively\npredicts the control signal of the current frame. The generated control signals\ntogether with the historical vision-action pairs are further conditioned to\npredict the future frames. With the predicted next frame, ADriver-I performs\nfurther control signal prediction. Such a process can be repeated infinite\ntimes, ADriver-I achieves autonomous driving in the world created by itself.\nExtensive experiments are conducted on nuScenes and our large-scale private\ndatasets. ADriver-I shows impressive performance compared to several\nconstructed baselines. We hope our ADriver-I can provide some new insights for\nfuture autonomous driving and embodied intelligence.",
        "translated": ""
    },
    {
        "title": "Medical Image Retrieval Using Pretrained Embeddings",
        "url": "http://arxiv.org/abs/2311.13547v1",
        "pub_date": "2023-11-22",
        "summary": "A wide range of imaging techniques and data formats available for medical\nimages make accurate retrieval from image databases challenging.\n  Efficient retrieval systems are crucial in advancing medical research,\nenabling large-scale studies and innovative diagnostic tools. Thus, addressing\nthe challenges of medical image retrieval is essential for the continued\nenhancement of healthcare and research.\n  In this study, we evaluated the feasibility of employing four\nstate-of-the-art pretrained models for medical image retrieval at modality,\nbody region, and organ levels and compared the results of two similarity\nindexing approaches. Since the employed networks take 2D images, we analyzed\nthe impacts of weighting and sampling strategies to incorporate 3D information\nduring retrieval of 3D volumes. We showed that medical image retrieval is\nfeasible using pretrained networks without any additional training or\nfine-tuning steps. Using pretrained embeddings, we achieved a recall of 1 for\nvarious tasks at modality, body region, and organ level.",
        "translated": ""
    },
    {
        "title": "SEGIC: Unleashing the Emergent Correspondence for In-Context\n  Segmentation",
        "url": "http://arxiv.org/abs/2311.14671v1",
        "pub_date": "2023-11-24",
        "summary": "In-context segmentation aims at segmenting novel images using a few labeled\nexample images, termed as \"in-context examples\", exploring content similarities\nbetween examples and the target. The resulting models can be generalized\nseamlessly to novel segmentation tasks, significantly reducing the labeling and\ntraining costs compared with conventional pipelines. However, in-context\nsegmentation is more challenging than classic ones due to its meta-learning\nnature, requiring the model to learn segmentation rules conditioned on a few\nsamples, not just the segmentation. Unlike previous work with ad-hoc or\nnon-end-to-end designs, we propose SEGIC, an end-to-end segment-in-context\nframework built upon a single vision foundation model (VFM). In particular,\nSEGIC leverages the emergent correspondence within VFM to capture dense\nrelationships between target images and in-context samples. As such,\ninformation from in-context samples is then extracted into three types of\ninstructions, i.e. geometric, visual, and meta instructions, serving as\nexplicit conditions for the final mask prediction. SEGIC is a straightforward\nyet effective approach that yields state-of-the-art performance on one-shot\nsegmentation benchmarks. Notably, SEGIC can be easily generalized to diverse\ntasks, including video object segmentation and open-vocabulary segmentation.\nCode will be available at \\url{https://github.com/MengLcool/SEGIC}.",
        "translated": ""
    },
    {
        "title": "Understanding Self-Supervised Features for Learning Unsupervised\n  Instance Segmentation",
        "url": "http://arxiv.org/abs/2311.14665v1",
        "pub_date": "2023-11-24",
        "summary": "Self-supervised learning (SSL) can be used to solve complex visual tasks\nwithout human labels. Self-supervised representations encode useful semantic\ninformation about images, and as a result, they have already been used for\ntasks such as unsupervised semantic segmentation. In this paper, we investigate\nself-supervised representations for instance segmentation without any manual\nannotations. We find that the features of different SSL methods vary in their\nlevel of instance-awareness. In particular, DINO features, which are known to\nbe excellent semantic descriptors, lack behind MAE features in their\nsensitivity for separating instances.",
        "translated": ""
    },
    {
        "title": "Charting New Territories: Exploring the Geographic and Geospatial\n  Capabilities of Multimodal LLMs",
        "url": "http://arxiv.org/abs/2311.14656v1",
        "pub_date": "2023-11-24",
        "summary": "Multimodal large language models (MLLMs) have shown remarkable capabilities\nacross a broad range of tasks but their knowledge and abilities in the\ngeographic and geospatial domains are yet to be explored, despite potential\nwide-ranging benefits to navigation, environmental research, urban development,\nand disaster response. We conduct a series of experiments exploring various\nvision capabilities of MLLMs within these domains, particularly focusing on the\nfrontier model GPT-4V, and benchmark its performance against open-source\ncounterparts. Our methodology involves challenging these models with a\nsmall-scale geographic benchmark consisting of a suite of visual tasks, testing\ntheir abilities across a spectrum of complexity. The analysis uncovers not only\nwhere such models excel, including instances where they outperform humans, but\nalso where they falter, providing a balanced view of their capabilities in the\ngeographic domain. To enable the comparison and evaluation of future models,\nour benchmark will be publicly released.",
        "translated": ""
    },
    {
        "title": "Continuous football player tracking from discrete broadcast data",
        "url": "http://arxiv.org/abs/2311.14642v1",
        "pub_date": "2023-11-24",
        "summary": "Player tracking data remains out of reach for many professional football\nteams as their video feeds are not sufficiently high quality for computer\nvision technologies to be used. To help bridge this gap, we present a method\nthat can estimate continuous full-pitch tracking data from discrete data made\nfrom broadcast footage. Such data could be collected by clubs or players at a\nsimilar cost to event data, which is widely available down to semi-professional\nlevel. We test our method using open-source tracking data, and include a\nversion that can be applied to a large set of over 200 games with such discrete\ndata.",
        "translated": ""
    },
    {
        "title": "Unsupervised high-throughput segmentation of cells and cell nuclei in\n  quantitative phase images",
        "url": "http://arxiv.org/abs/2311.14639v1",
        "pub_date": "2023-11-24",
        "summary": "In the effort to aid cytologic diagnostics by establishing automatic single\ncell screening using high throughput digital holographic microscopy for\nclinical studies thousands of images and millions of cells are captured. The\nbottleneck lies in an automatic, fast, and unsupervised segmentation technique\nthat does not limit the types of cells which might occur. We propose an\nunsupervised multistage method that segments correctly without confusing noise\nor reflections with cells and without missing cells that also includes the\ndetection of relevant inner structures, especially the cell nucleus in the\nunstained cell. In an effort to make the information reasonable and\ninterpretable for cytopathologists, we also introduce new cytoplasmic and\nnuclear features of potential help for cytologic diagnoses which exploit the\nquantitative phase information inherent to the measurement scheme. We show that\nthe segmentation provides consistently good results over many experiments on\npatient samples in a reasonable per cell analysis time.",
        "translated": ""
    },
    {
        "title": "Automated Detection and Counting of Windows using UAV Imagery based\n  Remote Sensing",
        "url": "http://arxiv.org/abs/2311.14635v1",
        "pub_date": "2023-11-24",
        "summary": "Despite the technological advancements in the construction and surveying\nsector, the inspection of salient features like windows in an\nunder-construction or existing building is predominantly a manual process.\nMoreover, the number of windows present in a building is directly related to\nthe magnitude of deformation it suffers under earthquakes. In this research, a\nmethod to accurately detect and count the number of windows of a building by\ndeploying an Unmanned Aerial Vehicle (UAV) based remote sensing system is\nproposed. The proposed two-stage method automates the identification and\ncounting of windows by developing computer vision pipelines that utilize data\nfrom UAV's onboard camera and other sensors. Quantitative and Qualitative\nresults show the effectiveness of our proposed approach in accurately detecting\nand counting the windows compared to the existing method.",
        "translated": ""
    },
    {
        "title": "One Strike, You're Out: Detecting Markush Structures in Low\n  Signal-to-Noise Ratio Images",
        "url": "http://arxiv.org/abs/2311.14633v1",
        "pub_date": "2023-11-24",
        "summary": "Modern research increasingly relies on automated methods to assist\nresearchers. An example of this is Optical Chemical Structure Recognition\n(OCSR), which aids chemists in retrieving information about chemicals from\nlarge amounts of documents. Markush structures are chemical structures that\ncannot be parsed correctly by OCSR and cause errors. The focus of this research\nwas to propose and test a novel method for classifying Markush structures.\nWithin this method, a comparison was made between fixed-feature extraction and\nend-to-end learning (CNN). The end-to-end method performed significantly better\nthan the fixed-feature method, achieving 0.928 (0.035 SD) Macro F1 compared to\nthe fixed-feature method's 0.701 (0.052 SD). Because of the nature of the\nexperiment, these figures are a lower bound and can be improved further. These\nresults suggest that Markush structures can be filtered out effectively and\naccurately using the proposed method. When implemented into OCSR pipelines,\nthis method can improve their performance and use to other researchers.",
        "translated": ""
    },
    {
        "title": "CatVersion: Concatenating Embeddings for Diffusion-Based Text-to-Image\n  Personalization",
        "url": "http://arxiv.org/abs/2311.14631v1",
        "pub_date": "2023-11-24",
        "summary": "We propose CatVersion, an inversion-based method that learns the personalized\nconcept through a handful of examples. Subsequently, users can utilize text\nprompts to generate images that embody the personalized concept, thereby\nachieving text-to-image personalization. In contrast to existing approaches\nthat emphasize word embedding learning or parameter fine-tuning for the\ndiffusion model, which potentially causes concept dilution or overfitting, our\nmethod concatenates embeddings on the feature-dense space of the text encoder\nin the diffusion model to learn the gap between the personalized concept and\nits base class, aiming to maximize the preservation of prior knowledge in\ndiffusion models while restoring the personalized concepts. To this end, we\nfirst dissect the text encoder's integration in the image generation process to\nidentify the feature-dense space of the encoder. Afterward, we concatenate\nembeddings on the Keys and Values in this space to learn the gap between the\npersonalized concept and its base class. In this way, the concatenated\nembeddings ultimately manifest as a residual on the original attention output.\nTo more accurately and unbiasedly quantify the results of personalized image\ngeneration, we improve the CLIP image alignment score based on masks.\nQualitatively and quantitatively, CatVersion helps to restore personalization\nconcepts more faithfully and enables more robust editing.",
        "translated": ""
    },
    {
        "title": "ARIA: On the interaction between Architectures, Aggregation methods and\n  Initializations in federated visual classification",
        "url": "http://arxiv.org/abs/2311.14625v1",
        "pub_date": "2023-11-24",
        "summary": "Federated Learning (FL) is a collaborative training paradigm that allows for\nprivacy-preserving learning of cross-institutional models by eliminating the\nexchange of sensitive data and instead relying on the exchange of model\nparameters between the clients and a server. Despite individual studies on how\nclient models are aggregated, and, more recently, on the benefits of ImageNet\npre-training, there is a lack of understanding of the effect the architecture\nchosen for the federation has, and of how the aforementioned elements\ninterconnect. To this end, we conduct the first joint\nARchitecture-Initialization-Aggregation study and benchmark ARIAs across a\nrange of medical image classification tasks. We find that, contrary to current\npractices, ARIA elements have to be chosen together to achieve the best\npossible performance. Our results also shed light on good choices for each\nelement depending on the task, the effect of normalisation layers, and the\nutility of SSL pre-training, pointing to potential directions for designing\nFL-specific architectures and training pipelines.",
        "translated": ""
    },
    {
        "title": "Neural Style Transfer for Computer Games",
        "url": "http://arxiv.org/abs/2311.14617v1",
        "pub_date": "2023-11-24",
        "summary": "Neural Style Transfer (NST) research has been applied to images, videos, 3D\nmeshes and radiance fields, but its application to 3D computer games remains\nrelatively unexplored. Whilst image and video NST systems can be used as a\npost-processing effect for a computer game, this results in undesired artefacts\nand diminished post-processing effects. Here, we present an approach for\ninjecting depth-aware NST as part of the 3D rendering pipeline. Qualitative and\nquantitative experiments are used to validate our in-game stylisation\nframework. We demonstrate temporally consistent results of artistically\nstylised game scenes, outperforming state-of-the-art image and video NST\nmethods.",
        "translated": ""
    },
    {
        "title": "Video-Bench: A Comprehensive Benchmark and Toolkit for Evaluating\n  Video-based Large Language Models",
        "url": "http://arxiv.org/abs/2311.16103v1",
        "pub_date": "2023-11-27",
        "summary": "Video-based large language models (Video-LLMs) have been recently introduced,\ntargeting both fundamental improvements in perception and comprehension, and a\ndiverse range of user inquiries. In pursuit of the ultimate goal of achieving\nartificial general intelligence, a truly intelligent Video-LLM model should not\nonly see and understand the surroundings, but also possess human-level\ncommonsense, and make well-informed decisions for the users. To guide the\ndevelopment of such a model, the establishment of a robust and comprehensive\nevaluation system becomes crucial. To this end, this paper proposes\n\\textit{Video-Bench}, a new comprehensive benchmark along with a toolkit\nspecifically designed for evaluating Video-LLMs. The benchmark comprises 10\nmeticulously crafted tasks, evaluating the capabilities of Video-LLMs across\nthree distinct levels: Video-exclusive Understanding, Prior Knowledge-based\nQuestion-Answering, and Comprehension and Decision-making. In addition, we\nintroduce an automatic toolkit tailored to process model outputs for various\ntasks, facilitating the calculation of metrics and generating convenient final\nscores. We evaluate 8 representative Video-LLMs using \\textit{Video-Bench}. The\nfindings reveal that current Video-LLMs still fall considerably short of\nachieving human-like comprehension and analysis of real-world videos, offering\nvaluable insights for future research directions. The benchmark and toolkit are\navailable at: \\url{https://github.com/PKU-YuanGroup/Video-Bench}.",
        "translated": ""
    },
    {
        "title": "Test-time Adaptation of Discriminative Models via Diffusion Generative\n  Feedback",
        "url": "http://arxiv.org/abs/2311.16102v1",
        "pub_date": "2023-11-27",
        "summary": "The advancements in generative modeling, particularly the advent of diffusion\nmodels, have sparked a fundamental question: how can these models be\neffectively used for discriminative tasks? In this work, we find that\ngenerative models can be great test-time adapters for discriminative models.\nOur method, Diffusion-TTA, adapts pre-trained discriminative models such as\nimage classifiers, segmenters and depth predictors, to each unlabelled example\nin the test set using generative feedback from a diffusion model. We achieve\nthis by modulating the conditioning of the diffusion model using the output of\nthe discriminative model. We then maximize the image likelihood objective by\nbackpropagating the gradients to discriminative model's parameters. We show\nDiffusion-TTA significantly enhances the accuracy of various large-scale\npre-trained discriminative models, such as, ImageNet classifiers, CLIP models,\nimage pixel labellers and image depth predictors. Diffusion-TTA outperforms\nexisting test-time adaptation methods, including TTT-MAE and TENT, and\nparticularly shines in online adaptation setups, where the discriminative model\nis continually adapted to each example in the test set. We provide access to\ncode, results, and visualizations on our website:\nhttps://diffusion-tta.github.io/.",
        "translated": ""
    },
    {
        "title": "How Many Unicorns Are in This Image? A Safety Evaluation Benchmark for\n  Vision LLMs",
        "url": "http://arxiv.org/abs/2311.16101v1",
        "pub_date": "2023-11-27",
        "summary": "This work focuses on the potential of Vision LLMs (VLLMs) in visual\nreasoning. Different from prior studies, we shift our focus from evaluating\nstandard performance to introducing a comprehensive safety evaluation suite,\ncovering both out-of-distribution (OOD) generalization and adversarial\nrobustness. For the OOD evaluation, we present two novel VQA datasets, each\nwith one variant, designed to test model performance under challenging\nconditions. In exploring adversarial robustness, we propose a straightforward\nattack strategy for misleading VLLMs to produce visual-unrelated responses.\nMoreover, we assess the efficacy of two jailbreaking strategies, targeting\neither the vision or language component of VLLMs. Our evaluation of 21 diverse\nmodels, ranging from open-source VLLMs to GPT-4V, yields interesting\nobservations: 1) Current VLLMs struggle with OOD texts but not images, unless\nthe visual information is limited; and 2) These VLLMs can be easily misled by\ndeceiving vision encoders only, and their vision-language training often\ncompromise safety protocols. We release this safety evaluation suite at\nhttps://github.com/UCSC-VLAA/vllm-safety-benchmark.",
        "translated": ""
    },
    {
        "title": "GART: Gaussian Articulated Template Models",
        "url": "http://arxiv.org/abs/2311.16099v1",
        "pub_date": "2023-11-27",
        "summary": "We introduce Gaussian Articulated Template Model GART, an explicit,\nefficient, and expressive representation for non-rigid articulated subject\ncapturing and rendering from monocular videos. GART utilizes a mixture of\nmoving 3D Gaussians to explicitly approximate a deformable subject's geometry\nand appearance. It takes advantage of a categorical template model prior (SMPL,\nSMAL, etc.) with learnable forward skinning while further generalizing to more\ncomplex non-rigid deformations with novel latent bones. GART can be\nreconstructed via differentiable rendering from monocular videos in seconds or\nminutes and rendered in novel poses faster than 150fps.",
        "translated": ""
    },
    {
        "title": "On Bringing Robots Home",
        "url": "http://arxiv.org/abs/2311.16098v1",
        "pub_date": "2023-11-27",
        "summary": "Throughout history, we have successfully integrated various machines into our\nhomes. Dishwashers, laundry machines, stand mixers, and robot vacuums are a few\nrecent examples. However, these machines excel at performing only a single task\neffectively. The concept of a \"generalist machine\" in homes - a domestic\nassistant that can adapt and learn from our needs, all while remaining\ncost-effective - has long been a goal in robotics that has been steadily\npursued for decades. In this work, we initiate a large-scale effort towards\nthis goal by introducing Dobb-E, an affordable yet versatile general-purpose\nsystem for learning robotic manipulation within household settings. Dobb-E can\nlearn a new task with only five minutes of a user showing it how to do it,\nthanks to a demonstration collection tool (\"The Stick\") we built out of cheap\nparts and iPhones. We use the Stick to collect 13 hours of data in 22 homes of\nNew York City, and train Home Pretrained Representations (HPR). Then, in a\nnovel home environment, with five minutes of demonstrations and fifteen minutes\nof adapting the HPR model, we show that Dobb-E can reliably solve the task on\nthe Stretch, a mobile robot readily available on the market. Across roughly 30\ndays of experimentation in homes of New York City and surrounding areas, we\ntest our system in 10 homes, with a total of 109 tasks in different\nenvironments, and finally achieve a success rate of 81%. Beyond success\npercentages, our experiments reveal a plethora of unique challenges absent or\nignored in lab robotics. These range from effects of strong shadows, to\nvariable demonstration quality by non-expert users. With the hope of\naccelerating research on home robots, and eventually seeing robot butlers in\nevery home, we open-source Dobb-E software stack and models, our data, and our\nhardware designs at https://dobb-e.com",
        "translated": ""
    },
    {
        "title": "CG-HOI: Contact-Guided 3D Human-Object Interaction Generation",
        "url": "http://arxiv.org/abs/2311.16097v1",
        "pub_date": "2023-11-27",
        "summary": "We propose CG-HOI, the first method to address the task of generating dynamic\n3D human-object interactions (HOIs) from text. We model the motion of both\nhuman and object in an interdependent fashion, as semantically rich human\nmotion rarely happens in isolation without any interactions. Our key insight is\nthat explicitly modeling contact between the human body surface and object\ngeometry can be used as strong proxy guidance, both during training and\ninference. Using this guidance to bridge human and object motion enables\ngenerating more realistic and physically plausible interaction sequences, where\nthe human body and corresponding object move in a coherent manner. Our method\nfirst learns to model human motion, object motion, and contact in a joint\ndiffusion process, inter-correlated through cross-attention. We then leverage\nthis learned contact for guidance during inference synthesis of realistic,\ncoherent HOIs. Extensive evaluation shows that our joint contact-based\nhuman-object interaction approach generates realistic and physically plausible\nsequences, and we show two applications highlighting the capabilities of our\nmethod. Conditioned on a given object trajectory, we can generate the\ncorresponding human motion without re-training, demonstrating strong\nhuman-object interdependency learning. Our approach is also flexible, and can\nbe applied to static real-world 3D scene scans.",
        "translated": ""
    },
    {
        "title": "Animatable Gaussians: Learning Pose-dependent Gaussian Maps for\n  High-fidelity Human Avatar Modeling",
        "url": "http://arxiv.org/abs/2311.16096v1",
        "pub_date": "2023-11-27",
        "summary": "Modeling animatable human avatars from RGB videos is a long-standing and\nchallenging problem. Recent works usually adopt MLP-based neural radiance\nfields (NeRF) to represent 3D humans, but it remains difficult for pure MLPs to\nregress pose-dependent garment details. To this end, we introduce Animatable\nGaussians, a new avatar representation that leverages powerful 2D CNNs and 3D\nGaussian splatting to create high-fidelity avatars. To associate 3D Gaussians\nwith the animatable avatar, we learn a parametric template from the input\nvideos, and then parameterize the template on two front \\&amp; back canonical\nGaussian maps where each pixel represents a 3D Gaussian. The learned template\nis adaptive to the wearing garments for modeling looser clothes like dresses.\nSuch template-guided 2D parameterization enables us to employ a powerful\nStyleGAN-based CNN to learn the pose-dependent Gaussian maps for modeling\ndetailed dynamic appearances. Furthermore, we introduce a pose projection\nstrategy for better generalization given novel poses. Overall, our method can\ncreate lifelike avatars with dynamic, realistic and generalized appearances.\nExperiments show that our method outperforms other state-of-the-art approaches.\nCode: https://github.com/lizhe00/AnimatableGaussians",
        "translated": ""
    },
    {
        "title": "Street TryOn: Learning In-the-Wild Virtual Try-On from Unpaired Person\n  Images",
        "url": "http://arxiv.org/abs/2311.16094v1",
        "pub_date": "2023-11-27",
        "summary": "Virtual try-on has become a popular research topic, but most existing methods\nfocus on studio images with a clean background. They can achieve plausible\nresults for this studio try-on setting by learning to warp a garment image to\nfit a person's body from paired training data, i.e., garment images paired with\nimages of people wearing the same garment. Such data is often collected from\ncommercial websites, where each garment is demonstrated both by itself and on\nseveral models. By contrast, it is hard to collect paired data for in-the-wild\nscenes, and therefore, virtual try-on for casual images of people against\ncluttered backgrounds is rarely studied.\n  In this work, we fill the gap in the current virtual try-on research by (1)\nintroducing a Street TryOn benchmark to evaluate performance on street scenes\nand (2) proposing a novel method that can learn without paired data, from a set\nof in-the-wild person images directly. Our method can achieve robust\nperformance across shop and street domains using a novel DensePose warping\ncorrection method combined with diffusion-based inpainting controlled by pose\nand semantic segmentation. Our experiments demonstrate competitive performance\nfor standard studio try-on tasks and SOTA performance for street try-on and\ncross-domain try-on tasks.",
        "translated": ""
    },
    {
        "title": "Interactive Autonomous Navigation with Internal State Inference and\n  Interactivity Estimation",
        "url": "http://arxiv.org/abs/2311.16091v1",
        "pub_date": "2023-11-27",
        "summary": "Deep reinforcement learning (DRL) provides a promising way for intelligent\nagents (e.g., autonomous vehicles) to learn to navigate complex scenarios.\nHowever, DRL with neural networks as function approximators is typically\nconsidered a black box with little explainability and often suffers from\nsuboptimal performance, especially for autonomous navigation in highly\ninteractive multi-agent environments. To address these issues, we propose three\nauxiliary tasks with spatio-temporal relational reasoning and integrate them\ninto the standard DRL framework, which improves the decision making performance\nand provides explainable intermediate indicators. We propose to explicitly\ninfer the internal states (i.e., traits and intentions) of surrounding agents\n(e.g., human drivers) as well as to predict their future trajectories in the\nsituations with and without the ego agent through counterfactual reasoning.\nThese auxiliary tasks provide additional supervision signals to infer the\nbehavior patterns of other interactive agents. Multiple variants of framework\nintegration strategies are compared. We also employ a spatio-temporal graph\nneural network to encode relations between dynamic entities, which enhances\nboth internal state inference and decision making of the ego agent. Moreover,\nwe propose an interactivity estimation mechanism based on the difference\nbetween predicted trajectories in these two situations, which indicates the\ndegree of influence of the ego agent on other agents. To validate the proposed\nmethod, we design an intersection driving simulator based on the Intelligent\nIntersection Driver Model (IIDM) that simulates vehicles and pedestrians. Our\napproach achieves robust and state-of-the-art performance in terms of standard\nevaluation metrics and provides explainable intermediate indicators (i.e.,\ninternal states, and interactivity scores) for decision making.",
        "translated": ""
    },
    {
        "title": "Self-correcting LLM-controlled Diffusion Models",
        "url": "http://arxiv.org/abs/2311.16090v1",
        "pub_date": "2023-11-27",
        "summary": "Text-to-image generation has witnessed significant progress with the advent\nof diffusion models. Despite the ability to generate photorealistic images,\ncurrent text-to-image diffusion models still often struggle to accurately\ninterpret and follow complex input text prompts. In contrast to existing models\nthat aim to generate images only with their best effort, we introduce\nSelf-correcting LLM-controlled Diffusion (SLD). SLD is a framework that\ngenerates an image from the input prompt, assesses its alignment with the\nprompt, and performs self-corrections on the inaccuracies in the generated\nimage. Steered by an LLM controller, SLD turns text-to-image generation into an\niterative closed-loop process, ensuring correctness in the resulting image. SLD\nis not only training-free but can also be seamlessly integrated with diffusion\nmodels behind API access, such as DALL-E 3, to further boost the performance of\nstate-of-the-art diffusion models. Experimental results show that our approach\ncan rectify a majority of incorrect generations, particularly in generative\nnumeracy, attribute binding, and spatial relationships. Furthermore, by simply\nadjusting the instructions to the LLM, SLD can perform image editing tasks,\nbridging the gap between text-to-image generation and image editing pipelines.\nWe will make our code available for future research and applications.",
        "translated": ""
    },
    {
        "title": "Material Palette: Extraction of Materials from a Single Image",
        "url": "http://arxiv.org/abs/2311.17060v1",
        "pub_date": "2023-11-28",
        "summary": "In this paper, we propose a method to extract physically-based rendering\n(PBR) materials from a single real-world image. We do so in two steps: first,\nwe map regions of the image to material concepts using a diffusion model, which\nallows the sampling of texture images resembling each material in the scene.\nSecond, we benefit from a separate network to decompose the generated textures\ninto Spatially Varying BRDFs (SVBRDFs), providing us with materials ready to be\nused in rendering applications. Our approach builds on existing synthetic\nmaterial libraries with SVBRDF ground truth, but also exploits a\ndiffusion-generated RGB texture dataset to allow generalization to new samples\nusing unsupervised domain adaptation (UDA). Our contributions are thoroughly\nevaluated on synthetic and real-world datasets. We further demonstrate the\napplicability of our method for editing 3D scenes with materials estimated from\nreal photographs. The code and models will be made open-source. Project page:\nhttps://astra-vision.github.io/MaterialPalette/",
        "translated": ""
    },
    {
        "title": "HumanGaussian: Text-Driven 3D Human Generation with Gaussian Splatting",
        "url": "http://arxiv.org/abs/2311.17061v1",
        "pub_date": "2023-11-28",
        "summary": "Realistic 3D human generation from text prompts is a desirable yet\nchallenging task. Existing methods optimize 3D representations like mesh or\nneural fields via score distillation sampling (SDS), which suffers from\ninadequate fine details or excessive training time. In this paper, we propose\nan efficient yet effective framework, HumanGaussian, that generates\nhigh-quality 3D humans with fine-grained geometry and realistic appearance. Our\nkey insight is that 3D Gaussian Splatting is an efficient renderer with\nperiodic Gaussian shrinkage or growing, where such adaptive density control can\nbe naturally guided by intrinsic human structures. Specifically, 1) we first\npropose a Structure-Aware SDS that simultaneously optimizes human appearance\nand geometry. The multi-modal score function from both RGB and depth space is\nleveraged to distill the Gaussian densification and pruning process. 2)\nMoreover, we devise an Annealed Negative Prompt Guidance by decomposing SDS\ninto a noisier generative score and a cleaner classifier score, which well\naddresses the over-saturation issue. The floating artifacts are further\neliminated based on Gaussian size in a prune-only phase to enhance generation\nsmoothness. Extensive experiments demonstrate the superior efficiency and\ncompetitive quality of our framework, rendering vivid 3D humans under diverse\nscenarios. Project Page: https://alvinliu0.github.io/projects/HumanGaussian",
        "translated": ""
    },
    {
        "title": "Panoptic Video Scene Graph Generation",
        "url": "http://arxiv.org/abs/2311.17058v1",
        "pub_date": "2023-11-28",
        "summary": "Towards building comprehensive real-world visual perception systems, we\npropose and study a new problem called panoptic scene graph generation (PVSG).\nPVSG relates to the existing video scene graph generation (VidSGG) problem,\nwhich focuses on temporal interactions between humans and objects grounded with\nbounding boxes in videos. However, the limitation of bounding boxes in\ndetecting non-rigid objects and backgrounds often causes VidSGG to miss key\ndetails crucial for comprehensive video understanding. In contrast, PVSG\nrequires nodes in scene graphs to be grounded by more precise, pixel-level\nsegmentation masks, which facilitate holistic scene understanding. To advance\nresearch in this new area, we contribute the PVSG dataset, which consists of\n400 videos (289 third-person + 111 egocentric videos) with a total of 150K\nframes labeled with panoptic segmentation masks as well as fine, temporal scene\ngraphs. We also provide a variety of baseline methods and share useful design\npractices for future work.",
        "translated": ""
    },
    {
        "title": "ReMoS: Reactive 3D Motion Synthesis for Two-Person Interactions",
        "url": "http://arxiv.org/abs/2311.17057v1",
        "pub_date": "2023-11-28",
        "summary": "Current approaches for 3D human motion synthesis can generate high-quality 3D\nanimations of digital humans performing a wide variety of actions and gestures.\nHowever, there is still a notable technological gap in addressing the complex\ndynamics of multi-human interactions within this paradigm. In this work, we\nintroduce ReMoS, a denoising diffusion-based probabilistic model for reactive\nmotion synthesis that explores two-person interactions. Given the motion of one\nperson, we synthesize the reactive motion of the second person to complete the\ninteractions between the two. In addition to synthesizing the full-body\nmotions, we also synthesize plausible hand interactions. We show the\nperformance of ReMoS under a wide range of challenging two-person scenarios\nincluding pair-dancing, Ninjutsu, kickboxing, and acrobatics, where one\nperson's movements have complex and diverse influences on the motions of the\nother. We further propose the ReMoCap dataset for two-person interactions\nconsisting of full-body and hand motions. We evaluate our approach through\nmultiple quantitative metrics, qualitative visualizations, and a user study.\nOur results are usable in interactive applications while also providing an\nadequate amount of control for animators.",
        "translated": ""
    },
    {
        "title": "Self-Supervised Motion Magnification by Backpropagating Through Optical\n  Flow",
        "url": "http://arxiv.org/abs/2311.17056v1",
        "pub_date": "2023-11-28",
        "summary": "This paper presents a simple, self-supervised method for magnifying subtle\nmotions in video: given an input video and a magnification factor, we\nmanipulate the video such that its new optical flow is scaled by the desired\namount. To train our model, we propose a loss function that estimates the\noptical flow of the generated video and penalizes how far if deviates from the\ngiven magnification factor. Thus, training involves differentiating through a\npretrained optical flow network. Since our model is self-supervised, we can\nfurther improve its performance through test-time adaptation, by finetuning it\non the input video. It can also be easily extended to magnify the motions of\nonly user-selected objects. Our approach avoids the need for synthetic\nmagnification datasets that have been used to train prior learning-based\napproaches. Instead, it leverages the existing capabilities of off-the-shelf\nmotion estimators. We demonstrate the effectiveness of our method through\nevaluations of both visual quality and quantitative metrics on a range of\nreal-world and synthetic videos, and we show our method works for both\nsupervised and unsupervised optical flow methods.",
        "translated": ""
    },
    {
        "title": "Rethinking Directional Integration in Neural Radiance Fields",
        "url": "http://arxiv.org/abs/2311.16504v1",
        "pub_date": "2023-11-28",
        "summary": "Recent works use the Neural radiance field (NeRF) to perform multi-view 3D\nreconstruction, providing a significant leap in rendering photorealistic\nscenes. However, despite its efficacy, NeRF exhibits limited capability of\nlearning view-dependent effects compared to light field rendering or\nimage-based view synthesis. To that end, we introduce a modification to the\nNeRF rendering equation which is as simple as a few lines of code change for\nany NeRF variations, while greatly improving the rendering quality of\nview-dependent effects. By swapping the integration operator and the direction\ndecoder network, we only integrate the positional features along the ray and\nmove the directional terms out of the integration, resulting in a\ndisentanglement of the view-dependent and independent components. The modified\nequation is equivalent to the classical volumetric rendering in ideal cases on\nobject surfaces with Dirac densities. Furthermore, we prove that with the\nerrors caused by network approximation and numerical integration, our rendering\nequation exhibits better convergence properties with lower error accumulations\ncompared to the classical NeRF. We also show that the modified equation can be\ninterpreted as light field rendering with learned ray embeddings. Experiments\non different NeRF variations show consistent improvements in the quality of\nview-dependent effects with our simple modification.",
        "translated": ""
    },
    {
        "title": "No Representation Rules Them All in Category Discovery",
        "url": "http://arxiv.org/abs/2311.17055v1",
        "pub_date": "2023-11-28",
        "summary": "In this paper we tackle the problem of Generalized Category Discovery (GCD).\nSpecifically, given a dataset with labelled and unlabelled images, the task is\nto cluster all images in the unlabelled subset, whether or not they belong to\nthe labelled categories. Our first contribution is to recognize that most\nexisting GCD benchmarks only contain labels for a single clustering of the\ndata, making it difficult to ascertain whether models are using the available\nlabels to solve the GCD task, or simply solving an unsupervised clustering\nproblem. As such, we present a synthetic dataset, named 'Clevr-4', for category\ndiscovery. Clevr-4 contains four equally valid partitions of the data, i.e\nbased on object shape, texture, color or count. To solve the task, models are\nrequired to extrapolate the taxonomy specified by the labelled set, rather than\nsimply latching onto a single natural grouping of the data. We use this dataset\nto demonstrate the limitations of unsupervised clustering in the GCD setting,\nshowing that even very strong unsupervised models fail on Clevr-4. We further\nuse Clevr-4 to examine the weaknesses of existing GCD algorithms, and propose a\nnew method which addresses these shortcomings, leveraging consistent findings\nfrom the representation learning literature to do so. Our simple solution,\nwhich is based on 'mean teachers' and termed $\\mu$GCD, substantially\noutperforms implemented baselines on Clevr-4. Finally, when we transfer these\nfindings to real data on the challenging Semantic Shift Benchmark (SSB), we\nfind that $\\mu$GCD outperforms all prior work, setting a new state-of-the-art.\nFor the project webpage, see https://www.robots.ox.ac.uk/~vgg/data/clevr4/",
        "translated": ""
    },
    {
        "title": "DiffuseBot: Breeding Soft Robots With Physics-Augmented Generative\n  Diffusion Models",
        "url": "http://arxiv.org/abs/2311.17053v1",
        "pub_date": "2023-11-28",
        "summary": "Nature evolves creatures with a high complexity of morphological and\nbehavioral intelligence, meanwhile computational methods lag in approaching\nthat diversity and efficacy. Co-optimization of artificial creatures'\nmorphology and control in silico shows promise for applications in physical\nsoft robotics and virtual character creation; such approaches, however, require\ndeveloping new learning algorithms that can reason about function atop pure\nstructure. In this paper, we present DiffuseBot, a physics-augmented diffusion\nmodel that generates soft robot morphologies capable of excelling in a wide\nspectrum of tasks. DiffuseBot bridges the gap between virtually generated\ncontent and physical utility by (i) augmenting the diffusion process with a\nphysical dynamical simulation which provides a certificate of performance, and\n(ii) introducing a co-design procedure that jointly optimizes physical design\nand control by leveraging information about physical sensitivities from\ndifferentiable simulation. We showcase a range of simulated and fabricated\nrobots along with their capabilities. Check our website at\nhttps://diffusebot.github.io/",
        "translated": ""
    },
    {
        "title": "Surf-D: High-Quality Surface Generation for Arbitrary Topologies using\n  Diffusion Models",
        "url": "http://arxiv.org/abs/2311.17050v1",
        "pub_date": "2023-11-28",
        "summary": "In this paper, we present Surf-D, a novel method for generating high-quality\n3D shapes as Surfaces with arbitrary topologies using Diffusion models.\nSpecifically, we adopt Unsigned Distance Field (UDF) as the surface\nrepresentation, as it excels in handling arbitrary topologies, enabling the\ngeneration of complex shapes. While the prior methods explored shape generation\nwith different representations, they suffer from limited topologies and\ngeometry details. Moreover, it's non-trivial to directly extend prior diffusion\nmodels to UDF because they lack spatial continuity due to the discrete volume\nstructure. However, UDF requires accurate gradients for mesh extraction and\nlearning. To tackle the issues, we first leverage a point-based auto-encoder to\nlearn a compact latent space, which supports gradient querying for any input\npoint through differentiation to effectively capture intricate geometry at a\nhigh resolution. Since the learning difficulty for various shapes can differ, a\ncurriculum learning strategy is employed to efficiently embed various surfaces,\nenhancing the whole embedding process. With pretrained shape latent space, we\nemploy a latent diffusion model to acquire the distribution of various shapes.\nOur approach demonstrates superior performance in shape generation across\nmultiple modalities and conducts extensive experiments in unconditional\ngeneration, category conditional generation, 3D reconstruction from images, and\ntext-to-shape tasks.",
        "translated": ""
    },
    {
        "title": "MobileCLIP: Fast Image-Text Models through Multi-Modal Reinforced\n  Training",
        "url": "http://arxiv.org/abs/2311.17049v1",
        "pub_date": "2023-11-28",
        "summary": "Contrastive pretraining of image-text foundation models, such as CLIP,\ndemonstrated excellent zero-shot performance and improved robustness on a wide\nrange of downstream tasks. However, these models utilize large\ntransformer-based encoders with significant memory and latency overhead which\npose challenges for deployment on mobile devices. In this work, we introduce\nMobileCLIP -- a new family of efficient image-text models optimized for runtime\nperformance along with a novel and efficient training approach, namely\nmulti-modal reinforced training. The proposed training approach leverages\nknowledge transfer from an image captioning model and an ensemble of strong\nCLIP encoders to improve the accuracy of efficient models. Our approach avoids\ntrain-time compute overhead by storing the additional knowledge in a reinforced\ndataset. MobileCLIP sets a new state-of-the-art latency-accuracy tradeoff for\nzero-shot classification and retrieval tasks on several datasets. Our\nMobileCLIP-S2 variant is 2.3$\\times$ faster while more accurate compared to\nprevious best CLIP model based on ViT-B/16. We further demonstrate the\neffectiveness of our multi-modal reinforced training by training a CLIP model\nbased on ViT-B/16 image backbone and achieving +2.9% average performance\nimprovement on 38 evaluation benchmarks compared to the previous best.\nMoreover, we show that the proposed approach achieves 10$\\times$-1000$\\times$\nimproved learning efficiency when compared with non-reinforced CLIP training.",
        "translated": ""
    },
    {
        "title": "Visual Anagrams: Generating Multi-View Optical Illusions with Diffusion\n  Models",
        "url": "http://arxiv.org/abs/2311.17919v1",
        "pub_date": "2023-11-29",
        "summary": "We address the problem of synthesizing multi-view optical illusions: images\nthat change appearance upon a transformation, such as a flip or rotation. We\npropose a simple, zero-shot method for obtaining these illusions from\noff-the-shelf text-to-image diffusion models. During the reverse diffusion\nprocess, we estimate the noise from different views of a noisy image. We then\ncombine these noise estimates together and denoise the image. A theoretical\nanalysis suggests that this method works precisely for views that can be\nwritten as orthogonal transformations, of which permutations are a subset. This\nleads to the idea of a visual anagram--an image that changes appearance under\nsome rearrangement of pixels. This includes rotations and flips, but also more\nexotic pixel permutations such as a jigsaw rearrangement. Our approach also\nnaturally extends to illusions with more than two views. We provide both\nqualitative and quantitative results demonstrating the effectiveness and\nflexibility of our method. Please see our project webpage for additional\nvisualizations and results: https://dangeng.github.io/visual_anagrams/",
        "translated": ""
    },
    {
        "title": "Do text-free diffusion models learn discriminative visual\n  representations?",
        "url": "http://arxiv.org/abs/2311.17921v1",
        "pub_date": "2023-11-29",
        "summary": "While many unsupervised learning models focus on one family of tasks, either\ngenerative or discriminative, we explore the possibility of a unified\nrepresentation learner: a model which addresses both families of tasks\nsimultaneously. We identify diffusion models, a state-of-the-art method for\ngenerative tasks, as a prime candidate. Such models involve training a U-Net to\niteratively predict and remove noise, and the resulting model can synthesize\nhigh-fidelity, diverse, novel images. We find that the intermediate feature\nmaps of the U-Net are diverse, discriminative feature representations. We\npropose a novel attention mechanism for pooling feature maps and further\nleverage this mechanism as DifFormer, a transformer feature fusion of features\nfrom different diffusion U-Net blocks and noise steps. We also develop DifFeed,\na novel feedback mechanism tailored to diffusion. We find that diffusion models\nare better than GANs, and, with our fusion and feedback mechanisms, can compete\nwith state-of-the-art unsupervised image representation learning methods for\ndiscriminative tasks - image classification with full and semi-supervision,\ntransfer for fine-grained classification, object detection and segmentation,\nand semantic segmentation. Our project website\n(https://mgwillia.github.io/diffssl/) and code\n(https://github.com/soumik-kanad/diffssl) are available publicly.",
        "translated": ""
    },
    {
        "title": "A Simple Recipe for Language-guided Domain Generalized Segmentation",
        "url": "http://arxiv.org/abs/2311.17922v1",
        "pub_date": "2023-11-29",
        "summary": "Generalization to new domains not seen during training is one of the\nlong-standing goals and challenges in deploying neural networks in real-world\napplications. Existing generalization techniques necessitate substantial data\naugmentation, potentially sourced from external datasets, and aim at learning\ninvariant representations by imposing various alignment constraints.\nLarge-scale pretraining has recently shown promising generalization\ncapabilities, along with the potential of bridging different modalities. For\ninstance, the recent advent of vision-language models like CLIP has opened the\ndoorway for vision models to exploit the textual modality. In this paper, we\nintroduce a simple framework for generalizing semantic segmentation networks by\nemploying language as the source of randomization. Our recipe comprises three\nkey ingredients: i) the preservation of the intrinsic CLIP robustness through\nminimal fine-tuning, ii) language-driven local style augmentation, and iii)\nrandomization by locally mixing the source and augmented styles during\ntraining. Extensive experiments report state-of-the-art results on various\ngeneralization benchmarks. The code will be made available.",
        "translated": ""
    },
    {
        "title": "Driving into the Future: Multiview Visual Forecasting and Planning with\n  World Model for Autonomous Driving",
        "url": "http://arxiv.org/abs/2311.17918v1",
        "pub_date": "2023-11-29",
        "summary": "In autonomous driving, predicting future events in advance and evaluating the\nforeseeable risks empowers autonomous vehicles to better plan their actions,\nenhancing safety and efficiency on the road. To this end, we propose Drive-WM,\nthe first driving world model compatible with existing end-to-end planning\nmodels. Through a joint spatial-temporal modeling facilitated by view\nfactorization, our model generates high-fidelity multiview videos in driving\nscenes. Building on its powerful generation ability, we showcase the potential\nof applying the world model for safe driving planning for the first time.\nParticularly, our Drive-WM enables driving into multiple futures based on\ndistinct driving maneuvers, and determines the optimal trajectory according to\nthe image-based rewards. Evaluation on real-world driving datasets verifies\nthat our method could generate high-quality, consistent, and controllable\nmultiview videos, opening up possibilities for real-world simulations and safe\nplanning.",
        "translated": ""
    },
    {
        "title": "AvatarStudio: High-fidelity and Animatable 3D Avatar Creation from Text",
        "url": "http://arxiv.org/abs/2311.17917v1",
        "pub_date": "2023-11-29",
        "summary": "We study the problem of creating high-fidelity and animatable 3D avatars from\nonly textual descriptions. Existing text-to-avatar methods are either limited\nto static avatars which cannot be animated or struggle to generate animatable\navatars with promising quality and precise pose control. To address these\nlimitations, we propose AvatarStudio, a coarse-to-fine generative model that\ngenerates explicit textured 3D meshes for animatable human avatars.\nSpecifically, AvatarStudio begins with a low-resolution NeRF-based\nrepresentation for coarse generation, followed by incorporating SMPL-guided\narticulation into the explicit mesh representation to support avatar animation\nand high resolution rendering. To ensure view consistency and pose\ncontrollability of the resulting avatars, we introduce a 2D diffusion model\nconditioned on DensePose for Score Distillation Sampling supervision. By\neffectively leveraging the synergy between the articulated mesh representation\nand the DensePose-conditional diffusion model, AvatarStudio can create\nhigh-quality avatars from text that are ready for animation, significantly\noutperforming previous methods. Moreover, it is competent for many\napplications, e.g., multimodal avatar animations and style-guided avatar\ncreation. For more results, please refer to our project page:\nhttp://jeff95.me/projects/avatarstudio.html",
        "translated": ""
    },
    {
        "title": "OPERA: Alleviating Hallucination in Multi-Modal Large Language Models\n  via Over-Trust Penalty and Retrospection-Allocation",
        "url": "http://arxiv.org/abs/2311.17911v1",
        "pub_date": "2023-11-29",
        "summary": "Hallucination, posed as a pervasive challenge of multi-modal large language\nmodels (MLLMs), has significantly impeded their real-world usage that demands\nprecise judgment. Existing methods mitigate this issue with either training\nwith specific designed data or inferencing with external knowledge from other\nsources, incurring inevitable additional costs. In this paper, we present\nOPERA, a novel MLLM decoding method grounded in an Over-trust Penalty and a\nRetrospection-Allocation strategy, serving as a nearly free lunch to alleviate\nthe hallucination issue without additional data, knowledge, or training. Our\napproach begins with an interesting observation that, most hallucinations are\nclosely tied to the knowledge aggregation patterns manifested in the\nself-attention matrix, i.e., MLLMs tend to generate new tokens by focusing on a\nfew summary tokens, but not all the previous tokens. Such partial over-trust\ninclination results in the neglecting of image tokens and describes the image\ncontent with hallucination. Statistically, we observe an 80%$\\sim$95%\nco-currency rate between hallucination contents and such knowledge aggregation\npatterns. Based on the observation, OPERA introduces a penalty term on the\nmodel logits during the beam-search decoding to mitigate the over-trust issue,\nalong with a rollback strategy that retrospects the presence of summary tokens\nin the previously generated tokens, and re-allocate the token selection if\nnecessary. With extensive experiments, OPERA shows significant\nhallucination-mitigating performance on different MLLMs and metrics, proving\nits effectiveness and generality. Our code is available at:\nhttps://github.com/shikiw/OPERA.",
        "translated": ""
    },
    {
        "title": "HUGS: Human Gaussian Splats",
        "url": "http://arxiv.org/abs/2311.17910v1",
        "pub_date": "2023-11-29",
        "summary": "Recent advances in neural rendering have improved both training and rendering\ntimes by orders of magnitude. While these methods demonstrate state-of-the-art\nquality and speed, they are designed for photogrammetry of static scenes and do\nnot generalize well to freely moving humans in the environment. In this work,\nwe introduce Human Gaussian Splats (HUGS) that represents an animatable human\ntogether with the scene using 3D Gaussian Splatting (3DGS). Our method takes\nonly a monocular video with a small number of (50-100) frames, and it\nautomatically learns to disentangle the static scene and a fully animatable\nhuman avatar within 30 minutes. We utilize the SMPL body model to initialize\nthe human Gaussians. To capture details that are not modeled by SMPL (e.g.\ncloth, hairs), we allow the 3D Gaussians to deviate from the human body model.\nUtilizing 3D Gaussians for animated humans brings new challenges, including the\nartifacts created when articulating the Gaussians. We propose to jointly\noptimize the linear blend skinning weights to coordinate the movements of\nindividual Gaussians during animation. Our approach enables novel-pose\nsynthesis of human and novel view synthesis of both the human and the scene. We\nachieve state-of-the-art rendering quality with a rendering speed of 60 FPS\nwhile being ~100x faster to train over previous work. Our code will be\nannounced here: https://github.com/apple/ml-hugs",
        "translated": ""
    },
    {
        "title": "CG3D: Compositional Generation for Text-to-3D via Gaussian Splatting",
        "url": "http://arxiv.org/abs/2311.17907v1",
        "pub_date": "2023-11-29",
        "summary": "With the onset of diffusion-based generative models and their ability to\ngenerate text-conditioned images, content generation has received a massive\ninvigoration. Recently, these models have been shown to provide useful guidance\nfor the generation of 3D graphics assets. However, existing work in\ntext-conditioned 3D generation faces fundamental constraints: (i) inability to\ngenerate detailed, multi-object scenes, (ii) inability to textually control\nmulti-object configurations, and (iii) physically realistic scene composition.\nIn this work, we propose CG3D, a method for compositionally generating scalable\n3D assets that resolves these constraints. We find that explicit Gaussian\nradiance fields, parameterized to allow for compositions of objects, possess\nthe capability to enable semantically and physically consistent scenes. By\nutilizing a guidance framework built around this explicit representation, we\nshow state of the art results, capable of even exceeding the guiding diffusion\nmodel in terms of object combinations and physics accuracy.",
        "translated": ""
    },
    {
        "title": "Language-conditioned Detection Transformer",
        "url": "http://arxiv.org/abs/2311.17902v1",
        "pub_date": "2023-11-29",
        "summary": "We present a new open-vocabulary detection framework. Our framework uses both\nimage-level labels and detailed detection annotations when available. Our\nframework proceeds in three steps. We first train a language-conditioned object\ndetector on fully-supervised detection data. This detector gets to see the\npresence or absence of ground truth classes during training, and conditions\nprediction on the set of present classes. We use this detector to pseudo-label\nimages with image-level labels. Our detector provides much more accurate\npseudo-labels than prior approaches with its conditioning mechanism. Finally,\nwe train an unconditioned open-vocabulary detector on the pseudo-annotated\nimages. The resulting detector, named DECOLA, shows strong zero-shot\nperformance in open-vocabulary LVIS benchmark as well as direct zero-shot\ntransfer benchmarks on LVIS, COCO, Object365, and OpenImages. DECOLA\noutperforms the prior arts by 17.1 AP-rare and 9.4 mAP on zero-shot LVIS\nbenchmark. DECOLA achieves state-of-the-art results in various model sizes,\narchitectures, and datasets by only training on open-sourced data and\nacademic-scale computing. Code is available at\nhttps://github.com/janghyuncho/DECOLA.",
        "translated": ""
    },
    {
        "title": "SODA: Bottleneck Diffusion Models for Representation Learning",
        "url": "http://arxiv.org/abs/2311.17901v1",
        "pub_date": "2023-11-29",
        "summary": "We introduce SODA, a self-supervised diffusion model, designed for\nrepresentation learning. The model incorporates an image encoder, which\ndistills a source view into a compact representation, that, in turn, guides the\ngeneration of related novel views. We show that by imposing a tight bottleneck\nbetween the encoder and a denoising decoder, and leveraging novel view\nsynthesis as a self-supervised objective, we can turn diffusion models into\nstrong representation learners, capable of capturing visual semantics in an\nunsupervised manner. To the best of our knowledge, SODA is the first diffusion\nmodel to succeed at ImageNet linear-probe classification, and, at the same\ntime, it accomplishes reconstruction, editing and synthesis tasks across a wide\nrange of datasets. Further investigation reveals the disentangled nature of its\nemergent latent space, that serves as an effective interface to control and\nmanipulate the model's produced images. All in all, we aim to shed light on the\nexciting and promising potential of diffusion models, not only for image\ngeneration, but also for learning rich and robust representations.",
        "translated": ""
    },
    {
        "title": "Dataset Distillation in Large Data Era",
        "url": "http://arxiv.org/abs/2311.18838v1",
        "pub_date": "2023-11-30",
        "summary": "Dataset distillation aims to generate a smaller but representative subset\nfrom a large dataset, which allows a model to be trained efficiently, meanwhile\nevaluating on the original testing data distribution to achieve decent\nperformance. Many prior works have aimed to align with diverse aspects of the\noriginal datasets, such as matching the training weight trajectories, gradient,\nfeature/BatchNorm distributions, etc. In this work, we show how to distill\nvarious large-scale datasets such as full ImageNet-1K/21K under a conventional\ninput resolution of 224$\\times$224 to achieve the best accuracy over all\nprevious approaches, including SRe$^2$L, TESLA and MTT. To achieve this, we\nintroduce a simple yet effective ${\\bf C}$urriculum ${\\bf D}$ata ${\\bf\nA}$ugmentation ($\\texttt{CDA}$) during data synthesis that obtains the accuracy\non large-scale ImageNet-1K and 21K with 63.2% under IPC (Images Per Class) 50\nand 36.1% under IPC 20, respectively. Finally, we show that, by integrating all\nour enhancements together, the proposed model beats the current\nstate-of-the-art by more than 4% Top-1 accuracy on ImageNet-1K/21K and for the\nfirst time, reduces the gap to its full-data training counterpart to less than\nabsolute 15%. Moreover, this work represents the inaugural success in dataset\ndistillation on larger-scale ImageNet-21K under the standard 224$\\times$224\nresolution. Our code and distilled ImageNet-21K dataset of 20 IPC, 2K recovery\nbudget are available at https://github.com/VILA-Lab/SRe2L/tree/main/CDA.",
        "translated": ""
    },
    {
        "title": "TrafficMOT: A Challenging Dataset for Multi-Object Tracking in Complex\n  Traffic Scenarios",
        "url": "http://arxiv.org/abs/2311.18839v1",
        "pub_date": "2023-11-30",
        "summary": "Multi-object tracking in traffic videos is a crucial research area, offering\nimmense potential for enhancing traffic monitoring accuracy and promoting road\nsafety measures through the utilisation of advanced machine learning\nalgorithms. However, existing datasets for multi-object tracking in traffic\nvideos often feature limited instances or focus on single classes, which cannot\nwell simulate the challenges encountered in complex traffic scenarios. To\naddress this gap, we introduce TrafficMOT, an extensive dataset designed to\nencompass diverse traffic situations with complex scenarios. To validate the\ncomplexity and challenges presented by TrafficMOT, we conducted comprehensive\nempirical studies using three different settings: fully-supervised,\nsemi-supervised, and a recent powerful zero-shot foundation model Tracking\nAnything Model (TAM). The experimental results highlight the inherent\ncomplexity of this dataset, emphasising its value in driving advancements in\nthe field of traffic monitoring and multi-object tracking.",
        "translated": ""
    },
    {
        "title": "Just Add $π$! Pose Induced Video Transformers for Understanding\n  Activities of Daily Living",
        "url": "http://arxiv.org/abs/2311.18840v1",
        "pub_date": "2023-11-30",
        "summary": "Video transformers have become the de facto standard for human action\nrecognition, yet their exclusive reliance on the RGB modality still limits\ntheir adoption in certain domains. One such domain is Activities of Daily\nLiving (ADL), where RGB alone is not sufficient to distinguish between visually\nsimilar actions, or actions observed from multiple viewpoints. To facilitate\nthe adoption of video transformers for ADL, we hypothesize that the\naugmentation of RGB with human pose information, known for its sensitivity to\nfine-grained motion and multiple viewpoints, is essential. Consequently, we\nintroduce the first Pose Induced Video Transformer: PI-ViT (or $\\pi$-ViT), a\nnovel approach that augments the RGB representations learned by video\ntransformers with 2D and 3D pose information. The key elements of $\\pi$-ViT are\ntwo plug-in modules, 2D Skeleton Induction Module and 3D Skeleton Induction\nModule, that are responsible for inducing 2D and 3D pose information into the\nRGB representations. These modules operate by performing pose-aware auxiliary\ntasks, a design choice that allows $\\pi$-ViT to discard the modules during\ninference. Notably, $\\pi$-ViT achieves the state-of-the-art performance on\nthree prominent ADL datasets, encompassing both real-world and large-scale\nRGB-D datasets, without requiring poses or additional computational overhead at\ninference.",
        "translated": ""
    },
    {
        "title": "PoseGPT: Chatting about 3D Human Pose",
        "url": "http://arxiv.org/abs/2311.18836v1",
        "pub_date": "2023-11-30",
        "summary": "We introduce PoseGPT, a framework employing Large Language Models (LLMs) to\nunderstand and reason about 3D human poses from images or textual descriptions.\nOur work is motivated by the human ability to intuitively understand postures\nfrom a single image or a brief description, a process that intertwines image\ninterpretation, world knowledge, and an understanding of body language.\nTraditional human pose estimation methods, whether image-based or text-based,\noften lack holistic scene comprehension and nuanced reasoning, leading to a\ndisconnect between visual data and its real-world implications. PoseGPT\naddresses these limitations by embedding SMPL poses as a distinct signal token\nwithin a multi-modal LLM, enabling direct generation of 3D body poses from both\ntextual and visual inputs. This approach not only simplifies pose prediction\nbut also empowers LLMs to apply their world knowledge in reasoning about human\nposes, fostering two advanced tasks: speculative pose generation and reasoning\nabout pose estimation. These tasks involve reasoning about humans to generate\n3D poses from subtle text queries, possibly accompanied by images. We establish\nbenchmarks for these tasks, moving beyond traditional 3D pose generation and\nestimation methods. Our results show that PoseGPT outperforms existing\nmultimodal LLMs and task-sepcific methods on these newly proposed tasks.\nFurthermore, PoseGPT's ability to understand and generate 3D human poses based\non complex reasoning opens new directions in human pose analysis.",
        "translated": ""
    },
    {
        "title": "VIDiff: Translating Videos via Multi-Modal Instructions with Diffusion\n  Models",
        "url": "http://arxiv.org/abs/2311.18837v1",
        "pub_date": "2023-11-30",
        "summary": "Diffusion models have achieved significant success in image and video\ngeneration. This motivates a growing interest in video editing tasks, where\nvideos are edited according to provided text descriptions. However, most\nexisting approaches only focus on video editing for short clips and rely on\ntime-consuming tuning or inference. We are the first to propose Video\nInstruction Diffusion (VIDiff), a unified foundation model designed for a wide\nrange of video tasks. These tasks encompass both understanding tasks (such as\nlanguage-guided video object segmentation) and generative tasks (video editing\nand enhancement). Our model can edit and translate the desired results within\nseconds based on user instructions. Moreover, we design an iterative\nauto-regressive method to ensure consistency in editing and enhancing long\nvideos. We provide convincing generative results for diverse input videos and\nwritten instructions, both qualitatively and quantitatively. More examples can\nbe found at our website https://ChenHsing.github.io/VIDiff.",
        "translated": ""
    },
    {
        "title": "InstructSeq: Unifying Vision Tasks with Instruction-conditioned\n  Multi-modal Sequence Generation",
        "url": "http://arxiv.org/abs/2311.18835v1",
        "pub_date": "2023-11-30",
        "summary": "Empowering models to dynamically accomplish tasks specified through natural\nlanguage instructions represents a promising path toward more capable and\ngeneral artificial intelligence. In this work, we introduce InstructSeq, an\ninstruction-conditioned multi-modal modeling framework that unifies diverse\nvision tasks through flexible natural language control and handling of both\nvisual and textual data. InstructSeq employs a multimodal transformer\narchitecture encompassing visual, language, and sequential modeling. We utilize\na visual encoder to extract image features and a text encoder to encode\ninstructions. An autoregressive transformer fuses the representations and\ngenerates sequential task outputs. By training with LLM-generated natural\nlanguage instructions, InstructSeq acquires a strong comprehension of free-form\ninstructions for specifying visual tasks. This provides an intuitive interface\nfor directing capabilities using flexible natural instructions. Without any\ntask-specific tuning, InstructSeq achieves compelling performance on semantic\nsegmentation, referring expression segmentation/comprehension, and image\ncaptioning. The flexible control and multi-task unification empower the model\nwith more human-like versatility and generalizability for computer vision. The\ncode will be released soon at https://github.com/rongyaofang/InstructSeq.",
        "translated": ""
    },
    {
        "title": "ART$\\boldsymbol{\\cdot}$V: Auto-Regressive Text-to-Video Generation with\n  Diffusion Models",
        "url": "http://arxiv.org/abs/2311.18834v1",
        "pub_date": "2023-11-30",
        "summary": "We present ART$\\boldsymbol{\\cdot}$V, an efficient framework for\nauto-regressive video generation with diffusion models. Unlike existing methods\nthat generate entire videos in one-shot, ART$\\boldsymbol{\\cdot}$V generates a\nsingle frame at a time, conditioned on the previous ones. The framework offers\nthree distinct advantages. First, it only learns simple continual motions\nbetween adjacent frames, therefore avoiding modeling complex long-range motions\nthat require huge training data. Second, it preserves the high-fidelity\ngeneration ability of the pre-trained image diffusion models by making only\nminimal network modifications. Third, it can generate arbitrarily long videos\nconditioned on a variety of prompts such as text, image or their combinations,\nmaking it highly versatile and flexible. To combat the common drifting issue in\nAR models, we propose masked diffusion model which implicitly learns which\ninformation can be drawn from reference images rather than network predictions,\nin order to reduce the risk of generating inconsistent appearances that cause\ndrifting. Moreover, we further enhance generation coherence by conditioning it\non the initial frame, which typically contains minimal noise. This is\nparticularly useful for long video generation. When trained for only two weeks\non four GPUs, ART$\\boldsymbol{\\cdot}$V already can generate videos with natural\nmotions, rich details and a high level of aesthetic quality. Besides, it\nenables various appealing applications, e.g., composing a long video from\nmultiple text prompts.",
        "translated": ""
    },
    {
        "title": "Exploiting Diffusion Prior for Generalizable Pixel-Level Semantic\n  Prediction",
        "url": "http://arxiv.org/abs/2311.18832v1",
        "pub_date": "2023-11-30",
        "summary": "Contents generated by recent advanced Text-to-Image (T2I) diffusion models\nare sometimes too imaginative for existing off-the-shelf property semantic\npredictors to estimate due to the immitigable domain gap. We introduce DMP, a\npipeline utilizing pre-trained T2I models as a prior for pixel-level semantic\nprediction tasks. To address the misalignment between deterministic prediction\ntasks and stochastic T2I models, we reformulate the diffusion process through a\nsequence of interpolations, establishing a deterministic mapping between input\nRGB images and output prediction distributions. To preserve generalizability,\nwe use low-rank adaptation to fine-tune pre-trained models. Extensive\nexperiments across five tasks, including 3D property estimation, semantic\nsegmentation, and intrinsic image decomposition, showcase the efficacy of the\nproposed method. Despite limited-domain training data, the approach yields\nfaithful estimations for arbitrary images, surpassing existing state-of-the-art\nalgorithms.",
        "translated": ""
    },
    {
        "title": "MotionEditor: Editing Video Motion via Content-Aware Diffusion",
        "url": "http://arxiv.org/abs/2311.18830v1",
        "pub_date": "2023-11-30",
        "summary": "Existing diffusion-based video editing models have made gorgeous advances for\nediting attributes of a source video over time but struggle to manipulate the\nmotion information while preserving the original protagonist's appearance and\nbackground. To address this, we propose MotionEditor, a diffusion model for\nvideo motion editing. MotionEditor incorporates a novel content-aware motion\nadapter into ControlNet to capture temporal motion correspondence. While\nControlNet enables direct generation based on skeleton poses, it encounters\nchallenges when modifying the source motion in the inverted noise due to\ncontradictory signals between the noise (source) and the condition (reference).\nOur adapter complements ControlNet by involving source content to transfer\nadapted control signals seamlessly. Further, we build up a two-branch\narchitecture (a reconstruction branch and an editing branch) with a\nhigh-fidelity attention injection mechanism facilitating branch interaction.\nThis mechanism enables the editing branch to query the key and value from the\nreconstruction branch in a decoupled manner, making the editing branch retain\nthe original background and protagonist appearance. We also propose a skeleton\nalignment algorithm to address the discrepancies in pose size and position.\nExperiments demonstrate the promising motion editing ability of MotionEditor,\nboth qualitatively and quantitatively.",
        "translated": ""
    },
    {
        "title": "MicroCinema: A Divide-and-Conquer Approach for Text-to-Video Generation",
        "url": "http://arxiv.org/abs/2311.18829v1",
        "pub_date": "2023-11-30",
        "summary": "We present MicroCinema, a straightforward yet effective framework for\nhigh-quality and coherent text-to-video generation. Unlike existing approaches\nthat align text prompts with video directly, MicroCinema introduces a\nDivide-and-Conquer strategy which divides the text-to-video into a two-stage\nprocess: text-to-image generation and image\\&amp;text-to-video generation. This\nstrategy offers two significant advantages. a) It allows us to take full\nadvantage of the recent advances in text-to-image models, such as Stable\nDiffusion, Midjourney, and DALLE, to generate photorealistic and highly\ndetailed images. b) Leveraging the generated image, the model can allocate less\nfocus to fine-grained appearance details, prioritizing the efficient learning\nof motion dynamics. To implement this strategy effectively, we introduce two\ncore designs. First, we propose the Appearance Injection Network, enhancing the\npreservation of the appearance of the given image. Second, we introduce the\nAppearance Noise Prior, a novel mechanism aimed at maintaining the capabilities\nof pre-trained 2D diffusion models. These design elements empower MicroCinema\nto generate high-quality videos with precise motion, guided by the provided\ntext prompts. Extensive experiments demonstrate the superiority of the proposed\nframework. Concretely, MicroCinema achieves SOTA zero-shot FVD of 342.86 on\nUCF-101 and 377.40 on MSR-VTT. See\nhttps://wangyanhui666.github.io/MicroCinema.github.io/ for video samples.",
        "translated": ""
    },
    {
        "title": "Dense Optical Tracking: Connecting the Dots",
        "url": "http://arxiv.org/abs/2312.00786v1",
        "pub_date": "2023-12-01",
        "summary": "Recent approaches to point tracking are able to recover the trajectory of any\nscene point through a large portion of a video despite the presence of\nocclusions. They are, however, too slow in practice to track every point\nobserved in a single frame in a reasonable amount of time. This paper\nintroduces DOT, a novel, simple and efficient method for solving this problem.\nIt first extracts a small set of tracks from key regions at motion boundaries\nusing an off-the-shelf point tracking algorithm. Given source and target\nframes, DOT then computes rough initial estimates of a dense flow field and\nvisibility mask through nearest-neighbor interpolation, before refining them\nusing a learnable optical flow estimator that explicitly handles occlusions and\ncan be trained on synthetic data with ground-truth correspondences. We show\nthat DOT is significantly more accurate than current optical flow techniques,\noutperforms sophisticated \"universal\" trackers like OmniMotion, and is on par\nwith, or better than, the best point tracking algorithms like CoTracker while\nbeing at least two orders of magnitude faster. Quantitative and qualitative\nexperiments with synthetic and real videos validate the promise of the proposed\napproach. Code, data, and videos showcasing the capabilities of our approach\nare available in the project webpage: https://16lemoing.github.io/dot .",
        "translated": ""
    },
    {
        "title": "Sequential Modeling Enables Scalable Learning for Large Vision Models",
        "url": "http://arxiv.org/abs/2312.00785v1",
        "pub_date": "2023-12-01",
        "summary": "We introduce a novel sequential modeling approach which enables learning a\nLarge Vision Model (LVM) without making use of any linguistic data. To do this,\nwe define a common format, \"visual sentences\", in which we can represent raw\nimages and videos as well as annotated data sources such as semantic\nsegmentations and depth reconstructions without needing any meta-knowledge\nbeyond the pixels. Once this wide variety of visual data (comprising 420\nbillion tokens) is represented as sequences, the model can be trained to\nminimize a cross-entropy loss for next token prediction. By training across\nvarious scales of model architecture and data diversity, we provide empirical\nevidence that our models scale effectively. Many different vision tasks can be\nsolved by designing suitable visual prompts at test time.",
        "translated": ""
    },
    {
        "title": "Making Large Multimodal Models Understand Arbitrary Visual Prompts",
        "url": "http://arxiv.org/abs/2312.00784v1",
        "pub_date": "2023-12-01",
        "summary": "While existing large vision-language multimodal models focus on whole image\nunderstanding, there is a prominent gap in achieving region-specific\ncomprehension. Current approaches that use textual coordinates or spatial\nencodings often fail to provide a user-friendly interface for visual prompting.\nTo address this challenge, we introduce a novel multimodal model capable of\ndecoding arbitrary visual prompts. This allows users to intuitively mark images\nand interact with the model using natural cues like a \"red bounding box\" or\n\"pointed arrow\". Our simple design directly overlays visual markers onto the\nRGB image, eliminating the need for complex region encodings, yet achieves\nstate-of-the-art performance on region-understanding tasks like Visual7W,\nPointQA, and Visual Commonsense Reasoning benchmark. Furthermore, we present\nViP-Bench, a comprehensive benchmark to assess the capability of models in\nunderstanding visual prompts across multiple dimensions, enabling future\nresearch in this domain. Code, data, and model are publicly available.",
        "translated": ""
    },
    {
        "title": "MorpheuS: Neural Dynamic 360° Surface Reconstruction from Monocular\n  RGB-D Video",
        "url": "http://arxiv.org/abs/2312.00778v1",
        "pub_date": "2023-12-01",
        "summary": "Neural rendering has demonstrated remarkable success in dynamic scene\nreconstruction. Thanks to the expressiveness of neural representations, prior\nworks can accurately capture the motion and achieve high-fidelity\nreconstruction of the target object. Despite this, real-world video scenarios\noften feature large unobserved regions where neural representations struggle to\nachieve realistic completion. To tackle this challenge, we introduce MorpheuS,\na framework for dynamic 360{\\deg} surface reconstruction from a casually\ncaptured RGB-D video. Our approach models the target scene as a canonical field\nthat encodes its geometry and appearance, in conjunction with a deformation\nfield that warps points from the current frame to the canonical space. We\nleverage a view-dependent diffusion prior and distill knowledge from it to\nachieve realistic completion of unobserved regions. Experimental results on\nvarious real-world and synthetic datasets show that our method can achieve\nhigh-fidelity 360{\\deg} surface reconstruction of a deformable object from a\nmonocular RGB-D video.",
        "translated": ""
    },
    {
        "title": "VideoBooth: Diffusion-based Video Generation with Image Prompts",
        "url": "http://arxiv.org/abs/2312.00777v1",
        "pub_date": "2023-12-01",
        "summary": "Text-driven video generation witnesses rapid progress. However, merely using\ntext prompts is not enough to depict the desired subject appearance that\naccurately aligns with users' intents, especially for customized content\ncreation. In this paper, we study the task of video generation with image\nprompts, which provide more accurate and direct content control beyond the text\nprompts. Specifically, we propose a feed-forward framework VideoBooth, with two\ndedicated designs: 1) We propose to embed image prompts in a coarse-to-fine\nmanner. Coarse visual embeddings from image encoder provide high-level\nencodings of image prompts, while fine visual embeddings from the proposed\nattention injection module provide multi-scale and detailed encoding of image\nprompts. These two complementary embeddings can faithfully capture the desired\nappearance. 2) In the attention injection module at fine level, multi-scale\nimage prompts are fed into different cross-frame attention layers as additional\nkeys and values. This extra spatial information refines the details in the\nfirst frame and then it is propagated to the remaining frames, which maintains\ntemporal consistency. Extensive experiments demonstrate that VideoBooth\nachieves state-of-the-art performance in generating customized high-quality\nvideos with subjects specified in image prompts. Notably, VideoBooth is a\ngeneralizable framework where a single model works for a wide range of image\nprompts with feed-forward pass.",
        "translated": ""
    },
    {
        "title": "Towards Generalizable Zero-Shot Manipulation via Translating Human\n  Interaction Plans",
        "url": "http://arxiv.org/abs/2312.00775v1",
        "pub_date": "2023-12-01",
        "summary": "We pursue the goal of developing robots that can interact zero-shot with\ngeneric unseen objects via a diverse repertoire of manipulation skills and show\nhow passive human videos can serve as a rich source of data for learning such\ngeneralist robots. Unlike typical robot learning approaches which directly\nlearn how a robot should act from interaction data, we adopt a factorized\napproach that can leverage large-scale human videos to learn how a human would\naccomplish a desired task (a human plan), followed by translating this plan to\nthe robots embodiment. Specifically, we learn a human plan predictor that,\ngiven a current image of a scene and a goal image, predicts the future hand and\nobject configurations. We combine this with a translation module that learns a\nplan-conditioned robot manipulation policy, and allows following humans plans\nfor generic manipulation tasks in a zero-shot manner with no deployment-time\ntraining. Importantly, while the plan predictor can leverage large-scale human\nvideos for learning, the translation module only requires a small amount of\nin-domain data, and can generalize to tasks not seen during training. We show\nthat our learned system can perform over 16 manipulation skills that generalize\nto 40 objects, encompassing 100 real-world tasks for table-top manipulation and\ndiverse in-the-wild manipulation. https://homangab.github.io/hopman/",
        "translated": ""
    },
    {
        "title": "Automated Material Properties Extraction For Enhanced Beauty Product\n  Discovery and Makeup Virtual Try-on",
        "url": "http://arxiv.org/abs/2312.00766v1",
        "pub_date": "2023-12-01",
        "summary": "The multitude of makeup products available can make it challenging to find\nthe ideal match for desired attributes. An intelligent approach for product\ndiscovery is required to enhance the makeup shopping experience to make it more\nconvenient and satisfying. However, enabling accurate and efficient product\ndiscovery requires extracting detailed attributes like color and finish type.\nOur work introduces an automated pipeline that utilizes multiple customized\nmachine learning models to extract essential material attributes from makeup\nproduct images. Our pipeline is versatile and capable of handling various\nmakeup products. To showcase the efficacy of our pipeline, we conduct extensive\nexperiments on eyeshadow products (both single and multi-shade ones), a\nchallenging makeup product known for its diverse range of shapes, colors, and\nfinish types. Furthermore, we demonstrate the applicability of our approach by\nsuccessfully extending it to other makeup categories like lipstick and\nfoundation, showcasing its adaptability and effectiveness across different\nbeauty products. Additionally, we conduct ablation experiments to demonstrate\nthe superiority of our machine learning pipeline over human labeling methods in\nterms of reliability. Our proposed method showcases its effectiveness in\ncross-category product discovery, specifically in recommending makeup products\nthat perfectly match a specified outfit. Lastly, we also demonstrate the\napplication of these material attributes in enabling virtual-try-on experiences\nwhich makes makeup shopping experience significantly more engaging.",
        "translated": ""
    },
    {
        "title": "Deep Unlearning: Fast and Efficient Training-free Approach to Controlled\n  Forgetting",
        "url": "http://arxiv.org/abs/2312.00761v1",
        "pub_date": "2023-12-01",
        "summary": "Machine unlearning has emerged as a prominent and challenging area of\ninterest, driven in large part by the rising regulatory demands for industries\nto delete user data upon request and the heightened awareness of privacy.\nExisting approaches either retrain models from scratch or use several\nfinetuning steps for every deletion request, often constrained by computational\nresource limitations and restricted access to the original training data. In\nthis work, we introduce a novel class unlearning algorithm designed to\nstrategically eliminate an entire class or a group of classes from the learned\nmodel. To that end, our algorithm first estimates the Retain Space and the\nForget Space, representing the feature or activation spaces for samples from\nclasses to be retained and unlearned, respectively. To obtain these spaces, we\npropose a novel singular value decomposition-based technique that requires\nlayer wise collection of network activations from a few forward passes through\nthe network. We then compute the shared information between these spaces and\nremove it from the forget space to isolate class-discriminatory feature space\nfor unlearning. Finally, we project the model weights in the orthogonal\ndirection of the class-discriminatory space to obtain the unlearned model. We\ndemonstrate our algorithm's efficacy on ImageNet using a Vision Transformer\nwith only $\\sim$1.5% drop in retain accuracy compared to the original model\nwhile maintaining under 1% accuracy on the unlearned class samples. Further,\nour algorithm consistently performs well when subject to Membership Inference\nAttacks showing 7.8% improvement on average across a variety of image\nclassification datasets and network architectures, as compared to other\nbaselines while being $\\sim$6x more computationally efficient.",
        "translated": ""
    },
    {
        "title": "Adversarial Score Distillation: When score distillation meets GAN",
        "url": "http://arxiv.org/abs/2312.00739v1",
        "pub_date": "2023-12-01",
        "summary": "Existing score distillation methods are sensitive to classifier-free guidance\n(CFG) scale: manifested as over-smoothness or instability at small CFG scales,\nwhile over-saturation at large ones. To explain and analyze these issues, we\nrevisit the derivation of Score Distillation Sampling (SDS) and decipher\nexisting score distillation with the Wasserstein Generative Adversarial Network\n(WGAN) paradigm. With the WGAN paradigm, we find that existing score\ndistillation either employs a fixed sub-optimal discriminator or conducts\nincomplete discriminator optimization, resulting in the scale-sensitive issue.\nWe propose the Adversarial Score Distillation (ASD), which maintains an\noptimizable discriminator and updates it using the complete optimization\nobjective. Experiments show that the proposed ASD performs favorably in 2D\ndistillation and text-to-3D tasks against existing methods. Furthermore, to\nexplore the generalization ability of our WGAN paradigm, we extend ASD to the\nimage editing task, which achieves competitive results. The project page and\ncode are at https://github.com/2y7c3/ASD.",
        "translated": ""
    },
    {
        "title": "Gaussian Grouping: Segment and Edit Anything in 3D Scenes",
        "url": "http://arxiv.org/abs/2312.00732v1",
        "pub_date": "2023-12-01",
        "summary": "The recent Gaussian Splatting achieves high-quality and real-time novel-view\nsynthesis of the 3D scenes. However, it is solely concentrated on the\nappearance and geometry modeling, while lacking in fine-grained object-level\nscene understanding. To address this issue, we propose Gaussian Grouping, which\nextends Gaussian Splatting to jointly reconstruct and segment anything in\nopen-world 3D scenes. We augment each Gaussian with a compact Identity\nEncoding, allowing the Gaussians to be grouped according to their object\ninstance or stuff membership in the 3D scene. Instead of resorting to expensive\n3D labels, we supervise the Identity Encodings during the differentiable\nrendering by leveraging the 2D mask predictions by SAM, along with introduced\n3D spatial consistency regularization. Comparing to the implicit NeRF\nrepresentation, we show that the discrete and grouped 3D Gaussians can\nreconstruct, segment and edit anything in 3D with high visual quality, fine\ngranularity and efficiency. Based on Gaussian Grouping, we further propose a\nlocal Gaussian Editing scheme, which shows efficacy in versatile scene editing\napplications, including 3D object removal, inpainting, colorization and scene\nrecomposition. Our code and models will be at\nhttps://github.com/lkeab/gaussian-grouping.",
        "translated": ""
    },
    {
        "title": "PaSCo: Urban 3D Panoptic Scene Completion with Uncertainty Awareness",
        "url": "http://arxiv.org/abs/2312.02158v1",
        "pub_date": "2023-12-04",
        "summary": "We propose the task of Panoptic Scene Completion (PSC) which extends the\nrecently popular Semantic Scene Completion (SSC) task with instance-level\ninformation to produce a richer understanding of the 3D scene. Our PSC proposal\nutilizes a hybrid mask-based technique on the non-empty voxels from sparse\nmulti-scale completions. Whereas the SSC literature overlooks uncertainty which\nis critical for robotics applications, we instead propose an efficient\nensembling to estimate both voxel-wise and instance-wise uncertainties along\nPSC. This is achieved by building on a multi-input multi-output (MIMO)\nstrategy, while improving performance and yielding better uncertainty for\nlittle additional compute. Additionally, we introduce a technique to aggregate\npermutation-invariant mask predictions. Our experiments demonstrate that our\nmethod surpasses all baselines in both Panoptic Scene Completion and\nuncertainty estimation on three large-scale autonomous driving datasets. Our\ncode and data are available at https://astra-vision.github.io/PaSCo .",
        "translated": ""
    },
    {
        "title": "Mesh-Guided Neural Implicit Field Editing",
        "url": "http://arxiv.org/abs/2312.02157v1",
        "pub_date": "2023-12-04",
        "summary": "Neural implicit fields have emerged as a powerful 3D representation for\nreconstructing and rendering photo-realistic views, yet they possess limited\neditability. Conversely, explicit 3D representations, such as polygonal meshes,\noffer ease of editing but may not be as suitable for rendering high-quality\nnovel views. To harness the strengths of both representations, we propose a new\napproach that employs a mesh as a guiding mechanism in editing the neural\nradiance field. We first introduce a differentiable method using marching\ntetrahedra for polygonal mesh extraction from the neural implicit field and\nthen design a differentiable color extractor to assign colors obtained from the\nvolume renderings to this extracted mesh. This differentiable colored mesh\nallows gradient back-propagation from the explicit mesh to the implicit fields,\nempowering users to easily manipulate the geometry and color of neural implicit\nfields. To enhance user control from coarse-grained to fine-grained levels, we\nintroduce an octree-based structure into its optimization. This structure\nprioritizes the edited regions and the surface part, making our method achieve\nfine-grained edits to the neural implicit field and accommodate various user\nmodifications, including object additions, component removals, specific area\ndeformations, and adjustments to local and global colors. Through extensive\nexperiments involving diverse scenes and editing operations, we have\ndemonstrated the capabilities and effectiveness of our method. Our project page\nis: \\url{https://cassiepython.github.io/MNeuEdit/}",
        "translated": ""
    },
    {
        "title": "GPS-Gaussian: Generalizable Pixel-wise 3D Gaussian Splatting for\n  Real-time Human Novel View Synthesis",
        "url": "http://arxiv.org/abs/2312.02155v1",
        "pub_date": "2023-12-04",
        "summary": "We present a new approach, termed GPS-Gaussian, for synthesizing novel views\nof a character in a real-time manner. The proposed method enables 2K-resolution\nrendering under a sparse-view camera setting. Unlike the original Gaussian\nSplatting or neural implicit rendering methods that necessitate per-subject\noptimizations, we introduce Gaussian parameter maps defined on the source views\nand regress directly Gaussian Splatting properties for instant novel view\nsynthesis without any fine-tuning or optimization. To this end, we train our\nGaussian parameter regression module on a large amount of human scan data,\njointly with a depth estimation module to lift 2D parameter maps to 3D space.\nThe proposed framework is fully differentiable and experiments on several\ndatasets demonstrate that our method outperforms state-of-the-art methods while\nachieving an exceeding rendering speed.",
        "translated": ""
    },
    {
        "title": "Latent Feature-Guided Diffusion Models for Shadow Removal",
        "url": "http://arxiv.org/abs/2312.02156v1",
        "pub_date": "2023-12-04",
        "summary": "Recovering textures under shadows has remained a challenging problem due to\nthe difficulty of inferring shadow-free scenes from shadow images. In this\npaper, we propose the use of diffusion models as they offer a promising\napproach to gradually refine the details of shadow regions during the diffusion\nprocess. Our method improves this process by conditioning on a learned latent\nfeature space that inherits the characteristics of shadow-free images, thus\navoiding the limitation of conventional methods that condition on degraded\nimages only. Additionally, we propose to alleviate potential local optima\nduring training by fusing noise features with the diffusion network. We\ndemonstrate the effectiveness of our approach which outperforms the previous\nbest method by 13% in terms of RMSE on the AISTD dataset. Further, we explore\ninstance-level shadow removal, where our model outperforms the previous best\nmethod by 82% in terms of RMSE on the DESOBA dataset.",
        "translated": ""
    },
    {
        "title": "Aligning and Prompting Everything All at Once for Universal Visual\n  Perception",
        "url": "http://arxiv.org/abs/2312.02153v1",
        "pub_date": "2023-12-04",
        "summary": "Vision foundation models have been explored recently to build general-purpose\nvision systems. However, predominant paradigms, driven by casting\ninstance-level tasks as an object-word alignment, bring heavy cross-modality\ninteraction, which is not effective in prompting object detection and visual\ngrounding. Another line of work that focuses on pixel-level tasks often\nencounters a large annotation gap of things and stuff, and suffers from mutual\ninterference between foreground-object and background-class segmentation. In\nstark contrast to the prevailing methods, we present APE, a universal visual\nperception model for aligning and prompting everything all at once in an image\nto perform diverse tasks, i.e., detection, segmentation, and grounding, as an\ninstance-level sentence-object matching paradigm. Specifically, APE advances\nthe convergence of detection and grounding by reformulating language-guided\ngrounding as open-vocabulary detection, which efficiently scales up model\nprompting to thousands of category vocabularies and region descriptions while\nmaintaining the effectiveness of cross-modality fusion. To bridge the\ngranularity gap of different pixel-level tasks, APE equalizes semantic and\npanoptic segmentation to proxy instance learning by considering any isolated\nregions as individual instances. APE aligns vision and language representation\non broad data with natural and challenging characteristics all at once without\ntask-specific fine-tuning. The extensive experiments on over 160 datasets\ndemonstrate that, with only one-suit of weights, APE outperforms (or is on par\nwith) the state-of-the-art models, proving that an effective yet universal\nperception for anything aligning and prompting is indeed feasible. Codes and\ntrained models are released at https://github.com/shenyunhang/APE.",
        "translated": ""
    },
    {
        "title": "Steerers: A framework for rotation equivariant keypoint descriptors",
        "url": "http://arxiv.org/abs/2312.02152v1",
        "pub_date": "2023-12-04",
        "summary": "Image keypoint descriptions that are discriminative and matchable over large\nchanges in viewpoint are vital for 3D reconstruction. However, descriptions\noutput by learned descriptors are typically not robust to camera rotation.\nWhile they can be made more robust by, e.g., data augmentation, this degrades\nperformance on upright images. Another approach is test-time augmentation,\nwhich incurs a significant increase in runtime. We instead learn a linear\ntransform in description space that encodes rotations of the input image. We\ncall this linear transform a steerer since it allows us to transform the\ndescriptions as if the image was rotated. From representation theory we know\nall possible steerers for the rotation group. Steerers can be optimized (A)\ngiven a fixed descriptor, (B) jointly with a descriptor or (C) we can optimize\na descriptor given a fixed steerer. We perform experiments in all of these\nthree settings and obtain state-of-the-art results on the rotation invariant\nimage matching benchmarks AIMS and Roto-360. We publish code and model weights\nat github.com/georg-bn/rotation-steerers.",
        "translated": ""
    },
    {
        "title": "Guarding Barlow Twins Against Overfitting with Mixed Samples",
        "url": "http://arxiv.org/abs/2312.02151v1",
        "pub_date": "2023-12-04",
        "summary": "Self-supervised Learning (SSL) aims to learn transferable feature\nrepresentations for downstream applications without relying on labeled data.\nThe Barlow Twins algorithm, renowned for its widespread adoption and\nstraightforward implementation compared to its counterparts like contrastive\nlearning methods, minimizes feature redundancy while maximizing invariance to\ncommon corruptions. Optimizing for the above objective forces the network to\nlearn useful representations, while avoiding noisy or constant features,\nresulting in improved downstream task performance with limited adaptation.\nDespite Barlow Twins' proven effectiveness in pre-training, the underlying SSL\nobjective can inadvertently cause feature overfitting due to the lack of strong\ninteraction between the samples unlike the contrastive learning approaches.\nFrom our experiments, we observe that optimizing for the Barlow Twins objective\ndoesn't necessarily guarantee sustained improvements in representation quality\nbeyond a certain pre-training phase, and can potentially degrade downstream\nperformance on some datasets. To address this challenge, we introduce Mixed\nBarlow Twins, which aims to improve sample interaction during Barlow Twins\ntraining via linearly interpolated samples. This results in an additional\nregularization term to the original Barlow Twins objective, assuming linear\ninterpolation in the input space translates to linearly interpolated features\nin the feature space. Pre-training with this regularization effectively\nmitigates feature overfitting and further enhances the downstream performance\non CIFAR-10, CIFAR-100, TinyImageNet, STL-10, and ImageNet datasets. The code\nand checkpoints are available at: https://github.com/wgcban/mix-bt.git",
        "translated": ""
    },
    {
        "title": "Readout Guidance: Learning Control from Diffusion Features",
        "url": "http://arxiv.org/abs/2312.02150v1",
        "pub_date": "2023-12-04",
        "summary": "We present Readout Guidance, a method for controlling text-to-image diffusion\nmodels with learned signals. Readout Guidance uses readout heads, lightweight\nnetworks trained to extract signals from the features of a pre-trained, frozen\ndiffusion model at every timestep. These readouts can encode single-image\nproperties, such as pose, depth, and edges; or higher-order properties that\nrelate multiple images, such as correspondence and appearance similarity.\nFurthermore, by comparing the readout estimates to a user-defined target, and\nback-propagating the gradient through the readout head, these estimates can be\nused to guide the sampling process. Compared to prior methods for conditional\ngeneration, Readout Guidance requires significantly fewer added parameters and\ntraining samples, and offers a convenient and simple recipe for reproducing\ndifferent forms of conditional control under a single framework, with a single\narchitecture and sampling procedure. We showcase these benefits in the\napplications of drag-based manipulation, identity-consistent generation, and\nspatially aligned control. Project page: https://readout-guidance.github.io.",
        "translated": ""
    },
    {
        "title": "Generative Powers of Ten",
        "url": "http://arxiv.org/abs/2312.02149v1",
        "pub_date": "2023-12-04",
        "summary": "We present a method that uses a text-to-image model to generate consistent\ncontent across multiple image scales, enabling extreme semantic zooms into a\nscene, e.g., ranging from a wide-angle landscape view of a forest to a macro\nshot of an insect sitting on one of the tree branches. We achieve this through\na joint multi-scale diffusion sampling approach that encourages consistency\nacross different scales while preserving the integrity of each individual\nsampling process. Since each generated scale is guided by a different text\nprompt, our method enables deeper levels of zoom than traditional\nsuper-resolution methods that may struggle to create new contextual structure\nat vastly different scales. We compare our method qualitatively with\nalternative techniques in image super-resolution and outpainting, and show that\nour method is most effective at generating consistent multi-scale content.",
        "translated": ""
    },
    {
        "title": "Rejuvenating image-GPT as Strong Visual Representation Learners",
        "url": "http://arxiv.org/abs/2312.02147v1",
        "pub_date": "2023-12-04",
        "summary": "This paper enhances image-GPT (iGPT), one of the pioneering works that\nintroduce autoregressive pretraining to predict next pixels for visual\nrepresentation learning. Two simple yet essential changes are made. First, we\nshift the prediction target from raw pixels to semantic tokens, enabling a\nhigher-level understanding of visual content. Second, we supplement the\nautoregressive modeling by instructing the model to predict not only the next\ntokens but also the visible tokens. This pipeline is particularly effective\nwhen semantic tokens are encoded by discriminatively trained models, such as\nCLIP. We introduce this novel approach as D-iGPT. Extensive experiments\nshowcase that D-iGPT excels as a strong learner of visual representations: A\nnotable achievement of D-iGPT is its compelling performance on the ImageNet-1K\ndataset -- by training on publicly available datasets, D-iGPT achieves 89.5\\%\ntop-1 accuracy with a vanilla ViT-Large model. This model also shows strong\ngeneralization on the downstream task and robustness on out-of-distribution\nsamples. Code is avaiable at\n\\href{https://github.com/OliverRensu/D-iGPT}{https://github.com/OliverRensu/D-iGPT}.",
        "translated": ""
    },
    {
        "title": "ReconFusion: 3D Reconstruction with Diffusion Priors",
        "url": "http://arxiv.org/abs/2312.02981v1",
        "pub_date": "2023-12-05",
        "summary": "3D reconstruction methods such as Neural Radiance Fields (NeRFs) excel at\nrendering photorealistic novel views of complex scenes. However, recovering a\nhigh-quality NeRF typically requires tens to hundreds of input images,\nresulting in a time-consuming capture process. We present ReconFusion to\nreconstruct real-world scenes using only a few photos. Our approach leverages a\ndiffusion prior for novel view synthesis, trained on synthetic and multiview\ndatasets, which regularizes a NeRF-based 3D reconstruction pipeline at novel\ncamera poses beyond those captured by the set of input images. Our method\nsynthesizes realistic geometry and texture in underconstrained regions while\npreserving the appearance of observed regions. We perform an extensive\nevaluation across various real-world datasets, including forward-facing and\n360-degree scenes, demonstrating significant performance improvements over\nprevious few-view NeRF reconstruction approaches.",
        "translated": ""
    },
    {
        "title": "GPT4Point: A Unified Framework for Point-Language Understanding and\n  Generation",
        "url": "http://arxiv.org/abs/2312.02980v1",
        "pub_date": "2023-12-05",
        "summary": "Multimodal Large Language Models (MLLMs) have excelled in 2D image-text\ncomprehension and image generation, but their understanding of the 3D world is\nnotably deficient, limiting progress in 3D language understanding and\ngeneration. To solve this problem, we introduce GPT4Point, an innovative\ngroundbreaking point-language multimodal model designed specifically for\nunified 3D object understanding and generation within the MLLM framework.\nGPT4Point as a powerful 3D MLLM seamlessly can execute a variety of point-text\nreference tasks such as point-cloud captioning and Q&amp;A. Additionally, GPT4Point\nis equipped with advanced capabilities for controllable 3D generation, it can\nget high-quality results through a low-quality point-text feature maintaining\nthe geometric shapes and colors. To support the expansive needs of 3D\nobject-text pairs, we develop Pyramid-XL, a point-language dataset annotation\nengine. It constructs a large-scale database over 1M objects of varied text\ngranularity levels from the Objaverse-XL dataset, essential for training\nGPT4Point. A comprehensive benchmark has been proposed to evaluate 3D\npoint-language understanding capabilities. In extensive evaluations, GPT4Point\nhas demonstrated superior performance in understanding and generation.",
        "translated": ""
    },
    {
        "title": "Imitating Shortest Paths in Simulation Enables Effective Navigation and\n  Manipulation in the Real World",
        "url": "http://arxiv.org/abs/2312.02976v1",
        "pub_date": "2023-12-05",
        "summary": "Reinforcement learning (RL) with dense rewards and imitation learning (IL)\nwith human-generated trajectories are the most widely used approaches for\ntraining modern embodied agents. RL requires extensive reward shaping and\nauxiliary losses and is often too slow and ineffective for long-horizon tasks.\nWhile IL with human supervision is effective, collecting human trajectories at\nscale is extremely expensive. In this work, we show that imitating\nshortest-path planners in simulation produces agents that, given a language\ninstruction, can proficiently navigate, explore, and manipulate objects in both\nsimulation and in the real world using only RGB sensors (no depth map or GPS\ncoordinates). This surprising result is enabled by our end-to-end,\ntransformer-based, SPOC architecture, powerful visual encoders paired with\nextensive image augmentation, and the dramatic scale and diversity of our\ntraining data: millions of frames of shortest-path-expert trajectories\ncollected inside approximately 200,000 procedurally generated houses containing\n40,000 unique 3D assets. Our models, data, training code, and newly proposed\n10-task benchmarking suite CHORES will be open-sourced.",
        "translated": ""
    },
    {
        "title": "Dexterous Functional Grasping",
        "url": "http://arxiv.org/abs/2312.02975v1",
        "pub_date": "2023-12-05",
        "summary": "While there have been significant strides in dexterous manipulation, most of\nit is limited to benchmark tasks like in-hand reorientation which are of\nlimited utility in the real world. The main benefit of dexterous hands over\ntwo-fingered ones is their ability to pickup tools and other objects (including\nthin ones) and grasp them firmly to apply force. However, this task requires\nboth a complex understanding of functional affordances as well as precise\nlow-level control. While prior work obtains affordances from human data this\napproach doesn't scale to low-level control. Similarly, simulation training\ncannot give the robot an understanding of real-world semantics. In this paper,\nwe aim to combine the best of both worlds to accomplish functional grasping for\nin-the-wild objects. We use a modular approach. First, affordances are obtained\nby matching corresponding regions of different objects and then a low-level\npolicy trained in sim is run to grasp it. We propose a novel application of\neigengrasps to reduce the search space of RL using a small amount of human data\nand find that it leads to more stable and physically realistic motion. We find\nthat eigengrasp action space beats baselines in simulation and outperforms\nhardcoded grasping in real and matches or outperforms a trained human\nteleoperator. Results visualizations and videos at https://dexfunc.github.io/",
        "translated": ""
    },
    {
        "title": "Describing Differences in Image Sets with Natural Language",
        "url": "http://arxiv.org/abs/2312.02974v1",
        "pub_date": "2023-12-05",
        "summary": "How do two sets of images differ? Discerning set-level differences is crucial\nfor understanding model behaviors and analyzing datasets, yet manually sifting\nthrough thousands of images is impractical. To aid in this discovery process,\nwe explore the task of automatically describing the differences between two\n$\\textbf{sets}$ of images, which we term Set Difference Captioning. This task\ntakes in image sets $D_A$ and $D_B$, and outputs a description that is more\noften true on $D_A$ than $D_B$. We outline a two-stage approach that first\nproposes candidate difference descriptions from image sets and then re-ranks\nthe candidates by checking how well they can differentiate the two sets. We\nintroduce VisDiff, which first captions the images and prompts a language model\nto propose candidate descriptions, then re-ranks these descriptions using CLIP.\nTo evaluate VisDiff, we collect VisDiffBench, a dataset with 187 paired image\nsets with ground truth difference descriptions. We apply VisDiff to various\ndomains, such as comparing datasets (e.g., ImageNet vs. ImageNetV2), comparing\nclassification models (e.g., zero-shot CLIP vs. supervised ResNet), summarizing\nmodel failure modes (supervised ResNet), characterizing differences between\ngenerative models (e.g., StableDiffusionV1 and V2), and discovering what makes\nimages memorable. Using VisDiff, we are able to find interesting and previously\nunknown differences in datasets and models, demonstrating its utility in\nrevealing nuanced insights.",
        "translated": ""
    },
    {
        "title": "GauHuman: Articulated Gaussian Splatting from Monocular Human Videos",
        "url": "http://arxiv.org/abs/2312.02973v1",
        "pub_date": "2023-12-05",
        "summary": "We present, GauHuman, a 3D human model with Gaussian Splatting for both fast\ntraining (1 ~ 2 minutes) and real-time rendering (up to 189 FPS), compared with\nexisting NeRF-based implicit representation modelling frameworks demanding\nhours of training and seconds of rendering per frame. Specifically, GauHuman\nencodes Gaussian Splatting in the canonical space and transforms 3D Gaussians\nfrom canonical space to posed space with linear blend skinning (LBS), in which\neffective pose and LBS refinement modules are designed to learn fine details of\n3D humans under negligible computational cost. Moreover, to enable fast\noptimization of GauHuman, we initialize and prune 3D Gaussians with 3D human\nprior, while splitting/cloning via KL divergence guidance, along with a novel\nmerge operation for further speeding up. Extensive experiments on ZJU_Mocap and\nMonoCap datasets demonstrate that GauHuman achieves state-of-the-art\nperformance quantitatively and qualitatively with fast training and real-time\nrendering speed. Notably, without sacrificing rendering quality, GauHuman can\nfast model the 3D human performer with ~13k 3D Gaussians.",
        "translated": ""
    },
    {
        "title": "Alchemist: Parametric Control of Material Properties with Diffusion\n  Models",
        "url": "http://arxiv.org/abs/2312.02970v1",
        "pub_date": "2023-12-05",
        "summary": "We propose a method to control material attributes of objects like roughness,\nmetallic, albedo, and transparency in real images. Our method capitalizes on\nthe generative prior of text-to-image models known for photorealism, employing\na scalar value and instructions to alter low-level material properties.\nAddressing the lack of datasets with controlled material attributes, we\ngenerated an object-centric synthetic dataset with physically-based materials.\nFine-tuning a modified pre-trained text-to-image model on this synthetic\ndataset enables us to edit material properties in real-world images while\npreserving all other attributes. We show the potential application of our model\nto material edited NeRFs.",
        "translated": ""
    },
    {
        "title": "AmbiGen: Generating Ambigrams from Pre-trained Diffusion Model",
        "url": "http://arxiv.org/abs/2312.02967v1",
        "pub_date": "2023-12-05",
        "summary": "Ambigrams are calligraphic designs that have different meanings depending on\nthe viewing orientation. Creating ambigrams is a challenging task even for\nskilled artists, as it requires maintaining the meaning under two different\nviewpoints at the same time. In this work, we propose to generate ambigrams by\ndistilling a large-scale vision and language diffusion model, namely DeepFloyd\nIF, to optimize the letters' outline for legibility in the two viewing\norientations. Empirically, we demonstrate that our approach outperforms\nexisting ambigram generation methods. On the 500 most common words in English,\nour method achieves more than an 11.6% increase in word accuracy and at least a\n41.9% reduction in edit distance.",
        "translated": ""
    },
    {
        "title": "Diffusion-SS3D: Diffusion Model for Semi-supervised 3D Object Detection",
        "url": "http://arxiv.org/abs/2312.02966v1",
        "pub_date": "2023-12-05",
        "summary": "Semi-supervised object detection is crucial for 3D scene understanding,\nefficiently addressing the limitation of acquiring large-scale 3D bounding box\nannotations. Existing methods typically employ a teacher-student framework with\npseudo-labeling to leverage unlabeled point clouds. However, producing reliable\npseudo-labels in a diverse 3D space still remains challenging. In this work, we\npropose Diffusion-SS3D, a new perspective of enhancing the quality of\npseudo-labels via the diffusion model for semi-supervised 3D object detection.\nSpecifically, we include noises to produce corrupted 3D object size and class\nlabel distributions, and then utilize the diffusion model as a denoising\nprocess to obtain bounding box outputs. Moreover, we integrate the diffusion\nmodel into the teacher-student framework, so that the denoised bounding boxes\ncan be used to improve pseudo-label generation, as well as the entire\nsemi-supervised learning process. We conduct experiments on the ScanNet and SUN\nRGB-D benchmark datasets to demonstrate that our approach achieves\nstate-of-the-art performance against existing methods. We also present\nextensive analysis to understand how our diffusion model design affects\nperformance in semi-supervised learning.",
        "translated": ""
    },
    {
        "title": "MVHumanNet: A Large-scale Dataset of Multi-view Daily Dressing Human\n  Captures",
        "url": "http://arxiv.org/abs/2312.02963v1",
        "pub_date": "2023-12-05",
        "summary": "In this era, the success of large language models and text-to-image models\ncan be attributed to the driving force of large-scale datasets. However, in the\nrealm of 3D vision, while remarkable progress has been made with models trained\non large-scale synthetic and real-captured object data like Objaverse and\nMVImgNet, a similar level of progress has not been observed in the domain of\nhuman-centric tasks partially due to the lack of a large-scale human dataset.\nExisting datasets of high-fidelity 3D human capture continue to be mid-sized\ndue to the significant challenges in acquiring large-scale high-quality 3D\nhuman data. To bridge this gap, we present MVHumanNet, a dataset that comprises\nmulti-view human action sequences of 4,500 human identities. The primary focus\nof our work is on collecting human data that features a large number of diverse\nidentities and everyday clothing using a multi-view human capture system, which\nfacilitates easily scalable data collection. Our dataset contains 9,000 daily\noutfits, 60,000 motion sequences and 645 million frames with extensive\nannotations, including human masks, camera parameters, 2D and 3D keypoints,\nSMPL/SMPLX parameters, and corresponding textual descriptions. To explore the\npotential of MVHumanNet in various 2D and 3D visual tasks, we conducted pilot\nstudies on view-consistent action recognition, human NeRF reconstruction,\ntext-driven view-unconstrained human image generation, as well as 2D\nview-unconstrained human image and 3D avatar generation. Extensive experiments\ndemonstrate the performance improvements and effective applications enabled by\nthe scale provided by MVHumanNet. As the current largest-scale 3D human\ndataset, we hope that the release of MVHumanNet data with annotations will\nfoster further innovations in the domain of 3D human-centric tasks at scale.",
        "translated": ""
    },
    {
        "title": "Relightable Gaussian Codec Avatars",
        "url": "http://arxiv.org/abs/2312.03704v1",
        "pub_date": "2023-12-06",
        "summary": "The fidelity of relighting is bounded by both geometry and appearance\nrepresentations. For geometry, both mesh and volumetric approaches have\ndifficulty modeling intricate structures like 3D hair geometry. For appearance,\nexisting relighting models are limited in fidelity and often too slow to render\nin real-time with high-resolution continuous environments. In this work, we\npresent Relightable Gaussian Codec Avatars, a method to build high-fidelity\nrelightable head avatars that can be animated to generate novel expressions.\nOur geometry model based on 3D Gaussians can capture 3D-consistent\nsub-millimeter details such as hair strands and pores on dynamic face\nsequences. To support diverse materials of human heads such as the eyes, skin,\nand hair in a unified manner, we present a novel relightable appearance model\nbased on learnable radiance transfer. Together with global illumination-aware\nspherical harmonics for the diffuse components, we achieve real-time relighting\nwith spatially all-frequency reflections using spherical Gaussians. This\nappearance model can be efficiently relit under both point light and continuous\nillumination. We further improve the fidelity of eye reflections and enable\nexplicit gaze control by introducing relightable explicit eye models. Our\nmethod outperforms existing approaches without compromising real-time\nperformance. We also demonstrate real-time relighting of avatars on a tethered\nconsumer VR headset, showcasing the efficiency and fidelity of our avatars.",
        "translated": ""
    },
    {
        "title": "Skeleton-in-Context: Unified Skeleton Sequence Modeling with In-Context\n  Learning",
        "url": "http://arxiv.org/abs/2312.03703v1",
        "pub_date": "2023-12-06",
        "summary": "In-context learning provides a new perspective for multi-task modeling for\nvision and NLP. Under this setting, the model can perceive tasks from prompts\nand accomplish them without any extra task-specific head predictions or model\nfine-tuning. However, Skeleton sequence modeling via in-context learning\nremains unexplored. Directly applying existing in-context models from other\nareas onto skeleton sequences fails due to the inter-frame and cross-task pose\nsimilarity that makes it outstandingly hard to perceive the task correctly from\na subtle context. To address this challenge, we propose Skeleton-in-Context\n(SiC), an effective framework for in-context skeleton sequence modeling. Our\nSiC is able to handle multiple skeleton-based tasks simultaneously after a\nsingle training process and accomplish each task from context according to the\ngiven prompt. It can further generalize to new, unseen tasks according to\ncustomized prompts. To facilitate context perception, we additionally propose a\ntask-unified prompt, which adaptively learns tasks of different natures, such\nas partial joint-level generation, sequence-level prediction, or 2D-to-3D\nmotion prediction. We conduct extensive experiments to evaluate the\neffectiveness of our SiC on multiple tasks, including motion prediction, pose\nestimation, joint completion, and future pose estimation. We also evaluate its\ngeneralization capability on unseen tasks such as motion-in-between. These\nexperiments show that our model achieves state-of-the-art multi-task\nperformance and even outperforms single-task methods on certain tasks.",
        "translated": ""
    },
    {
        "title": "Self-conditioned Image Generation via Generating Representations",
        "url": "http://arxiv.org/abs/2312.03701v1",
        "pub_date": "2023-12-06",
        "summary": "This paper presents $\\textbf{R}$epresentation-$\\textbf{C}$onditioned image\n$\\textbf{G}$eneration (RCG), a simple yet effective image generation framework\nwhich sets a new benchmark in class-unconditional image generation. RCG does\nnot condition on any human annotations. Instead, it conditions on a\nself-supervised representation distribution which is mapped from the image\ndistribution using a pre-trained encoder. During generation, RCG samples from\nsuch representation distribution using a representation diffusion model (RDM),\nand employs a pixel generator to craft image pixels conditioned on the sampled\nrepresentation. Such a design provides substantial guidance during the\ngenerative process, resulting in high-quality image generation. Tested on\nImageNet 256$\\times$256, RCG achieves a Frechet Inception Distance (FID) of\n3.31 and an Inception Score (IS) of 253.4. These results not only significantly\nimprove the state-of-the-art of class-unconditional image generation but also\nrival the current leading methods in class-conditional image generation,\nbridging the long-standing performance gap between these two tasks. Code is\navailable at https://github.com/LTH14/rcg.",
        "translated": ""
    },
    {
        "title": "OneLLM: One Framework to Align All Modalities with Language",
        "url": "http://arxiv.org/abs/2312.03700v1",
        "pub_date": "2023-12-06",
        "summary": "Multimodal large language models (MLLMs) have gained significant attention\ndue to their strong multimodal understanding capability. However, existing\nworks rely heavily on modality-specific encoders, which usually differ in\narchitecture and are limited to common modalities. In this paper, we present\nOneLLM, an MLLM that aligns eight modalities to language using a unified\nframework. We achieve this through a unified multimodal encoder and a\nprogressive multimodal alignment pipeline. In detail, we first train an image\nprojection module to connect a vision encoder with LLM. Then, we build a\nuniversal projection module (UPM) by mixing multiple image projection modules\nand dynamic routing. Finally, we progressively align more modalities to LLM\nwith the UPM. To fully leverage the potential of OneLLM in following\ninstructions, we also curated a comprehensive multimodal instruction dataset,\nincluding 2M items from image, audio, video, point cloud, depth/normal map, IMU\nand fMRI brain activity. OneLLM is evaluated on 25 diverse benchmarks,\nencompassing tasks such as multimodal captioning, question answering and\nreasoning, where it delivers excellent performance. Code, data, model and\nonline demo are available at https://github.com/csuhan/OneLLM",
        "translated": ""
    },
    {
        "title": "Intrinsic Harmonization for Illumination-Aware Compositing",
        "url": "http://arxiv.org/abs/2312.03698v1",
        "pub_date": "2023-12-06",
        "summary": "Despite significant advancements in network-based image harmonization\ntechniques, there still exists a domain disparity between typical training\npairs and real-world composites encountered during inference. Most existing\nmethods are trained to reverse global edits made on segmented image regions,\nwhich fail to accurately capture the lighting inconsistencies between the\nforeground and background found in composited images. In this work, we\nintroduce a self-supervised illumination harmonization approach formulated in\nthe intrinsic image domain. First, we estimate a simple global lighting model\nfrom mid-level vision representations to generate a rough shading for the\nforeground region. A network then refines this inferred shading to generate a\nharmonious re-shading that aligns with the background scene. In order to match\nthe color appearance of the foreground and background, we utilize ideas from\nprior harmonization approaches to perform parameterized image edits in the\nalbedo domain. To validate the effectiveness of our approach, we present\nresults from challenging real-world composites and conduct a user study to\nobjectively measure the enhanced realism achieved compared to state-of-the-art\nharmonization methods.",
        "translated": ""
    },
    {
        "title": "Memory Triggers: Unveiling Memorization in Text-To-Image Generative\n  Models through Word-Level Duplication",
        "url": "http://arxiv.org/abs/2312.03692v1",
        "pub_date": "2023-12-06",
        "summary": "Diffusion-based models, such as the Stable Diffusion model, have\nrevolutionized text-to-image synthesis with their ability to produce\nhigh-quality, high-resolution images. These advancements have prompted\nsignificant progress in image generation and editing tasks. However, these\nmodels also raise concerns due to their tendency to memorize and potentially\nreplicate exact training samples, posing privacy risks and enabling adversarial\nattacks. Duplication in training datasets is recognized as a major factor\ncontributing to memorization, and various forms of memorization have been\nstudied so far. This paper focuses on two distinct and underexplored types of\nduplication that lead to replication during inference in diffusion-based\nmodels, particularly in the Stable Diffusion model. We delve into these\nlesser-studied duplication phenomena and their implications through two case\nstudies, aiming to contribute to the safer and more responsible use of\ngenerative models in various applications.",
        "translated": ""
    },
    {
        "title": "Hybrid Functional Maps for Crease-Aware Non-Isometric Shape Matching",
        "url": "http://arxiv.org/abs/2312.03678v1",
        "pub_date": "2023-12-06",
        "summary": "Non-isometric shape correspondence remains a fundamental challenge in\ncomputer vision. Traditional methods using Laplace-Beltrami operator (LBO)\neigenmodes face limitations in characterizing high-frequency extrinsic shape\nchanges like bending and creases. We propose a novel approach of combining the\nnon-orthogonal extrinsic basis of eigenfunctions of the elastic thin-shell\nhessian with the intrinsic ones of the LBO, creating a hybrid spectral space in\nwhich we construct functional maps. To this end, we present a theoretical\nframework to effectively integrate non-orthogonal basis functions into\ndescriptor- and learning-based functional map methods. Our approach can be\nincorporated easily into existing functional map pipelines across varying\napplications and is able to handle complex deformations beyond isometries. We\nshow extensive evaluations across various supervised and unsupervised settings\nand demonstrate significant improvements. Notably, our approach achieves up to\n15% better mean geodesic error for non-isometric correspondence settings and up\nto 45% improvement in scenarios with topological noise.",
        "translated": ""
    },
    {
        "title": "WarpDiffusion: Efficient Diffusion Model for High-Fidelity Virtual\n  Try-on",
        "url": "http://arxiv.org/abs/2312.03667v1",
        "pub_date": "2023-12-06",
        "summary": "Image-based Virtual Try-On (VITON) aims to transfer an in-shop garment image\nonto a target person. While existing methods focus on warping the garment to\nfit the body pose, they often overlook the synthesis quality around the\ngarment-skin boundary and realistic effects like wrinkles and shadows on the\nwarped garments. These limitations greatly reduce the realism of the generated\nresults and hinder the practical application of VITON techniques. Leveraging\nthe notable success of diffusion-based models in cross-modal image synthesis,\nsome recent diffusion-based methods have ventured to tackle this issue.\nHowever, they tend to either consume a significant amount of training resources\nor struggle to achieve realistic try-on effects and retain garment details. For\nefficient and high-fidelity VITON, we propose WarpDiffusion, which bridges the\nwarping-based and diffusion-based paradigms via a novel informative and local\ngarment feature attention mechanism. Specifically, WarpDiffusion incorporates\nlocal texture attention to reduce resource consumption and uses a novel\nauto-mask module that effectively retains only the critical areas of the warped\ngarment while disregarding unrealistic or erroneous portions. Notably,\nWarpDiffusion can be integrated as a plug-and-play component into existing\nVITON methodologies, elevating their synthesis quality. Extensive experiments\non high-resolution VITON benchmarks and an in-the-wild test set demonstrate the\nsuperiority of WarpDiffusion, surpassing state-of-the-art methods both\nqualitatively and quantitatively.",
        "translated": ""
    },
    {
        "title": "Reason2Drive: Towards Interpretable and Chain-based Reasoning for\n  Autonomous Driving",
        "url": "http://arxiv.org/abs/2312.03661v1",
        "pub_date": "2023-12-06",
        "summary": "Large vision-language models (VLMs) have garnered increasing interest in\nautonomous driving areas, due to their advanced capabilities in complex\nreasoning tasks essential for highly autonomous vehicle behavior. Despite their\npotential, research in autonomous systems is hindered by the lack of datasets\nwith annotated reasoning chains that explain the decision-making processes in\ndriving. To bridge this gap, we present Reason2Drive, a benchmark dataset with\nover 600K video-text pairs, aimed at facilitating the study of interpretable\nreasoning in complex driving environments. We distinctly characterize the\nautonomous driving process as a sequential combination of perception,\nprediction, and reasoning steps, and the question-answer pairs are\nautomatically collected from a diverse range of open-source outdoor driving\ndatasets, including nuScenes, Waymo and ONCE. Moreover, we introduce a novel\naggregated evaluation metric to assess chain-based reasoning performance in\nautonomous systems, addressing the semantic ambiguities of existing metrics\nsuch as BLEU and CIDEr. Based on the proposed benchmark, we conduct experiments\nto assess various existing VLMs, revealing insights into their reasoning\ncapabilities. Additionally, we develop an efficient approach to empower VLMs to\nleverage object-level perceptual elements in both feature extraction and\nprediction, further enhancing their reasoning accuracy. The code and dataset\nwill be released.",
        "translated": ""
    },
    {
        "title": "Editable Stain Transformation Of Histological Images Using Unpaired GANs",
        "url": "http://arxiv.org/abs/2312.03647v1",
        "pub_date": "2023-12-06",
        "summary": "Double staining in histopathology, particularly for metaplastic breast\ncancer, typically employs H&amp;E and P63 dyes. However, P63's tissue damage and\nhigh cost necessitate alternative methods. This study introduces xAI-CycleGAN,\nan advanced architecture combining Mask CycleGAN with explainability features\nand structure-preserving capabilities for transforming H&amp;E stained breast\ntissue images into P63-like images. The architecture allows for output editing,\nenhancing resemblance to actual images and enabling further model refinement.\nWe showcase xAI-CycleGAN's efficacy in maintaining structural integrity and\ngenerating high-quality images. Additionally, a histopathologist survey\nindicates the generated images' realism is often comparable to actual images,\nvalidating our model's high-quality output.",
        "translated": ""
    },
    {
        "title": "Scaling Laws of Synthetic Images for Model Training ... for Now",
        "url": "http://arxiv.org/abs/2312.04567v1",
        "pub_date": "2023-12-07",
        "summary": "Recent significant advances in text-to-image models unlock the possibility of\ntraining vision systems using synthetic images, potentially overcoming the\ndifficulty of collecting curated data at scale. It is unclear, however, how\nthese models behave at scale, as more synthetic data is added to the training\nset. In this paper we study the scaling laws of synthetic images generated by\nstate of the art text-to-image models, for the training of supervised models:\nimage classifiers with label supervision, and CLIP with language supervision.\nWe identify several factors, including text prompts, classifier-free guidance\nscale, and types of text-to-image models, that significantly affect scaling\nbehavior. After tuning these factors, we observe that synthetic images\ndemonstrate a scaling trend similar to, but slightly less effective than, real\nimages in CLIP training, while they significantly underperform in scaling when\ntraining supervised image classifiers. Our analysis indicates that the main\nreason for this underperformance is the inability of off-the-shelf\ntext-to-image models to generate certain concepts, a limitation that\nsignificantly impairs the training of image classifiers. Our findings also\nsuggest that scaling synthetic data can be particularly effective in scenarios\nsuch as: (1) when there is a limited supply of real images for a supervised\nproblem (e.g., fewer than 0.5 million images in ImageNet), (2) when the\nevaluation dataset diverges significantly from the training data, indicating\nthe out-of-distribution scenario, or (3) when synthetic data is used in\nconjunction with real images, as demonstrated in the training of CLIP models.",
        "translated": ""
    },
    {
        "title": "Gen2Det: Generate to Detect",
        "url": "http://arxiv.org/abs/2312.04566v1",
        "pub_date": "2023-12-07",
        "summary": "Recently diffusion models have shown improvement in synthetic image quality\nas well as better control in generation. We motivate and present Gen2Det, a\nsimple modular pipeline to create synthetic training data for object detection\nfor free by leveraging state-of-the-art grounded image generation methods.\nUnlike existing works which generate individual object instances, require\nidentifying foreground followed by pasting on other images, we simplify to\ndirectly generating scene-centric images. In addition to the synthetic data,\nGen2Det also proposes a suite of techniques to best utilize the generated data,\nincluding image-level filtering, instance-level filtering, and better training\nrecipe to account for imperfections in the generation. Using Gen2Det, we show\nhealthy improvements on object detection and segmentation tasks under various\nsettings and agnostic to detection methods. In the long-tailed detection\nsetting on LVIS, Gen2Det improves the performance on rare categories by a large\nmargin while also significantly improving the performance on other categories,\ne.g. we see an improvement of 2.13 Box AP and 1.84 Mask AP over just training\non real data on LVIS with Mask R-CNN. In the low-data regime setting on COCO,\nGen2Det consistently improves both Box and Mask AP by 2.27 and 1.85 points. In\nthe most general detection setting, Gen2Det still demonstrates robust\nperformance gains, e.g. it improves the Box and Mask AP on COCO by 0.45 and\n0.32 points.",
        "translated": ""
    },
    {
        "title": "MuRF: Multi-Baseline Radiance Fields",
        "url": "http://arxiv.org/abs/2312.04565v1",
        "pub_date": "2023-12-07",
        "summary": "We present Multi-Baseline Radiance Fields (MuRF), a general feed-forward\napproach to solving sparse view synthesis under multiple different baseline\nsettings (small and large baselines, and different number of input views). To\nrender a target novel view, we discretize the 3D space into planes parallel to\nthe target image plane, and accordingly construct a target view frustum volume.\nSuch a target volume representation is spatially aligned with the target view,\nwhich effectively aggregates relevant information from the input views for\nhigh-quality rendering. It also facilitates subsequent radiance field\nregression with a convolutional network thanks to its axis-aligned nature. The\n3D context modeled by the convolutional network enables our method to synthesis\nsharper scene structures than prior works. Our MuRF achieves state-of-the-art\nperformance across multiple different baseline settings and diverse scenarios\nranging from simple objects (DTU) to complex indoor and outdoor scenes\n(RealEstate10K and LLFF). We also show promising zero-shot generalization\nabilities on the Mip-NeRF 360 dataset, demonstrating the general applicability\nof MuRF.",
        "translated": ""
    },
    {
        "title": "EAGLES: Efficient Accelerated 3D Gaussians with Lightweight EncodingS",
        "url": "http://arxiv.org/abs/2312.04564v1",
        "pub_date": "2023-12-07",
        "summary": "Recently, 3D Gaussian splatting (3D-GS) has gained popularity in novel-view\nscene synthesis. It addresses the challenges of lengthy training times and slow\nrendering speeds associated with Neural Radiance Fields (NeRFs). Through rapid,\ndifferentiable rasterization of 3D Gaussians, 3D-GS achieves real-time\nrendering and accelerated training. They, however, demand substantial memory\nresources for both training and storage, as they require millions of Gaussians\nin their point cloud representation for each scene. We present a technique\nutilizing quantized embeddings to significantly reduce memory storage\nrequirements and a coarse-to-fine training strategy for a faster and more\nstable optimization of the Gaussian point clouds. Our approach results in scene\nrepresentations with fewer Gaussians and quantized representations, leading to\nfaster training times and rendering speeds for real-time rendering of high\nresolution scenes. We reduce memory by more than an order of magnitude all\nwhile maintaining the reconstruction quality. We validate the effectiveness of\nour approach on a variety of datasets and scenes preserving the visual quality\nwhile consuming 10-20x less memory and faster training/inference speed. Project\npage and code is available https://efficientgaussian.github.io",
        "translated": ""
    },
    {
        "title": "Visual Geometry Grounded Deep Structure From Motion",
        "url": "http://arxiv.org/abs/2312.04563v1",
        "pub_date": "2023-12-07",
        "summary": "Structure-from-motion (SfM) is a long-standing problem in the computer vision\ncommunity, which aims to reconstruct the camera poses and 3D structure of a\nscene from a set of unconstrained 2D images. Classical frameworks solve this\nproblem in an incremental manner by detecting and matching keypoints,\nregistering images, triangulating 3D points, and conducting bundle adjustment.\nRecent research efforts have predominantly revolved around harnessing the power\nof deep learning techniques to enhance specific elements (e.g., keypoint\nmatching), but are still based on the original, non-differentiable pipeline.\nInstead, we propose a new deep pipeline VGGSfM, where each component is fully\ndifferentiable and thus can be trained in an end-to-end manner. To this end, we\nintroduce new mechanisms and simplifications. First, we build on recent\nadvances in deep 2D point tracking to extract reliable pixel-accurate tracks,\nwhich eliminates the need for chaining pairwise matches. Furthermore, we\nrecover all cameras simultaneously based on the image and track features\ninstead of gradually registering cameras. Finally, we optimise the cameras and\ntriangulate 3D points via a differentiable bundle adjustment layer. We attain\nstate-of-the-art performance on three popular datasets, CO3D, IMC Phototourism,\nand ETH3D.",
        "translated": ""
    },
    {
        "title": "NeRFiller: Completing Scenes via Generative 3D Inpainting",
        "url": "http://arxiv.org/abs/2312.04560v1",
        "pub_date": "2023-12-07",
        "summary": "We propose NeRFiller, an approach that completes missing portions of a 3D\ncapture via generative 3D inpainting using off-the-shelf 2D visual generative\nmodels. Often parts of a captured 3D scene or object are missing due to mesh\nreconstruction failures or a lack of observations (e.g., contact regions, such\nas the bottom of objects, or hard-to-reach areas). We approach this challenging\n3D inpainting problem by leveraging a 2D inpainting diffusion model. We\nidentify a surprising behavior of these models, where they generate more 3D\nconsistent inpaints when images form a 2$\\times$2 grid, and show how to\ngeneralize this behavior to more than four images. We then present an iterative\nframework to distill these inpainted regions into a single consistent 3D scene.\nIn contrast to related works, we focus on completing scenes rather than\ndeleting foreground objects, and our approach does not require tight 2D object\nmasks or text. We compare our approach to relevant baselines adapted to our\nsetting on a variety of scenes, where NeRFiller creates the most 3D consistent\nand plausible scene completions. Our project page is at\nhttps://ethanweber.me/nerfiller.",
        "translated": ""
    },
    {
        "title": "GenDeF: Learning Generative Deformation Field for Video Generation",
        "url": "http://arxiv.org/abs/2312.04561v1",
        "pub_date": "2023-12-07",
        "summary": "We offer a new perspective on approaching the task of video generation.\nInstead of directly synthesizing a sequence of frames, we propose to render a\nvideo by warping one static image with a generative deformation field (GenDeF).\nSuch a pipeline enjoys three appealing advantages. First, we can sufficiently\nreuse a well-trained image generator to synthesize the static image (also\ncalled canonical image), alleviating the difficulty in producing a video and\nthereby resulting in better visual quality. Second, we can easily convert a\ndeformation field to optical flows, making it possible to apply explicit\nstructural regularizations for motion modeling, leading to temporally\nconsistent results. Third, the disentanglement between content and motion\nallows users to process a synthesized video through processing its\ncorresponding static image without any tuning, facilitating many applications\nlike video editing, keypoint tracking, and video segmentation. Both qualitative\nand quantitative results on three common video generation benchmarks\ndemonstrate the superiority of our GenDeF method.",
        "translated": ""
    },
    {
        "title": "PrimDiffusion: Volumetric Primitives Diffusion for 3D Human Generation",
        "url": "http://arxiv.org/abs/2312.04559v1",
        "pub_date": "2023-12-07",
        "summary": "We present PrimDiffusion, the first diffusion-based framework for 3D human\ngeneration. Devising diffusion models for 3D human generation is difficult due\nto the intensive computational cost of 3D representations and the articulated\ntopology of 3D humans. To tackle these challenges, our key insight is operating\nthe denoising diffusion process directly on a set of volumetric primitives,\nwhich models the human body as a number of small volumes with radiance and\nkinematic information. This volumetric primitives representation marries the\ncapacity of volumetric representations with the efficiency of primitive-based\nrendering. Our PrimDiffusion framework has three appealing properties: 1)\ncompact and expressive parameter space for the diffusion model, 2) flexible 3D\nrepresentation that incorporates human prior, and 3) decoder-free rendering for\nefficient novel-view and novel-pose synthesis. Extensive experiments validate\nthat PrimDiffusion outperforms state-of-the-art methods in 3D human generation.\nNotably, compared to GAN-based methods, our PrimDiffusion supports real-time\nrendering of high-quality 3D humans at a resolution of $512\\times512$ once the\ndenoising process is done. We also demonstrate the flexibility of our framework\non training-free conditional generation such as texture transfer and 3D\ninpainting.",
        "translated": ""
    },
    {
        "title": "MonoGaussianAvatar: Monocular Gaussian Point-based Head Avatar",
        "url": "http://arxiv.org/abs/2312.04558v1",
        "pub_date": "2023-12-07",
        "summary": "The ability to animate photo-realistic head avatars reconstructed from\nmonocular portrait video sequences represents a crucial step in bridging the\ngap between the virtual and real worlds. Recent advancements in head avatar\ntechniques, including explicit 3D morphable meshes (3DMM), point clouds, and\nneural implicit representation have been exploited for this ongoing research.\nHowever, 3DMM-based methods are constrained by their fixed topologies,\npoint-based approaches suffer from a heavy training burden due to the extensive\nquantity of points involved, and the last ones suffer from limitations in\ndeformation flexibility and rendering efficiency. In response to these\nchallenges, we propose MonoGaussianAvatar (Monocular Gaussian Point-based Head\nAvatar), a novel approach that harnesses 3D Gaussian point representation\ncoupled with a Gaussian deformation field to learn explicit head avatars from\nmonocular portrait videos. We define our head avatars with Gaussian points\ncharacterized by adaptable shapes, enabling flexible topology. These points\nexhibit movement with a Gaussian deformation field in alignment with the target\npose and expression of a person, facilitating efficient deformation.\nAdditionally, the Gaussian points have controllable shape, size, color, and\nopacity combined with Gaussian splatting, allowing for efficient training and\nrendering. Experiments demonstrate the superior performance of our method,\nwhich achieves state-of-the-art results among previous methods.",
        "translated": ""
    },
    {
        "title": "GenTron: Delving Deep into Diffusion Transformers for Image and Video\n  Generation",
        "url": "http://arxiv.org/abs/2312.04557v1",
        "pub_date": "2023-12-07",
        "summary": "In this study, we explore Transformer-based diffusion models for image and\nvideo generation. Despite the dominance of Transformer architectures in various\nfields due to their flexibility and scalability, the visual generative domain\nprimarily utilizes CNN-based U-Net architectures, particularly in\ndiffusion-based models. We introduce GenTron, a family of Generative models\nemploying Transformer-based diffusion, to address this gap. Our initial step\nwas to adapt Diffusion Transformers (DiTs) from class to text conditioning, a\nprocess involving thorough empirical exploration of the conditioning mechanism.\nWe then scale GenTron from approximately 900M to over 3B parameters, observing\nsignificant improvements in visual quality. Furthermore, we extend GenTron to\ntext-to-video generation, incorporating novel motion-free guidance to enhance\nvideo quality. In human evaluations against SDXL, GenTron achieves a 51.1% win\nrate in visual quality (with a 19.8% draw rate), and a 42.3% win rate in text\nalignment (with a 42.9% draw rate). GenTron also excels in the T2I-CompBench,\nunderscoring its strengths in compositional generation. We believe this work\nwill provide meaningful insights and serve as a valuable reference for future\nresearch.",
        "translated": ""
    },
    {
        "title": "Reconstructing Hands in 3D with Transformers",
        "url": "http://arxiv.org/abs/2312.05251v1",
        "pub_date": "2023-12-08",
        "summary": "We present an approach that can reconstruct hands in 3D from monocular input.\nOur approach for Hand Mesh Recovery, HaMeR, follows a fully transformer-based\narchitecture and can analyze hands with significantly increased accuracy and\nrobustness compared to previous work. The key to HaMeR's success lies in\nscaling up both the data used for training and the capacity of the deep network\nfor hand reconstruction. For training data, we combine multiple datasets that\ncontain 2D or 3D hand annotations. For the deep model, we use a large scale\nVision Transformer architecture. Our final model consistently outperforms the\nprevious baselines on popular 3D hand pose benchmarks. To further evaluate the\neffect of our design in non-controlled settings, we annotate existing\nin-the-wild datasets with 2D hand keypoint annotations. On this newly collected\ndataset of annotations, HInt, we demonstrate significant improvements over\nexisting baselines. We make our code, data and models available on the project\nwebsite: https://geopavlakos.github.io/hamer/.",
        "translated": ""
    },
    {
        "title": "CAD: Photorealistic 3D Generation via Adversarial Distillation",
        "url": "http://arxiv.org/abs/2312.06663v1",
        "pub_date": "2023-12-11",
        "summary": "The increased demand for 3D data in AR/VR, robotics and gaming applications,\ngave rise to powerful generative pipelines capable of synthesizing high-quality\n3D objects. Most of these models rely on the Score Distillation Sampling (SDS)\nalgorithm to optimize a 3D representation such that the rendered image\nmaintains a high likelihood as evaluated by a pre-trained diffusion model.\nHowever, finding a correct mode in the high-dimensional distribution produced\nby the diffusion model is challenging and often leads to issues such as\nover-saturation, over-smoothing, and Janus-like artifacts. In this paper, we\npropose a novel learning paradigm for 3D synthesis that utilizes pre-trained\ndiffusion models. Instead of focusing on mode-seeking, our method directly\nmodels the distribution discrepancy between multi-view renderings and diffusion\npriors in an adversarial manner, which unlocks the generation of high-fidelity\nand photorealistic 3D content, conditioned on a single image and prompt.\nMoreover, by harnessing the latent space of GANs and expressive diffusion model\npriors, our method facilitates a wide variety of 3D applications including\nsingle-view reconstruction, high diversity generation and continuous 3D\ninterpolation in the open domain. The experiments demonstrate the superiority\nof our pipeline compared to previous works in terms of generation quality and\ndiversity.",
        "translated": ""
    },
    {
        "title": "Photorealistic Video Generation with Diffusion Models",
        "url": "http://arxiv.org/abs/2312.06662v1",
        "pub_date": "2023-12-11",
        "summary": "We present W.A.L.T, a transformer-based approach for photorealistic video\ngeneration via diffusion modeling. Our approach has two key design decisions.\nFirst, we use a causal encoder to jointly compress images and videos within a\nunified latent space, enabling training and generation across modalities.\nSecond, for memory and training efficiency, we use a window attention\narchitecture tailored for joint spatial and spatiotemporal generative modeling.\nTaken together these design decisions enable us to achieve state-of-the-art\nperformance on established video (UCF-101 and Kinetics-600) and image\n(ImageNet) generation benchmarks without using classifier free guidance.\nFinally, we also train a cascade of three models for the task of text-to-video\ngeneration consisting of a base latent video diffusion model, and two video\nsuper-resolution diffusion models to generate videos of $512 \\times 896$\nresolution at $8$ frames per second.",
        "translated": ""
    },
    {
        "title": "UpFusion: Novel View Diffusion from Unposed Sparse View Observations",
        "url": "http://arxiv.org/abs/2312.06661v1",
        "pub_date": "2023-12-11",
        "summary": "We propose UpFusion, a system that can perform novel view synthesis and infer\n3D representations for an object given a sparse set of reference images without\ncorresponding pose information. Current sparse-view 3D inference methods\ntypically rely on camera poses to geometrically aggregate information from\ninput views, but are not robust in-the-wild when such information is\nunavailable/inaccurate. In contrast, UpFusion sidesteps this requirement by\nlearning to implicitly leverage the available images as context in a\nconditional generative model for synthesizing novel views. We incorporate two\ncomplementary forms of conditioning into diffusion models for leveraging the\ninput views: a) via inferring query-view aligned features using a scene-level\ntransformer, b) via intermediate attentional layers that can directly observe\nthe input image tokens. We show that this mechanism allows generating\nhigh-fidelity novel views while improving the synthesis quality given\nadditional (unposed) images. We evaluate our approach on the Co3Dv2 and Google\nScanned Objects datasets and demonstrate the benefits of our method over\npose-reliant sparse-view methods as well as single-view methods that cannot\nleverage additional views. Finally, we also show that our learned model can\ngeneralize beyond the training categories and even allow reconstruction from\nself-captured images of generic objects in-the-wild.",
        "translated": ""
    },
    {
        "title": "EdgeSAM: Prompt-In-the-Loop Distillation for On-Device Deployment of SAM",
        "url": "http://arxiv.org/abs/2312.06660v1",
        "pub_date": "2023-12-11",
        "summary": "This paper presents EdgeSAM, an accelerated variant of the Segment Anything\nModel (SAM), optimized for efficient execution on edge devices with minimal\ncompromise in performance. Our approach involves distilling the original\nViT-based SAM image encoder into a purely CNN-based architecture, better suited\nfor edge devices. We carefully benchmark various distillation strategies and\ndemonstrate that task-agnostic encoder distillation fails to capture the full\nknowledge embodied in SAM. To overcome this bottleneck, we include both the\nprompt encoder and mask decoder in the distillation process, with box and point\nprompts in the loop, so that the distilled model can accurately capture the\nintricate dynamics between user input and mask generation. To mitigate dataset\nbias issues stemming from point prompt distillation, we incorporate a\nlightweight module within the encoder. EdgeSAM achieves a 40-fold speed\nincrease compared to the original SAM, and it also outperforms MobileSAM, being\n14 times as fast when deployed on edge devices while enhancing the mIoUs on\nCOCO and LVIS by 2.3 and 3.2 respectively. It is also the first SAM variant\nthat can run at over 30 FPS on an iPhone 14. Code and models are available at\nhttps://github.com/chongzhou96/EdgeSAM.",
        "translated": ""
    },
    {
        "title": "Learning Naturally Aggregated Appearance for Efficient 3D Editing",
        "url": "http://arxiv.org/abs/2312.06657v1",
        "pub_date": "2023-12-11",
        "summary": "Neural radiance fields, which represent a 3D scene as a color field and a\ndensity field, have demonstrated great progress in novel view synthesis yet are\nunfavorable for editing due to the implicitness. In view of such a deficiency,\nwe propose to replace the color field with an explicit 2D appearance\naggregation, also called canonical image, with which users can easily customize\ntheir 3D editing via 2D image processing. To avoid the distortion effect and\nfacilitate convenient editing, we complement the canonical image with a\nprojection field that maps 3D points onto 2D pixels for texture lookup. This\nfield is carefully initialized with a pseudo canonical camera model and\noptimized with offset regularity to ensure naturalness of the aggregated\nappearance. Extensive experimental results on three datasets suggest that our\nrepresentation, dubbed AGAP, well supports various ways of 3D editing (e.g.,\nstylization, interactive drawing, and content extraction) with no need of\nre-optimization for each case, demonstrating its generalizability and\nefficiency. Project page is available at https://felixcheng97.github.io/AGAP/.",
        "translated": ""
    },
    {
        "title": "Sherpa3D: Boosting High-Fidelity Text-to-3D Generation via Coarse 3D\n  Prior",
        "url": "http://arxiv.org/abs/2312.06655v1",
        "pub_date": "2023-12-11",
        "summary": "Recently, 3D content creation from text prompts has demonstrated remarkable\nprogress by utilizing 2D and 3D diffusion models. While 3D diffusion models\nensure great multi-view consistency, their ability to generate high-quality and\ndiverse 3D assets is hindered by the limited 3D data. In contrast, 2D diffusion\nmodels find a distillation approach that achieves excellent generalization and\nrich details without any 3D data. However, 2D lifting methods suffer from\ninherent view-agnostic ambiguity thereby leading to serious multi-face Janus\nissues, where text prompts fail to provide sufficient guidance to learn\ncoherent 3D results. Instead of retraining a costly viewpoint-aware model, we\nstudy how to fully exploit easily accessible coarse 3D knowledge to enhance the\nprompts and guide 2D lifting optimization for refinement. In this paper, we\npropose Sherpa3D, a new text-to-3D framework that achieves high-fidelity,\ngeneralizability, and geometric consistency simultaneously. Specifically, we\ndesign a pair of guiding strategies derived from the coarse 3D prior generated\nby the 3D diffusion model: a structural guidance for geometric fidelity and a\nsemantic guidance for 3D coherence. Employing the two types of guidance, the 2D\ndiffusion model enriches the 3D content with diversified and high-quality\nresults. Extensive experiments show the superiority of our Sherpa3D over the\nstate-of-the-art text-to-3D methods in terms of quality and 3D consistency.",
        "translated": ""
    },
    {
        "title": "LightSim: Neural Lighting Simulation for Urban Scenes",
        "url": "http://arxiv.org/abs/2312.06654v1",
        "pub_date": "2023-12-11",
        "summary": "Different outdoor illumination conditions drastically alter the appearance of\nurban scenes, and they can harm the performance of image-based robot perception\nsystems if not seen during training. Camera simulation provides a\ncost-effective solution to create a large dataset of images captured under\ndifferent lighting conditions. Towards this goal, we propose LightSim, a neural\nlighting camera simulation system that enables diverse, realistic, and\ncontrollable data generation. LightSim automatically builds lighting-aware\ndigital twins at scale from collected raw sensor data and decomposes the scene\ninto dynamic actors and static background with accurate geometry, appearance,\nand estimated scene lighting. These digital twins enable actor insertion,\nmodification, removal, and rendering from a new viewpoint, all in a\nlighting-aware manner. LightSim then combines physically-based and learnable\ndeferred rendering to perform realistic relighting of modified scenes, such as\naltering the sun location and modifying the shadows or changing the sun\nbrightness, producing spatially- and temporally-consistent camera videos. Our\nexperiments show that LightSim generates more realistic relighting results than\nprior work. Importantly, training perception models on data generated by\nLightSim can significantly improve their performance.",
        "translated": ""
    },
    {
        "title": "Adaptive Human Trajectory Prediction via Latent Corridors",
        "url": "http://arxiv.org/abs/2312.06653v1",
        "pub_date": "2023-12-11",
        "summary": "Human trajectory prediction is typically posed as a zero-shot generalization\nproblem: a predictor is learnt on a dataset of human motion in training scenes,\nand then deployed on unseen test scenes. While this paradigm has yielded\ntremendous progress, it fundamentally assumes that trends in human behavior\nwithin the deployment scene are constant over time. As such, current prediction\nmodels are unable to adapt to scene-specific transient human behaviors, such as\ncrowds temporarily gathering to see buskers, pedestrians hurrying through the\nrain and avoiding puddles, or a protest breaking out. We formalize the problem\nof scene-specific adaptive trajectory prediction and propose a new adaptation\napproach inspired by prompt tuning called latent corridors. By augmenting the\ninput of any pre-trained human trajectory predictor with learnable image\nprompts, the predictor can improve in the deployment scene by inferring trends\nfrom extremely small amounts of new data (e.g., 2 humans observed for 30\nseconds). With less than 0.1% additional model parameters, we see up to 23.9%\nADE improvement in MOTSynth simulated data and 16.4% ADE in MOT and Wildtrack\nreal pedestrian data. Qualitatively, we observe that latent corridors imbue\npredictors with an awareness of scene geometry and scene-specific human\nbehaviors that non-adaptive predictors struggle to capture. The project website\ncan be found at https://neerja.me/atp_latent_corridors/.",
        "translated": ""
    },
    {
        "title": "Nuvo: Neural UV Mapping for Unruly 3D Representations",
        "url": "http://arxiv.org/abs/2312.05283v1",
        "pub_date": "2023-12-11",
        "summary": "Existing UV mapping algorithms are designed to operate on well-behaved\nmeshes, instead of the geometry representations produced by state-of-the-art 3D\nreconstruction and generation techniques. As such, applying these methods to\nthe volume densities recovered by neural radiance fields and related techniques\n(or meshes triangulated from such fields) results in texture atlases that are\ntoo fragmented to be useful for tasks such as view synthesis or appearance\nediting. We present a UV mapping method designed to operate on geometry\nproduced by 3D reconstruction and generation techniques. Instead of computing a\nmapping defined on a mesh's vertices, our method Nuvo uses a neural field to\nrepresent a continuous UV mapping, and optimizes it to be a valid and\nwell-behaved mapping for just the set of visible points, i.e. only points that\naffect the scene's appearance. We show that our model is robust to the\nchallenges posed by ill-behaved geometry, and that it produces editable UV\nmappings that can represent detailed appearance.",
        "translated": ""
    },
    {
        "title": "4M: Massively Multimodal Masked Modeling",
        "url": "http://arxiv.org/abs/2312.06647v1",
        "pub_date": "2023-12-11",
        "summary": "Current machine learning models for vision are often highly specialized and\nlimited to a single modality and task. In contrast, recent large language\nmodels exhibit a wide range of capabilities, hinting at a possibility for\nsimilarly versatile models in computer vision. In this paper, we take a step in\nthis direction and propose a multimodal training scheme called 4M. It consists\nof training a single unified Transformer encoder-decoder using a masked\nmodeling objective across a wide range of input/output modalities - including\ntext, images, geometric, and semantic modalities, as well as neural network\nfeature maps. 4M achieves scalability by unifying the representation space of\nall modalities through mapping them into discrete tokens and performing\nmultimodal masked modeling on a small randomized subset of tokens.\n  4M leads to models that exhibit several key capabilities: (1) they can\nperform a diverse set of vision tasks out of the box, (2) they excel when\nfine-tuned for unseen downstream tasks or new input modalities, and (3) they\ncan function as a generative model that can be conditioned on arbitrary\nmodalities, enabling a wide variety of expressive multimodal editing\ncapabilities with remarkable flexibility.\n  Through experimental analyses, we demonstrate the potential of 4M for\ntraining versatile and scalable foundation models for vision tasks, setting the\nstage for further exploration in multimodal learning for vision and other\ndomains.",
        "translated": ""
    },
    {
        "title": "SMERF: Streamable Memory Efficient Radiance Fields for Real-Time\n  Large-Scene Exploration",
        "url": "http://arxiv.org/abs/2312.07541v1",
        "pub_date": "2023-12-12",
        "summary": "Recent techniques for real-time view synthesis have rapidly advanced in\nfidelity and speed, and modern methods are capable of rendering\nnear-photorealistic scenes at interactive frame rates. At the same time, a\ntension has arisen between explicit scene representations amenable to\nrasterization and neural fields built on ray marching, with state-of-the-art\ninstances of the latter surpassing the former in quality while being\nprohibitively expensive for real-time applications. In this work, we introduce\nSMERF, a view synthesis approach that achieves state-of-the-art accuracy among\nreal-time methods on large scenes with footprints up to 300 m$^2$ at a\nvolumetric resolution of 3.5 mm$^3$. Our method is built upon two primary\ncontributions: a hierarchical model partitioning scheme, which increases model\ncapacity while constraining compute and memory consumption, and a distillation\ntraining strategy that simultaneously yields high fidelity and internal\nconsistency. Our approach enables full six degrees of freedom (6DOF) navigation\nwithin a web browser and renders in real-time on commodity smartphones and\nlaptops. Extensive experiments show that our method exceeds the current\nstate-of-the-art in real-time novel view synthesis by 0.78 dB on standard\nbenchmarks and 1.78 dB on large scenes, renders frames three orders of\nmagnitude faster than state-of-the-art radiance field models, and achieves\nreal-time performance across a wide variety of commodity devices, including\nsmartphones. We encourage the reader to explore these models in person at our\nproject website: https://smerf-3d.github.io.",
        "translated": ""
    },
    {
        "title": "HeadArtist: Text-conditioned 3D Head Generation with Self Score\n  Distillation",
        "url": "http://arxiv.org/abs/2312.07539v1",
        "pub_date": "2023-12-12",
        "summary": "This work presents HeadArtist for 3D head generation from text descriptions.\nWith a landmark-guided ControlNet serving as the generative prior, we come up\nwith an efficient pipeline that optimizes a parameterized 3D head model under\nthe supervision of the prior distillation itself. We call such a process self\nscore distillation (SSD). In detail, given a sampled camera pose, we first\nrender an image and its corresponding landmarks from the head model, and add\nsome particular level of noise onto the image. The noisy image, landmarks, and\ntext condition are then fed into the frozen ControlNet twice for noise\nprediction. Two different classifier-free guidance (CFG) weights are applied\nduring these two predictions, and the prediction difference offers a direction\non how the rendered image can better match the text of interest. Experimental\nresults suggest that our approach delivers high-quality 3D head sculptures with\nadequate geometry and photorealistic appearance, significantly outperforming\nstate-ofthe-art methods. We also show that the same pipeline well supports\nediting the generated heads, including both geometry deformation and appearance\nchange.",
        "translated": ""
    },
    {
        "title": "Anatomically Constrained Implicit Face Models",
        "url": "http://arxiv.org/abs/2312.07538v1",
        "pub_date": "2023-12-12",
        "summary": "Coordinate based implicit neural representations have gained rapid popularity\nin recent years as they have been successfully used in image, geometry and\nscene modeling tasks. In this work, we present a novel use case for such\nimplicit representations in the context of learning anatomically constrained\nface models. Actor specific anatomically constrained face models are the state\nof the art in both facial performance capture and performance retargeting.\nDespite their practical success, these anatomical models are slow to evaluate\nand often require extensive data capture to be built. We propose the anatomical\nimplicit face model; an ensemble of implicit neural networks that jointly learn\nto model the facial anatomy and the skin surface with high-fidelity, and can\nreadily be used as a drop in replacement to conventional blendshape models.\nGiven an arbitrary set of skin surface meshes of an actor and only a neutral\nshape with estimated skull and jaw bones, our method can recover a dense\nanatomical substructure which constrains every point on the facial surface. We\ndemonstrate the usefulness of our approach in several tasks ranging from shape\nfitting, shape editing, and performance retargeting.",
        "translated": ""
    },
    {
        "title": "FreeInit: Bridging Initialization Gap in Video Diffusion Models",
        "url": "http://arxiv.org/abs/2312.07537v1",
        "pub_date": "2023-12-12",
        "summary": "Though diffusion-based video generation has witnessed rapid progress, the\ninference results of existing models still exhibit unsatisfactory temporal\nconsistency and unnatural dynamics. In this paper, we delve deep into the noise\ninitialization of video diffusion models, and discover an implicit\ntraining-inference gap that attributes to the unsatisfactory inference quality.\nOur key findings are: 1) the spatial-temporal frequency distribution of the\ninitial latent at inference is intrinsically different from that for training,\nand 2) the denoising process is significantly influenced by the low-frequency\ncomponents of the initial noise. Motivated by these observations, we propose a\nconcise yet effective inference sampling strategy, FreeInit, which\nsignificantly improves temporal consistency of videos generated by diffusion\nmodels. Through iteratively refining the spatial-temporal low-frequency\ncomponents of the initial latent during inference, FreeInit is able to\ncompensate the initialization gap between training and inference, thus\neffectively improving the subject appearance and temporal consistency of\ngeneration results. Extensive experiments demonstrate that FreeInit\nconsistently enhances the generation results of various text-to-video\ngeneration models without additional training.",
        "translated": ""
    },
    {
        "title": "FreeControl: Training-Free Spatial Control of Any Text-to-Image\n  Diffusion Model with Any Condition",
        "url": "http://arxiv.org/abs/2312.07536v1",
        "pub_date": "2023-12-12",
        "summary": "Recent approaches such as ControlNet offer users fine-grained spatial control\nover text-to-image (T2I) diffusion models. However, auxiliary modules have to\nbe trained for each type of spatial condition, model architecture, and\ncheckpoint, putting them at odds with the diverse intents and preferences a\nhuman designer would like to convey to the AI models during the content\ncreation process. In this work, we present FreeControl, a training-free\napproach for controllable T2I generation that supports multiple conditions,\narchitectures, and checkpoints simultaneously. FreeControl designs structure\nguidance to facilitate the structure alignment with a guidance image, and\nappearance guidance to enable the appearance sharing between images generated\nusing the same seed. Extensive qualitative and quantitative experiments\ndemonstrate the superior performance of FreeControl across a variety of\npre-trained T2I models. In particular, FreeControl facilitates convenient\ntraining-free control over many different architectures and checkpoints, allows\nthe challenging input conditions on which most of the existing training-free\nmethods fail, and achieves competitive synthesis quality with training-based\napproaches.",
        "translated": ""
    },
    {
        "title": "VILA: On Pre-training for Visual Language Models",
        "url": "http://arxiv.org/abs/2312.07533v1",
        "pub_date": "2023-12-12",
        "summary": "Visual language models (VLMs) rapidly progressed with the recent success of\nlarge language models. There have been growing efforts on visual instruction\ntuning to extend the LLM with visual inputs, but lacks an in-depth study of the\nvisual language pre-training process, where the model learns to perform joint\nmodeling on both modalities. In this work, we examine the design options for\nVLM pre-training by augmenting LLM towards VLM through step-by-step\ncontrollable comparisons. We introduce three main findings: (1) freezing LLMs\nduring pre-training can achieve decent zero-shot performance, but lack\nin-context learning capability, which requires unfreezing the LLM; (2)\ninterleaved pre-training data is beneficial whereas image-text pairs alone are\nnot optimal; (3) re-blending text-only instruction data to image-text data\nduring instruction fine-tuning not only remedies the degradation of text-only\ntasks, but also boosts VLM task accuracy. With an enhanced pre-training recipe\nwe build VILA, a Visual Language model family that consistently outperforms the\nstate-of-the-art models, e.g., LLaVA-1.5, across main benchmarks without bells\nand whistles. Multi-modal pre-training also helps unveil appealing properties\nof VILA, including multi-image reasoning, enhanced in-context learning, and\nbetter world knowledge.",
        "translated": ""
    },
    {
        "title": "Interfacing Foundation Models' Embeddings",
        "url": "http://arxiv.org/abs/2312.07532v1",
        "pub_date": "2023-12-12",
        "summary": "We present FIND, a generalized interface for aligning foundation models'\nembeddings. As shown in teaser figure, a lightweight transformer interface\nwithout tuning any foundation model weights is enough for a unified image\n(segmentation) and dataset-level (retrieval) understanding. The proposed\ninterface has the following favorable attributes: (1) Generalizable. It applies\nto various tasks spanning retrieval, segmentation, \\textit{etc.}, under the\nsame architecture and weights. (2) Prototypable. Different tasks are able to be\nimplemented through prototyping attention masks and embedding types. (3)\nExtendable. The proposed interface is adaptive to new tasks, and new models.\n(4) Interleavable. With the benefit of multi-task multi-modal training, the\nproposed interface creates an interleaved shared embedding space. In light of\nthe interleaved embedding space, we introduce the FIND-Bench, which introduces\nnew training and evaluation annotations to the COCO dataset for interleave\nsegmentation and retrieval. Our approach achieves state-of-the-art performance\non FIND-Bench and competitive performance on standard retrieval and\nsegmentation settings. The training, evaluation, and demo code as well as the\ndataset have been released at https://github.com/UX-Decoder/FIND.",
        "translated": ""
    },
    {
        "title": "WHAM: Reconstructing World-grounded Humans with Accurate 3D Motion",
        "url": "http://arxiv.org/abs/2312.07531v1",
        "pub_date": "2023-12-12",
        "summary": "The estimation of 3D human motion from video has progressed rapidly but\ncurrent methods still have several key limitations. First, most methods\nestimate the human in camera coordinates. Second, prior work on estimating\nhumans in global coordinates often assumes a flat ground plane and produces\nfoot sliding. Third, the most accurate methods rely on computationally\nexpensive optimization pipelines, limiting their use to offline applications.\nFinally, existing video-based methods are surprisingly less accurate than\nsingle-frame methods. We address these limitations with WHAM (World-grounded\nHumans with Accurate Motion), which accurately and efficiently reconstructs 3D\nhuman motion in a global coordinate system from video. WHAM learns to lift 2D\nkeypoint sequences to 3D using motion capture data and fuses this with video\nfeatures, integrating motion context and visual information. WHAM exploits\ncamera angular velocity estimated from a SLAM method together with human motion\nto estimate the body's global trajectory. We combine this with a contact-aware\ntrajectory refinement method that lets WHAM capture human motion in diverse\nconditions, such as climbing stairs. WHAM outperforms all existing 3D human\nmotion recovery methods across multiple in-the-wild benchmarks. Code will be\navailable for research purposes at http://wham.is.tue.mpg.de/",
        "translated": ""
    },
    {
        "title": "Weakly Supervised 3D Object Detection via Multi-Level Visual Guidance",
        "url": "http://arxiv.org/abs/2312.07530v1",
        "pub_date": "2023-12-12",
        "summary": "Weakly supervised 3D object detection aims to learn a 3D detector with lower\nannotation cost, e.g., 2D labels. Unlike prior work which still relies on few\naccurate 3D annotations, we propose a framework to study how to leverage\nconstraints between 2D and 3D domains without requiring any 3D labels.\nSpecifically, we employ visual data from three perspectives to establish\nconnections between 2D and 3D domains. First, we design a feature-level\nconstraint to align LiDAR and image features based on object-aware regions.\nSecond, the output-level constraint is developed to enforce the overlap between\n2D and projected 3D box estimations. Finally, the training-level constraint is\nutilized by producing accurate and consistent 3D pseudo-labels that align with\nthe visual data. We conduct extensive experiments on the KITTI dataset to\nvalidate the effectiveness of the proposed three constraints. Without using any\n3D labels, our method achieves favorable performance against state-of-the-art\napproaches and is competitive with the method that uses 500-frame 3D\nannotations. Code and models will be made publicly available at\nhttps://github.com/kuanchihhuang/VG-W3D.",
        "translated": ""
    },
    {
        "title": "RTMO: Towards High-Performance One-Stage Real-Time Multi-Person Pose\n  Estimation",
        "url": "http://arxiv.org/abs/2312.07526v1",
        "pub_date": "2023-12-12",
        "summary": "Real-time multi-person pose estimation presents significant challenges in\nbalancing speed and precision. While two-stage top-down methods slow down as\nthe number of people in the image increases, existing one-stage methods often\nfail to simultaneously deliver high accuracy and real-time performance. This\npaper introduces RTMO, a one-stage pose estimation framework that seamlessly\nintegrates coordinate classification by representing keypoints using dual 1-D\nheatmaps within the YOLO architecture, achieving accuracy comparable to\ntop-down methods while maintaining high speed. We propose a dynamic coordinate\nclassifier and a tailored loss function for heatmap learning, specifically\ndesigned to address the incompatibilities between coordinate classification and\ndense prediction models. RTMO outperforms state-of-the-art one-stage pose\nestimators, achieving 1.1% higher AP on COCO while operating about 9 times\nfaster with the same backbone. Our largest model, RTMO-l, attains 74.8% AP on\nCOCO val2017 and 141 FPS on a single V100 GPU, demonstrating its efficiency and\naccuracy. The code and models are available at\nhttps://github.com/open-mmlab/mmpose/tree/dev-1.x/projects/rtmo.",
        "translated": ""
    },
    {
        "title": "SAM-guided Graph Cut for 3D Instance Segmentation",
        "url": "http://arxiv.org/abs/2312.08372v1",
        "pub_date": "2023-12-13",
        "summary": "This paper addresses the challenge of 3D instance segmentation by\nsimultaneously leveraging 3D geometric and multi-view image information. Many\nprevious works have applied deep learning techniques to 3D point clouds for\ninstance segmentation. However, these methods often failed to generalize to\nvarious types of scenes due to the scarcity and low-diversity of labeled 3D\npoint cloud data. Some recent works have attempted to lift 2D instance\nsegmentations to 3D within a bottom-up framework. The inconsistency in 2D\ninstance segmentations among views can substantially degrade the performance of\n3D segmentation. In this work, we introduce a novel 3D-to-2D query framework to\neffectively exploit 2D segmentation models for 3D instance segmentation.\nSpecifically, we pre-segment the scene into several superpoints in 3D,\nformulating the task into a graph cut problem. The superpoint graph is\nconstructed based on 2D segmentation models, where node features are obtained\nfrom multi-view image features and edge weights are computed based on\nmulti-view segmentation results, enabling the better generalization ability. To\nprocess the graph, we train a graph neural network using pseudo 3D labels from\n2D segmentation models. Experimental results on the ScanNet, ScanNet++ and\nKITTI-360 datasets demonstrate that our method achieves robust segmentation\nperformance and can generalize across different types of scenes. Our project\npage is available at https://zju3dv.github.io/sam_graph.",
        "translated": ""
    },
    {
        "title": "PTT: Point-Trajectory Transformer for Efficient Temporal 3D Object\n  Detection",
        "url": "http://arxiv.org/abs/2312.08371v1",
        "pub_date": "2023-12-13",
        "summary": "Recent temporal LiDAR-based 3D object detectors achieve promising performance\nbased on the two-stage proposal-based approach. They generate 3D box candidates\nfrom the first-stage dense detector, followed by different temporal aggregation\nmethods. However, these approaches require per-frame objects or whole point\nclouds, posing challenges related to memory bank utilization. Moreover, point\nclouds and trajectory features are combined solely based on concatenation,\nwhich may neglect effective interactions between them. In this paper, we\npropose a point-trajectory transformer with long short-term memory for\nefficient temporal 3D object detection. To this end, we only utilize point\nclouds of current-frame objects and their historical trajectories as input to\nminimize the memory bank storage requirement. Furthermore, we introduce modules\nto encode trajectory features, focusing on long short-term and future-aware\nperspectives, and then effectively aggregate them with point cloud features. We\nconduct extensive experiments on the large-scale Waymo dataset to demonstrate\nthat our approach performs well against state-of-the-art methods. Code and\nmodels will be made publicly available at https://github.com/kuanchihhuang/PTT.",
        "translated": ""
    },
    {
        "title": "VLAP: Efficient Video-Language Alignment via Frame Prompting and\n  Distilling for Video Question Answering",
        "url": "http://arxiv.org/abs/2312.08367v1",
        "pub_date": "2023-12-13",
        "summary": "In this work, we propose an efficient Video-Language Alignment via\nFrame-Prompting and Distilling (VLAP) network. Our VLAP model addresses both\nefficient frame sampling and effective cross-modal alignment in a unified way.\nIn our VLAP network, we design a new learnable question-aware Frame-Prompter\ntogether with a new cross-modal distillation (QFormer-Distiller) module.\nPre-trained large image-language models have shown promising results on\nproblems such as visual question answering. However, how to efficiently and\neffectively sample image frames when adapting pre-trained large image-language\nmodel to video-language alignment is still the major challenge. Compared with\nprior work, our VLAP model demonstrates the capability of selecting key frames\nwith critical contents, thus improving the video-language alignment accuracy\nwhile reducing the inference latency (+3.3% on NExT-QA Temporal with 3.0X speed\nup). Overall, our VLAP network outperforms (e.g. +4.6% on STAR Interaction and\n+2.2% on STAR average with 3.0X speed up, ours 2-frames out-perform SeViLA\n4-frames on VLEP with 4.2X speed up) the state-of-the-art methods on the video\nquestion-answering benchmarks.",
        "translated": ""
    },
    {
        "title": "See, Say, and Segment: Teaching LMMs to Overcome False Premises",
        "url": "http://arxiv.org/abs/2312.08366v1",
        "pub_date": "2023-12-13",
        "summary": "Current open-source Large Multimodal Models (LMMs) excel at tasks such as\nopen-vocabulary language grounding and segmentation but can suffer under false\npremises when queries imply the existence of something that is not actually\npresent in the image. We observe that existing methods that fine-tune an LMM to\nsegment images significantly degrade their ability to reliably determine\n(\"see\") if an object is present and to interact naturally with humans (\"say\"),\na form of catastrophic forgetting. In this work, we propose a cascading and\njoint training approach for LMMs to solve this task, avoiding catastrophic\nforgetting of previous skills. Our resulting model can \"see\" by detecting\nwhether objects are present in an image, \"say\" by telling the user if they are\nnot, proposing alternative queries or correcting semantic errors in the query,\nand finally \"segment\" by outputting the mask of the desired objects if they\nexist. Additionally, we introduce a novel False Premise Correction benchmark\ndataset, an extension of existing RefCOCO(+/g) referring segmentation datasets\n(which we call FP-RefCOCO(+/g)). The results show that our method not only\ndetects false premises up to 55% better than existing approaches, but under\nfalse premise conditions produces relative cIOU improvements of more than 31%\nover baselines, and produces natural language feedback judged helpful up to 67%\nof the time.",
        "translated": ""
    },
    {
        "title": "View-Dependent Octree-based Mesh Extraction in Unbounded Scenes for\n  Procedural Synthetic Data",
        "url": "http://arxiv.org/abs/2312.08364v1",
        "pub_date": "2023-12-13",
        "summary": "Procedural synthetic data generation has received increasing attention in\ncomputer vision. Procedural signed distance functions (SDFs) are a powerful\ntool for modeling large-scale detailed scenes, but existing mesh extraction\nmethods have artifacts or performance profiles that limit their use for\nsynthetic data. We propose OcMesher, a mesh extraction algorithm that\nefficiently handles high-detail unbounded scenes with perfect view-consistency,\nwith easy export to downstream real-time engines. The main novelty of our\nsolution is an algorithm to construct an octree based on a given SDF and\nmultiple camera views. We performed extensive experiments, and show our\nsolution produces better synthetic data for training and evaluation of computer\nvision models.",
        "translated": ""
    },
    {
        "title": "FoundationPose: Unified 6D Pose Estimation and Tracking of Novel Objects",
        "url": "http://arxiv.org/abs/2312.08344v1",
        "pub_date": "2023-12-13",
        "summary": "We present FoundationPose, a unified foundation model for 6D object pose\nestimation and tracking, supporting both model-based and model-free setups. Our\napproach can be instantly applied at test-time to a novel object without\nfine-tuning, as long as its CAD model is given, or a small number of reference\nimages are captured. We bridge the gap between these two setups with a neural\nimplicit representation that allows for effective novel view synthesis, keeping\nthe downstream pose estimation modules invariant under the same unified\nframework. Strong generalizability is achieved via large-scale synthetic\ntraining, aided by a large language model (LLM), a novel transformer-based\narchitecture, and contrastive learning formulation. Extensive evaluation on\nmultiple public datasets involving challenging scenarios and objects indicate\nour unified approach outperforms existing methods specialized for each task by\na large margin. In addition, it even achieves comparable results to\ninstance-level methods despite the reduced assumptions. Project page:\nhttps://nvlabs.github.io/FoundationPose/",
        "translated": ""
    },
    {
        "title": "Ehancing CT Image synthesis from multi-modal MRI data based on a\n  multi-task neural network framework",
        "url": "http://arxiv.org/abs/2312.08343v1",
        "pub_date": "2023-12-13",
        "summary": "Image segmentation, real-value prediction, and cross-modal translation are\ncritical challenges in medical imaging. In this study, we propose a versatile\nmulti-task neural network framework, based on an enhanced Transformer U-Net\narchitecture, capable of simultaneously, selectively, and adaptively addressing\nthese medical image tasks. Validation is performed on a public repository of\nhuman brain MR and CT images. We decompose the traditional problem of\nsynthesizing CT images into distinct subtasks, which include skull\nsegmentation, Hounsfield unit (HU) value prediction, and image sequential\nreconstruction. To enhance the framework's versatility in handling multi-modal\ndata, we expand the model with multiple image channels. Comparisons between\nsynthesized CT images derived from T1-weighted and T2-Flair images were\nconducted, evaluating the model's capability to integrate multi-modal\ninformation from both morphological and pixel value perspectives.",
        "translated": ""
    },
    {
        "title": "Global Latent Neural Rendering",
        "url": "http://arxiv.org/abs/2312.08338v1",
        "pub_date": "2023-12-13",
        "summary": "A recent trend among generalizable novel view synthesis methods is to learn a\nrendering operator acting over single camera rays. This approach is promising\nbecause it removes the need for explicit volumetric rendering, but it\neffectively treats target images as collections of independent pixels. Here, we\npropose to learn a global rendering operator acting over all camera rays\njointly. We show that the right representation to enable such rendering is the\n5-dimensional plane sweep volume, consisting of the projection of the input\nimages on a set of planes facing the target camera. Based on this\nunderstanding, we introduce our Convolutional Global Latent Renderer (ConvGLR),\nan efficient convolutional architecture that performs the rendering operation\nglobally in a low-resolution latent space. Experiments on various datasets\nunder sparse and generalizable setups show that our approach consistently\noutperforms existing methods by significant margins.",
        "translated": ""
    },
    {
        "title": "LD-SDM: Language-Driven Hierarchical Species Distribution Modeling",
        "url": "http://arxiv.org/abs/2312.08334v1",
        "pub_date": "2023-12-13",
        "summary": "We focus on the problem of species distribution modeling using global-scale\npresence-only data. Most previous studies have mapped the range of a given\nspecies using geographical and environmental features alone. To capture a\nstronger implicit relationship between species, we encode the taxonomic\nhierarchy of species using a large language model. This enables range mapping\nfor any taxonomic rank and unseen species without additional supervision.\nFurther, we propose a novel proximity-aware evaluation metric that enables\nevaluating species distribution models using any pixel-level representation of\nground-truth species range map. The proposed metric penalizes the predictions\nof a model based on its proximity to the ground truth. We describe the\neffectiveness of our model by systematically evaluating on the task of species\nrange prediction, zero-shot prediction and geo-feature regression against the\nstate-of-the-art. Results show our model outperforms the strong baselines when\ntrained with a variety of multi-label learning losses.",
        "translated": ""
    },
    {
        "title": "PnPNet: Pull-and-Push Networks for Volumetric Segmentation with Boundary\n  Confusion",
        "url": "http://arxiv.org/abs/2312.08323v1",
        "pub_date": "2023-12-13",
        "summary": "Precise boundary segmentation of volumetric images is a critical task for\nimage-guided diagnosis and computer-assisted intervention, especially for\nboundary confusion in clinical practice. However, U-shape networks cannot\neffectively resolve this challenge due to the lack of boundary shape\nconstraints. Besides, existing methods of refining boundaries overemphasize the\nslender structure, which results in the overfitting phenomenon due to networks'\nlimited abilities to model tiny objects. In this paper, we reconceptualize the\nmechanism of boundary generation by encompassing the interaction dynamics with\nadjacent regions. Moreover, we propose a unified network termed PnPNet to model\nshape characteristics of the confused boundary region. Core ingredients of\nPnPNet contain the pushing and pulling branches. Specifically, based on\ndiffusion theory, we devise the semantic difference module (SDM) from the\npushing branch to squeeze the boundary region. Explicit and implicit\ndifferential information inside SDM significantly boost representation\nabilities for inter-class boundaries. Additionally, motivated by the K-means\nalgorithm, the class clustering module (CCM) from the pulling branch is\nintroduced to stretch the intersected boundary region. Thus, pushing and\npulling branches will shrink and enlarge the boundary uncertainty respectively.\nThey furnish two adversarial forces to promote models to output a more precise\ndelineation of boundaries. We carry out experiments on three challenging public\ndatasets and one in-house dataset, containing three types of boundary confusion\nin model predictions. Experimental results demonstrate the superiority of\nPnPNet over other segmentation networks, especially on evaluation metrics of HD\nand ASSD. Besides, pushing and pulling branches can serve as plug-and-play\nmodules to enhance classic U-shape baseline models. Codes are available.",
        "translated": ""
    },
    {
        "title": "LIME: Localized Image Editing via Attention Regularization in Diffusion\n  Models",
        "url": "http://arxiv.org/abs/2312.09256v1",
        "pub_date": "2023-12-14",
        "summary": "Diffusion models (DMs) have gained prominence due to their ability to\ngenerate high-quality, varied images, with recent advancements in text-to-image\ngeneration. The research focus is now shifting towards the controllability of\nDMs. A significant challenge within this domain is localized editing, where\nspecific areas of an image are modified without affecting the rest of the\ncontent. This paper introduces LIME for localized image editing in diffusion\nmodels that do not require user-specified regions of interest (RoI) or\nadditional text input. Our method employs features from pre-trained methods and\na simple clustering technique to obtain precise semantic segmentation maps.\nThen, by leveraging cross-attention maps, it refines these segments for\nlocalized edits. Finally, we propose a novel cross-attention regularization\ntechnique that penalizes unrelated cross-attention scores in the RoI during the\ndenoising steps, ensuring localized edits. Our approach, without re-training\nand fine-tuning, consistently improves the performance of existing methods in\nvarious editing benchmarks.",
        "translated": ""
    },
    {
        "title": "Revisiting Depth Completion from a Stereo Matching Perspective for\n  Cross-domain Generalization",
        "url": "http://arxiv.org/abs/2312.09254v1",
        "pub_date": "2023-12-14",
        "summary": "This paper proposes a new framework for depth completion robust against\ndomain-shifting issues. It exploits the generalization capability of modern\nstereo networks to face depth completion, by processing fictitious stereo pairs\nobtained through a virtual pattern projection paradigm. Any stereo network or\ntraditional stereo matcher can be seamlessly plugged into our framework,\nallowing for the deployment of a virtual stereo setup that is future-proof\nagainst advancement in the stereo field. Exhaustive experiments on cross-domain\ngeneralization support our claims. Hence, we argue that our framework can help\ndepth completion to reach new deployment scenarios.",
        "translated": ""
    },
    {
        "title": "VL-GPT: A Generative Pre-trained Transformer for Vision and Language\n  Understanding and Generation",
        "url": "http://arxiv.org/abs/2312.09251v1",
        "pub_date": "2023-12-14",
        "summary": "In this work, we introduce Vision-Language Generative Pre-trained Transformer\n(VL-GPT), a transformer model proficient at concurrently perceiving and\ngenerating visual and linguistic data. VL-GPT achieves a unified pre-training\napproach for both image and text modalities by employing a straightforward\nauto-regressive objective, thereby enabling the model to process image and text\nas seamlessly as a language model processes text. To accomplish this, we\ninitially propose a novel image tokenizer-detokenizer framework for visual\ndata, specifically designed to transform raw images into a sequence of\ncontinuous embeddings and reconstruct them accordingly. In combination with the\nexisting text tokenizer and detokenizer, this framework allows for the encoding\nof interleaved image-text data into a multimodal sequence, which can\nsubsequently be fed into the transformer model. Consequently, VL-GPT can\nperform large-scale pre-training on multimodal corpora utilizing a unified\nauto-regressive objective (i.e., next-token prediction). Upon completion of\npre-training, VL-GPT exhibits remarkable zero-shot and few-shot performance\nacross a diverse range of vision and language understanding and generation\ntasks, including image captioning, visual question answering, text-to-image\ngeneration, and more. Additionally, the pre-trained model retrains in-context\nlearning capabilities when provided with multimodal prompts. We further conduct\ninstruction tuning on our VL-GPT, highlighting its exceptional potential for\nmultimodal assistance. The source code and model weights shall be released.",
        "translated": ""
    },
    {
        "title": "FineControlNet: Fine-level Text Control for Image Generation with\n  Spatially Aligned Text Control Injection",
        "url": "http://arxiv.org/abs/2312.09252v1",
        "pub_date": "2023-12-14",
        "summary": "Recently introduced ControlNet has the ability to steer the text-driven image\ngeneration process with geometric input such as human 2D pose, or edge\nfeatures. While ControlNet provides control over the geometric form of the\ninstances in the generated image, it lacks the capability to dictate the visual\nappearance of each instance. We present FineControlNet to provide fine control\nover each instance's appearance while maintaining the precise pose control\ncapability. Specifically, we develop and demonstrate FineControlNet with\ngeometric control via human pose images and appearance control via\ninstance-level text prompts. The spatial alignment of instance-specific text\nprompts and 2D poses in latent space enables the fine control capabilities of\nFineControlNet. We evaluate the performance of FineControlNet with rigorous\ncomparison against state-of-the-art pose-conditioned text-to-image diffusion\nmodels. FineControlNet achieves superior performance in generating images that\nfollow the user-provided instance-specific text prompts and poses compared with\nexisting methods. Project webpage:\nhttps://samsunglabs.github.io/FineControlNet-project-page",
        "translated": ""
    },
    {
        "title": "Single Mesh Diffusion Models with Field Latents for Texture Generation",
        "url": "http://arxiv.org/abs/2312.09250v1",
        "pub_date": "2023-12-14",
        "summary": "We introduce a framework for intrinsic latent diffusion models operating\ndirectly on the surfaces of 3D shapes, with the goal of synthesizing\nhigh-quality textures. Our approach is underpinned by two contributions: field\nlatents, a latent representation encoding textures as discrete vector fields on\nthe mesh vertices, and field latent diffusion models, which learn to denoise a\ndiffusion process in the learned latent space on the surface. We consider a\nsingle-textured-mesh paradigm, where our models are trained to generate\nvariations of a given texture on a mesh. We show the synthesized textures are\nof superior fidelity compared those from existing single-textured-mesh\ngenerative models. Our models can also be adapted for user-controlled editing\ntasks such as inpainting and label-guided generation. The efficacy of our\napproach is due in part to the equivariance of our proposed framework under\nisometries, allowing our models to seamlessly reproduce details across locally\nsimilar regions and opening the door to a notion of generative texture\ntransfer.",
        "translated": ""
    },
    {
        "title": "ZeroRF: Fast Sparse View 360° Reconstruction with Zero Pretraining",
        "url": "http://arxiv.org/abs/2312.09249v1",
        "pub_date": "2023-12-14",
        "summary": "We present ZeroRF, a novel per-scene optimization method addressing the\nchallenge of sparse view 360{\\deg} reconstruction in neural field\nrepresentations. Current breakthroughs like Neural Radiance Fields (NeRF) have\ndemonstrated high-fidelity image synthesis but struggle with sparse input\nviews. Existing methods, such as Generalizable NeRFs and per-scene optimization\napproaches, face limitations in data dependency, computational cost, and\ngeneralization across diverse scenarios. To overcome these challenges, we\npropose ZeroRF, whose key idea is to integrate a tailored Deep Image Prior into\na factorized NeRF representation. Unlike traditional methods, ZeroRF\nparametrizes feature grids with a neural network generator, enabling efficient\nsparse view 360{\\deg} reconstruction without any pretraining or additional\nregularization. Extensive experiments showcase ZeroRF's versatility and\nsuperiority in terms of both quality and speed, achieving state-of-the-art\nresults on benchmark datasets. ZeroRF's significance extends to applications in\n3D content generation and editing. Project page:\nhttps://sarahweiii.github.io/zerorf/",
        "translated": ""
    },
    {
        "title": "SHAP-EDITOR: Instruction-guided Latent 3D Editing in Seconds",
        "url": "http://arxiv.org/abs/2312.09246v1",
        "pub_date": "2023-12-14",
        "summary": "We propose a novel feed-forward 3D editing framework called Shap-Editor.\nPrior research on editing 3D objects primarily concentrated on editing\nindividual objects by leveraging off-the-shelf 2D image editing networks. This\nis achieved via a process called distillation, which transfers knowledge from\nthe 2D network to 3D assets. Distillation necessitates at least tens of minutes\nper asset to attain satisfactory editing results, and is thus not very\npractical. In contrast, we ask whether 3D editing can be carried out directly\nby a feed-forward network, eschewing test-time optimisation. In particular, we\nhypothesise that editing can be greatly simplified by first encoding 3D objects\nin a suitable latent space. We validate this hypothesis by building upon the\nlatent space of Shap-E. We demonstrate that direct 3D editing in this space is\npossible and efficient by building a feed-forward editor network that only\nrequires approximately one second per edit. Our experiments show that\nShap-Editor generalises well to both in-distribution and out-of-distribution 3D\nassets with different prompts, exhibiting comparable performance with methods\nthat carry out test-time optimisation for each edited instance.",
        "translated": ""
    },
    {
        "title": "DriveMLM: Aligning Multi-Modal Large Language Models with Behavioral\n  Planning States for Autonomous Driving",
        "url": "http://arxiv.org/abs/2312.09245v1",
        "pub_date": "2023-12-14",
        "summary": "Large language models (LLMs) have opened up new possibilities for intelligent\nagents, endowing them with human-like thinking and cognitive abilities. In this\nwork, we delve into the potential of large language models (LLMs) in autonomous\ndriving (AD). We introduce DriveMLM, an LLM-based AD framework that can perform\nclose-loop autonomous driving in realistic simulators. To this end, (1) we\nbridge the gap between the language decisions and the vehicle control commands\nby standardizing the decision states according to the off-the-shelf motion\nplanning module. (2) We employ a multi-modal LLM (MLLM) to model the behavior\nplanning module of a module AD system, which uses driving rules, user commands,\nand inputs from various sensors (e.g., camera, lidar) as input and makes\ndriving decisions and provide explanations; This model can plug-and-play in\nexisting AD systems such as Apollo for close-loop driving. (3) We design an\neffective data engine to collect a dataset that includes decision state and\ncorresponding explanation annotation for model training and evaluation. We\nconduct extensive experiments and show that our model achieves 76.1 driving\nscore on the CARLA Town05 Long, and surpasses the Apollo baseline by 4.7 points\nunder the same settings, demonstrating the effectiveness of our model. We hope\nthis work can serve as a baseline for autonomous driving with LLMs. Code and\nmodels shall be released at https://github.com/OpenGVLab/DriveMLM.",
        "translated": ""
    },
    {
        "title": "OccNeRF: Self-Supervised Multi-Camera Occupancy Prediction with Neural\n  Radiance Fields",
        "url": "http://arxiv.org/abs/2312.09243v1",
        "pub_date": "2023-12-14",
        "summary": "As a fundamental task of vision-based perception, 3D occupancy prediction\nreconstructs 3D structures of surrounding environments. It provides detailed\ninformation for autonomous driving planning and navigation. However, most\nexisting methods heavily rely on the LiDAR point clouds to generate occupancy\nground truth, which is not available in the vision-based system. In this paper,\nwe propose an OccNeRF method for self-supervised multi-camera occupancy\nprediction. Different from bounded 3D occupancy labels, we need to consider\nunbounded scenes with raw image supervision. To solve the issue, we\nparameterize the reconstructed occupancy fields and reorganize the sampling\nstrategy. The neural rendering is adopted to convert occupancy fields to\nmulti-camera depth maps, supervised by multi-frame photometric consistency.\nMoreover, for semantic occupancy prediction, we design several strategies to\npolish the prompts and filter the outputs of a pretrained open-vocabulary 2D\nsegmentation model. Extensive experiments for both self-supervised depth\nestimation and semantic occupancy prediction tasks on nuScenes dataset\ndemonstrate the effectiveness of our method.",
        "translated": ""
    },
    {
        "title": "Text2Immersion: Generative Immersive Scene with 3D Gaussians",
        "url": "http://arxiv.org/abs/2312.09242v1",
        "pub_date": "2023-12-14",
        "summary": "We introduce Text2Immersion, an elegant method for producing high-quality 3D\nimmersive scenes from text prompts. Our proposed pipeline initiates by\nprogressively generating a Gaussian cloud using pre-trained 2D diffusion and\ndepth estimation models. This is followed by a refining stage on the Gaussian\ncloud, interpolating and refining it to enhance the details of the generated\nscene. Distinct from prevalent methods that focus on single object or indoor\nscenes, or employ zoom-out trajectories, our approach generates diverse scenes\nwith various objects, even extending to the creation of imaginary scenes.\nConsequently, Text2Immersion can have wide-ranging implications for various\napplications such as virtual reality, game development, and automated content\ncreation. Extensive evaluations demonstrate that our system surpasses other\nmethods in rendering quality and diversity, further progressing towards\ntext-driven 3D scene generation. We will make the source code publicly\naccessible at the project page.",
        "translated": ""
    },
    {
        "title": "Point Transformer V3: Simpler, Faster, Stronger",
        "url": "http://arxiv.org/abs/2312.10035v1",
        "pub_date": "2023-12-15",
        "summary": "This paper is not motivated to seek innovation within the attention\nmechanism. Instead, it focuses on overcoming the existing trade-offs between\naccuracy and efficiency within the context of point cloud processing,\nleveraging the power of scale. Drawing inspiration from recent advances in 3D\nlarge-scale representation learning, we recognize that model performance is\nmore influenced by scale than by intricate design. Therefore, we present Point\nTransformer V3 (PTv3), which prioritizes simplicity and efficiency over the\naccuracy of certain mechanisms that are minor to the overall performance after\nscaling, such as replacing the precise neighbor search by KNN with an efficient\nserialized neighbor mapping of point clouds organized with specific patterns.\nThis principle enables significant scaling, expanding the receptive field from\n16 to 1024 points while remaining efficient (a 3x increase in processing speed\nand a 10x improvement in memory efficiency compared with its predecessor,\nPTv2). PTv3 attains state-of-the-art results on over 20 downstream tasks that\nspan both indoor and outdoor scenarios. Further enhanced with multi-dataset\njoint training, PTv3 pushes these results to a higher level.",
        "translated": ""
    },
    {
        "title": "SlimmeRF: Slimmable Radiance Fields",
        "url": "http://arxiv.org/abs/2312.10034v1",
        "pub_date": "2023-12-15",
        "summary": "Neural Radiance Field (NeRF) and its variants have recently emerged as\nsuccessful methods for novel view synthesis and 3D scene reconstruction.\nHowever, most current NeRF models either achieve high accuracy using large\nmodel sizes, or achieve high memory-efficiency by trading off accuracy. This\nlimits the applicable scope of any single model, since high-accuracy models\nmight not fit in low-memory devices, and memory-efficient models might not\nsatisfy high-quality requirements. To this end, we present SlimmeRF, a model\nthat allows for instant test-time trade-offs between model size and accuracy\nthrough slimming, thus making the model simultaneously suitable for scenarios\nwith different computing budgets. We achieve this through a newly proposed\nalgorithm named Tensorial Rank Incrementation (TRaIn) which increases the rank\nof the model's tensorial representation gradually during training. We also\nobserve that our model allows for more effective trade-offs in sparse-view\nscenarios, at times even achieving higher accuracy after being slimmed. We\ncredit this to the fact that erroneous information such as floaters tend to be\nstored in components corresponding to higher ranks. Our implementation is\navailable at https://github.com/Shiran-Yuan/SlimmeRF.",
        "translated": ""
    },
    {
        "title": "Osprey: Pixel Understanding with Visual Instruction Tuning",
        "url": "http://arxiv.org/abs/2312.10032v1",
        "pub_date": "2023-12-15",
        "summary": "Multimodal large language models (MLLMs) have recently achieved impressive\ngeneral-purpose vision-language capabilities through visual instruction tuning.\nHowever, current MLLMs primarily focus on image-level or box-level\nunderstanding, falling short of achieving fine-grained vision-language\nalignment at the pixel level. Besides, the lack of mask-based instruction data\nlimits their advancements. In this paper, we propose Osprey, a mask-text\ninstruction tuning approach, to extend MLLMs by incorporating fine-grained mask\nregions into language instruction, aiming at achieving pixel-wise visual\nunderstanding. To achieve this goal, we first meticulously curate a mask-based\nregion-text dataset with 724K samples, and then design a vision-language model\nby injecting pixel-level representation into LLM. Especially, Osprey adopts a\nconvolutional CLIP backbone as the vision encoder and employs a mask-aware\nvisual extractor to extract precise visual mask features from high resolution\ninput. Experimental results demonstrate Osprey's superiority in various region\nunderstanding tasks, showcasing its new capability for pixel-level instruction\ntuning. In particular, Osprey can be integrated with Segment Anything Model\n(SAM) seamlessly to obtain multi-granularity semantics. The source code,\ndataset and demo can be found at https://github.com/CircleRadon/Osprey.",
        "translated": ""
    },
    {
        "title": "One Self-Configurable Model to Solve Many Abstract Visual Reasoning\n  Problems",
        "url": "http://arxiv.org/abs/2312.09997v1",
        "pub_date": "2023-12-15",
        "summary": "Abstract Visual Reasoning (AVR) comprises a wide selection of various\nproblems similar to those used in human IQ tests. Recent years have brought\ndynamic progress in solving particular AVR tasks, however, in the contemporary\nliterature AVR problems are largely dealt with in isolation, leading to highly\nspecialized task-specific methods. With the aim of developing universal\nlearning systems in the AVR domain, we propose the unified model for solving\nSingle-Choice Abstract visual Reasoning tasks (SCAR), capable of solving\nvarious single-choice AVR tasks, without making any a priori assumptions about\nthe task structure, in particular the number and location of panels. The\nproposed model relies on a novel Structure-Aware dynamic Layer (SAL), which\nadapts its weights to the structure of the considered AVR problem. Experiments\nconducted on Raven's Progressive Matrices, Visual Analogy Problems, and Odd One\nOut problems show that SCAR (SAL-based models, in general) effectively solves\ndiverse AVR tasks, and its performance is on par with the state-of-the-art\ntask-specific baselines. What is more, SCAR demonstrates effective knowledge\nreuse in multi-task and transfer learning settings. To our knowledge, this work\nis the first successful attempt to construct a general single-choice AVR solver\nrelying on self-configurable architecture and unified solving method. With this\nwork we aim to stimulate and foster progress on task-independent research paths\nin the AVR domain, with the long-term goal of development of a general AVR\nsolver.",
        "translated": ""
    },
    {
        "title": "Towards Architecture-Insensitive Untrained Network Priors for\n  Accelerated MRI Reconstruction",
        "url": "http://arxiv.org/abs/2312.09988v1",
        "pub_date": "2023-12-15",
        "summary": "Untrained neural networks pioneered by Deep Image Prior (DIP) have recently\nenabled MRI reconstruction without requiring fully-sampled measurements for\ntraining. Their success is widely attributed to the implicit regularization\ninduced by suitable network architectures. However, the lack of understanding\nof such architectural priors results in superfluous design choices and\nsub-optimal outcomes. This work aims to simplify the architectural design\ndecisions for DIP-MRI to facilitate its practical deployment. We observe that\ncertain architectural components are more prone to causing overfitting\nregardless of the number of parameters, incurring severe reconstruction\nartifacts by hindering accurate extrapolation on the un-acquired measurements.\nWe interpret this phenomenon from a frequency perspective and find that the\narchitectural characteristics favoring low frequencies, i.e., deep and narrow\nwith unlearnt upsampling, can lead to enhanced generalization and hence better\nreconstruction. Building on this insight, we propose two architecture-agnostic\nremedies: one to constrain the frequency range of the white-noise input and the\nother to penalize the Lipschitz constants of the network. We demonstrate that\neven with just one extra line of code on the input, the performance gap between\nthe ill-designed models and the high-performing ones can be closed. These\nresults signify that for the first time, architectural biases on untrained MRI\nreconstruction can be mitigated without architectural modifications.",
        "translated": ""
    },
    {
        "title": "Human Perception-Inspired Grain Segmentation Refinement Using\n  Conditional Random Fields",
        "url": "http://arxiv.org/abs/2312.09968v1",
        "pub_date": "2023-12-15",
        "summary": "Accurate segmentation of interconnected line networks, such as grain\nboundaries in polycrystalline material microstructures, poses a significant\nchallenge due to the fragmented masks produced by conventional computer vision\nalgorithms, including convolutional neural networks. These algorithms struggle\nwith thin masks, often necessitating intricate post-processing for effective\ncontour closure and continuity. Addressing this issue, this paper introduces a\nfast, high-fidelity post-processing technique, leveraging domain knowledge\nabout grain boundary connectivity and employing conditional random fields and\nperceptual grouping rules. This approach significantly enhances segmentation\nmask accuracy, achieving a 79% segment identification accuracy in validation\nwith a U-Net model on electron microscopy images of a polycrystalline oxide.\nAdditionally, a novel grain alignment metric is introduced, showing a 51%\nimprovement in grain alignment, providing a more detailed assessment of\nsegmentation performance for complex microstructures. This method not only\nenables rapid and accurate segmentation but also facilitates an unprecedented\nlevel of data analysis, significantly improving the statistical representation\nof grain boundary networks, making it suitable for a range of disciplines where\nprecise segmentation of interconnected line networks is essential.",
        "translated": ""
    },
    {
        "title": "DHFormer: A Vision Transformer-Based Attention Module for Image Dehazing",
        "url": "http://arxiv.org/abs/2312.09955v1",
        "pub_date": "2023-12-15",
        "summary": "Images acquired in hazy conditions have degradations induced in them.\nDehazing such images is a vexed and ill-posed problem. Scores of prior-based\nand learning-based approaches have been proposed to mitigate the effect of haze\nand generate haze-free images. Many conventional methods are constrained by\ntheir lack of awareness regarding scene depth and their incapacity to capture\nlong-range dependencies. In this paper, a method that uses residual learning\nand vision transformers in an attention module is proposed. It essentially\ncomprises two networks: In the first one, the network takes the ratio of a hazy\nimage and the approximated transmission matrix to estimate a residual map. The\nsecond network takes this residual image as input and passes it through\nconvolution layers before superposing it on the generated feature maps. It is\nthen passed through global context and depth-aware transformer encoders to\nobtain channel attention. The attention module then infers the spatial\nattention map before generating the final haze-free image. Experimental\nresults, including several quantitative metrics, demonstrate the efficiency and\nscalability of the suggested methodology.",
        "translated": ""
    },
    {
        "title": "LogoStyleFool: Vitiating Video Recognition Systems via Logo Style\n  Transfer",
        "url": "http://arxiv.org/abs/2312.09935v1",
        "pub_date": "2023-12-15",
        "summary": "Video recognition systems are vulnerable to adversarial examples. Recent\nstudies show that style transfer-based and patch-based unrestricted\nperturbations can effectively improve attack efficiency. These attacks,\nhowever, face two main challenges: 1) Adding large stylized perturbations to\nall pixels reduces the naturalness of the video and such perturbations can be\neasily detected. 2) Patch-based video attacks are not extensible to targeted\nattacks due to the limited search space of reinforcement learning that has been\nwidely used in video attacks recently. In this paper, we focus on the video\nblack-box setting and propose a novel attack framework named LogoStyleFool by\nadding a stylized logo to the clean video. We separate the attack into three\nstages: style reference selection, reinforcement-learning-based logo style\ntransfer, and perturbation optimization. We solve the first challenge by\nscaling down the perturbation range to a regional logo, while the second\nchallenge is addressed by complementing an optimization stage after\nreinforcement learning. Experimental results substantiate the overall\nsuperiority of LogoStyleFool over three state-of-the-art patch-based attacks in\nterms of attack performance and semantic preservation. Meanwhile, LogoStyleFool\nstill maintains its performance against two existing patch-based defense\nmethods. We believe that our research is beneficial in increasing the attention\nof the security community to such subregional style transfer attacks.",
        "translated": ""
    },
    {
        "title": "CNC-Net: Self-Supervised Learning for CNC Machining Operations",
        "url": "http://arxiv.org/abs/2312.09925v1",
        "pub_date": "2023-12-15",
        "summary": "CNC manufacturing is a process that employs computer numerical control (CNC)\nmachines to govern the movements of various industrial tools and machinery,\nencompassing equipment ranging from grinders and lathes to mills and CNC\nrouters. However, the reliance on manual CNC programming has become a\nbottleneck, and the requirement for expert knowledge can result in significant\ncosts. Therefore, we introduce a pioneering approach named CNC-Net,\nrepresenting the use of deep neural networks (DNNs) to simulate CNC machines\nand grasp intricate operations when supplied with raw materials. CNC-Net\nconstitutes a self-supervised framework that exclusively takes an input 3D\nmodel and subsequently generates the essential operation parameters required by\nthe CNC machine to construct the object. Our method has the potential to\ntransformative automation in manufacturing by offering a cost-effective\nalternative to the high costs of manual CNC programming while maintaining\nexceptional precision in 3D object production. Our experiments underscore the\neffectiveness of our CNC-Net in constructing the desired 3D objects through the\nutilization of CNC operations. Notably, it excels in preserving finer local\ndetails, exhibiting a marked enhancement in precision compared to the\nstate-of-the-art 3D CAD reconstruction approaches.",
        "translated": ""
    },
    {
        "title": "A Unifying Tensor View for Lightweight CNNs",
        "url": "http://arxiv.org/abs/2312.09922v1",
        "pub_date": "2023-12-15",
        "summary": "Despite the decomposition of convolutional kernels for lightweight CNNs being\nwell studied, existing works that rely on tensor network diagrams or\nhyperdimensional abstraction lack geometry intuition. This work devises a new\nperspective by linking a 3D-reshaped kernel tensor to its various slice-wise\nand rank-1 decompositions, permitting a straightforward connection between\nvarious tensor approximations and efficient CNN modules. Specifically, it is\ndiscovered that a pointwise-depthwise-pointwise (PDP) configuration constitutes\na viable construct for lightweight CNNs. Moreover, a novel link to the latest\nShiftNet is established, inspiring a first-ever shift layer pruning that\nachieves nearly 50% compression with &lt; 1% drop in accuracy for ShiftResNet.",
        "translated": ""
    },
    {
        "title": "Generative Multimodal Models are In-Context Learners",
        "url": "http://arxiv.org/abs/2312.13286v1",
        "pub_date": "2023-12-20",
        "summary": "The human ability to easily solve multimodal tasks in context (i.e., with\nonly a few demonstrations or simple instructions), is what current multimodal\nsystems have largely struggled to imitate. In this work, we demonstrate that\nthe task-agnostic in-context learning capabilities of large multimodal models\ncan be significantly enhanced by effective scaling-up. We introduce Emu2, a\ngenerative multimodal model with 37 billion parameters, trained on large-scale\nmultimodal sequences with a unified autoregressive objective. Emu2 exhibits\nstrong multimodal in-context learning abilities, even emerging to solve tasks\nthat require on-the-fly reasoning, such as visual prompting and object-grounded\ngeneration. The model sets a new record on multiple multimodal understanding\ntasks in few-shot settings. When instruction-tuned to follow specific\ninstructions, Emu2 further achieves new state-of-the-art on challenging tasks\nsuch as question answering benchmarks for large multimodal models and\nopen-ended subject-driven generation. These achievements demonstrate that Emu2\ncan serve as a base model and general-purpose interface for a wide range of\nmultimodal tasks. Code and models are publicly available to facilitate future\nresearch.",
        "translated": ""
    },
    {
        "title": "UniSDF: Unifying Neural Representations for High-Fidelity 3D\n  Reconstruction of Complex Scenes with Reflections",
        "url": "http://arxiv.org/abs/2312.13285v1",
        "pub_date": "2023-12-20",
        "summary": "Neural 3D scene representations have shown great potential for 3D\nreconstruction from 2D images. However, reconstructing real-world captures of\ncomplex scenes still remains a challenge. Existing generic 3D reconstruction\nmethods often struggle to represent fine geometric details and do not\nadequately model reflective surfaces of large-scale scenes. Techniques that\nexplicitly focus on reflective surfaces can model complex and detailed\nreflections by exploiting better reflection parameterizations. However, we\nobserve that these methods are often not robust in real unbounded scenarios\nwhere non-reflective as well as reflective components are present. In this\nwork, we propose UniSDF, a general purpose 3D reconstruction method that can\nreconstruct large complex scenes with reflections. We investigate both\nview-based as well as reflection-based color prediction parameterization\ntechniques and find that explicitly blending these representations in 3D space\nenables reconstruction of surfaces that are more geometrically accurate,\nespecially for reflective surfaces. We further combine this representation with\na multi-resolution grid backbone that is trained in a coarse-to-fine manner,\nenabling faster reconstructions than prior methods. Extensive experiments on\nobject-level datasets DTU, Shiny Blender as well as unbounded datasets Mip-NeRF\n360 and Ref-NeRF real demonstrate that our method is able to robustly\nreconstruct complex large-scale scenes with fine details and reflective\nsurfaces. Please see our project page at\nhttps://fangjinhuawang.github.io/UniSDF.",
        "translated": ""
    },
    {
        "title": "Deep Learning on 3D Neural Fields",
        "url": "http://arxiv.org/abs/2312.13277v1",
        "pub_date": "2023-12-20",
        "summary": "In recent years, Neural Fields (NFs) have emerged as an effective tool for\nencoding diverse continuous signals such as images, videos, audio, and 3D\nshapes. When applied to 3D data, NFs offer a solution to the fragmentation and\nlimitations associated with prevalent discrete representations. However, given\nthat NFs are essentially neural networks, it remains unclear whether and how\nthey can be seamlessly integrated into deep learning pipelines for solving\ndownstream tasks. This paper addresses this research problem and introduces\nnf2vec, a framework capable of generating a compact latent representation for\nan input NF in a single inference pass. We demonstrate that nf2vec effectively\nembeds 3D objects represented by the input NFs and showcase how the resulting\nembeddings can be employed in deep learning pipelines to successfully address\nvarious tasks, all while processing exclusively NFs. We test this framework on\nseveral NFs used to represent 3D surfaces, such as unsigned/signed distance and\noccupancy fields. Moreover, we demonstrate the effectiveness of our approach\nwith more complex NFs that encompass both geometry and appearance of 3D objects\nsuch as neural radiance fields.",
        "translated": ""
    },
    {
        "title": "Repaint123: Fast and High-quality One Image to 3D Generation with\n  Progressive Controllable 2D Repainting",
        "url": "http://arxiv.org/abs/2312.13271v1",
        "pub_date": "2023-12-20",
        "summary": "Recent one image to 3D generation methods commonly adopt Score Distillation\nSampling (SDS). Despite the impressive results, there are multiple deficiencies\nincluding multi-view inconsistency, over-saturated and over-smoothed textures,\nas well as the slow generation speed. To address these deficiencies, we present\nRepaint123 to alleviate multi-view bias as well as texture degradation and\nspeed up the generation process. The core idea is to combine the powerful image\ngeneration capability of the 2D diffusion model and the texture alignment\nability of the repainting strategy for generating high-quality multi-view\nimages with consistency. We further propose visibility-aware adaptive\nrepainting strength for overlap regions to enhance the generated image quality\nin the repainting process. The generated high-quality and multi-view consistent\nimages enable the use of simple Mean Square Error (MSE) loss for fast 3D\ncontent generation. We conduct extensive experiments and show that our method\nhas a superior ability to generate high-quality 3D content with multi-view\nconsistency and fine textures in 2 minutes from scratch. Code is at\nhttps://github.com/junwuzhang19/repaint123.",
        "translated": ""
    },
    {
        "title": "ClassLIE: Structure- and Illumination-Adaptive Classification for\n  Low-Light Image Enhancement",
        "url": "http://arxiv.org/abs/2312.13265v1",
        "pub_date": "2023-12-20",
        "summary": "Low-light images often suffer from limited visibility and multiple types of\ndegradation, rendering low-light image enhancement (LIE) a non-trivial task.\nSome endeavors have been recently made to enhance low-light images using\nconvolutional neural networks (CNNs). However, they have low efficiency in\nlearning the structural information and diverse illumination levels at the\nlocal regions of an image. Consequently, the enhanced results are affected by\nunexpected artifacts, such as unbalanced exposure, blur, and color bias. To\nthis end, this paper proposes a novel framework, called ClassLIE, that combines\nthe potential of CNNs and transformers. It classifies and adaptively learns the\nstructural and illumination information from the low-light images in a holistic\nand regional manner, thus showing better enhancement performance. Our framework\nfirst employs a structure and illumination classification (SIC) module to learn\nthe degradation information adaptively. In SIC, we decompose an input image\ninto an illumination map and a reflectance map. A class prediction block is\nthen designed to classify the degradation information by calculating the\nstructure similarity scores on the reflectance map and mean square error on the\nillumination map. As such, each input image can be divided into patches with\nthree enhancement difficulty levels. Then, a feature learning and fusion (FLF)\nmodule is proposed to adaptively learn the feature information with CNNs for\ndifferent enhancement difficulty levels while learning the long-range\ndependencies for the patches in a holistic manner. Experiments on five\nbenchmark datasets consistently show our ClassLIE achieves new state-of-the-art\nperformance, with 25.74 PSNR and 0.92 SSIM on the LOL dataset.",
        "translated": ""
    },
    {
        "title": "Conditional Image Generation with Pretrained Generative Model",
        "url": "http://arxiv.org/abs/2312.13253v1",
        "pub_date": "2023-12-20",
        "summary": "In recent years, diffusion models have gained popularity for their ability to\ngenerate higher-quality images in comparison to GAN models. However, like any\nother large generative models, these models require a huge amount of data,\ncomputational resources, and meticulous tuning for successful training. This\nposes a significant challenge, rendering it infeasible for most individuals. As\na result, the research community has devised methods to leverage pre-trained\nunconditional diffusion models with additional guidance for the purpose of\nconditional image generative. These methods enable conditional image\ngenerations on diverse inputs and, most importantly, circumvent the need for\ntraining the diffusion model. In this paper, our objective is to reduce the\ntime-required and computational overhead introduced by the addition of guidance\nin diffusion models -- while maintaining comparable image quality. We propose a\nset of methods based on our empirical analysis, demonstrating a reduction in\ncomputation time by approximately threefold.",
        "translated": ""
    },
    {
        "title": "Zero-Shot Metric Depth with a Field-of-View Conditioned Diffusion Model",
        "url": "http://arxiv.org/abs/2312.13252v1",
        "pub_date": "2023-12-20",
        "summary": "While methods for monocular depth estimation have made significant strides on\nstandard benchmarks, zero-shot metric depth estimation remains unsolved.\nChallenges include the joint modeling of indoor and outdoor scenes, which often\nexhibit significantly different distributions of RGB and depth, and the\ndepth-scale ambiguity due to unknown camera intrinsics. Recent work has\nproposed specialized multi-head architectures for jointly modeling indoor and\noutdoor scenes. In contrast, we advocate a generic, task-agnostic diffusion\nmodel, with several advancements such as log-scale depth parameterization to\nenable joint modeling of indoor and outdoor scenes, conditioning on the\nfield-of-view (FOV) to handle scale ambiguity and synthetically augmenting FOV\nduring training to generalize beyond the limited camera intrinsics in training\ndatasets. Furthermore, by employing a more diverse training mixture than is\ncommon, and an efficient diffusion parameterization, our method, DMD (Diffusion\nfor Metric Depth) achieves a 25\\% reduction in relative error (REL) on\nzero-shot indoor and 33\\% reduction on zero-shot outdoor datasets over the\ncurrent SOTA using only a small number of denoising steps. For an overview see\nhttps://diffusion-vision.github.io/dmd",
        "translated": ""
    },
    {
        "title": "The role of data embedding in equivariant quantum convolutional neural\n  networks",
        "url": "http://arxiv.org/abs/2312.13250v1",
        "pub_date": "2023-12-20",
        "summary": "Geometric deep learning refers to the scenario in which the symmetries of a\ndataset are used to constrain the parameter space of a neural network and thus,\nimprove their trainability and generalization. Recently this idea has been\nincorporated into the field of quantum machine learning, which has given rise\nto equivariant quantum neural networks (EQNNs). In this work, we investigate\nthe role of classical-to-quantum embedding on the performance of equivariant\nquantum convolutional neural networks (EQCNNs) for the classification of\nimages. We discuss the connection between the data embedding method and the\nresulting representation of a symmetry group and analyze how changing\nrepresentation affects the expressibility of an EQCNN. We numerically compare\nthe classification accuracy of EQCNNs with three different basis-permuted\namplitude embeddings to the one obtained from a non-equivariant quantum\nconvolutional neural network (QCNN). Our results show that all the EQCNNs\nachieve higher classification accuracy than the non-equivariant QCNN for small\nnumbers of training iterations, while for large iterations this improvement\ncrucially depends on the used embedding. It is expected that the results of\nthis work can be useful to the community for a better understanding of the\nimportance of data embedding choice in the context of geometric quantum machine\nlearning.",
        "translated": ""
    },
    {
        "title": "Efficient Verification-Based Face Identification",
        "url": "http://arxiv.org/abs/2312.13240v1",
        "pub_date": "2023-12-20",
        "summary": "We study the problem of performing face verification with an efficient neural\nmodel $f$. The efficiency of $f$ stems from simplifying the face verification\nproblem from an embedding nearest neighbor search into a binary problem; each\nuser has its own neural network $f$. To allow information sharing between\ndifferent individuals in the training set, we do not train $f$ directly but\ninstead generate the model weights using a hypernetwork $h$. This leads to the\ngeneration of a compact personalized model for face identification that can be\ndeployed on edge devices. Key to the method's success is a novel way of\ngenerating hard negatives and carefully scheduling the training objectives. Our\nmodel leads to a substantially small $f$ requiring only 23k parameters and 5M\nfloating point operations (FLOPS). We use six face verification datasets to\ndemonstrate that our method is on par or better than state-of-the-art models,\nwith a significantly reduced number of parameters and computational burden.\nFurthermore, we perform an extensive ablation study to demonstrate the\nimportance of each element in our method.",
        "translated": ""
    },
    {
        "title": "Diffusion Models With Learned Adaptive Noise",
        "url": "http://arxiv.org/abs/2312.13236v1",
        "pub_date": "2023-12-20",
        "summary": "Diffusion models have gained traction as powerful algorithms for synthesizing\nhigh-quality images. Central to these algorithms is the diffusion process,\nwhich maps data to noise according to equations inspired by thermodynamics and\ncan significantly impact performance. A widely held assumption is that the ELBO\nobjective of a diffusion model is invariant to the noise process (Kingma et\nal.,2021). In this work, we dispel this assumption -- we propose multivariate\nlearned adaptive noise (MuLAN), a learned diffusion process that applies\nGaussian noise at different rates across an image. Our method consists of three\ncomponents -- a multivariate noise schedule, instance-conditional diffusion,\nand auxiliary variables -- which ensure that the learning objective is no\nlonger invariant to the choice of the noise schedule as in previous works. Our\nwork is grounded in Bayesian inference and casts the learned diffusion process\nas an approximate variational posterior that yields a tighter lower bound on\nmarginal likelihood. Empirically, MuLAN sets a new state-of-the-art in density\nestimation on CIFAR-10 and ImageNet compared to classical diffusion. Code is\navailable at https://github.com/s-sahoo/MuLAN",
        "translated": ""
    },
    {
        "title": "3D Pose Estimation of Two Interacting Hands from a Monocular Event\n  Camera",
        "url": "http://arxiv.org/abs/2312.14157v1",
        "pub_date": "2023-12-21",
        "summary": "3D hand tracking from a monocular video is a very challenging problem due to\nhand interactions, occlusions, left-right hand ambiguity, and fast motion. Most\nexisting methods rely on RGB inputs, which have severe limitations under\nlow-light conditions and suffer from motion blur. In contrast, event cameras\ncapture local brightness changes instead of full image frames and do not suffer\nfrom the described effects. Unfortunately, existing image-based techniques\ncannot be directly applied to events due to significant differences in the data\nmodalities. In response to these challenges, this paper introduces the first\nframework for 3D tracking of two fast-moving and interacting hands from a\nsingle monocular event camera. Our approach tackles the left-right hand\nambiguity with a novel semi-supervised feature-wise attention mechanism and\nintegrates an intersection loss to fix hand collisions. To facilitate advances\nin this research domain, we release a new synthetic large-scale dataset of two\ninteracting hands, Ev2Hands-S, and a new real benchmark with real event streams\nand ground-truth 3D annotations, Ev2Hands-R. Our approach outperforms existing\nmethods in terms of the 3D reconstruction accuracy and generalises to real data\nunder severe light conditions.",
        "translated": ""
    },
    {
        "title": "Virtual Pets: Animatable Animal Generation in 3D Scenes",
        "url": "http://arxiv.org/abs/2312.14154v1",
        "pub_date": "2023-12-21",
        "summary": "Toward unlocking the potential of generative models in immersive 4D\nexperiences, we introduce Virtual Pet, a novel pipeline to model realistic and\ndiverse motions for target animal species within a 3D environment. To\ncircumvent the limited availability of 3D motion data aligned with\nenvironmental geometry, we leverage monocular internet videos and extract\ndeformable NeRF representations for the foreground and static NeRF\nrepresentations for the background. For this, we develop a reconstruction\nstrategy, encompassing species-level shared template learning and per-video\nfine-tuning. Utilizing the reconstructed data, we then train a conditional 3D\nmotion model to learn the trajectory and articulation of foreground animals in\nthe context of 3D backgrounds. We showcase the efficacy of our pipeline with\ncomprehensive qualitative and quantitative evaluations using cat videos. We\nalso demonstrate versatility across unseen cats and indoor environments,\nproducing temporally coherent 4D outputs for enriched virtual experiences.",
        "translated": ""
    },
    {
        "title": "DriveLM: Driving with Graph Visual Question Answering",
        "url": "http://arxiv.org/abs/2312.14150v1",
        "pub_date": "2023-12-21",
        "summary": "We study how vision-language models (VLMs) trained on web-scale data can be\nintegrated into end-to-end driving systems to boost generalization and enable\ninteractivity with human users. While recent approaches adapt VLMs to driving\nvia single-round visual question answering (VQA), human drivers reason about\ndecisions in multiple steps. Starting from the localization of key objects,\nhumans estimate object interactions before taking actions. The key insight is\nthat with our proposed task, Graph VQA, where we model graph-structured\nreasoning through perception, prediction and planning question-answer pairs, we\nobtain a suitable proxy task to mimic the human reasoning process. We\ninstantiate datasets (DriveLM-Data) built upon nuScenes and CARLA, and propose\na VLM-based baseline approach (DriveLM-Agent) for jointly performing Graph VQA\nand end-to-end driving. The experiments demonstrate that Graph VQA provides a\nsimple, principled framework for reasoning about a driving scene, and\nDriveLM-Data provides a challenging benchmark for this task. Our DriveLM-Agent\nbaseline performs end-to-end autonomous driving competitively in comparison to\nstate-of-the-art driving-specific architectures. Notably, its benefits are\npronounced when it is evaluated zero-shot on unseen objects or sensor\nconfigurations. We hope this work can be the starting point to shed new light\non how to apply VLMs for autonomous driving. To facilitate future research, all\ncode, data, and models are available to the public.",
        "translated": ""
    },
    {
        "title": "TagAlign: Improving Vision-Language Alignment with Multi-Tag\n  Classification",
        "url": "http://arxiv.org/abs/2312.14149v1",
        "pub_date": "2023-12-21",
        "summary": "The crux of learning vision-language models is to extract semantically\naligned information from visual and linguistic data. Existing attempts usually\nface the problem of coarse alignment, \\textit{e.g.}, the vision encoder\nstruggles in localizing an attribute-specified object. In this work, we propose\nan embarrassingly simple approach to better align image and text features with\nno need of additional data formats other than image-text pairs. Concretely,\ngiven an image and its paired text, we manage to parse objects (\\textit{e.g.},\ncat) and attributes (\\textit{e.g.}, black) from the description, which are\nhighly likely to exist in the image. It is noteworthy that the parsing pipeline\nis fully automatic and thus enjoys good scalability. With these parsed\nsemantics as supervision signals, we can complement the commonly used\nimage-text contrastive loss with the multi-tag classification loss. Extensive\nexperimental results on a broad suite of semantic segmentation datasets\nsubstantiate the average 3.65\\% improvement of our framework over existing\nalternatives. Furthermore, the visualization results indicate that attribute\nsupervision makes vision-language models accurately localize\nattribute-specified objects. Project page can be found at\nhttps://qinying-liu.github.io/Tag-Align/",
        "translated": ""
    },
    {
        "title": "HeadCraft: Modeling High-Detail Shape Variations for Animated 3DMMs",
        "url": "http://arxiv.org/abs/2312.14140v1",
        "pub_date": "2023-12-21",
        "summary": "Current advances in human head modeling allow to generate plausible-looking\n3D head models via neural representations. Nevertheless, constructing complete\nhigh-fidelity head models with explicitly controlled animation remains an\nissue. Furthermore, completing the head geometry based on a partial\nobservation, e.g. coming from a depth sensor, while preserving details is often\nproblematic for the existing methods. We introduce a generative model for\ndetailed 3D head meshes on top of an articulated 3DMM which allows explicit\nanimation and high-detail preservation at the same time. Our method is trained\nin two stages. First, we register a parametric head model with vertex\ndisplacements to each mesh of the recently introduced NPHM dataset of accurate\n3D head scans. The estimated displacements are baked into a hand-crafted UV\nlayout. Second, we train a StyleGAN model in order to generalize over the UV\nmaps of displacements. The decomposition of the parametric model and\nhigh-quality vertex displacements allows us to animate the model and modify it\nsemantically. We demonstrate the results of unconditional generation and\nfitting to the full or partial observation. The project page is available at\nhttps://seva100.github.io/headcraft.",
        "translated": ""
    },
    {
        "title": "Revisiting Foreground and Background Separation in Weakly-supervised\n  Temporal Action Localization: A Clustering-based Approach",
        "url": "http://arxiv.org/abs/2312.14138v1",
        "pub_date": "2023-12-21",
        "summary": "Weakly-supervised temporal action localization aims to localize action\ninstances in videos with only video-level action labels. Existing methods\nmainly embrace a localization-by-classification pipeline that optimizes the\nsnippet-level prediction with a video classification loss. However, this\nformulation suffers from the discrepancy between classification and detection,\nresulting in inaccurate separation of foreground and background (F\\&amp;B)\nsnippets. To alleviate this problem, we propose to explore the underlying\nstructure among the snippets by resorting to unsupervised snippet clustering,\nrather than heavily relying on the video classification loss. Specifically, we\npropose a novel clustering-based F\\&amp;B separation algorithm. It comprises two\ncore components: a snippet clustering component that groups the snippets into\nmultiple latent clusters and a cluster classification component that further\nclassifies the cluster as foreground or background. As there are no\nground-truth labels to train these two components, we introduce a unified\nself-labeling mechanism based on optimal transport to produce high-quality\npseudo-labels that match several plausible prior distributions. This ensures\nthat the cluster assignments of the snippets can be accurately associated with\ntheir F\\&amp;B labels, thereby boosting the F\\&amp;B separation. We evaluate our method\non three benchmarks: THUMOS14, ActivityNet v1.2 and v1.3. Our method achieves\npromising performance on all three benchmarks while being significantly more\nlightweight than previous methods. Code is available at\nhttps://github.com/Qinying-Liu/CASE",
        "translated": ""
    },
    {
        "title": "$\\textit{V}^*$: Guided Visual Search as a Core Mechanism in Multimodal\n  LLMs",
        "url": "http://arxiv.org/abs/2312.14135v1",
        "pub_date": "2023-12-21",
        "summary": "When we look around and perform complex tasks, how we see and selectively\nprocess what we see is crucial. However, the lack of this visual search\nmechanism in current multimodal LLMs (MLLMs) hinders their ability to focus on\nimportant visual details, especially when handling high-resolution and visually\ncrowded images. To address this, we introduce $\\textit{V}^*$, an LLM-guided\nvisual search mechanism that employs the world knowledge in LLMs for efficient\nvisual querying. When combined with an MLLM, this mechanism enhances\ncollaborative reasoning, contextual understanding, and precise targeting of\nspecific visual elements. This integration results in a new MLLM\nmeta-architecture, named $\\textbf{S}$how, s$\\textbf{EA}$rch, and\nTel$\\textbf{L}$ (SEAL). We further create $\\textit{V}^*$Bench, a benchmark\nspecifically designed to evaluate MLLMs in their ability to process\nhigh-resolution images and focus on visual details. Our study highlights the\nnecessity of incorporating visual search capabilities into multimodal systems.\nThe code is available https://github.com/penghao-wu/vstar.",
        "translated": ""
    },
    {
        "title": "Diffusion Reward: Learning Rewards via Conditional Video Diffusion",
        "url": "http://arxiv.org/abs/2312.14134v1",
        "pub_date": "2023-12-21",
        "summary": "Learning rewards from expert videos offers an affordable and effective\nsolution to specify the intended behaviors for reinforcement learning tasks. In\nthis work, we propose Diffusion Reward, a novel framework that learns rewards\nfrom expert videos via conditional video diffusion models for solving complex\nvisual RL problems. Our key insight is that lower generative diversity is\nobserved when conditioned on expert trajectories. Diffusion Reward is\naccordingly formalized by the negative of conditional entropy that encourages\nproductive exploration of expert-like behaviors. We show the efficacy of our\nmethod over 10 robotic manipulation tasks from MetaWorld and Adroit with visual\ninput and sparse reward. Moreover, Diffusion Reward could even solve unseen\ntasks successfully and effectively, largely surpassing baseline methods.\nProject page and code: https://diffusion-reward.github.io/.",
        "translated": ""
    },
    {
        "title": "DUSt3R: Geometric 3D Vision Made Easy",
        "url": "http://arxiv.org/abs/2312.14132v1",
        "pub_date": "2023-12-21",
        "summary": "Multi-view stereo reconstruction (MVS) in the wild requires to first estimate\nthe camera parameters e.g. intrinsic and extrinsic parameters. These are\nusually tedious and cumbersome to obtain, yet they are mandatory to triangulate\ncorresponding pixels in 3D space, which is the core of all best performing MVS\nalgorithms. In this work, we take an opposite stance and introduce DUSt3R, a\nradically novel paradigm for Dense and Unconstrained Stereo 3D Reconstruction\nof arbitrary image collections, i.e. operating without prior information about\ncamera calibration nor viewpoint poses. We cast the pairwise reconstruction\nproblem as a regression of pointmaps, relaxing the hard constraints of usual\nprojective camera models. We show that this formulation smoothly unifies the\nmonocular and binocular reconstruction cases. In the case where more than two\nimages are provided, we further propose a simple yet effective global alignment\nstrategy that expresses all pairwise pointmaps in a common reference frame. We\nbase our network architecture on standard Transformer encoders and decoders,\nallowing us to leverage powerful pretrained models. Our formulation directly\nprovides a 3D model of the scene as well as depth information, but\ninterestingly, we can seamlessly recover from it, pixel matches, relative and\nabsolute camera. Exhaustive experiments on all these tasks showcase that the\nproposed DUSt3R can unify various 3D vision tasks and set new SoTAs on\nmonocular/multi-view depth estimation as well as relative pose estimation. In\nsummary, DUSt3R makes many geometric 3D vision tasks easy.",
        "translated": ""
    },
    {
        "title": "Entropic Open-set Active Learning",
        "url": "http://arxiv.org/abs/2312.14126v1",
        "pub_date": "2023-12-21",
        "summary": "Active Learning (AL) aims to enhance the performance of deep models by\nselecting the most informative samples for annotation from a pool of unlabeled\ndata. Despite impressive performance in closed-set settings, most AL methods\nfail in real-world scenarios where the unlabeled data contains unknown\ncategories. Recently, a few studies have attempted to tackle the AL problem for\nthe open-set setting. However, these methods focus more on selecting known\nsamples and do not efficiently utilize unknown samples obtained during AL\nrounds. In this work, we propose an Entropic Open-set AL (EOAL) framework which\nleverages both known and unknown distributions effectively to select\ninformative samples during AL rounds. Specifically, our approach employs two\ndifferent entropy scores. One measures the uncertainty of a sample with respect\nto the known-class distributions. The other measures the uncertainty of the\nsample with respect to the unknown-class distributions. By utilizing these two\nentropy scores we effectively separate the known and unknown samples from the\nunlabeled data resulting in better sampling. Through extensive experiments, we\nshow that the proposed method outperforms existing state-of-the-art methods on\nCIFAR-10, CIFAR-100, and TinyImageNet datasets. Code is available at\n\\url{https://github.com/bardisafa/EOAL}.",
        "translated": ""
    },
    {
        "title": "MACS: Mass Conditioned 3D Hand and Object Motion Synthesis",
        "url": "http://arxiv.org/abs/2312.14929v1",
        "pub_date": "2023-12-22",
        "summary": "The physical properties of an object, such as mass, significantly affect how\nwe manipulate it with our hands. Surprisingly, this aspect has so far been\nneglected in prior work on 3D motion synthesis. To improve the naturalness of\nthe synthesized 3D hand object motions, this work proposes MACS the first MAss\nConditioned 3D hand and object motion Synthesis approach. Our approach is based\non cascaded diffusion models and generates interactions that plausibly adjust\nbased on the object mass and interaction type. MACS also accepts a manually\ndrawn 3D object trajectory as input and synthesizes the natural 3D hand motions\nconditioned by the object mass. This flexibility enables MACS to be used for\nvarious downstream applications, such as generating synthetic training data for\nML tasks, fast animation of hands for graphics workflows, and generating\ncharacter interactions for computer games. We show experimentally that a\nsmall-scale dataset is sufficient for MACS to reasonably generalize across\ninterpolated and extrapolated object masses unseen during the training.\nFurthermore, MACS shows moderate generalization to unseen objects, thanks to\nthe mass-conditioned contact labels generated by our surface contact synthesis\nmodel ConNet. Our comprehensive user study confirms that the synthesized 3D\nhand-object interactions are highly plausible and realistic.",
        "translated": ""
    },
    {
        "title": "Training Convolutional Neural Networks with the Forward-Forward\n  algorithm",
        "url": "http://arxiv.org/abs/2312.14924v1",
        "pub_date": "2023-12-22",
        "summary": "The recent successes in analyzing images with deep neural networks are almost\nexclusively achieved with Convolutional Neural Networks (CNNs). The training of\nthese CNNs, and in fact of all deep neural network architectures, uses the\nbackpropagation algorithm where the output of the network is compared with the\ndesired result and the difference is then used to tune the weights of the\nnetwork towards the desired outcome. In a 2022 preprint, Geoffrey Hinton\nsuggested an alternative way of training which passes the desired results\ntogether with the images at the input of the network. This so called Forward\nForward (FF) algorithm has up to now only been used in fully connected\nnetworks. In this paper, we show how the FF paradigm can be extended to CNNs.\nOur FF-trained CNN, featuring a novel spatially-extended labeling technique,\nachieves a classification accuracy of 99.0% on the MNIST hand-written digits\ndataset. We show how different hyperparameters affect the performance of the\nproposed algorithm and compare the results with CNN trained with the standard\nbackpropagation approach. Furthermore, we use Class Activation Maps to\ninvestigate which type of features are learnt by the FF algorithm.",
        "translated": ""
    },
    {
        "title": "Lift-Attend-Splat: Bird's-eye-view camera-lidar fusion using\n  transformers",
        "url": "http://arxiv.org/abs/2312.14919v1",
        "pub_date": "2023-12-22",
        "summary": "Combining complementary sensor modalities is crucial to providing robust\nperception for safety-critical robotics applications such as autonomous driving\n(AD). Recent state-of-the-art camera-lidar fusion methods for AD rely on\nmonocular depth estimation which is a notoriously difficult task compared to\nusing depth information from the lidar directly. Here, we find that this\napproach does not leverage depth as expected and show that naively improving\ndepth estimation does not lead to improvements in object detection performance\nand that, strikingly, removing depth estimation altogether does not degrade\nobject detection performance. This suggests that relying on monocular depth\ncould be an unnecessary architectural bottleneck during camera-lidar fusion. In\nthis work, we introduce a novel fusion method that bypasses monocular depth\nestimation altogether and instead selects and fuses camera and lidar features\nin a bird's-eye-view grid using a simple attention mechanism. We show that our\nmodel can modulate its use of camera features based on the availability of\nlidar features and that it yields better 3D object detection on the nuScenes\ndataset than baselines relying on monocular depth estimation.",
        "translated": ""
    },
    {
        "title": "PoseGen: Learning to Generate 3D Human Pose Dataset with NeRF",
        "url": "http://arxiv.org/abs/2312.14915v1",
        "pub_date": "2023-12-22",
        "summary": "This paper proposes an end-to-end framework for generating 3D human pose\ndatasets using Neural Radiance Fields (NeRF). Public datasets generally have\nlimited diversity in terms of human poses and camera viewpoints, largely due to\nthe resource-intensive nature of collecting 3D human pose data. As a result,\npose estimators trained on public datasets significantly underperform when\napplied to unseen out-of-distribution samples. Previous works proposed\naugmenting public datasets by generating 2D-3D pose pairs or rendering a large\namount of random data. Such approaches either overlook image rendering or\nresult in suboptimal datasets for pre-trained models. Here we propose PoseGen,\nwhich learns to generate a dataset (human 3D poses and images) with a feedback\nloss from a given pre-trained pose estimator. In contrast to prior art, our\ngenerated data is optimized to improve the robustness of the pre-trained model.\nThe objective of PoseGen is to learn a distribution of data that maximizes the\nprediction error of a given pre-trained model. As the learned data distribution\ncontains OOD samples of the pre-trained model, sampling data from such a\ndistribution for further fine-tuning a pre-trained model improves the\ngeneralizability of the model. This is the first work that proposes NeRFs for\n3D human data generation. NeRFs are data-driven and do not require 3D scans of\nhumans. Therefore, using NeRF for data generation is a new direction for\nconvenient user-specific data generation. Our extensive experiments show that\nthe proposed PoseGen improves two baseline models (SPIN and HybrIK) on four\ndatasets with an average 6% relative improvement.",
        "translated": ""
    },
    {
        "title": "DRStageNet: Deep Learning for Diabetic Retinopathy Staging from Fundus\n  Images",
        "url": "http://arxiv.org/abs/2312.14891v1",
        "pub_date": "2023-12-22",
        "summary": "Diabetic retinopathy (DR) is a prevalent complication of diabetes associated\nwith a significant risk of vision loss. Timely identification is critical to\ncurb vision impairment. Algorithms for DR staging from digital fundus images\n(DFIs) have been recently proposed. However, models often fail to generalize\ndue to distribution shifts between the source domain on which the model was\ntrained and the target domain where it is deployed. A common and particularly\nchallenging shift is often encountered when the source- and target-domain\nsupports do not fully overlap. In this research, we introduce DRStageNet, a\ndeep learning model designed to mitigate this challenge. We used seven publicly\navailable datasets, comprising a total of 93,534 DFIs that cover a variety of\npatient demographics, ethnicities, geographic origins and comorbidities. We\nfine-tune DINOv2, a pretrained model of self-supervised vision transformer, and\nimplement a multi-source domain fine-tuning strategy to enhance generalization\nperformance. We benchmark and demonstrate the superiority of our method to two\nstate-of-the-art benchmarks, including a recently published foundation model.\nWe adapted the grad-rollout method to our regression task in order to provide\nhigh-resolution explainability heatmaps. The error analysis showed that 59\\% of\nthe main errors had incorrect reference labels. DRStageNet is accessible at URL\n[upon acceptance of the manuscript].",
        "translated": ""
    },
    {
        "title": "BrainVis: Exploring the Bridge between Brain and Visual Signals via\n  Image Reconstruction",
        "url": "http://arxiv.org/abs/2312.14871v1",
        "pub_date": "2023-12-22",
        "summary": "Analyzing and reconstructing visual stimuli from brain signals effectively\nadvances understanding of the human visual system. However, the EEG signals are\ncomplex and contain a amount of noise. This leads to substantial limitations in\nexisting works of visual stimuli reconstruction from EEG, such as difficulties\nin aligning EEG embeddings with the fine-grained semantic information and a\nheavy reliance on additional large self-collected dataset for training. To\naddress these challenges, we propose a novel approach called BrainVis. Firstly,\nwe divide the EEG signals into various units and apply a self-supervised\napproach on them to obtain EEG time-domain features, in an attempt to ease the\ntraining difficulty. Additionally, we also propose to utilize the\nfrequency-domain features to enhance the EEG representations. Then, we\nsimultaneously align EEG time-frequency embeddings with the interpolation of\nthe coarse and fine-grained semantics in the CLIP space, to highlight the\nprimary visual components and reduce the cross-modal alignment difficulty.\nFinally, we adopt the cascaded diffusion models to reconstruct images. Our\nproposed BrainVis outperforms state of the arts in both semantic fidelity\nreconstruction and generation quality. Notably, we reduce the training data\nscale to 10% of the previous work.",
        "translated": ""
    },
    {
        "title": "VIEScore: Towards Explainable Metrics for Conditional Image Synthesis\n  Evaluation",
        "url": "http://arxiv.org/abs/2312.14867v1",
        "pub_date": "2023-12-22",
        "summary": "In the rapidly advancing field of conditional image generation research,\nchallenges such as limited explainability lie in effectively evaluating the\nperformance and capabilities of various models. This paper introduces VIESCORE,\na Visual Instruction-guided Explainable metric for evaluating any conditional\nimage generation tasks. VIESCORE leverages general knowledge from Multimodal\nLarge Language Models (MLLMs) as the backbone and does not require training or\nfine-tuning. We evaluate VIESCORE on seven prominent tasks in conditional image\ntasks and found: (1) VIESCORE (GPT4-v) achieves a high Spearman correlation of\n0.3 with human evaluations, while the human-to-human correlation is 0.45. (2)\nVIESCORE (with open-source MLLM) is significantly weaker than GPT-4v in\nevaluating synthetic images. (3) VIESCORE achieves a correlation on par with\nhuman ratings in the generation tasks but struggles in editing tasks. With\nthese results, we believe VIESCORE shows its great potential to replace human\njudges in evaluating image synthesis tasks.",
        "translated": ""
    },
    {
        "title": "Prototype-Guided Text-based Person Search based on Rich Chinese\n  Descriptions",
        "url": "http://arxiv.org/abs/2312.14834v1",
        "pub_date": "2023-12-22",
        "summary": "Text-based person search aims to simultaneously localize and identify the\ntarget person based on query text from uncropped scene images, which can be\nregarded as the unified task of person detection and text-based person\nretrieval task. In this work, we propose a large-scale benchmark dataset named\nPRW-TPS-CN based on the widely used person search dataset PRW. Our dataset\ncontains 47,102 sentences, which means there is quite more information than\nexisting dataset. These texts precisely describe the person images from top to\nbottom, which in line with the natural description order. We also provide both\nChinese and English descriptions in our dataset for more comprehensive\nevaluation. These characteristics make our dataset more applicable. To\nalleviate the inconsistency between person detection and text-based person\nretrieval, we take advantage of the rich texts in PRW-TPS-CN dataset. We\npropose to aggregate multiple texts as text prototypes to maintain the\nprominent text features of a person, which can better reflect the whole\ncharacter of a person. The overall prototypes lead to generating the image\nattention map to eliminate the detection misalignment causing the decrease of\ntext-based person retrieval. Thus, the inconsistency between person detection\nand text-based person retrieval is largely alleviated. We conduct extensive\nexperiments on the PRW-TPS-CN dataset. The experimental results show the\nPRW-TPS-CN dataset's effectiveness and the state-of-the-art performance of our\napproach.",
        "translated": ""
    },
    {
        "title": "Dreaming of Electrical Waves: Generative Modeling of Cardiac Excitation\n  Waves using Diffusion Models",
        "url": "http://arxiv.org/abs/2312.14830v1",
        "pub_date": "2023-12-22",
        "summary": "Electrical waves in the heart form rotating spiral or scroll waves during\nlife-threatening arrhythmias such as atrial or ventricular fibrillation. The\nwave dynamics are typically modeled using coupled partial differential\nequations, which describe reaction-diffusion dynamics in excitable media. More\nrecently, data-driven generative modeling has emerged as an alternative to\ngenerate spatio-temporal patterns in physical and biological systems. Here, we\nexplore denoising diffusion probabilistic models for the generative modeling of\nelectrical wave patterns in cardiac tissue. We trained diffusion models with\nsimulated electrical wave patterns to be able to generate such wave patterns in\nunconditional and conditional generation tasks. For instance, we explored\ninpainting tasks, such as reconstructing three-dimensional wave dynamics from\nsuperficial two-dimensional measurements, and evolving and generating\nparameter-specific dynamics. We characterized and compared the\ndiffusion-generated solutions to solutions obtained with biophysical models and\nfound that diffusion models learn to replicate spiral and scroll waves dynamics\nso well that they could serve as an alternative data-driven approach for the\nmodeling of excitation waves in cardiac tissue. For instance, we found that it\nis possible to initiate ventricular fibrillation (VF) dynamics instantaneously\nwithout having to apply pacing protocols in order to induce wavebreak. The VF\ndynamics can be created in arbitrary ventricular geometries and can be evolved\nover time. However, we also found that diffusion models `hallucinate' wave\npatterns when given insufficient constraints. Regardless of these limitations,\ndiffusion models are an interesting and powerful tool with many potential\napplications in cardiac arrhythmia research and diagnostics.",
        "translated": ""
    },
    {
        "title": "Plan, Posture and Go: Towards Open-World Text-to-Motion Generation",
        "url": "http://arxiv.org/abs/2312.14828v1",
        "pub_date": "2023-12-22",
        "summary": "Conventional text-to-motion generation methods are usually trained on limited\ntext-motion pairs, making them hard to generalize to open-world scenarios. Some\nworks use the CLIP model to align the motion space and the text space, aiming\nto enable motion generation from natural language motion descriptions. However,\nthey are still constrained to generate limited and unrealistic in-place\nmotions. To address these issues, we present a divide-and-conquer framework\nnamed PRO-Motion, which consists of three modules as motion planner,\nposture-diffuser and go-diffuser. The motion planner instructs Large Language\nModels (LLMs) to generate a sequence of scripts describing the key postures in\nthe target motion. Differing from natural languages, the scripts can describe\nall possible postures following very simple text templates. This significantly\nreduces the complexity of posture-diffuser, which transforms a script to a\nposture, paving the way for open-world generation. Finally, go-diffuser,\nimplemented as another diffusion model, estimates whole-body translations and\nrotations for all postures, resulting in realistic motions. Experimental\nresults have shown the superiority of our method with other counterparts, and\ndemonstrated its capability of generating diverse and realistic motions from\ncomplex open-world prompts such as \"Experiencing a profound sense of joy\". The\nproject page is available at https://moonsliu.github.io/Pro-Motion.",
        "translated": ""
    },
    {
        "title": "EmbodiedScan: A Holistic Multi-Modal 3D Perception Suite Towards\n  Embodied AI",
        "url": "http://arxiv.org/abs/2312.16170v1",
        "pub_date": "2023-12-26",
        "summary": "In the realm of computer vision and robotics, embodied agents are expected to\nexplore their environment and carry out human instructions. This necessitates\nthe ability to fully understand 3D scenes given their first-person observations\nand contextualize them into language for interaction. However, traditional\nresearch focuses more on scene-level input and output setups from a global\nview. To address the gap, we introduce EmbodiedScan, a multi-modal, ego-centric\n3D perception dataset and benchmark for holistic 3D scene understanding. It\nencompasses over 5k scans encapsulating 1M ego-centric RGB-D views, 1M language\nprompts, 160k 3D-oriented boxes spanning over 760 categories, some of which\npartially align with LVIS, and dense semantic occupancy with 80 common\ncategories. Building upon this database, we introduce a baseline framework\nnamed Embodied Perceptron. It is capable of processing an arbitrary number of\nmulti-modal inputs and demonstrates remarkable 3D perception capabilities, both\nwithin the two series of benchmarks we set up, i.e., fundamental 3D perception\ntasks and language-grounded tasks, and in the wild. Codes, datasets, and\nbenchmarks will be available at https://github.com/OpenRobotLab/EmbodiedScan.",
        "translated": ""
    },
    {
        "title": "Social-Transmotion: Promptable Human Trajectory Prediction",
        "url": "http://arxiv.org/abs/2312.16168v1",
        "pub_date": "2023-12-26",
        "summary": "Accurate human trajectory prediction is crucial for applications such as\nautonomous vehicles, robotics, and surveillance systems. Yet, existing models\noften fail to fully leverage the non-verbal social cues human subconsciously\ncommunicate when navigating the space. To address this, we introduce\nSocial-Transmotion, a generic model that exploits the power of transformers to\nhandle diverse and numerous visual cues, capturing the multi-modal nature of\nhuman behavior. We translate the idea of a prompt from Natural Language\nProcessing (NLP) to the task of human trajectory prediction, where a prompt can\nbe a sequence of x-y coordinates on the ground, bounding boxes or body poses.\nThis, in turn, augments trajectory data, leading to enhanced human trajectory\nprediction. Our model exhibits flexibility and adaptability by capturing\nspatiotemporal interactions between pedestrians based on the available visual\ncues, whether they are poses, bounding boxes, or a combination thereof. By the\nmasking technique, we ensure our model's effectiveness even when certain visual\ncues are unavailable, although performance is further boosted with the presence\nof comprehensive visual data. We delve into the merits of using 2d versus 3d\nposes, and a limited set of poses. Additionally, we investigate the spatial and\ntemporal attention map to identify which keypoints and frames of poses are\nvital for optimizing human trajectory prediction. Our approach is validated on\nmultiple datasets, including JTA, JRDB, Pedestrians and Cyclists in Road\nTraffic, and ETH-UCY. The code is publicly available:\nhttps://github.com/vita-epfl/social-transmotion",
        "translated": ""
    },
    {
        "title": "Large-scale Long-tailed Disease Diagnosis on Radiology Images",
        "url": "http://arxiv.org/abs/2312.16151v1",
        "pub_date": "2023-12-26",
        "summary": "In this study, we aim to investigate the problem of large-scale,\nlarge-vocabulary disease classification for radiologic images, which can be\nformulated as a multi-modal, multi-anatomy, multi-label, long-tailed\nclassification. Our main contributions are three folds: (i), on dataset\nconstruction, we build up an academically accessible, large-scale diagnostic\ndataset that encompasses 5568 disorders linked with 930 unique ICD-10-CM codes,\ncontaining 39,026 cases (192,675 scans). (ii), on model design, we present a\nnovel architecture that enables to process arbitrary number of input scans,\nfrom various imaging modalities, which is trained with knowledge enhancement to\nleverage the rich domain knowledge; (iii), on evaluation, we initialize a new\nbenchmark for multi-modal multi-anatomy long-tailed diagnosis. Our method shows\nsuperior results on it. Additionally, our final model serves as a pre-trained\nmodel, and can be finetuned to benefit diagnosis on various external datasets.",
        "translated": ""
    },
    {
        "title": "One-dimensional Adapter to Rule Them All: Concepts, Diffusion Models and\n  Erasing Applications",
        "url": "http://arxiv.org/abs/2312.16145v1",
        "pub_date": "2023-12-26",
        "summary": "The prevalent use of commercial and open-source diffusion models (DMs) for\ntext-to-image generation prompts risk mitigation to prevent undesired\nbehaviors. Existing concept erasing methods in academia are all based on full\nparameter or specification-based fine-tuning, from which we observe the\nfollowing issues: 1) Generation alternation towards erosion: Parameter drift\nduring target elimination causes alternations and potential deformations across\nall generations, even eroding other concepts at varying degrees, which is more\nevident with multi-concept erased; 2) Transfer inability &amp; deployment\ninefficiency: Previous model-specific erasure impedes the flexible combination\nof concepts and the training-free transfer towards other models, resulting in\nlinear cost growth as the deployment scenarios increase. To achieve\nnon-invasive, precise, customizable, and transferable elimination, we ground\nour erasing framework on one-dimensional adapters to erase multiple concepts\nfrom most DMs at once across versatile erasing applications. The\nconcept-SemiPermeable structure is injected as a Membrane (SPM) into any DM to\nlearn targeted erasing, and meantime the alteration and erosion phenomenon is\neffectively mitigated via a novel Latent Anchoring fine-tuning strategy. Once\nobtained, SPMs can be flexibly combined and plug-and-play for other DMs without\nspecific re-tuning, enabling timely and efficient adaptation to diverse\nscenarios. During generation, our Facilitated Transport mechanism dynamically\nregulates the permeability of each SPM to respond to different input prompts,\nfurther minimizing the impact on other concepts. Quantitative and qualitative\nresults across ~40 concepts, 7 DMs and 4 erasing applications have demonstrated\nthe superior erasing of SPM. Our code and pre-tuned SPMs will be available on\nthe project page https://lyumengyao.github.io/projects/spm.",
        "translated": ""
    },
    {
        "title": "VirtualPainting: Addressing Sparsity with Virtual Points and\n  Distance-Aware Data Augmentation for 3D Object Detection",
        "url": "http://arxiv.org/abs/2312.16141v1",
        "pub_date": "2023-12-26",
        "summary": "In recent times, there has been a notable surge in multimodal approaches that\ndecorates raw LiDAR point clouds with camera-derived features to improve object\ndetection performance. However, we found that these methods still grapple with\nthe inherent sparsity of LiDAR point cloud data, primarily because fewer points\nare enriched with camera-derived features for sparsely distributed objects. We\npresent an innovative approach that involves the generation of virtual LiDAR\npoints using camera images and enhancing these virtual points with semantic\nlabels obtained from image-based segmentation networks to tackle this issue and\nfacilitate the detection of sparsely distributed objects, particularly those\nthat are occluded or distant. Furthermore, we integrate a distance aware data\naugmentation (DADA) technique to enhance the models capability to recognize\nthese sparsely distributed objects by generating specialized training samples.\nOur approach offers a versatile solution that can be seamlessly integrated into\nvarious 3D frameworks and 2D semantic segmentation methods, resulting in\nsignificantly improved overall detection accuracy. Evaluation on the KITTI and\nnuScenes datasets demonstrates substantial enhancements in both 3D and birds\neye view (BEV) detection benchmarks",
        "translated": ""
    },
    {
        "title": "Quantum-Hybrid Stereo Matching With Nonlinear Regularization and Spatial\n  Pyramids",
        "url": "http://arxiv.org/abs/2312.16118v1",
        "pub_date": "2023-12-26",
        "summary": "Quantum visual computing is advancing rapidly. This paper presents a new\nformulation for stereo matching with nonlinear regularizers and spatial\npyramids on quantum annealers as a maximum a posteriori inference problem that\nminimizes the energy of a Markov Random Field. Our approach is hybrid (i.e.,\nquantum-classical) and is compatible with modern D-Wave quantum annealers,\ni.e., it includes a quadratic unconstrained binary optimization (QUBO)\nobjective. Previous quantum annealing techniques for stereo matching are\nlimited to using linear regularizers, and thus, they do not exploit the\nfundamental advantages of the quantum computing paradigm in solving\ncombinatorial optimization problems. In contrast, our method utilizes the full\npotential of quantum annealing for stereo matching, as nonlinear regularizers\ncreate optimization problems which are NP-hard. On the Middlebury benchmark, we\nachieve an improved root mean squared accuracy over the previous state of the\nart in quantum stereo matching of 2% and 22.5% when using different solvers.",
        "translated": ""
    },
    {
        "title": "fMPI: Fast Novel View Synthesis in the Wild with Layered Scene\n  Representations",
        "url": "http://arxiv.org/abs/2312.16109v1",
        "pub_date": "2023-12-26",
        "summary": "In this study, we propose two novel input processing paradigms for novel view\nsynthesis (NVS) methods based on layered scene representations that\nsignificantly improve their runtime without compromising quality. Our approach\nidentifies and mitigates the two most time-consuming aspects of traditional\npipelines: building and processing the so-called plane sweep volume (PSV),\nwhich is a high-dimensional tensor of planar re-projections of the input camera\nviews. In particular, we propose processing this tensor in parallel groups for\nimproved compute efficiency as well as super-sampling adjacent input planes to\ngenerate denser, and hence more accurate scene representation. The proposed\nenhancements offer significant flexibility, allowing for a balance between\nperformance and speed, thus making substantial steps toward real-time\napplications. Furthermore, they are very general in the sense that any\nPSV-based method can make use of them, including methods that employ multiplane\nimages, multisphere images, and layered depth images. In a comprehensive set of\nexperiments, we demonstrate that our proposed paradigms enable the design of an\nNVS method that achieves state-of-the-art on public benchmarks while being up\nto $50x$ faster than existing state-of-the-art methods. It also beats the\ncurrent forerunner in terms of speed by over $3x$, while achieving\nsignificantly better rendering quality.",
        "translated": ""
    },
    {
        "title": "LaneSegNet: Map Learning with Lane Segment Perception for Autonomous\n  Driving",
        "url": "http://arxiv.org/abs/2312.16108v1",
        "pub_date": "2023-12-26",
        "summary": "A map, as crucial information for downstream applications of an autonomous\ndriving system, is usually represented in lanelines or centerlines. However,\nexisting literature on map learning primarily focuses on either detecting\ngeometry-based lanelines or perceiving topology relationships of centerlines.\nBoth of these methods ignore the intrinsic relationship of lanelines and\ncenterlines, that lanelines bind centerlines. While simply predicting both\ntypes of lane in one model is mutually excluded in learning objective, we\nadvocate lane segment as a new representation that seamlessly incorporates both\ngeometry and topology information. Thus, we introduce LaneSegNet, the first\nend-to-end mapping network generating lane segments to obtain a complete\nrepresentation of the road structure. Our algorithm features two key\nmodifications. One is a lane attention module to capture pivotal region details\nwithin the long-range feature space. Another is an identical initialization\nstrategy for reference points, which enhances the learning of positional priors\nfor lane attention. On the OpenLane-V2 dataset, LaneSegNet outperforms previous\ncounterparts by a substantial gain across three tasks, \\textit{i.e.}, map\nelement detection (+4.8 mAP), centerline perception (+6.9 DET$_l$), and the\nnewly defined one, lane segment perception (+5.6 mAP). Furthermore, it obtains\na real-time inference speed of 14.7 FPS. Code is accessible at\nhttps://github.com/OpenDriveLab/LaneSegNet.",
        "translated": ""
    },
    {
        "title": "LangSplat: 3D Language Gaussian Splatting",
        "url": "http://arxiv.org/abs/2312.16084v1",
        "pub_date": "2023-12-26",
        "summary": "Human lives in a 3D world and commonly uses natural language to interact with\na 3D scene. Modeling a 3D language field to support open-ended language queries\nin 3D has gained increasing attention recently. This paper introduces\nLangSplat, which constructs a 3D language field that enables precise and\nefficient open-vocabulary querying within 3D spaces. Unlike existing methods\nthat ground CLIP language embeddings in a NeRF model, LangSplat advances the\nfield by utilizing a collection of 3D Gaussians, each encoding language\nfeatures distilled from CLIP, to represent the language field. By employing a\ntile-based splatting technique for rendering language features, we circumvent\nthe costly rendering process inherent in NeRF. Instead of directly learning\nCLIP embeddings, LangSplat first trains a scene-wise language autoencoder and\nthen learns language features on the scene-specific latent space, thereby\nalleviating substantial memory demands imposed by explicit modeling. Existing\nmethods struggle with imprecise and vague 3D language fields, which fail to\ndiscern clear boundaries between objects. We delve into this issue and propose\nto learn hierarchical semantics using SAM, thereby eliminating the need for\nextensively querying the language field across various scales and the\nregularization of DINO features. Extensive experiments on open-vocabulary 3D\nobject localization and semantic segmentation demonstrate that LangSplat\nsignificantly outperforms the previous state-of-the-art method LERF by a large\nmargin. Notably, LangSplat is extremely efficient, achieving a {\\speed}\n$\\times$ speedup compared to LERF at the resolution of 1440 $\\times$ 1080. We\nstrongly recommend readers to check out our video results at\nhttps://langsplat.github.io",
        "translated": ""
    },
    {
        "title": "Inter-X: Towards Versatile Human-Human Interaction Analysis",
        "url": "http://arxiv.org/abs/2312.16051v1",
        "pub_date": "2023-12-26",
        "summary": "The analysis of the ubiquitous human-human interactions is pivotal for\nunderstanding humans as social beings. Existing human-human interaction\ndatasets typically suffer from inaccurate body motions, lack of hand gestures\nand fine-grained textual descriptions. To better perceive and generate\nhuman-human interactions, we propose Inter-X, a currently largest human-human\ninteraction dataset with accurate body movements and diverse interaction\npatterns, together with detailed hand gestures. The dataset includes ~11K\ninteraction sequences and more than 8.1M frames. We also equip Inter-X with\nversatile annotations of more than 34K fine-grained human part-level textual\ndescriptions, semantic interaction categories, interaction order, and the\nrelationship and personality of the subjects. Based on the elaborate\nannotations, we propose a unified benchmark composed of 4 categories of\ndownstream tasks from both the perceptual and generative directions. Extensive\nexperiments and comprehensive analysis show that Inter-X serves as a testbed\nfor promoting the development of versatile human-human interaction analysis.\nOur dataset and benchmark will be publicly available for research purposes.",
        "translated": ""
    },
    {
        "title": "Multiscale Vision Transformers meet Bipartite Matching for efficient\n  single-stage Action Localization",
        "url": "http://arxiv.org/abs/2312.17686v1",
        "pub_date": "2023-12-29",
        "summary": "Action Localization is a challenging problem that combines detection and\nrecognition tasks, which are often addressed separately. State-of-the-art\nmethods rely on off-the-shelf bounding box detections pre-computed at high\nresolution and propose transformer models that focus on the classification task\nalone. Such two-stage solutions are prohibitive for real-time deployment. On\nthe other hand, single-stage methods target both tasks by devoting part of the\nnetwork (generally the backbone) to sharing the majority of the workload,\ncompromising performance for speed. These methods build on adding a DETR head\nwith learnable queries that, after cross- and self-attention can be sent to\ncorresponding MLPs for detecting a person's bounding box and action. However,\nDETR-like architectures are challenging to train and can incur in big\ncomplexity.\n  In this paper, we observe that a straight bipartite matching loss can be\napplied to the output tokens of a vision transformer. This results in a\nbackbone + MLP architecture that can do both tasks without the need of an extra\nencoder-decoder head and learnable queries. We show that a single MViT-S\narchitecture trained with bipartite matching to perform both tasks surpasses\nthe same MViT-S when trained with RoI align on pre-computed bounding boxes.\nWith a careful design of token pooling and the proposed training pipeline, our\nMViTv2-S model achieves +3 mAP on AVA2.2. w.r.t. the two-stage counterpart.\nCode and models will be released after paper revision.",
        "translated": ""
    },
    {
        "title": "FlowVid: Taming Imperfect Optical Flows for Consistent Video-to-Video\n  Synthesis",
        "url": "http://arxiv.org/abs/2312.17681v1",
        "pub_date": "2023-12-29",
        "summary": "Diffusion models have transformed the image-to-image (I2I) synthesis and are\nnow permeating into videos. However, the advancement of video-to-video (V2V)\nsynthesis has been hampered by the challenge of maintaining temporal\nconsistency across video frames. This paper proposes a consistent V2V synthesis\nframework by jointly leveraging spatial conditions and temporal optical flow\nclues within the source video. Contrary to prior methods that strictly adhere\nto optical flow, our approach harnesses its benefits while handling the\nimperfection in flow estimation. We encode the optical flow via warping from\nthe first frame and serve it as a supplementary reference in the diffusion\nmodel. This enables our model for video synthesis by editing the first frame\nwith any prevalent I2I models and then propagating edits to successive frames.\nOur V2V model, FlowVid, demonstrates remarkable properties: (1) Flexibility:\nFlowVid works seamlessly with existing I2I models, facilitating various\nmodifications, including stylization, object swaps, and local edits. (2)\nEfficiency: Generation of a 4-second video with 30 FPS and 512x512 resolution\ntakes only 1.5 minutes, which is 3.1x, 7.2x, and 10.5x faster than CoDeF,\nRerender, and TokenFlow, respectively. (3) High-quality: In user studies, our\nFlowVid is preferred 45.7% of the time, outperforming CoDeF (3.5%), Rerender\n(10.2%), and TokenFlow (40.4%).",
        "translated": ""
    },
    {
        "title": "Benchmarking the CoW with the TopCoW Challenge: Topology-Aware\n  Anatomical Segmentation of the Circle of Willis for CTA and MRA",
        "url": "http://arxiv.org/abs/2312.17670v1",
        "pub_date": "2023-12-29",
        "summary": "The Circle of Willis (CoW) is an important network of arteries connecting\nmajor circulations of the brain. Its vascular architecture is believed to\naffect the risk, severity, and clinical outcome of serious neuro-vascular\ndiseases. However, characterizing the highly variable CoW anatomy is still a\nmanual and time-consuming expert task. The CoW is usually imaged by two\nangiographic imaging modalities, magnetic resonance angiography (MRA) and\ncomputed tomography angiography (CTA), but there exist limited public datasets\nwith annotations on CoW anatomy, especially for CTA. Therefore we organized the\nTopCoW Challenge in 2023 with the release of an annotated CoW dataset and\ninvited submissions worldwide for the CoW segmentation task, which attracted\nover 140 registered participants from four continents. TopCoW dataset was the\nfirst public dataset with voxel-level annotations for CoW's 13 vessel\ncomponents, made possible by virtual-reality (VR) technology. It was also the\nfirst dataset with paired MRA and CTA from the same patients. TopCoW challenge\naimed to tackle the CoW characterization problem as a multiclass anatomical\nsegmentation task with an emphasis on topological metrics. The top performing\nteams managed to segment many CoW components to Dice scores around 90%, but\nwith lower scores for communicating arteries and rare variants. There were also\ntopological mistakes for predictions with high Dice scores. Additional\ntopological analysis revealed further areas for improvement in detecting\ncertain CoW components and matching CoW variant's topology accurately. TopCoW\nrepresented a first attempt at benchmarking the CoW anatomical segmentation\ntask for MRA and CTA, both morphologically and topologically.",
        "translated": ""
    },
    {
        "title": "Shape-IoU: More Accurate Metric considering Bounding Box Shape and Scale",
        "url": "http://arxiv.org/abs/2312.17663v1",
        "pub_date": "2023-12-29",
        "summary": "As an important component of the detector localization branch, bounding box\nregression loss plays a significant role in object detection tasks. The\nexisting bounding box regression methods usually consider the geometric\nrelationship between the GT box and the predicted box, and calculate the loss\nby using the relative position and shape of the bounding boxes, while ignoring\nthe influence of inherent properties such as the shape and scale of the\nbounding boxes on bounding box regression. In order to make up for the\nshortcomings of existing research, this article proposes a bounding box\nregression method that focuses on the shape and scale of the bounding box\nitself. Firstly, we analyzed the regression characteristics of the bounding\nboxes and found that the shape and scale factors of the bounding boxes\nthemselves will have an impact on the regression results. Based on the above\nconclusions, we propose the Shape IoU method, which can calculate the loss by\nfocusing on the shape and scale of the bounding box itself, thereby making the\nbounding box regression more accurate. Finally, we validated our method through\na large number of comparative experiments, which showed that our method can\neffectively improve detection performance and outperform existing methods,\nachieving state-of-the-art performance in different detection tasks.Code is\navailable at https://github.com/malagoutou/Shape-IoU",
        "translated": ""
    },
    {
        "title": "Gemini in Reasoning: Unveiling Commonsense in Multimodal Large Language\n  Models",
        "url": "http://arxiv.org/abs/2312.17661v1",
        "pub_date": "2023-12-29",
        "summary": "The burgeoning interest in Multimodal Large Language Models (MLLMs), such as\nOpenAI's GPT-4V(ision), has significantly impacted both academic and industrial\nrealms. These models enhance Large Language Models (LLMs) with advanced visual\nunderstanding capabilities, facilitating their application in a variety of\nmultimodal tasks. Recently, Google introduced Gemini, a cutting-edge MLLM\ndesigned specifically for multimodal integration. Despite its advancements,\npreliminary benchmarks indicate that Gemini lags behind GPT models in\ncommonsense reasoning tasks. However, this assessment, based on a limited\ndataset (i.e., HellaSWAG), does not fully capture Gemini's authentic\ncommonsense reasoning potential. To address this gap, our study undertakes a\nthorough evaluation of Gemini's performance in complex reasoning tasks that\nnecessitate the integration of commonsense knowledge across modalities. We\ncarry out a comprehensive analysis of 12 commonsense reasoning datasets,\nranging from general to domain-specific tasks. This includes 11 datasets\nfocused solely on language, as well as one that incorporates multimodal\nelements. Our experiments across four LLMs and two MLLMs demonstrate Gemini's\ncompetitive commonsense reasoning capabilities. Additionally, we identify\ncommon challenges faced by current LLMs and MLLMs in addressing commonsense\nproblems, underscoring the need for further advancements in enhancing the\ncommonsense reasoning abilities of these models.",
        "translated": ""
    },
    {
        "title": "Visual Point Cloud Forecasting enables Scalable Autonomous Driving",
        "url": "http://arxiv.org/abs/2312.17655v1",
        "pub_date": "2023-12-29",
        "summary": "In contrast to extensive studies on general vision, pre-training for scalable\nvisual autonomous driving remains seldom explored. Visual autonomous driving\napplications require features encompassing semantics, 3D geometry, and temporal\ninformation simultaneously for joint perception, prediction, and planning,\nposing dramatic challenges for pre-training. To resolve this, we bring up a new\npre-training task termed as visual point cloud forecasting - predicting future\npoint clouds from historical visual input. The key merit of this task captures\nthe synergic learning of semantics, 3D structures, and temporal dynamics. Hence\nit shows superiority in various downstream tasks. To cope with this new\nproblem, we present ViDAR, a general model to pre-train downstream visual\nencoders. It first extracts historical embeddings by the encoder. These\nrepresentations are then transformed to 3D geometric space via a novel Latent\nRendering operator for future point cloud prediction. Experiments show\nsignificant gain in downstream tasks, e.g., 3.1% NDS on 3D detection, ~10%\nerror reduction on motion forecasting, and ~15% less collision rate on\nplanning.",
        "translated": ""
    },
    {
        "title": "Bridging Modality Gap for Visual Grounding with Effecitve Cross-modal\n  Distillation",
        "url": "http://arxiv.org/abs/2312.17648v1",
        "pub_date": "2023-12-29",
        "summary": "Visual grounding aims to align visual information of specific regions of\nimages with corresponding natural language expressions. Current visual\ngrounding methods leverage pre-trained visual and language backbones separately\nto obtain visual features and linguistic features. Although these two types of\nfeatures are then fused via delicately designed networks, the heterogeneity of\nthe features makes them inapplicable for multi-modal reasoning. This problem\narises from the domain gap between the single-modal pre-training backbone used\nin current visual grounding methods, which can hardly be overcome by the\ntraditional end-to-end training method. To alleviate this, our work proposes an\nEmpowering pre-trained model for Visual Grounding (EpmVG) framework, which\ndistills a multimodal pre-trained model to guide the visual grounding task.\nEpmVG is based on a novel cross-modal distillation mechanism, which can\neffectively introduce the consistency information of images and texts in the\npre-trained model, to reduce the domain gap existing in the backbone networks,\nthereby improving the performance of the model in the visual grounding task.\nExtensive experiments are carried out on five conventionally used datasets, and\nresults demonstrate that our method achieves better performance than\nstate-of-the-art methods.",
        "translated": ""
    },
    {
        "title": "Research on the Laws of Multimodal Perception and Cognition from a\n  Cross-cultural Perspective -- Taking Overseas Chinese Gardens as an Example",
        "url": "http://arxiv.org/abs/2312.17642v1",
        "pub_date": "2023-12-29",
        "summary": "This study aims to explore the complex relationship between perceptual and\ncognitive interactions in multimodal data analysis,with a specific emphasis on\nspatial experience design in overseas Chinese gardens. It is found that\nevaluation content and images on social media can reflect individuals' concerns\nand sentiment responses, providing a rich data base for cognitive research that\ncontains both sentimental and image-based cognitive information. Leveraging\ndeep learning techniques, we analyze textual and visual data from social media,\nthereby unveiling the relationship between people's perceptions and sentiment\ncognition within the context of overseas Chinese gardens. In addition, our\nstudy introduces a multi-agent system (MAS)alongside AI agents. Each agent\nexplores the laws of aesthetic cognition through chat scene simulation combined\nwith web search. This study goes beyond the traditional approach of translating\nperceptions into sentiment scores, allowing for an extension of the research\nmethodology in terms of directly analyzing texts and digging deeper into\nopinion data. This study provides new perspectives for understanding aesthetic\nexperience and its impact on architecture and landscape design across diverse\ncultural contexts, which is an essential contribution to the field of cultural\ncommunication and aesthetic understanding.",
        "translated": ""
    },
    {
        "title": "MoD2T:Model-Data-Driven Motion-Static Object Tracking Method",
        "url": "http://arxiv.org/abs/2312.17641v1",
        "pub_date": "2023-12-29",
        "summary": "The domain of Multi-Object Tracking (MOT) is of paramount significance within\nthe realm of video analysis. However, both traditional methodologies and deep\nlearning-based approaches within this domain exhibit inherent limitations. Deep\nlearning methods driven exclusively by data exhibit challenges in accurately\ndiscerning the motion states of objects, while traditional methods relying on\ncomprehensive mathematical models may suffer from suboptimal tracking\nprecision. To address these challenges, we introduce the Model-Data-Driven\nMotion-Static Object Tracking Method (MoD2T). We propose a novel architecture\nthat adeptly amalgamates traditional mathematical modeling with deep\nlearning-based MOT frameworks, thereby effectively mitigating the limitations\nassociated with sole reliance on established methodologies or advanced deep\nlearning techniques. MoD2T's fusion of mathematical modeling and deep learning\naugments the precision of object motion determination, consequently enhancing\ntracking accuracy. Our empirical experiments robustly substantiate MoD2T's\nefficacy across a diverse array of scenarios, including UAV aerial surveillance\nand street-level tracking. To assess MoD2T's proficiency in discerning object\nmotion states, we introduce MVF1 metric. This novel performance metric is\ndesigned to measure the accuracy of motion state classification, providing a\ncomprehensive evaluation of MoD2T's performance. Meticulous experiments\nsubstantiate the rationale behind MVF1's formulation. To provide a\ncomprehensive assessment of MoD2T's performance, we meticulously annotate\ndiverse datasets and subject MoD2T to rigorous testing. The achieved MVF1\nscores, which measure the accuracy of motion state classification, are\nparticularly noteworthy in scenarios marked by minimal or mild camera motion,\nwith values of 0.774 on the KITTI dataset, 0.521 on MOT17, and 0.827 on UAVDT.",
        "translated": ""
    },
    {
        "title": "One-Shot Multi-Rate Pruning of Graph Convolutional Networks",
        "url": "http://arxiv.org/abs/2312.17615v1",
        "pub_date": "2023-12-29",
        "summary": "In this paper, we devise a novel lightweight Graph Convolutional Network\n(GCN) design dubbed as Multi-Rate Magnitude Pruning (MRMP) that jointly trains\nnetwork topology and weights. Our method is variational and proceeds by\naligning the weight distribution of the learned networks with an a priori\ndistribution. In the one hand, this allows implementing any fixed pruning rate,\nand also enhancing the generalization performances of the designed lightweight\nGCNs. In the other hand, MRMP achieves a joint training of multiple GCNs, on\ntop of shared weights, in order to extrapolate accurate networks at any\ntargeted pruning rate without retraining their weights. Extensive experiments\nconducted on the challenging task of skeleton-based recognition show a\nsubstantial gain of our lightweight GCNs particularly at very high pruning\nregimes.",
        "translated": ""
    },
    {
        "title": "Street Gaussians for Modeling Dynamic Urban Scenes",
        "url": "http://arxiv.org/abs/2401.01339v1",
        "pub_date": "2024-01-02",
        "summary": "This paper aims to tackle the problem of modeling dynamic urban street scenes\nfrom monocular videos. Recent methods extend NeRF by incorporating tracked\nvehicle poses to animate vehicles, enabling photo-realistic view synthesis of\ndynamic urban street scenes. However, significant limitations are their slow\ntraining and rendering speed, coupled with the critical need for high precision\nin tracked vehicle poses. We introduce Street Gaussians, a new explicit scene\nrepresentation that tackles all these limitations. Specifically, the dynamic\nurban street is represented as a set of point clouds equipped with semantic\nlogits and 3D Gaussians, each associated with either a foreground vehicle or\nthe background. To model the dynamics of foreground object vehicles, each\nobject point cloud is optimized with optimizable tracked poses, along with a\ndynamic spherical harmonics model for the dynamic appearance. The explicit\nrepresentation allows easy composition of object vehicles and background, which\nin turn allows for scene editing operations and rendering at 133 FPS\n(1066$\\times$1600 resolution) within half an hour of training. The proposed\nmethod is evaluated on multiple challenging benchmarks, including KITTI and\nWaymo Open datasets. Experiments show that the proposed method consistently\noutperforms state-of-the-art methods across all datasets. Furthermore, the\nproposed representation delivers performance on par with that achieved using\nprecise ground-truth poses, despite relying only on poses from an off-the-shelf\ntracker. The code is available at https://zju3dv.github.io/street_gaussians/.",
        "translated": ""
    },
    {
        "title": "Integrating Edges into U-Net Models with Explainable Activation Maps for\n  Brain Tumor Segmentation using MR Images",
        "url": "http://arxiv.org/abs/2401.01303v1",
        "pub_date": "2024-01-02",
        "summary": "Manual delineation of tumor regions from magnetic resonance (MR) images is\ntime-consuming, requires an expert, and is prone to human error. In recent\nyears, deep learning models have been the go-to approach for the segmentation\nof brain tumors. U-Net and its' variants for semantic segmentation of medical\nimages have achieved good results in the literature. However, U-Net and its'\nvariants tend to over-segment tumor regions and may not accurately segment the\ntumor edges. The edges of the tumor are as important as the tumor regions for\naccurate diagnosis, surgical precision, and treatment planning. In the proposed\nwork, the authors aim to extract edges from the ground truth using a\nderivative-like filter followed by edge reconstruction to obtain an edge ground\ntruth in addition to the brain tumor ground truth. Utilizing both ground\ntruths, the author studies several U-Net and its' variant architectures with\nand without tumor edges ground truth as a target along with the tumor ground\ntruth for brain tumor segmentation. The author used the BraTS2020 benchmark\ndataset to perform the study and the results are tabulated for the dice and\nHausdorff95 metrics. The mean and median metrics are calculated for the whole\ntumor (WT), tumor core (TC), and enhancing tumor (ET) regions. Compared to the\nbaseline U-Net and its variants, the models that learned edges along with the\ntumor regions performed well in core tumor regions in both training and\nvalidation datasets. The improved performance of edge-trained models trained on\nbaseline models like U-Net and V-Net achieved performance similar to baseline\nstate-of-the-art models like Swin U-Net and hybrid MR-U-Net. The edge-target\ntrained models are capable of generating edge maps that can be useful for\ntreatment planning. Additionally, for further explainability of the results,\nthe activation map generated by the hybrid MR-U-Net has been studied.",
        "translated": ""
    },
    {
        "title": "Physics-informed Generalizable Wireless Channel Modeling with\n  Segmentation and Deep Learning: Fundamentals, Methodologies, and Challenges",
        "url": "http://arxiv.org/abs/2401.01288v1",
        "pub_date": "2024-01-02",
        "summary": "Channel modeling is fundamental in advancing wireless systems and has thus\nattracted considerable research focus. Recent trends have seen a growing\nreliance on data-driven techniques to facilitate the modeling process and yield\naccurate channel predictions. In this work, we first provide a concise overview\nof data-driven channel modeling methods, highlighting their limitations.\nSubsequently, we introduce the concept and advantages of physics-informed\nneural network (PINN)-based modeling and a summary of recent contributions in\nthis area. Our findings demonstrate that PINN-based approaches in channel\nmodeling exhibit promising attributes such as generalizability,\ninterpretability, and robustness. We offer a comprehensive architecture for\nPINN methodology, designed to inform and inspire future model development. A\ncase-study of our recent work on precise indoor channel prediction with\nsemantic segmentation and deep learning is presented. The study concludes by\naddressing the challenges faced and suggesting potential research directions in\nthis field.",
        "translated": ""
    },
    {
        "title": "A Comprehensive Study of Knowledge Editing for Large Language Models",
        "url": "http://arxiv.org/abs/2401.01286v1",
        "pub_date": "2024-01-02",
        "summary": "Large Language Models (LLMs) have shown extraordinary capabilities in\nunderstanding and generating text that closely mirrors human communication.\nHowever, a primary limitation lies in the significant computational demands\nduring training, arising from their extensive parameterization. This challenge\nis further intensified by the dynamic nature of the world, necessitating\nfrequent updates to LLMs to correct outdated information or integrate new\nknowledge, thereby ensuring their continued relevance. Note that many\napplications demand continual model adjustments post-training to address\ndeficiencies or undesirable behaviors. There is an increasing interest in\nefficient, lightweight methods for on-the-fly model modifications. To this end,\nrecent years have seen a burgeoning in the techniques of knowledge editing for\nLLMs, which aim to efficiently modify LLMs' behaviors within specific domains\nwhile preserving overall performance across various inputs. In this paper, we\nfirst define the knowledge editing problem and then provide a comprehensive\nreview of cutting-edge approaches. Drawing inspiration from educational and\ncognitive research theories, we propose a unified categorization criterion that\nclassifies knowledge editing methods into three groups: resorting to external\nknowledge, merging knowledge into the model, and editing intrinsic knowledge.\nFurthermore, we introduce a new benchmark, KnowEdit, for a comprehensive\nempirical evaluation of representative knowledge editing approaches.\nAdditionally, we provide an in-depth analysis of knowledge location, which can\nprovide a deeper understanding of the knowledge structures inherent within\nLLMs. Finally, we discuss several potential applications of knowledge editing,\noutlining its broad and impactful implications.",
        "translated": ""
    },
    {
        "title": "MOC-RVQ: Multilevel Codebook-assisted Digital Generative Semantic\n  Communication",
        "url": "http://arxiv.org/abs/2401.01272v1",
        "pub_date": "2024-01-02",
        "summary": "Vector quantization-based image semantic communication systems have\nsuccessfully boosted transmission efficiency, but face a challenge with\nconflicting requirements between codebook design and digital constellation\nmodulation. Traditional codebooks need a wide index range, while modulation\nfavors few discrete states. To address this, we propose a multilevel generative\nsemantic communication system with a two-stage training framework. In the first\nstage, we train a high-quality codebook, using a multi-head octonary codebook\n(MOC) to compress the index range. We also integrate a residual vector\nquantization (RVQ) mechanism for effective multilevel communication. In the\nsecond stage, a noise reduction block (NRB) based on Swin Transformer is\nintroduced, coupled with the multilevel codebook from the first stage, serving\nas a high-quality semantic knowledge base (SKB) for generative feature\nrestoration. Experimental results highlight MOC-RVQ's superior performance over\nmethods like BPG or JPEG, even without channel error correction coding.",
        "translated": ""
    },
    {
        "title": "VideoDrafter: Content-Consistent Multi-Scene Video Generation with LLM",
        "url": "http://arxiv.org/abs/2401.01256v1",
        "pub_date": "2024-01-02",
        "summary": "The recent innovations and breakthroughs in diffusion models have\nsignificantly expanded the possibilities of generating high-quality videos for\nthe given prompts. Most existing works tackle the single-scene scenario with\nonly one video event occurring in a single background. Extending to generate\nmulti-scene videos nevertheless is not trivial and necessitates to nicely\nmanage the logic in between while preserving the consistent visual appearance\nof key content across video scenes. In this paper, we propose a novel\nframework, namely VideoDrafter, for content-consistent multi-scene video\ngeneration. Technically, VideoDrafter leverages Large Language Models (LLM) to\nconvert the input prompt into comprehensive multi-scene script that benefits\nfrom the logical knowledge learnt by LLM. The script for each scene includes a\nprompt describing the event, the foreground/background entities, as well as\ncamera movement. VideoDrafter identifies the common entities throughout the\nscript and asks LLM to detail each entity. The resultant entity description is\nthen fed into a text-to-image model to generate a reference image for each\nentity. Finally, VideoDrafter outputs a multi-scene video by generating each\nscene video via a diffusion process that takes the reference images, the\ndescriptive prompt of the event and camera movement into account. The diffusion\nmodel incorporates the reference images as the condition and alignment to\nstrengthen the content consistency of multi-scene videos. Extensive experiments\ndemonstrate that VideoDrafter outperforms the SOTA video generation models in\nterms of visual quality, content consistency, and user preference.",
        "translated": ""
    },
    {
        "title": "Deep Learning-Based Computational Model for Disease Identification in\n  Cocoa Pods (Theobroma cacao L.)",
        "url": "http://arxiv.org/abs/2401.01247v1",
        "pub_date": "2024-01-02",
        "summary": "The early identification of diseases in cocoa pods is an important task to\nguarantee the production of high-quality cocoa. The use of artificial\nintelligence techniques such as machine learning, computer vision and deep\nlearning are promising solutions to help identify and classify diseases in\ncocoa pods. In this paper we introduce the development and evaluation of a deep\nlearning computational model applied to the identification of diseases in cocoa\npods, focusing on \"monilia\" and \"black pod\" diseases. An exhaustive review of\nstate-of-the-art of computational models was carried out, based on scientific\narticles related to the identification of plant diseases using computer vision\nand deep learning techniques. As a result of the search, EfficientDet-Lite4, an\nefficient and lightweight model for object detection, was selected. A dataset,\nincluding images of both healthy and diseased cocoa pods, has been utilized to\ntrain the model to detect and pinpoint disease manifestations with considerable\naccuracy. Significant enhancements in the model training and evaluation\ndemonstrate the capability of recognizing and classifying diseases through\nimage analysis. Furthermore, the functionalities of the model were integrated\ninto an Android native mobile with an user-friendly interface, allowing to\nyounger or inexperienced farmers a fast and accuracy identification of health\nstatus of cocoa pods",
        "translated": ""
    },
    {
        "title": "Temporal Adaptive RGBT Tracking with Modality Prompt",
        "url": "http://arxiv.org/abs/2401.01244v1",
        "pub_date": "2024-01-02",
        "summary": "RGBT tracking has been widely used in various fields such as robotics,\nsurveillance processing, and autonomous driving. Existing RGBT trackers fully\nexplore the spatial information between the template and the search region and\nlocate the target based on the appearance matching results. However, these RGBT\ntrackers have very limited exploitation of temporal information, either\nignoring temporal information or exploiting it through online sampling and\ntraining. The former struggles to cope with the object state changes, while the\nlatter neglects the correlation between spatial and temporal information. To\nalleviate these limitations, we propose a novel Temporal Adaptive RGBT Tracking\nframework, named as TATrack. TATrack has a spatio-temporal two-stream structure\nand captures temporal information by an online updated template, where the\ntwo-stream structure refers to the multi-modal feature extraction and\ncross-modal interaction for the initial template and the online update template\nrespectively. TATrack contributes to comprehensively exploit spatio-temporal\ninformation and multi-modal information for target localization. In addition,\nwe design a spatio-temporal interaction (STI) mechanism that bridges two\nbranches and enables cross-modal interaction to span longer time scales.\nExtensive experiments on three popular RGBT tracking benchmarks show that our\nmethod achieves state-of-the-art performance, while running at real-time speed.",
        "translated": ""
    },
    {
        "title": "IdentiFace : A VGG Based Multimodal Facial Biometric System",
        "url": "http://arxiv.org/abs/2401.01227v1",
        "pub_date": "2024-01-02",
        "summary": "The development of facial biometric systems has contributed greatly to the\ndevelopment of the computer vision field. Nowadays, there's always a need to\ndevelop a multimodal system that combines multiple biometric traits in an\nefficient, meaningful way. In this paper, we introduce \"IdentiFace\" which is a\nmultimodal facial biometric system that combines the core of facial recognition\nwith some of the most important soft biometric traits such as gender, face\nshape, and emotion. We also focused on developing the system using only VGG-16\ninspired architecture with minor changes across different subsystems. This\nunification allows for simpler integration across modalities. It makes it\neasier to interpret the learned features between the tasks which gives a good\nindication about the decision-making process across the facial modalities and\npotential connection. For the recognition problem, we acquired a 99.2% test\naccuracy for five classes with high intra-class variations using data collected\nfrom the FERET database[1]. We achieved 99.4% on our dataset and 95.15% on the\npublic dataset[2] in the gender recognition problem. We were also able to\nachieve a testing accuracy of 88.03% in the face-shape problem using the\ncelebrity face-shape dataset[3]. Finally, we achieved a decent testing accuracy\nof 66.13% in the emotion task which is considered a very acceptable accuracy\ncompared to related work on the FER2013 dataset[4].",
        "translated": ""
    },
    {
        "title": "Distribution Matching for Multi-Task Learning of Classification Tasks: a\n  Large-Scale Study on Faces &amp; Beyond",
        "url": "http://arxiv.org/abs/2401.01219v1",
        "pub_date": "2024-01-02",
        "summary": "Multi-Task Learning (MTL) is a framework, where multiple related tasks are\nlearned jointly and benefit from a shared representation space, or parameter\ntransfer. To provide sufficient learning support, modern MTL uses annotated\ndata with full, or sufficiently large overlap across tasks, i.e., each input\nsample is annotated for all, or most of the tasks. However, collecting such\nannotations is prohibitive in many real applications, and cannot benefit from\ndatasets available for individual tasks. In this work, we challenge this setup\nand show that MTL can be successful with classification tasks with little, or\nnon-overlapping annotations, or when there is big discrepancy in the size of\nlabeled data per task. We explore task-relatedness for co-annotation and\nco-training, and propose a novel approach, where knowledge exchange is enabled\nbetween the tasks via distribution matching. To demonstrate the general\napplicability of our method, we conducted diverse case studies in the domains\nof affective computing, face recognition, species recognition, and shopping\nitem classification using nine datasets. Our large-scale study of affective\ntasks for basic expression recognition and facial action unit detection\nillustrates that our approach is network agnostic and brings large performance\nimprovements compared to the state-of-the-art in both tasks and across all\nstudied databases. In all case studies, we show that co-training via\ntask-relatedness is advantageous and prevents negative transfer (which occurs\nwhen MT model's performance is worse than that of at least one single-task\nmodel).",
        "translated": ""
    },
    {
        "title": "LEAP-VO: Long-term Effective Any Point Tracking for Visual Odometry",
        "url": "http://arxiv.org/abs/2401.01887v1",
        "pub_date": "2024-01-03",
        "summary": "Visual odometry estimates the motion of a moving camera based on visual\ninput. Existing methods, mostly focusing on two-view point tracking, often\nignore the rich temporal context in the image sequence, thereby overlooking the\nglobal motion patterns and providing no assessment of the full trajectory\nreliability. These shortcomings hinder performance in scenarios with occlusion,\ndynamic objects, and low-texture areas. To address these challenges, we present\nthe Long-term Effective Any Point Tracking (LEAP) module. LEAP innovatively\ncombines visual, inter-track, and temporal cues with mindfully selected anchors\nfor dynamic track estimation. Moreover, LEAP's temporal probabilistic\nformulation integrates distribution updates into a learnable iterative\nrefinement module to reason about point-wise uncertainty. Based on these\ntraits, we develop LEAP-VO, a robust visual odometry system adept at handling\nocclusions and dynamic scenes. Our mindful integration showcases a novel\npractice by employing long-term point tracking as the front-end. Extensive\nexperiments demonstrate that the proposed pipeline significantly outperforms\nexisting baselines across various visual odometry benchmarks.",
        "translated": ""
    },
    {
        "title": "From Audio to Photoreal Embodiment: Synthesizing Humans in Conversations",
        "url": "http://arxiv.org/abs/2401.01885v1",
        "pub_date": "2024-01-03",
        "summary": "We present a framework for generating full-bodied photorealistic avatars that\ngesture according to the conversational dynamics of a dyadic interaction. Given\nspeech audio, we output multiple possibilities of gestural motion for an\nindividual, including face, body, and hands. The key behind our method is in\ncombining the benefits of sample diversity from vector quantization with the\nhigh-frequency details obtained through diffusion to generate more dynamic,\nexpressive motion. We visualize the generated motion using highly\nphotorealistic avatars that can express crucial nuances in gestures (e.g.\nsneers and smirks). To facilitate this line of research, we introduce a\nfirst-of-its-kind multi-view conversational dataset that allows for\nphotorealistic reconstruction. Experiments show our model generates appropriate\nand diverse gestures, outperforming both diffusion- and VQ-only methods.\nFurthermore, our perceptual evaluation highlights the importance of\nphotorealism (vs. meshes) in accurately assessing subtle motion details in\nconversational gestures. Code and dataset available online.",
        "translated": ""
    },
    {
        "title": "Step length measurement in the wild using FMCW radar",
        "url": "http://arxiv.org/abs/2401.01868v1",
        "pub_date": "2024-01-03",
        "summary": "With an aging population, numerous assistive and monitoring technologies are\nunder development to enable older adults to age in place. To facilitate aging\nin place predicting risk factors such as falls, and hospitalization and\nproviding early interventions are important. Much of the work on ambient\nmonitoring for risk prediction has centered on gait speed analysis, utilizing\nprivacy-preserving sensors like radar. Despite compelling evidence that\nmonitoring step length, in addition to gait speed, is crucial for predicting\nrisk, radar-based methods have not explored step length measurement in the\nhome. Furthermore, laboratory experiments on step length measurement using\nradars are limited to proof of concept studies with few healthy subjects. To\naddress this gap, a radar-based step length measurement system for the home is\nproposed based on detection and tracking using radar point cloud, followed by\nDoppler speed profiling of the torso to obtain step lengths in the home. The\nproposed method was evaluated in a clinical environment, involving 35 frail\nolder adults, to establish its validity. Additionally, the method was assessed\nin people's homes, with 21 frail older adults who had participated in the\nclinical assessment. The proposed radar-based step length measurement method\nwas compared to the gold standard Zeno Walkway Gait Analysis System, revealing\na 4.5cm/8.3% error in a clinical setting. Furthermore, it exhibited excellent\nreliability (ICC(2,k)=0.91, 95% CI 0.82 to 0.96) in uncontrolled home settings.\nThe method also proved accurate in uncontrolled home settings, as indicated by\na strong agreement (ICC(3,k)=0.81 (95% CI 0.53 to 0.92)) between home\nmeasurements and in-clinic assessments.",
        "translated": ""
    },
    {
        "title": "A Vision Check-up for Language Models",
        "url": "http://arxiv.org/abs/2401.01862v1",
        "pub_date": "2024-01-03",
        "summary": "What does learning to model relationships between strings teach large\nlanguage models (LLMs) about the visual world? We systematically evaluate LLMs'\nabilities to generate and recognize an assortment of visual concepts of\nincreasing complexity and then demonstrate how a preliminary visual\nrepresentation learning system can be trained using models of text. As language\nmodels lack the ability to consume or output visual information as pixels, we\nuse code to represent images in our study. Although LLM-generated images do not\nlook like natural images, results on image generation and the ability of models\nto correct these generated images indicate that precise modeling of strings can\nteach language models about numerous aspects of the visual world. Furthermore,\nexperiments on self-supervised visual representation learning, utilizing images\ngenerated with text models, highlight the potential to train vision models\ncapable of making semantic assessments of natural images using just LLMs.",
        "translated": ""
    },
    {
        "title": "Synthetic dataset of ID and Travel Document",
        "url": "http://arxiv.org/abs/2401.01858v1",
        "pub_date": "2024-01-03",
        "summary": "This paper presents a new synthetic dataset of ID and travel documents,\ncalled SIDTD. The SIDTD dataset is created to help training and evaluating\nforged ID documents detection systems. Such a dataset has become a necessity as\nID documents contain personal information and a public dataset of real\ndocuments can not be released. Moreover, forged documents are scarce, compared\nto legit ones, and the way they are generated varies from one fraudster to\nanother resulting in a class of high intra-variability. In this paper we\ntrained state-of-the-art models on this dataset and we compare them to the\nperformance achieved in larger, but private, datasets. The creation of this\ndataset will help to document image analysis community to progress in the task\nof ID document verification.",
        "translated": ""
    },
    {
        "title": "Frequency Domain Modality-invariant Feature Learning for\n  Visible-infrared Person Re-Identification",
        "url": "http://arxiv.org/abs/2401.01839v1",
        "pub_date": "2024-01-03",
        "summary": "Visible-infrared person re-identification (VI-ReID) is challenging due to the\nsignificant cross-modality discrepancies between visible and infrared images.\nWhile existing methods have focused on designing complex network architectures\nor using metric learning constraints to learn modality-invariant features, they\noften overlook which specific component of the image causes the modality\ndiscrepancy problem. In this paper, we first reveal that the difference in the\namplitude component of visible and infrared images is the primary factor that\ncauses the modality discrepancy and further propose a novel Frequency Domain\nmodality-invariant feature learning framework (FDMNet) to reduce modality\ndiscrepancy from the frequency domain perspective. Our framework introduces two\nnovel modules, namely the Instance-Adaptive Amplitude Filter (IAF) module and\nthe Phrase-Preserving Normalization (PPNorm) module, to enhance the\nmodality-invariant amplitude component and suppress the modality-specific\ncomponent at both the image- and feature-levels. Extensive experimental results\non two standard benchmarks, SYSU-MM01 and RegDB, demonstrate the superior\nperformance of our FDMNet against state-of-the-art methods.",
        "translated": ""
    },
    {
        "title": "Moonshot: Towards Controllable Video Generation and Editing with\n  Multimodal Conditions",
        "url": "http://arxiv.org/abs/2401.01827v1",
        "pub_date": "2024-01-03",
        "summary": "Most existing video diffusion models (VDMs) are limited to mere text\nconditions. Thereby, they are usually lacking in control over visual appearance\nand geometry structure of the generated videos. This work presents Moonshot, a\nnew video generation model that conditions simultaneously on multimodal inputs\nof image and text. The model builts upon a core module, called multimodal video\nblock (MVB), which consists of conventional spatialtemporal layers for\nrepresenting video features, and a decoupled cross-attention layer to address\nimage and text inputs for appearance conditioning. In addition, we carefully\ndesign the model architecture such that it can optionally integrate with\npre-trained image ControlNet modules for geometry visual conditions, without\nneeding of extra training overhead as opposed to prior methods. Experiments\nshow that with versatile multimodal conditioning mechanisms, Moonshot\ndemonstrates significant improvement on visual quality and temporal consistency\ncompared to existing models. In addition, the model can be easily repurposed\nfor a variety of generative applications, such as personalized video\ngeneration, image animation and video editing, unveiling its potential to serve\nas a fundamental architecture for controllable video generation. Models will be\nmade public on https://github.com/salesforce/LAVIS.",
        "translated": ""
    },
    {
        "title": "HawkRover: An Autonomous mmWave Vehicular Communication Testbed with\n  Multi-sensor Fusion and Deep Learning",
        "url": "http://arxiv.org/abs/2401.01822v1",
        "pub_date": "2024-01-03",
        "summary": "Connected and automated vehicles (CAVs) have become a transformative\ntechnology that can change our daily life. Currently, millimeter-wave (mmWave)\nbands are identified as the promising CAV connectivity solution. While it can\nprovide high data rate, their realization faces many challenges such as high\nattenuation during mmWave signal propagation and mobility management. Existing\nsolution has to initiate pilot signal to measure channel information, then\napply signal processing to calculate the best narrow beam towards the receiver\nend to guarantee sufficient signal power. This process takes significant\noverhead and time, hence not suitable for vehicles. In this study, we propose\nan autonomous and low-cost testbed to collect extensive co-located mmWave\nsignal and other sensors data such as LiDAR (Light Detection and Ranging),\ncameras, ultrasonic, etc, traditionally for ``automated'', to facilitate mmWave\nvehicular communications. Intuitively, these sensors can build a 3D map around\nthe vehicle and signal propagation path can be estimated, eliminating iterative\nthe process via pilot signals. This multimodal data fusion, together with AI,\nis expected to bring significant advances in ``connected'' research.",
        "translated": ""
    },
    {
        "title": "Detours for Navigating Instructional Videos",
        "url": "http://arxiv.org/abs/2401.01823v1",
        "pub_date": "2024-01-03",
        "summary": "We introduce the video detours problem for navigating instructional videos.\nGiven a source video and a natural language query asking to alter the how-to\nvideo's current path of execution in a certain way, the goal is to find a\nrelated ''detour video'' that satisfies the requested alteration. To address\nthis challenge, we propose VidDetours, a novel video-language approach that\nlearns to retrieve the targeted temporal segments from a large repository of\nhow-to's using video-and-text conditioned queries. Furthermore, we devise a\nlanguage-based pipeline that exploits how-to video narration text to create\nweakly supervised training data. We demonstrate our idea applied to the domain\nof how-to cooking videos, where a user can detour from their current recipe to\nfind steps with alternate ingredients, tools, and techniques. Validating on a\nground truth annotated dataset of 16K samples, we show our model's significant\nimprovements over best available methods for video retrieval and question\nanswering, with recall rates exceeding the state of the art by 35%.",
        "translated": ""
    },
    {
        "title": "aMUSEd: An Open MUSE Reproduction",
        "url": "http://arxiv.org/abs/2401.01808v1",
        "pub_date": "2024-01-03",
        "summary": "We present aMUSEd, an open-source, lightweight masked image model (MIM) for\ntext-to-image generation based on MUSE. With 10 percent of MUSE's parameters,\naMUSEd is focused on fast image generation. We believe MIM is under-explored\ncompared to latent diffusion, the prevailing approach for text-to-image\ngeneration. Compared to latent diffusion, MIM requires fewer inference steps\nand is more interpretable. Additionally, MIM can be fine-tuned to learn\nadditional styles with only a single image. We hope to encourage further\nexploration of MIM by demonstrating its effectiveness on large-scale\ntext-to-image generation and releasing reproducible training code. We also\nrelease checkpoints for two models which directly produce images at 256x256 and\n512x512 resolutions.",
        "translated": ""
    },
    {
        "title": "Learning to Prompt with Text Only Supervision for Vision-Language Models",
        "url": "http://arxiv.org/abs/2401.02418v1",
        "pub_date": "2024-01-04",
        "summary": "Foundational vision-language models such as CLIP are becoming a new paradigm\nin vision, due to their excellent generalization abilities. However, adapting\nthese models for downstream tasks while maintaining their generalization\nremains a challenge. In literature, one branch of methods adapts CLIP by\nlearning prompts using visual information. While effective, most of these works\nrequire labeled data which is not practical, and often struggle to generalize\ntowards new datasets due to over-fitting on the source data. An alternative\napproach resorts to training-free methods by generating class descriptions from\nlarge language models (LLMs) and perform prompt ensembling. However, these\nmethods often generate class specific prompts that cannot be transferred to\nother classes, which incur higher costs by generating LLM descriptions for each\nclass separately. In this work, we propose to combine the strengths of these\nboth streams of methods by learning prompts using only text data derived from\nLLMs. As supervised training of prompts is not trivial due to absence of\nimages, we develop a training approach that allows prompts to extract rich\ncontextual knowledge from LLM data. Moreover, with LLM contextual data mapped\nwithin the learned prompts, it enables zero-shot transfer of prompts to new\nclasses and datasets potentially cutting the LLM prompt engineering cost. To\nthe best of our knowledge, this is the first work that learns generalized\nprompts using text only data. We perform extensive evaluations on 4 benchmarks\nwhere our method improves over prior ensembling works while being competitive\nto those utilizing labeled images. Our code and pre-trained models are\navailable at https://github.com/muzairkhattak/ProText.",
        "translated": ""
    },
    {
        "title": "ODIN: A Single Model for 2D and 3D Perception",
        "url": "http://arxiv.org/abs/2401.02416v1",
        "pub_date": "2024-01-04",
        "summary": "State-of-the-art models on contemporary 3D perception benchmarks like ScanNet\nconsume and label dataset-provided 3D point clouds, obtained through post\nprocessing of sensed multiview RGB-D images. They are typically trained\nin-domain, forego large-scale 2D pre-training and outperform alternatives that\nfeaturize the posed RGB-D multiview images instead. The gap in performance\nbetween methods that consume posed images versus post-processed 3D point clouds\nhas fueled the belief that 2D and 3D perception require distinct model\narchitectures. In this paper, we challenge this view and propose ODIN\n(Omni-Dimensional INstance segmentation), a model that can segment and label\nboth 2D RGB images and 3D point clouds, using a transformer architecture that\nalternates between 2D within-view and 3D cross-view information fusion. Our\nmodel differentiates 2D and 3D feature operations through the positional\nencodings of the tokens involved, which capture pixel coordinates for 2D patch\ntokens and 3D coordinates for 3D feature tokens. ODIN achieves state-of-the-art\nperformance on ScanNet200, Matterport3D and AI2THOR 3D instance segmentation\nbenchmarks, and competitive performance on ScanNet, S3DIS and COCO. It\noutperforms all previous works by a wide margin when the sensed 3D point cloud\nis used in place of the point cloud sampled from 3D mesh. When used as the 3D\nperception engine in an instructable embodied agent architecture, it sets a new\nstate-of-the-art on the TEACh action-from-dialogue benchmark. Our code and\ncheckpoints can be found at the project website: https://odin-seg.github.io.",
        "translated": ""
    },
    {
        "title": "Bring Metric Functions into Diffusion Models",
        "url": "http://arxiv.org/abs/2401.02414v1",
        "pub_date": "2024-01-04",
        "summary": "We introduce a Cascaded Diffusion Model (Cas-DM) that improves a Denoising\nDiffusion Probabilistic Model (DDPM) by effectively incorporating additional\nmetric functions in training. Metric functions such as the LPIPS loss have been\nproven highly effective in consistency models derived from the score matching.\nHowever, for the diffusion counterparts, the methodology and efficacy of adding\nextra metric functions remain unclear. One major challenge is the mismatch\nbetween the noise predicted by a DDPM at each step and the desired clean image\nthat the metric function works well on. To address this problem, we propose\nCas-DM, a network architecture that cascades two network modules to effectively\napply metric functions to the diffusion model training. The first module,\nsimilar to a standard DDPM, learns to predict the added noise and is unaffected\nby the metric function. The second cascaded module learns to predict the clean\nimage, thereby facilitating the metric function computation. Experiment results\nshow that the proposed diffusion model backbone enables the effective use of\nthe LPIPS loss, leading to state-of-the-art image quality (FID, sFID, IS) on\nvarious established benchmarks.",
        "translated": ""
    },
    {
        "title": "LLM Augmented LLMs: Expanding Capabilities through Composition",
        "url": "http://arxiv.org/abs/2401.02412v1",
        "pub_date": "2024-01-04",
        "summary": "Foundational models with billions of parameters which have been trained on\nlarge corpora of data have demonstrated non-trivial skills in a variety of\ndomains. However, due to their monolithic structure, it is challenging and\nexpensive to augment them or impart new skills. On the other hand, due to their\nadaptation abilities, several new instances of these models are being trained\ntowards new domains and tasks. In this work, we study the problem of efficient\nand practical composition of existing foundation models with more specific\nmodels to enable newer capabilities. To this end, we propose CALM --\nComposition to Augment Language Models -- which introduces cross-attention\nbetween models to compose their representations and enable new capabilities.\nSalient features of CALM are: (i) Scales up LLMs on new tasks by 're-using'\nexisting LLMs along with a few additional parameters and data, (ii) Existing\nmodel weights are kept intact, and hence preserves existing capabilities, and\n(iii) Applies to diverse domains and settings. We illustrate that augmenting\nPaLM2-S with a smaller model trained on low-resource languages results in an\nabsolute improvement of up to 13\\% on tasks like translation into English and\narithmetic reasoning for low-resource languages. Similarly, when PaLM2-S is\naugmented with a code-specific model, we see a relative improvement of 40\\%\nover the base model for code generation and explanation tasks -- on-par with\nfully fine-tuned counterparts.",
        "translated": ""
    },
    {
        "title": "What You See is What You GAN: Rendering Every Pixel for High-Fidelity\n  Geometry in 3D GANs",
        "url": "http://arxiv.org/abs/2401.02411v1",
        "pub_date": "2024-01-04",
        "summary": "3D-aware Generative Adversarial Networks (GANs) have shown remarkable\nprogress in learning to generate multi-view-consistent images and 3D geometries\nof scenes from collections of 2D images via neural volume rendering. Yet, the\nsignificant memory and computational costs of dense sampling in volume\nrendering have forced 3D GANs to adopt patch-based training or employ\nlow-resolution rendering with post-processing 2D super resolution, which\nsacrifices multiview consistency and the quality of resolved geometry.\nConsequently, 3D GANs have not yet been able to fully resolve the rich 3D\ngeometry present in 2D images. In this work, we propose techniques to scale\nneural volume rendering to the much higher resolution of native 2D images,\nthereby resolving fine-grained 3D geometry with unprecedented detail. Our\napproach employs learning-based samplers for accelerating neural rendering for\n3D GAN training using up to 5 times fewer depth samples. This enables us to\nexplicitly \"render every pixel\" of the full-resolution image during training\nand inference without post-processing superresolution in 2D. Together with our\nstrategy to learn high-quality surface geometry, our method synthesizes\nhigh-resolution 3D geometry and strictly view-consistent images while\nmaintaining image quality on par with baselines relying on post-processing\nsuper resolution. We demonstrate state-of-the-art 3D gemetric quality on FFHQ\nand AFHQ, setting a new standard for unsupervised learning of 3D shapes in 3D\nGANs.",
        "translated": ""
    },
    {
        "title": "3D Open-Vocabulary Panoptic Segmentation with 2D-3D Vision-Language\n  Distillation",
        "url": "http://arxiv.org/abs/2401.02402v1",
        "pub_date": "2024-01-04",
        "summary": "3D panoptic segmentation is a challenging perception task, which aims to\npredict both semantic and instance annotations for 3D points in a scene.\nAlthough prior 3D panoptic segmentation approaches have achieved great\nperformance on closed-set benchmarks, generalizing to novel categories remains\nan open problem. For unseen object categories, 2D open-vocabulary segmentation\nhas achieved promising results that solely rely on frozen CLIP backbones and\nensembling multiple classification outputs. However, we find that simply\nextending these 2D models to 3D does not achieve good performance due to poor\nper-mask classification quality on novel categories. In this paper, we propose\nthe first method to tackle 3D open-vocabulary panoptic segmentation. Our model\ntakes advantage of the fusion between learnable LiDAR features and dense frozen\nvision CLIP features, using a single classification head to make predictions\nfor both base and novel classes. To further improve the classification\nperformance on novel classes and leverage the CLIP model, we propose two novel\nloss functions: object-level distillation loss and voxel-level distillation\nloss. Our experiments on the nuScenes and SemanticKITTI datasets show that our\nmethod outperforms strong baselines by a large margin.",
        "translated": ""
    },
    {
        "title": "Learning the 3D Fauna of the Web",
        "url": "http://arxiv.org/abs/2401.02400v1",
        "pub_date": "2024-01-04",
        "summary": "Learning 3D models of all animals on the Earth requires massively scaling up\nexisting solutions. With this ultimate goal in mind, we develop 3D-Fauna, an\napproach that learns a pan-category deformable 3D animal model for more than\n100 animal species jointly. One crucial bottleneck of modeling animals is the\nlimited availability of training data, which we overcome by simply learning\nfrom 2D Internet images. We show that prior category-specific attempts fail to\ngeneralize to rare species with limited training images. We address this\nchallenge by introducing the Semantic Bank of Skinned Models (SBSM), which\nautomatically discovers a small set of base animal shapes by combining\ngeometric inductive priors with semantic knowledge implicitly captured by an\noff-the-shelf self-supervised feature extractor. To train such a model, we also\ncontribute a new large-scale dataset of diverse animal species. At inference\ntime, given a single image of any quadruped animal, our model reconstructs an\narticulated 3D mesh in a feed-forward fashion within seconds.",
        "translated": ""
    },
    {
        "title": "ChartAssisstant: A Universal Chart Multimodal Language Model via\n  Chart-to-Table Pre-training and Multitask Instruction Tuning",
        "url": "http://arxiv.org/abs/2401.02384v1",
        "pub_date": "2024-01-04",
        "summary": "Charts play a vital role in data visualization, understanding data patterns,\nand informed decision-making. However, their unique combination of graphical\nelements (e.g., bars, lines) and textual components (e.g., labels, legends)\nposes challenges for general-purpose multimodal models. While vision-language\nmodels trained on chart data excel in comprehension, they struggle with\ngeneralization and require task-specific fine-tuning. To address these\nchallenges, we propose ChartAssistant, a chart-based vision-language model for\nuniversal chart comprehension and reasoning. ChartAssistant leverages ChartSFT,\na comprehensive dataset covering diverse chart-related tasks with basic and\nspecialized chart types. It undergoes a two-stage training process, starting\nwith pre-training on chart-to-table parsing to align chart and text, followed\nby multitask instruction-following fine-tuning. This approach enables\nChartAssistant to achieve competitive performance across various chart tasks\nwithout task-specific fine-tuning. Experimental results demonstrate significant\nperformance gains over the state-of-the-art UniChart method, outperforming\nOpenAI's GPT-4V(ision) on real-world chart data. The code and data are\navailable at https://github.com/OpenGVLab/ChartAst.",
        "translated": ""
    },
    {
        "title": "Survey of 3D Human Body Pose and Shape Estimation Methods for\n  Contemporary Dance Applications",
        "url": "http://arxiv.org/abs/2401.02383v1",
        "pub_date": "2024-01-04",
        "summary": "3D human body shape and pose estimation from RGB images is a challenging\nproblem with potential applications in augmented/virtual reality, healthcare\nand fitness technology and virtual retail. Recent solutions have focused on\nthree types of inputs: i) single images, ii) multi-view images and iii) videos.\nIn this study, we surveyed and compared 3D body shape and pose estimation\nmethods for contemporary dance and performing arts, with a special focus on\nhuman body pose and dressing, camera viewpoint, illumination conditions and\nbackground conditions. We demonstrated that multi-frame methods, such as PHALP,\nprovide better results than single-frame method for pose estimation when\ndancers are performing contemporary dances.",
        "translated": ""
    },
    {
        "title": "An Open and Comprehensive Pipeline for Unified Object Grounding and\n  Detection",
        "url": "http://arxiv.org/abs/2401.02361v1",
        "pub_date": "2024-01-04",
        "summary": "Grounding-DINO is a state-of-the-art open-set detection model that tackles\nmultiple vision tasks including Open-Vocabulary Detection (OVD), Phrase\nGrounding (PG), and Referring Expression Comprehension (REC). Its effectiveness\nhas led to its widespread adoption as a mainstream architecture for various\ndownstream applications. However, despite its significance, the original\nGrounding-DINO model lacks comprehensive public technical details due to the\nunavailability of its training code. To bridge this gap, we present\nMM-Grounding-DINO, an open-source, comprehensive, and user-friendly baseline,\nwhich is built with the MMDetection toolbox. It adopts abundant vision datasets\nfor pre-training and various detection and grounding datasets for fine-tuning.\nWe give a comprehensive analysis of each reported result and detailed settings\nfor reproduction. The extensive experiments on the benchmarks mentioned\ndemonstrate that our MM-Grounding-DINO-Tiny outperforms the Grounding-DINO-Tiny\nbaseline. We release all our models to the research community. Codes and\ntrained models are released at\nhttps://github.com/open-mmlab/mmdetection/configs/mm_grounding_dino.",
        "translated": ""
    },
    {
        "title": "Denoising Vision Transformers",
        "url": "http://arxiv.org/abs/2401.02957v1",
        "pub_date": "2024-01-05",
        "summary": "We delve into a nuanced but significant challenge inherent to Vision\nTransformers (ViTs): feature maps of these models exhibit grid-like artifacts,\nwhich detrimentally hurt the performance of ViTs in downstream tasks. Our\ninvestigations trace this fundamental issue down to the positional embeddings\nat the input stage. To address this, we propose a novel noise model, which is\nuniversally applicable to all ViTs. Specifically, the noise model dissects ViT\noutputs into three components: a semantics term free from noise artifacts and\ntwo artifact-related terms that are conditioned on pixel locations. Such a\ndecomposition is achieved by enforcing cross-view feature consistency with\nneural fields in a per-image basis. This per-image optimization process\nextracts artifact-free features from raw ViT outputs, providing clean features\nfor offline applications. Expanding the scope of our solution to support online\nfunctionality, we introduce a learnable denoiser to predict artifact-free\nfeatures directly from unprocessed ViT outputs, which shows remarkable\ngeneralization capabilities to novel data without the need for per-image\noptimization. Our two-stage approach, termed Denoising Vision Transformers\n(DVT), does not require re-training existing pre-trained ViTs and is\nimmediately applicable to any Transformer-based architecture. We evaluate our\nmethod on a variety of representative ViTs (DINO, MAE, DeiT-III, EVA02, CLIP,\nDINOv2, DINOv2-reg). Extensive evaluations demonstrate that our DVT\nconsistently and significantly improves existing state-of-the-art\ngeneral-purpose models in semantic and geometric tasks across multiple datasets\n(e.g., +3.84 mIoU). We hope our study will encourage a re-evaluation of ViT\ndesign, especially regarding the naive use of positional embeddings.",
        "translated": ""
    },
    {
        "title": "Open-Vocabulary SAM: Segment and Recognize Twenty-thousand Classes\n  Interactively",
        "url": "http://arxiv.org/abs/2401.02955v1",
        "pub_date": "2024-01-05",
        "summary": "The CLIP and Segment Anything Model (SAM) are remarkable vision foundation\nmodels (VFMs). SAM excels in segmentation tasks across diverse domains, while\nCLIP is renowned for its zero-shot recognition capabilities. This paper\npresents an in-depth exploration of integrating these two models into a unified\nframework. Specifically, we introduce the Open-Vocabulary SAM, a SAM-inspired\nmodel designed for simultaneous interactive segmentation and recognition,\nleveraging two unique knowledge transfer modules: SAM2CLIP and CLIP2SAM. The\nformer adapts SAM's knowledge into the CLIP via distillation and learnable\ntransformer adapters, while the latter transfers CLIP knowledge into SAM,\nenhancing its recognition capabilities. Extensive experiments on various\ndatasets and detectors show the effectiveness of Open-Vocabulary SAM in both\nsegmentation and recognition tasks, significantly outperforming the naive\nbaselines of simply combining SAM and CLIP. Furthermore, aided with image\nclassification data training, our method can segment and recognize\napproximately 22,000 classes.",
        "translated": ""
    },
    {
        "title": "Locally Adaptive Neural 3D Morphable Models",
        "url": "http://arxiv.org/abs/2401.02937v1",
        "pub_date": "2024-01-05",
        "summary": "We present the Locally Adaptive Morphable Model (LAMM), a highly flexible\nAuto-Encoder (AE) framework for learning to generate and manipulate 3D meshes.\nWe train our architecture following a simple self-supervised training scheme in\nwhich input displacements over a set of sparse control vertices are used to\noverwrite the encoded geometry in order to transform one training sample into\nanother. During inference, our model produces a dense output that adheres\nlocally to the specified sparse geometry while maintaining the overall\nappearance of the encoded object. This approach results in state-of-the-art\nperformance in both disentangling manipulated geometry and 3D mesh\nreconstruction. To the best of our knowledge LAMM is the first end-to-end\nframework that enables direct local control of 3D vertex geometry in a single\nforward pass. A very efficient computational graph allows our network to train\nwith only a fraction of the memory required by previous methods and run faster\nduring inference, generating 12k vertex meshes at $&gt;$60fps on a single CPU\nthread. We further leverage local geometry control as a primitive for higher\nlevel editing operations and present a set of derivative capabilities such as\nswapping and sampling object parts. Code and pretrained models can be found at\nhttps://github.com/michaeltrs/LAMM.",
        "translated": ""
    },
    {
        "title": "SPFormer: Enhancing Vision Transformer with Superpixel Representation",
        "url": "http://arxiv.org/abs/2401.02931v1",
        "pub_date": "2024-01-05",
        "summary": "In this work, we introduce SPFormer, a novel Vision Transformer enhanced by\nsuperpixel representation. Addressing the limitations of traditional Vision\nTransformers' fixed-size, non-adaptive patch partitioning, SPFormer employs\nsuperpixels that adapt to the image's content. This approach divides the image\ninto irregular, semantically coherent regions, effectively capturing intricate\ndetails and applicable at both initial and intermediate feature levels.\n  SPFormer, trainable end-to-end, exhibits superior performance across various\nbenchmarks. Notably, it exhibits significant improvements on the challenging\nImageNet benchmark, achieving a 1.4% increase over DeiT-T and 1.1% over DeiT-S\nrespectively. A standout feature of SPFormer is its inherent explainability.\nThe superpixel structure offers a window into the model's internal processes,\nproviding valuable insights that enhance the model's interpretability. This\nlevel of clarity significantly improves SPFormer's robustness, particularly in\nchallenging scenarios such as image rotations and occlusions, demonstrating its\nadaptability and resilience.",
        "translated": ""
    },
    {
        "title": "Uncovering the human motion pattern: Pattern Memory-based Diffusion\n  Model for Trajectory Prediction",
        "url": "http://arxiv.org/abs/2401.02916v1",
        "pub_date": "2024-01-05",
        "summary": "Human trajectory forecasting is a critical challenge in fields such as\nrobotics and autonomous driving. Due to the inherent uncertainty of human\nactions and intentions in real-world scenarios, various unexpected occurrences\nmay arise. To uncover latent motion patterns in human behavior, we introduce a\nnovel memory-based method, named Motion Pattern Priors Memory Network. Our\nmethod involves constructing a memory bank derived from clustered prior\nknowledge of motion patterns observed in the training set trajectories. We\nintroduce an addressing mechanism to retrieve the matched pattern and the\npotential target distributions for each prediction from the memory bank, which\nenables the identification and retrieval of natural motion patterns exhibited\nby agents, subsequently using the target priors memory token to guide the\ndiffusion model to generate predictions. Extensive experiments validate the\neffectiveness of our approach, achieving state-of-the-art trajectory prediction\naccuracy. The code will be made publicly available.",
        "translated": ""
    },
    {
        "title": "MLLM-Protector: Ensuring MLLM's Safety without Hurting Performance",
        "url": "http://arxiv.org/abs/2401.02906v1",
        "pub_date": "2024-01-05",
        "summary": "The deployment of multimodal large language models (MLLMs) has brought forth\na unique vulnerability: susceptibility to malicious attacks through visual\ninputs. We delve into the novel challenge of defending MLLMs against such\nattacks. We discovered that images act as a \"foreign language\" that is not\nconsidered during alignment, which can make MLLMs prone to producing harmful\nresponses. Unfortunately, unlike the discrete tokens considered in text-based\nLLMs, the continuous nature of image signals presents significant alignment\nchallenges, which poses difficulty to thoroughly cover the possible scenarios.\nThis vulnerability is exacerbated by the fact that open-source MLLMs are\npredominantly fine-tuned on limited image-text pairs that is much less than the\nextensive text-based pretraining corpus, which makes the MLLMs more prone to\ncatastrophic forgetting of their original abilities during explicit alignment\ntuning. To tackle these challenges, we introduce MLLM-Protector, a\nplug-and-play strategy combining a lightweight harm detector and a response\ndetoxifier. The harm detector's role is to identify potentially harmful outputs\nfrom the MLLM, while the detoxifier corrects these outputs to ensure the\nresponse stipulates to the safety standards. This approach effectively\nmitigates the risks posed by malicious visual inputs without compromising the\nmodel's overall performance. Our results demonstrate that MLLM-Protector offers\na robust solution to a previously unaddressed aspect of MLLM security.",
        "translated": ""
    },
    {
        "title": "Reversing the Irreversible: A Survey on Inverse Biometrics",
        "url": "http://arxiv.org/abs/2401.02861v1",
        "pub_date": "2024-01-05",
        "summary": "With the widespread use of biometric recognition, several issues related to\nthe privacy and security provided by this technology have been recently raised\nand analysed. As a result, the early common belief among the biometrics\ncommunity of templates irreversibility has been proven wrong. It is now an\naccepted fact that it is possible to reconstruct from an unprotected template a\nsynthetic sample that matches the bona fide one. This reverse engineering\nprocess, commonly referred to as \\textit{inverse biometrics}, constitutes a\nsevere threat for biometric systems from two different angles: on the one hand,\nsensitive personal data (i.e., biometric data) can be derived from compromised\nunprotected templates; on the other hand, other powerful attacks can be\nlaunched building upon these reconstructed samples. Given its important\nimplications, biometric stakeholders have produced over the last fifteen years\nnumerous works analysing the different aspects related to inverse biometrics:\ndevelopment of reconstruction algorithms for different characteristics;\nproposal of methodologies to assess the vulnerabilities of biometric systems to\nthe aforementioned algorithms; development of countermeasures to reduce the\npossible effects of attacks. The present article is an effort to condense all\nthis information in one comprehensive review of: the problem itself, the\nevaluation of the problem, and the mitigation of the problem. The present\narticle is an effort to condense all this information in one comprehensive\nreview of: the problem itself, the evaluation of the problem, and the\nmitigation of the problem.",
        "translated": ""
    },
    {
        "title": "Generating Non-Stationary Textures using Self-Rectification",
        "url": "http://arxiv.org/abs/2401.02847v1",
        "pub_date": "2024-01-05",
        "summary": "This paper addresses the challenge of example-based non-stationary texture\nsynthesis. We introduce a novel twostep approach wherein users first modify a\nreference texture using standard image editing tools, yielding an initial rough\ntarget for the synthesis. Subsequently, our proposed method, termed\n\"self-rectification\", automatically refines this target into a coherent,\nseamless texture, while faithfully preserving the distinct visual\ncharacteristics of the reference exemplar. Our method leverages a pre-trained\ndiffusion network, and uses self-attention mechanisms, to gradually align the\nsynthesized texture with the reference, ensuring the retention of the\nstructures in the provided target. Through experimental validation, our\napproach exhibits exceptional proficiency in handling non-stationary textures,\ndemonstrating significant advancements in texture synthesis when compared to\nexisting state-of-the-art techniques. Code is available at\nhttps://github.com/xiaorongjun000/Self-Rectification",
        "translated": ""
    },
    {
        "title": "Multi-Stage Contrastive Regression for Action Quality Assessment",
        "url": "http://arxiv.org/abs/2401.02841v1",
        "pub_date": "2024-01-05",
        "summary": "In recent years, there has been growing interest in the video-based action\nquality assessment (AQA). Most existing methods typically solve AQA problem by\nconsidering the entire video yet overlooking the inherent stage-level\ncharacteristics of actions. To address this issue, we design a novel\nMulti-stage Contrastive Regression (MCoRe) framework for the AQA task. This\napproach allows us to efficiently extract spatial-temporal information, while\nsimultaneously reducing computational costs by segmenting the input video into\nmultiple stages or procedures. Inspired by the graph contrastive learning, we\npropose a new stage-wise contrastive learning loss function to enhance\nperformance. As a result, MCoRe demonstrates the state-of-the-art result so far\non the widely-adopted fine-grained AQA dataset.",
        "translated": ""
    },
    {
        "title": "CrisisViT: A Robust Vision Transformer for Crisis Image Classification",
        "url": "http://arxiv.org/abs/2401.02838v1",
        "pub_date": "2024-01-05",
        "summary": "In times of emergency, crisis response agencies need to quickly and\naccurately assess the situation on the ground in order to deploy relevant\nservices and resources. However, authorities often have to make decisions based\non limited information, as data on affected regions can be scarce until local\nresponse services can provide first-hand reports. Fortunately, the widespread\navailability of smartphones with high-quality cameras has made citizen\njournalism through social media a valuable source of information for crisis\nresponders. However, analyzing the large volume of images posted by citizens\nrequires more time and effort than is typically available. To address this\nissue, this paper proposes the use of state-of-the-art deep neural models for\nautomatic image classification/tagging, specifically by adapting\ntransformer-based architectures for crisis image classification (CrisisViT). We\nleverage the new Incidents1M crisis image dataset to develop a range of new\ntransformer-based image classification models. Through experimentation over the\nstandard Crisis image benchmark dataset, we demonstrate that the CrisisViT\nmodels significantly outperform previous approaches in emergency type, image\nrelevance, humanitarian category, and damage severity classification.\nAdditionally, we show that the new Incidents1M dataset can further augment the\nCrisisViT models resulting in an additional 1.25% absolute accuracy gain.",
        "translated": ""
    },
    {
        "title": "Dr$^2$Net: Dynamic Reversible Dual-Residual Networks for\n  Memory-Efficient Finetuning",
        "url": "http://arxiv.org/abs/2401.04105v1",
        "pub_date": "2024-01-08",
        "summary": "Large pretrained models are increasingly crucial in modern computer vision\ntasks. These models are typically used in downstream tasks by end-to-end\nfinetuning, which is highly memory-intensive for tasks with high-resolution\ndata, e.g., video understanding, small object detection, and point cloud\nanalysis. In this paper, we propose Dynamic Reversible Dual-Residual Networks,\nor Dr$^2$Net, a novel family of network architectures that acts as a surrogate\nnetwork to finetune a pretrained model with substantially reduced memory\nconsumption. Dr$^2$Net contains two types of residual connections, one\nmaintaining the residual structure in the pretrained models, and the other\nmaking the network reversible. Due to its reversibility, intermediate\nactivations, which can be reconstructed from output, are cleared from memory\nduring training. We use two coefficients on either type of residual connections\nrespectively, and introduce a dynamic training strategy that seamlessly\ntransitions the pretrained model to a reversible network with much higher\nnumerical precision. We evaluate Dr$^2$Net on various pretrained models and\nvarious tasks, and show that it can reach comparable performance to\nconventional finetuning but with significantly less memory usage.",
        "translated": ""
    },
    {
        "title": "AGG: Amortized Generative 3D Gaussians for Single Image to 3D",
        "url": "http://arxiv.org/abs/2401.04099v1",
        "pub_date": "2024-01-08",
        "summary": "Given the growing need for automatic 3D content creation pipelines, various\n3D representations have been studied to generate 3D objects from a single\nimage. Due to its superior rendering efficiency, 3D Gaussian splatting-based\nmodels have recently excelled in both 3D reconstruction and generation. 3D\nGaussian splatting approaches for image to 3D generation are often\noptimization-based, requiring many computationally expensive score-distillation\nsteps. To overcome these challenges, we introduce an Amortized Generative 3D\nGaussian framework (AGG) that instantly produces 3D Gaussians from a single\nimage, eliminating the need for per-instance optimization. Utilizing an\nintermediate hybrid representation, AGG decomposes the generation of 3D\nGaussian locations and other appearance attributes for joint optimization.\nMoreover, we propose a cascaded pipeline that first generates a coarse\nrepresentation of the 3D data and later upsamples it with a 3D Gaussian\nsuper-resolution module. Our method is evaluated against existing\noptimization-based 3D Gaussian frameworks and sampling-based pipelines\nutilizing other 3D representations, where AGG showcases competitive generation\nabilities both qualitatively and quantitatively while being several orders of\nmagnitude faster. Project page: https://ir1d.github.io/AGG/",
        "translated": ""
    },
    {
        "title": "GPT-4V(ision) is a Human-Aligned Evaluator for Text-to-3D Generation",
        "url": "http://arxiv.org/abs/2401.04092v1",
        "pub_date": "2024-01-08",
        "summary": "Despite recent advances in text-to-3D generative methods, there is a notable\nabsence of reliable evaluation metrics. Existing metrics usually focus on a\nsingle criterion each, such as how well the asset aligned with the input text.\nThese metrics lack the flexibility to generalize to different evaluation\ncriteria and might not align well with human preferences. Conducting user\npreference studies is an alternative that offers both adaptability and\nhuman-aligned results. User studies, however, can be very expensive to scale.\nThis paper presents an automatic, versatile, and human-aligned evaluation\nmetric for text-to-3D generative models. To this end, we first develop a prompt\ngenerator using GPT-4V to generate evaluating prompts, which serve as input to\ncompare text-to-3D models. We further design a method instructing GPT-4V to\ncompare two 3D assets according to user-defined criteria. Finally, we use these\npairwise comparison results to assign these models Elo ratings. Experimental\nresults suggest our metric strongly align with human preference across\ndifferent evaluation criteria.",
        "translated": ""
    },
    {
        "title": "RudolfV: A Foundation Model by Pathologists for Pathologists",
        "url": "http://arxiv.org/abs/2401.04079v1",
        "pub_date": "2024-01-08",
        "summary": "Histopathology plays a central role in clinical medicine and biomedical\nresearch. While artificial intelligence shows promising results on many\npathological tasks, generalization and dealing with rare diseases, where\ntraining data is scarce, remains a challenge. Distilling knowledge from\nunlabeled data into a foundation model before learning from, potentially\nlimited, labeled data provides a viable path to address these challenges. In\nthis work, we extend the state of the art of foundation models for digital\npathology whole slide images by semi-automated data curation and incorporating\npathologist domain knowledge. Specifically, we combine computational and\npathologist domain knowledge (1) to curate a diverse dataset of 103k slides\ncorresponding to 750 million image patches covering data from different\nfixation, staining, and scanning protocols as well as data from different\nindications and labs across the EU and US, (2) for grouping semantically\nsimilar slides and tissue patches, and (3) to augment the input images during\ntraining. We evaluate the resulting model on a set of public and internal\nbenchmarks and show that although our foundation model is trained with an order\nof magnitude less slides, it performs on par or better than competing models.\nWe expect that scaling our approach to more data and larger models will further\nincrease its performance and capacity to deal with increasingly complex real\nworld tasks in diagnostics and biomedical research.",
        "translated": ""
    },
    {
        "title": "Fun with Flags: Robust Principal Directions via Flag Manifolds",
        "url": "http://arxiv.org/abs/2401.04071v1",
        "pub_date": "2024-01-08",
        "summary": "Principal component analysis (PCA), along with its extensions to manifolds\nand outlier contaminated data, have been indispensable in computer vision and\nmachine learning. In this work, we present a unifying formalism for PCA and its\nvariants, and introduce a framework based on the flags of linear subspaces, \\ie\na hierarchy of nested linear subspaces of increasing dimension, which not only\nallows for a common implementation but also yields novel variants, not explored\npreviously. We begin by generalizing traditional PCA methods that either\nmaximize variance or minimize reconstruction error. We expand these\ninterpretations to develop a wide array of new dimensionality reduction\nalgorithms by accounting for outliers and the data manifold. To devise a common\ncomputational approach, we recast robust and dual forms of PCA as optimization\nproblems on flag manifolds. We then integrate tangent space approximations of\nprincipal geodesic analysis (tangent-PCA) into this flag-based framework,\ncreating novel robust and dual geodesic PCA variations. The remarkable\nflexibility offered by the 'flagification' introduced here enables even more\nalgorithmic variants identified by specific flag types. Last but not least, we\npropose an effective convergent solver for these flag-formulations employing\nthe Stiefel manifold. Our empirical results on both real-world and synthetic\nscenarios, demonstrate the superiority of our novel algorithms, especially in\nterms of robustness to outliers on manifolds.",
        "translated": ""
    },
    {
        "title": "Efficient Multiscale Multimodal Bottleneck Transformer for Audio-Video\n  Classification",
        "url": "http://arxiv.org/abs/2401.04023v1",
        "pub_date": "2024-01-08",
        "summary": "In recent years, researchers combine both audio and video signals to deal\nwith challenges where actions are not well represented or captured by visual\ncues. However, how to effectively leverage the two modalities is still under\ndevelopment. In this work, we develop a multiscale multimodal Transformer (MMT)\nthat leverages hierarchical representation learning. Particularly, MMT is\ncomposed of a novel multiscale audio Transformer (MAT) and a multiscale video\nTransformer [43]. To learn a discriminative cross-modality fusion, we further\ndesign multimodal supervised contrastive objectives called audio-video\ncontrastive loss (AVC) and intra-modal contrastive loss (IMC) that robustly\nalign the two modalities. MMT surpasses previous state-of-the-art approaches by\n7.3% and 2.1% on Kinetics-Sounds and VGGSound in terms of the top-1 accuracy\nwithout external training data. Moreover, the proposed MAT significantly\noutperforms AST [28] by 22.2%, 4.4% and 4.7% on three public benchmark\ndatasets, and is about 3% more efficient based on the number of FLOPs and 9.8%\nmore efficient based on GPU memory usage.",
        "translated": ""
    },
    {
        "title": "Behavioural Cloning in VizDoom",
        "url": "http://arxiv.org/abs/2401.03993v1",
        "pub_date": "2024-01-08",
        "summary": "This paper describes methods for training autonomous agents to play the game\n\"Doom 2\" through Imitation Learning (IL) using only pixel data as input. We\nalso explore how Reinforcement Learning (RL) compares to IL for humanness by\ncomparing camera movement and trajectory data. Through behavioural cloning, we\nexamine the ability of individual models to learn varying behavioural traits.\nWe attempt to mimic the behaviour of real players with different play styles,\nand find we can train agents that behave aggressively, passively, or simply\nmore human-like than traditional AIs. We propose these methods of introducing\nmore depth and human-like behaviour to agents in video games. The trained IL\nagents perform on par with the average players in our dataset, whilst\noutperforming the worst players. While performance was not as strong as common\nRL approaches, it provides much stronger human-like behavioural traits to the\nagent.",
        "translated": ""
    },
    {
        "title": "MS-DETR: Efficient DETR Training with Mixed Supervision",
        "url": "http://arxiv.org/abs/2401.03989v1",
        "pub_date": "2024-01-08",
        "summary": "DETR accomplishes end-to-end object detection through iteratively generating\nmultiple object candidates based on image features and promoting one candidate\nfor each ground-truth object. The traditional training procedure using\none-to-one supervision in the original DETR lacks direct supervision for the\nobject detection candidates.\n  We aim at improving the DETR training efficiency by explicitly supervising\nthe candidate generation procedure through mixing one-to-one supervision and\none-to-many supervision. Our approach, namely MS-DETR, is simple, and places\none-to-many supervision to the object queries of the primary decoder that is\nused for inference. In comparison to existing DETR variants with one-to-many\nsupervision, such as Group DETR and Hybrid DETR, our approach does not need\nadditional decoder branches or object queries. The object queries of the\nprimary decoder in our approach directly benefit from one-to-many supervision\nand thus are superior in object candidate prediction. Experimental results show\nthat our approach outperforms related DETR variants, such as DN-DETR, Hybrid\nDETR, and Group DETR, and the combination with related DETR variants further\nimproves the performance.",
        "translated": ""
    },
    {
        "title": "Multi-scale attention-based instance segmentation for measuring crystals\n  with large size variation",
        "url": "http://arxiv.org/abs/2401.03939v1",
        "pub_date": "2024-01-08",
        "summary": "Quantitative measurement of crystals in high-resolution images allows for\nimportant insights into underlying material characteristics. Deep learning has\nshown great progress in vision-based automatic crystal size measurement, but\ncurrent instance segmentation methods reach their limits with images that have\nlarge variation in crystal size or hard to detect crystal boundaries. Even\nsmall image segmentation errors, such as incorrectly fused or separated\nsegments, can significantly lower the accuracy of the measured results. Instead\nof improving the existing pixel-wise boundary segmentation methods, we propose\nto use an instance-based segmentation method, which gives more robust\nsegmentation results to improve measurement accuracy. Our novel method enhances\nflow maps with a size-aware multi-scale attention module. The attention module\nadaptively fuses information from multiple scales and focuses on the most\nrelevant scale for each segmented image area. We demonstrate that our proposed\nattention fusion strategy outperforms state-of-the-art instance and boundary\nsegmentation methods, as well as simple average fusion of multi-scale\npredictions. We evaluate our method on a refractory raw material dataset of\nhigh-resolution images with large variation in crystal size and show that our\nmodel can be used to calculate the crystal size more accurately than existing\nmethods.",
        "translated": ""
    },
    {
        "title": "Structure-focused Neurodegeneration Convolutional Neural Network for\n  Modeling and Classification of Alzheimer's Disease",
        "url": "http://arxiv.org/abs/2401.03922v1",
        "pub_date": "2024-01-08",
        "summary": "Alzheimer's disease (AD), the predominant form of dementia, poses a growing\nglobal challenge and underscores the urgency of accurate and early diagnosis.\nThe clinical technique radiologists adopt for distinguishing between mild\ncognitive impairment (MCI) and AD using Machine Resonance Imaging (MRI)\nencounter hurdles because they are not consistent and reliable. Machine\nlearning has been shown to offer promise for early AD diagnosis. However,\nexisting models focused on focal fine-grain features without considerations to\nfocal structural features that give off information on neurodegeneration of the\nbrain cerebral cortex. Therefore, this paper proposes a machine learning (ML)\nframework that integrates Gamma correction, an image enhancement technique, and\nincludes a structure-focused neurodegeneration convolutional neural network\n(CNN) architecture called SNeurodCNN for discriminating between AD and MCI. The\nML framework leverages the mid-sagittal and para-sagittal brain image\nviewpoints of the structure-focused Alzheimer's Disease Neuroimaging Initiative\n(ADNI) dataset. Through experiments, our proposed machine learning framework\nshows exceptional performance. The parasagittal viewpoint set achieves 97.8%\naccuracy, with 97.0% specificity and 98.5% sensitivity. The midsagittal\nviewpoint is shown to present deeper insights into the structural brain changes\ngiven the increase in accuracy, specificity, and sensitivity, which are 98.1%\n97.2%, and 99.0%, respectively. Using GradCAM technique, we show that our\nproposed model is capable of capturing the structural dynamics of MCI and AD\nwhich exist about the frontal lobe, occipital lobe, cerebellum, and parietal\nlobe. Therefore, our model itself as a potential brain structural change\nDigi-Biomarker for early diagnosis of AD.",
        "translated": ""
    },
    {
        "title": "A Simple Baseline for Spoken Language to Sign Language Translation with\n  3D Avatars",
        "url": "http://arxiv.org/abs/2401.04730v1",
        "pub_date": "2024-01-09",
        "summary": "The objective of this paper is to develop a functional system for translating\nspoken languages into sign languages, referred to as Spoken2Sign translation.\nThe Spoken2Sign task is orthogonal and complementary to traditional sign\nlanguage to spoken language (Sign2Spoken) translation. To enable Spoken2Sign\ntranslation, we present a simple baseline consisting of three steps: 1)\ncreating a gloss-video dictionary using existing Sign2Spoken benchmarks; 2)\nestimating a 3D sign for each sign video in the dictionary; 3) training a\nSpoken2Sign model, which is composed of a Text2Gloss translator, a sign\nconnector, and a rendering module, with the aid of the yielded gloss-3D sign\ndictionary. The translation results are then displayed through a sign avatar.\nAs far as we know, we are the first to present the Spoken2Sign task in an\noutput format of 3D signs. In addition to its capability of Spoken2Sign\ntranslation, we also demonstrate that two by-products of our approach-3D\nkeypoint augmentation and multi-view understanding-can assist in keypoint-based\nsign language understanding. Code and models will be available at\nhttps://github.com/FangyunWei/SLRT",
        "translated": ""
    },
    {
        "title": "Morphable Diffusion: 3D-Consistent Diffusion for Single-image Avatar\n  Creation",
        "url": "http://arxiv.org/abs/2401.04728v1",
        "pub_date": "2024-01-09",
        "summary": "Recent advances in generative diffusion models have enabled the previously\nunfeasible capability of generating 3D assets from a single input image or a\ntext prompt. In this work, we aim to enhance the quality and functionality of\nthese models for the task of creating controllable, photorealistic human\navatars. We achieve this by integrating a 3D morphable model into the\nstate-of-the-art multiview-consistent diffusion approach. We demonstrate that\naccurate conditioning of a generative pipeline on the articulated 3D model\nenhances the baseline model performance on the task of novel view synthesis\nfrom a single image. More importantly, this integration facilitates a seamless\nand accurate incorporation of facial expression and body pose control into the\ngeneration process. To the best of our knowledge, our proposed framework is the\nfirst diffusion model to enable the creation of fully 3D-consistent,\nanimatable, and photorealistic human avatars from a single image of an unseen\nsubject; extensive quantitative and qualitative evaluations demonstrate the\nadvantages of our approach over existing state-of-the-art avatar creation\nmodels on both novel view and novel expression synthesis tasks.",
        "translated": ""
    },
    {
        "title": "Revisiting Adversarial Training at Scale",
        "url": "http://arxiv.org/abs/2401.04727v1",
        "pub_date": "2024-01-09",
        "summary": "The machine learning community has witnessed a drastic change in the training\npipeline, pivoted by those ''foundation models'' with unprecedented scales.\nHowever, the field of adversarial training is lagging behind, predominantly\ncentered around small model sizes like ResNet-50, and tiny and low-resolution\ndatasets like CIFAR-10. To bridge this transformation gap, this paper provides\na modern re-examination with adversarial training, investigating its potential\nbenefits when applied at scale. Additionally, we introduce an efficient and\neffective training strategy to enable adversarial training with giant models\nand web-scale data at an affordable computing cost. We denote this newly\nintroduced framework as AdvXL.\n  Empirical results demonstrate that AdvXL establishes new state-of-the-art\nrobust accuracy records under AutoAttack on ImageNet-1K. For example, by\ntraining on DataComp-1B dataset, our AdvXL empowers a vanilla ViT-g model to\nsubstantially surpass the previous records of $l_{\\infty}$-, $l_{2}$-, and\n$l_{1}$-robust accuracy by margins of 11.4%, 14.2% and 12.9%, respectively.\nThis achievement posits AdvXL as a pioneering approach, charting a new\ntrajectory for the efficient training of robust visual representations at\nsignificantly larger scales. Our code is available at\nhttps://github.com/UCSC-VLAA/AdvXL.",
        "translated": ""
    },
    {
        "title": "U-Mamba: Enhancing Long-range Dependency for Biomedical Image\n  Segmentation",
        "url": "http://arxiv.org/abs/2401.04722v1",
        "pub_date": "2024-01-09",
        "summary": "Convolutional Neural Networks (CNNs) and Transformers have been the most\npopular architectures for biomedical image segmentation, but both of them have\nlimited ability to handle long-range dependencies because of inherent locality\nor computational complexity. To address this challenge, we introduce U-Mamba, a\ngeneral-purpose network for biomedical image segmentation. Inspired by the\nState Space Sequence Models (SSMs), a new family of deep sequence models known\nfor their strong capability in handling long sequences, we design a hybrid\nCNN-SSM block that integrates the local feature extraction power of\nconvolutional layers with the abilities of SSMs for capturing the long-range\ndependency. Moreover, U-Mamba enjoys a self-configuring mechanism, allowing it\nto automatically adapt to various datasets without manual intervention. We\nconduct extensive experiments on four diverse tasks, including the 3D abdominal\norgan segmentation in CT and MR images, instrument segmentation in endoscopy\nimages, and cell segmentation in microscopy images. The results reveal that\nU-Mamba outperforms state-of-the-art CNN-based and Transformer-based\nsegmentation networks across all tasks. This opens new avenues for efficient\nlong-range dependency modeling in biomedical image analysis. The code, models,\nand data are publicly available at https://wanglab.ai/u-mamba.html.",
        "translated": ""
    },
    {
        "title": "Low-resource finetuning of foundation models beats state-of-the-art in\n  histopathology",
        "url": "http://arxiv.org/abs/2401.04720v1",
        "pub_date": "2024-01-09",
        "summary": "To handle the large scale of whole slide images in computational pathology,\nmost approaches first tessellate the images into smaller patches, extract\nfeatures from these patches, and finally aggregate the feature vectors with\nweakly-supervised learning. The performance of this workflow strongly depends\non the quality of the extracted features. Recently, foundation models in\ncomputer vision showed that leveraging huge amounts of data through supervised\nor self-supervised learning improves feature quality and generalizability for a\nvariety of tasks. In this study, we benchmark the most popular vision\nfoundation models as feature extractors for histopathology data. We evaluate\nthe models in two settings: slide-level classification and patch-level\nclassification. We show that foundation models are a strong baseline. Our\nexperiments demonstrate that by finetuning a foundation model on a single GPU\nfor only two hours or three days depending on the dataset, we can match or\noutperform state-of-the-art feature extractors for computational pathology.\nThese findings imply that even with little resources one can finetune a feature\nextractor tailored towards a specific downstream task and dataset. This is a\nconsiderable shift from the current state, where only few institutions with\nlarge amounts of resources and datasets are able to train a feature extractor.\nWe publish all code used for training and evaluation as well as the finetuned\nmodels.",
        "translated": ""
    },
    {
        "title": "Jump Cut Smoothing for Talking Heads",
        "url": "http://arxiv.org/abs/2401.04718v1",
        "pub_date": "2024-01-09",
        "summary": "A jump cut offers an abrupt, sometimes unwanted change in the viewing\nexperience. We present a novel framework for smoothing these jump cuts, in the\ncontext of talking head videos. We leverage the appearance of the subject from\nthe other source frames in the video, fusing it with a mid-level representation\ndriven by DensePose keypoints and face landmarks. To achieve motion, we\ninterpolate the keypoints and landmarks between the end frames around the cut.\nWe then use an image translation network from the keypoints and source frames,\nto synthesize pixels. Because keypoints can contain errors, we propose a\ncross-modal attention scheme to select and pick the most appropriate source\namongst multiple options for each key point. By leveraging this mid-level\nrepresentation, our method can achieve stronger results than a strong video\ninterpolation baseline. We demonstrate our method on various jump cuts in the\ntalking head videos, such as cutting filler words, pauses, and even random\ncuts. Our experiments show that we can achieve seamless transitions, even in\nthe challenging cases where the talking head rotates or moves drastically in\nthe jump cut.",
        "translated": ""
    },
    {
        "title": "Low-Resource Vision Challenges for Foundation Models",
        "url": "http://arxiv.org/abs/2401.04716v1",
        "pub_date": "2024-01-09",
        "summary": "Low-resource settings are well-established in natural language processing,\nwhere many languages lack sufficient data for machine learning at scale.\nHowever, low-resource problems are under-explored in computer vision. In this\npaper, we strive to address this gap and explore the challenges of low-resource\nimage tasks with vision foundation models. Thus, we first collect a benchmark\nof genuinely low-resource image data, covering historic maps, circuit diagrams,\nand mechanical drawings. These low-resource settings all share the three\nchallenges of data scarcity, fine-grained differences, and the distribution\nshift from natural images to the specialized domain of interest. While existing\nfoundation models have shown impressive generalizability, we find they cannot\ntransfer well to our low-resource tasks. To begin to tackle the challenges of\nlow-resource vision, we introduce one simple baseline per challenge.\nSpecifically, we propose to i) enlarge the data space by generative models, ii)\nadopt the best sub-kernels to encode local regions for fine-grained difference\ndiscovery and iii) learn attention for specialized domains. Experiments on the\nthree low-resource data sources in our benchmark demonstrate our proposals\nalready provide a better baseline than common transfer learning, data\naugmentation, and fine-grained methods. This highlights the unique\ncharacteristics and challenges of low-resource vision for foundation models\nthat warrant further investigation. Project website:\nhttps://xiaobai1217.github.io/Low-Resource-Vision/.",
        "translated": ""
    },
    {
        "title": "CoordGate: Efficiently Computing Spatially-Varying Convolutions in\n  Convolutional Neural Networks",
        "url": "http://arxiv.org/abs/2401.04680v1",
        "pub_date": "2024-01-09",
        "summary": "Optical imaging systems are inherently limited in their resolution due to the\npoint spread function (PSF), which applies a static, yet spatially-varying,\nconvolution to the image. This degradation can be addressed via Convolutional\nNeural Networks (CNNs), particularly through deblurring techniques. However,\ncurrent solutions face certain limitations in efficiently computing\nspatially-varying convolutions. In this paper we propose CoordGate, a novel\nlightweight module that uses a multiplicative gate and a coordinate encoding\nnetwork to enable efficient computation of spatially-varying convolutions in\nCNNs. CoordGate allows for selective amplification or attenuation of filters\nbased on their spatial position, effectively acting like a locally connected\nneural network. The effectiveness of the CoordGate solution is demonstrated\nwithin the context of U-Nets and applied to the challenging problem of image\ndeblurring. The experimental results show that CoordGate outperforms\nconventional approaches, offering a more robust and spatially aware solution\nfor CNNs in various computer vision applications.",
        "translated": ""
    },
    {
        "title": "Benchmark Analysis of Various Pre-trained Deep Learning Models on ASSIRA\n  Cats and Dogs Dataset",
        "url": "http://arxiv.org/abs/2401.04666v1",
        "pub_date": "2024-01-09",
        "summary": "As the most basic application and implementation of deep learning, image\nclassification has grown in popularity. Various datasets are provided by\nrenowned data science communities for benchmarking machine learning algorithms\nand pre-trained models. The ASSIRA Cats &amp; Dogs dataset is one of them and is\nbeing used in this research for its overall acceptance and benchmark standards.\nA comparison of various pre-trained models is demonstrated by using different\ntypes of optimizers and loss functions. Hyper-parameters are changed to gain\nthe best result from a model. By applying this approach, we have got higher\naccuracy without major changes in the training model. To run the experiment, we\nused three different computer architectures: a laptop equipped with NVIDIA\nGeForce GTX 1070, a laptop equipped with NVIDIA GeForce RTX 3080Ti, and a\ndesktop equipped with NVIDIA GeForce RTX 3090. The acquired results demonstrate\nsupremacy in terms of accuracy over the previously done experiments on this\ndataset. From this experiment, the highest accuracy which is 99.65% is gained\nusing the NASNet Large.",
        "translated": ""
    },
    {
        "title": "Learning to Prompt Segment Anything Models",
        "url": "http://arxiv.org/abs/2401.04651v1",
        "pub_date": "2024-01-09",
        "summary": "Segment Anything Models (SAMs) like SEEM and SAM have demonstrated great\npotential in learning to segment anything. The core design of SAMs lies with\nPromptable Segmentation, which takes a handcrafted prompt as input and returns\nthe expected segmentation mask. SAMs work with two types of prompts including\nspatial prompts (e.g., points) and semantic prompts (e.g., texts), which work\ntogether to prompt SAMs to segment anything on downstream datasets. Despite the\nimportant role of prompts, how to acquire suitable prompts for SAMs is largely\nunder-explored. In this work, we examine the architecture of SAMs and identify\ntwo challenges for learning effective prompts for SAMs. To this end, we propose\nspatial-semantic prompt learning (SSPrompt) that learns effective semantic and\nspatial prompts for better SAMs. Specifically, SSPrompt introduces spatial\nprompt learning and semantic prompt learning, which optimize spatial prompts\nand semantic prompts directly over the embedding space and selectively leverage\nthe knowledge encoded in pre-trained prompt encoders. Extensive experiments\nshow that SSPrompt achieves superior image segmentation performance\nconsistently across multiple widely adopted datasets.",
        "translated": ""
    },
    {
        "title": "InseRF: Text-Driven Generative Object Insertion in Neural 3D Scenes",
        "url": "http://arxiv.org/abs/2401.05335v1",
        "pub_date": "2024-01-10",
        "summary": "We introduce InseRF, a novel method for generative object insertion in the\nNeRF reconstructions of 3D scenes. Based on a user-provided textual description\nand a 2D bounding box in a reference viewpoint, InseRF generates new objects in\n3D scenes. Recently, methods for 3D scene editing have been profoundly\ntransformed, owing to the use of strong priors of text-to-image diffusion\nmodels in 3D generative modeling. Existing methods are mostly effective in\nediting 3D scenes via style and appearance changes or removing existing\nobjects. Generating new objects, however, remains a challenge for such methods,\nwhich we address in this study. Specifically, we propose grounding the 3D\nobject insertion to a 2D object insertion in a reference view of the scene. The\n2D edit is then lifted to 3D using a single-view object reconstruction method.\nThe reconstructed object is then inserted into the scene, guided by the priors\nof monocular depth estimation methods. We evaluate our method on various 3D\nscenes and provide an in-depth analysis of the proposed components. Our\nexperiments with generative insertion of objects in several 3D scenes indicate\nthe effectiveness of our method compared to the existing methods. InseRF is\ncapable of controllable and 3D-consistent object insertion without requiring\nexplicit 3D information as input. Please visit our project page at\nhttps://mohamad-shahbazi.github.io/inserf.",
        "translated": ""
    },
    {
        "title": "Towards Online Sign Language Recognition and Translation",
        "url": "http://arxiv.org/abs/2401.05336v1",
        "pub_date": "2024-01-10",
        "summary": "The objective of sign language recognition is to bridge the communication gap\nbetween the deaf and the hearing. Numerous previous works train their models\nusing the well-established connectionist temporal classification (CTC) loss.\nDuring the inference stage, the CTC-based models typically take the entire sign\nvideo as input to make predictions. This type of inference scheme is referred\nto as offline recognition. In contrast, while mature speech recognition systems\ncan efficiently recognize spoken words on the fly, sign language recognition\nstill falls short due to the lack of practical online solutions. In this work,\nwe take the first step towards filling this gap. Our approach comprises three\nphases: 1) developing a sign language dictionary encompassing all glosses\npresent in a target sign language dataset; 2) training an isolated sign\nlanguage recognition model on augmented signs using both conventional\nclassification loss and our novel saliency loss; 3) employing a sliding window\napproach on the input sign sequence and feeding each sign clip to the\nwell-optimized model for online recognition. Furthermore, our online\nrecognition model can be extended to boost the performance of any offline\nmodel, and to support online translation by appending a gloss-to-text network\nonto the recognition model. By integrating our online framework with the\npreviously best-performing offline model, TwoStream-SLR, we achieve new\nstate-of-the-art performance on three benchmarks: Phoenix-2014, Phoenix-2014T,\nand CSL-Daily. Code and models will be available at\nhttps://github.com/FangyunWei/SLRT",
        "translated": ""
    },
    {
        "title": "URHand: Universal Relightable Hands",
        "url": "http://arxiv.org/abs/2401.05334v1",
        "pub_date": "2024-01-10",
        "summary": "Existing photorealistic relightable hand models require extensive\nidentity-specific observations in different views, poses, and illuminations,\nand face challenges in generalizing to natural illuminations and novel\nidentities. To bridge this gap, we present URHand, the first universal\nrelightable hand model that generalizes across viewpoints, poses,\nilluminations, and identities. Our model allows few-shot personalization using\nimages captured with a mobile phone, and is ready to be photorealistically\nrendered under novel illuminations. To simplify the personalization process\nwhile retaining photorealism, we build a powerful universal relightable prior\nbased on neural relighting from multi-view images of hands captured in a light\nstage with hundreds of identities. The key challenge is scaling the\ncross-identity training while maintaining personalized fidelity and sharp\ndetails without compromising generalization under natural illuminations. To\nthis end, we propose a spatially varying linear lighting model as the neural\nrenderer that takes physics-inspired shading as input feature. By removing\nnon-linear activations and bias, our specifically designed lighting model\nexplicitly keeps the linearity of light transport. This enables single-stage\ntraining from light-stage data while generalizing to real-time rendering under\narbitrary continuous illuminations across diverse identities. In addition, we\nintroduce the joint learning of a physically based model and our neural\nrelighting model, which further improves fidelity and generalization. Extensive\nexperiments show that our approach achieves superior performance over existing\nmethods in terms of both quality and generalizability. We also demonstrate\nquick personalization of URHand from a short phone scan of an unseen identity.",
        "translated": ""
    },
    {
        "title": "ANIM-400K: A Large-Scale Dataset for Automated End-To-End Dubbing of\n  Video",
        "url": "http://arxiv.org/abs/2401.05314v1",
        "pub_date": "2024-01-10",
        "summary": "The Internet's wealth of content, with up to 60% published in English,\nstarkly contrasts the global population, where only 18.8% are English speakers,\nand just 5.1% consider it their native language, leading to disparities in\nonline information access. Unfortunately, automated processes for dubbing of\nvideo - replacing the audio track of a video with a translated alternative -\nremains a complex and challenging task due to pipelines, necessitating precise\ntiming, facial movement synchronization, and prosody matching. While end-to-end\ndubbing offers a solution, data scarcity continues to impede the progress of\nboth end-to-end and pipeline-based methods. In this work, we introduce\nAnim-400K, a comprehensive dataset of over 425K aligned animated video segments\nin Japanese and English supporting various video-related tasks, including\nautomated dubbing, simultaneous translation, guided video summarization, and\ngenre/theme/style classification. Our dataset is made publicly available for\nresearch purposes at https://github.com/davidmchan/Anim400K.",
        "translated": ""
    },
    {
        "title": "Strategic Client Selection to Address Non-IIDness in HAPS-enabled FL\n  Networks",
        "url": "http://arxiv.org/abs/2401.05308v1",
        "pub_date": "2024-01-10",
        "summary": "The deployment of federated learning (FL) within vertical heterogeneous\nnetworks, such as those enabled by high-altitude platform station (HAPS),\noffers the opportunity to engage a wide array of clients, each endowed with\ndistinct communication and computational capabilities. This diversity not only\nenhances the training accuracy of FL models but also hastens their convergence.\nYet, applying FL in these expansive networks presents notable challenges,\nparticularly the significant non-IIDness in client data distributions. Such\ndata heterogeneity often results in slower convergence rates and reduced\neffectiveness in model training performance. Our study introduces a client\nselection strategy tailored to address this issue, leveraging user network\ntraffic behaviour. This strategy involves the prediction and classification of\nclients based on their network usage patterns while prioritizing user privacy.\nBy strategically selecting clients whose data exhibit similar patterns for\nparticipation in FL training, our approach fosters a more uniform and\nrepresentative data distribution across the network. Our simulations\ndemonstrate that this targeted client selection methodology significantly\nreduces the training loss of FL models in HAPS networks, thereby effectively\ntackling a crucial challenge in implementing large-scale FL systems.",
        "translated": ""
    },
    {
        "title": "Enhanced Muscle and Fat Segmentation for CT-Based Body Composition\n  Analysis: A Comparative Study",
        "url": "http://arxiv.org/abs/2401.05294v1",
        "pub_date": "2024-01-10",
        "summary": "Purpose: Body composition measurements from routine abdominal CT can yield\npersonalized risk assessments for asymptomatic and diseased patients. In\nparticular, attenuation and volume measures of muscle and fat are associated\nwith important clinical outcomes, such as cardiovascular events, fractures, and\ndeath. This study evaluates the reliability of an Internal tool for the\nsegmentation of muscle and fat (subcutaneous and visceral) as compared to the\nwell-established public TotalSegmentator tool.\n  Methods: We assessed the tools across 900 CT series from the publicly\navailable SAROS dataset, focusing on muscle, subcutaneous fat, and visceral\nfat. The Dice score was employed to assess accuracy in subcutaneous fat and\nmuscle segmentation. Due to the lack of ground truth segmentations for visceral\nfat, Cohen's Kappa was utilized to assess segmentation agreement between the\ntools.\n  Results: Our Internal tool achieved a 3% higher Dice (83.8 vs. 80.8) for\nsubcutaneous fat and a 5% improvement (87.6 vs. 83.2) for muscle segmentation\nrespectively. A Wilcoxon signed-rank test revealed that our results were\nstatistically different with p&lt;0.01. For visceral fat, the Cohen's kappa score\nof 0.856 indicated near-perfect agreement between the two tools. Our internal\ntool also showed very strong correlations for muscle volume (R^2=0.99), muscle\nattenuation (R^2=0.93), and subcutaneous fat volume (R^2=0.99) with a moderate\ncorrelation for subcutaneous fat attenuation (R^2=0.45).\n  Conclusion: Our findings indicated that our Internal tool outperformed\nTotalSegmentator in measuring subcutaneous fat and muscle. The high Cohen's\nKappa score for visceral fat suggests a reliable level of agreement between the\ntwo tools. These results demonstrate the potential of our tool in advancing the\naccuracy of body composition analysis.",
        "translated": ""
    },
    {
        "title": "Score Distillation Sampling with Learned Manifold Corrective",
        "url": "http://arxiv.org/abs/2401.05293v1",
        "pub_date": "2024-01-10",
        "summary": "Score Distillation Sampling (SDS) is a recent but already widely popular\nmethod that relies on an image diffusion model to control optimization problems\nusing text prompts. In this paper, we conduct an in-depth analysis of the SDS\nloss function, identify an inherent problem with its formulation, and propose a\nsurprisingly easy but effective fix. Specifically, we decompose the loss into\ndifferent factors and isolate the component responsible for noisy gradients. In\nthe original formulation, high text guidance is used to account for the noise,\nleading to unwanted side effects. Instead, we train a shallow network mimicking\nthe timestep-dependent denoising deficiency of the image diffusion model in\norder to effectively factor it out. We demonstrate the versatility and the\neffectiveness of our novel loss formulation through several qualitative and\nquantitative experiments, including optimization-based image synthesis and\nediting, zero-shot image translation network training, and text-to-3D\nsynthesis.",
        "translated": ""
    },
    {
        "title": "PIXART-δ: Fast and Controllable Image Generation with Latent\n  Consistency Models",
        "url": "http://arxiv.org/abs/2401.05252v1",
        "pub_date": "2024-01-10",
        "summary": "This technical report introduces PIXART-{\\delta}, a text-to-image synthesis\nframework that integrates the Latent Consistency Model (LCM) and ControlNet\ninto the advanced PIXART-{\\alpha} model. PIXART-{\\alpha} is recognized for its\nability to generate high-quality images of 1024px resolution through a\nremarkably efficient training process. The integration of LCM in\nPIXART-{\\delta} significantly accelerates the inference speed, enabling the\nproduction of high-quality images in just 2-4 steps. Notably, PIXART-{\\delta}\nachieves a breakthrough 0.5 seconds for generating 1024x1024 pixel images,\nmarking a 7x improvement over the PIXART-{\\alpha}. Additionally,\nPIXART-{\\delta} is designed to be efficiently trainable on 32GB V100 GPUs\nwithin a single day. With its 8-bit inference capability (von Platen et al.,\n2023), PIXART-{\\delta} can synthesize 1024px images within 8GB GPU memory\nconstraints, greatly enhancing its usability and accessibility. Furthermore,\nincorporating a ControlNet-like module enables fine-grained control over\ntext-to-image diffusion models. We introduce a novel ControlNet-Transformer\narchitecture, specifically tailored for Transformers, achieving explicit\ncontrollability alongside high-quality image generation. As a state-of-the-art,\nopen-source image generation model, PIXART-{\\delta} offers a promising\nalternative to the Stable Diffusion family of models, contributing\nsignificantly to text-to-image synthesis.",
        "translated": ""
    },
    {
        "title": "Structure from Duplicates: Neural Inverse Graphics from a Pile of\n  Objects",
        "url": "http://arxiv.org/abs/2401.05236v1",
        "pub_date": "2024-01-10",
        "summary": "Our world is full of identical objects (\\emphe.g., cans of coke, cars of same\nmodel). These duplicates, when seen together, provide additional and strong\ncues for us to effectively reason about 3D. Inspired by this observation, we\nintroduce Structure from Duplicates (SfD), a novel inverse graphics framework\nthat reconstructs geometry, material, and illumination from a single image\ncontaining multiple identical objects. SfD begins by identifying multiple\ninstances of an object within an image, and then jointly estimates the 6DoF\npose for all instances.An inverse graphics pipeline is subsequently employed to\njointly reason about the shape, material of the object, and the environment\nlight, while adhering to the shared geometry and material constraint across\ninstances. Our primary contributions involve utilizing object duplicates as a\nrobust prior for single-image inverse graphics and proposing an in-plane\nrotation-robust Structure from Motion (SfM) formulation for joint 6-DoF object\npose estimation. By leveraging multi-view cues from a single image, SfD\ngenerates more realistic and detailed 3D reconstructions, significantly\noutperforming existing single image reconstruction models and multi-view\nreconstruction approaches with a similar or greater number of observations.",
        "translated": ""
    },
    {
        "title": "Measuring Natural Scenes SFR of Automotive Fisheye Cameras",
        "url": "http://arxiv.org/abs/2401.05232v1",
        "pub_date": "2024-01-10",
        "summary": "The Modulation Transfer Function (MTF) is an important image quality metric\ntypically used in the automotive domain. However, despite the fact that optical\nquality has an impact on the performance of computer vision in vehicle\nautomation, for many public datasets, this metric is unknown. Additionally,\nwide field-of-view (FOV) cameras have become increasingly popular, particularly\nfor low-speed vehicle automation applications. To investigate image quality in\ndatasets, this paper proposes an adaptation of the Natural Scenes Spatial\nFrequency Response (NS-SFR) algorithm to suit cameras with a wide\nfield-of-view.",
        "translated": ""
    },
    {
        "title": "Distilling Vision-Language Models on Millions of Videos",
        "url": "http://arxiv.org/abs/2401.06129v1",
        "pub_date": "2024-01-11",
        "summary": "The recent advance in vision-language models is largely attributed to the\nabundance of image-text data. We aim to replicate this success for\nvideo-language models, but there simply is not enough human-curated video-text\ndata available. We thus resort to fine-tuning a video-language model from a\nstrong image-language baseline with synthesized instructional data. The\nresulting video-language model is then used to auto-label millions of videos to\ngenerate high-quality captions. We show the adapted video-language model\nperforms well on a wide range of video-language benchmarks. For instance, it\nsurpasses the best prior result on open-ended NExT-QA by 2.8%. Besides, our\nmodel generates detailed descriptions for previously unseen videos, which\nprovide better textual supervision than existing methods. Experiments show that\na video-language dual-encoder model contrastively trained on these\nauto-generated captions is 3.8% better than the strongest baseline that also\nleverages vision-language models. Our best model outperforms state-of-the-art\nmethods on MSR-VTT zero-shot text-to-video retrieval by 6%.",
        "translated": ""
    },
    {
        "title": "E$^{2}$GAN: Efficient Training of Efficient GANs for Image-to-Image\n  Translation",
        "url": "http://arxiv.org/abs/2401.06127v1",
        "pub_date": "2024-01-11",
        "summary": "One highly promising direction for enabling flexible real-time on-device\nimage editing is utilizing data distillation by leveraging large-scale\ntext-to-image diffusion models, such as Stable Diffusion, to generate paired\ndatasets used for training generative adversarial networks (GANs). This\napproach notably alleviates the stringent requirements typically imposed by\nhigh-end commercial GPUs for performing image editing with diffusion models.\nHowever, unlike text-to-image diffusion models, each distilled GAN is\nspecialized for a specific image editing task, necessitating costly training\nefforts to obtain models for various concepts. In this work, we introduce and\naddress a novel research direction: can the process of distilling GANs from\ndiffusion models be made significantly more efficient? To achieve this goal, we\npropose a series of innovative techniques. First, we construct a base GAN model\nwith generalized features, adaptable to different concepts through fine-tuning,\neliminating the need for training from scratch. Second, we identify crucial\nlayers within the base GAN model and employ Low-Rank Adaptation (LoRA) with a\nsimple yet effective rank search process, rather than fine-tuning the entire\nbase model. Third, we investigate the minimal amount of data necessary for\nfine-tuning, further reducing the overall training time. Extensive experiments\nshow that we can efficiently empower GANs with the ability to perform real-time\nhigh-quality image editing on mobile devices with remarkable reduced training\ncost and storage for each concept.",
        "translated": ""
    },
    {
        "title": "Dubbing for Everyone: Data-Efficient Visual Dubbing using Neural\n  Rendering Priors",
        "url": "http://arxiv.org/abs/2401.06126v1",
        "pub_date": "2024-01-11",
        "summary": "Visual dubbing is the process of generating lip motions of an actor in a\nvideo to synchronise with given audio. Recent advances have made progress\ntowards this goal but have not been able to produce an approach suitable for\nmass adoption. Existing methods are split into either person-generic or\nperson-specific models. Person-specific models produce results almost\nindistinguishable from reality but rely on long training times using large\nsingle-person datasets. Person-generic works have allowed for the visual\ndubbing of any video to any audio without further training, but these fail to\ncapture the person-specific nuances and often suffer from visual artefacts. Our\nmethod, based on data-efficient neural rendering priors, overcomes the\nlimitations of existing approaches. Our pipeline consists of learning a\ndeferred neural rendering prior network and actor-specific adaptation using\nneural textures. This method allows for $\\textbf{high-quality visual dubbing\nwith just a few seconds of data}$, that enables video dubbing for any actor -\nfrom A-list celebrities to background actors. We show that we achieve\nstate-of-the-art in terms of $\\textbf{visual quality}$ and\n$\\textbf{recognisability}$ both quantitatively, and qualitatively through two\nuser studies. Our prior learning and adaptation method $\\textbf{generalises to\nlimited data}$ better and is more $\\textbf{scalable}$ than existing\nperson-specific models. Our experiments on real-world, limited data scenarios\nfind that our model is preferred over all others. The project page may be found\nat https://dubbingforeveryone.github.io/",
        "translated": ""
    },
    {
        "title": "Manipulating Feature Visualizations with Gradient Slingshots",
        "url": "http://arxiv.org/abs/2401.06122v1",
        "pub_date": "2024-01-11",
        "summary": "Deep Neural Networks (DNNs) are capable of learning complex and versatile\nrepresentations, however, the semantic nature of the learned concepts remains\nunknown. A common method used to explain the concepts learned by DNNs is\nActivation Maximization (AM), which generates a synthetic input signal that\nmaximally activates a particular neuron in the network. In this paper, we\ninvestigate the vulnerability of this approach to adversarial model\nmanipulations and introduce a novel method for manipulating feature\nvisualization without altering the model architecture or significantly\nimpacting the model's decision-making process. We evaluate the effectiveness of\nour method on several neural network models and demonstrate its capabilities to\nhide the functionality of specific neurons by masking the original explanations\nof neurons with chosen target explanations during model auditing. As a remedy,\nwe propose a protective measure against such manipulations and provide\nquantitative evidence which substantiates our findings.",
        "translated": ""
    },
    {
        "title": "Gaussian Shadow Casting for Neural Characters",
        "url": "http://arxiv.org/abs/2401.06116v1",
        "pub_date": "2024-01-11",
        "summary": "Neural character models can now reconstruct detailed geometry and texture\nfrom video, but they lack explicit shadows and shading, leading to artifacts\nwhen generating novel views and poses or during relighting. It is particularly\ndifficult to include shadows as they are a global effect and the required\ncasting of secondary rays is costly. We propose a new shadow model using a\nGaussian density proxy that replaces sampling with a simple analytic formula.\nIt supports dynamic motion and is tailored for shadow computation, thereby\navoiding the affine projection approximation and sorting required by the\nclosely related Gaussian splatting. Combined with a deferred neural rendering\nmodel, our Gaussian shadows enable Lambertian shading and shadow casting with\nminimal overhead. We demonstrate improved reconstructions, with better\nseparation of albedo, shading, and shadows in challenging outdoor scenes with\ndirect sun light and hard shadows. Our method is able to optimize the light\ndirection without any input from the user. As a result, novel poses have fewer\nshadow artifacts and relighting in novel scenes is more realistic compared to\nthe state-of-the-art methods, providing new ways to pose neural characters in\nnovel environments, increasing their applicability.",
        "translated": ""
    },
    {
        "title": "PALP: Prompt Aligned Personalization of Text-to-Image Models",
        "url": "http://arxiv.org/abs/2401.06105v1",
        "pub_date": "2024-01-11",
        "summary": "Content creators often aim to create personalized images using personal\nsubjects that go beyond the capabilities of conventional text-to-image models.\nAdditionally, they may want the resulting image to encompass a specific\nlocation, style, ambiance, and more. Existing personalization methods may\ncompromise personalization ability or the alignment to complex textual prompts.\nThis trade-off can impede the fulfillment of user prompts and subject fidelity.\nWe propose a new approach focusing on personalization methods for a\n\\emph{single} prompt to address this issue. We term our approach prompt-aligned\npersonalization. While this may seem restrictive, our method excels in\nimproving text alignment, enabling the creation of images with complex and\nintricate prompts, which may pose a challenge for current techniques. In\nparticular, our method keeps the personalized model aligned with a target\nprompt using an additional score distillation sampling term. We demonstrate the\nversatility of our method in multi- and single-shot settings and further show\nthat it can compose multiple subjects or use inspiration from reference images,\nsuch as artworks. We compare our approach quantitatively and qualitatively with\nexisting baselines and state-of-the-art techniques.",
        "translated": ""
    },
    {
        "title": "LEGO:Language Enhanced Multi-modal Grounding Model",
        "url": "http://arxiv.org/abs/2401.06071v1",
        "pub_date": "2024-01-11",
        "summary": "Multi-modal large language models have demonstrated impressive performance\nacross various tasks in different modalities. However, existing multi-modal\nmodels primarily emphasize capturing global information within each modality\nwhile neglecting the importance of perceiving local information across\nmodalities. Consequently, these models lack the ability to effectively\nunderstand the fine-grained details of input data, limiting their performance\nin tasks that require a more nuanced understanding. To address this limitation,\nthere is a compelling need to develop models that enable fine-grained\nunderstanding across multiple modalities, thereby enhancing their applicability\nto a wide range of tasks. In this paper, we propose LEGO, a language enhanced\nmulti-modal grounding model. Beyond capturing global information like other\nmulti-modal models, our proposed model excels at tasks demanding a detailed\nunderstanding of local information within the input. It demonstrates precise\nidentification and localization of specific regions in images or moments in\nvideos. To achieve this objective, we design a diversified dataset construction\npipeline, resulting in a multi-modal, multi-granularity dataset for model\ntraining. The code, dataset, and demo of our model can be found at https:\n//github.com/lzw-lzw/LEGO.",
        "translated": ""
    },
    {
        "title": "MatSynth: A Modern PBR Materials Dataset",
        "url": "http://arxiv.org/abs/2401.06056v1",
        "pub_date": "2024-01-11",
        "summary": "We introduce MatSynth, a dataset of $4,000+$ CC0 ultra-high resolution PBR\nmaterials. Materials are crucial components of virtual relightable assets,\ndefining the interaction of light at the surface of geometries. Given their\nimportance, significant research effort was dedicated to their representation,\ncreation and acquisition. However, in the past 6 years, most research in\nmaterial acquisiton or generation relied either on the same unique dataset, or\non company-owned huge library of procedural materials. With this dataset we\npropose a significantly larger, more diverse, and higher resolution set of\nmaterials than previously publicly available. We carefully discuss the data\ncollection process and demonstrate the benefits of this dataset on material\nacquisition and generation applications. The complete data further contains\nmetadata with each material's origin, license, category, tags, creation method\nand, when available, descriptions and physical size, as well as 3M+ renderings\nof the augmented materials, in 1K, under various environment lightings. The\nMatSynth dataset is released through the project page at:\nhttps://www.gvecchio.com/matsynth.",
        "translated": ""
    },
    {
        "title": "Fast High Dynamic Range Radiance Fields for Dynamic Scenes",
        "url": "http://arxiv.org/abs/2401.06052v1",
        "pub_date": "2024-01-11",
        "summary": "Neural Radiances Fields (NeRF) and their extensions have shown great success\nin representing 3D scenes and synthesizing novel-view images. However, most\nNeRF methods take in low-dynamic-range (LDR) images, which may lose details,\nespecially with nonuniform illumination. Some previous NeRF methods attempt to\nintroduce high-dynamic-range (HDR) techniques but mainly target static scenes.\nTo extend HDR NeRF methods to wider applications, we propose a dynamic HDR NeRF\nframework, named HDR-HexPlane, which can learn 3D scenes from dynamic 2D images\ncaptured with various exposures. A learnable exposure mapping function is\nconstructed to obtain adaptive exposure values for each image. Based on the\nmonotonically increasing prior, a camera response function is designed for\nstable learning. With the proposed model, high-quality novel-view images at any\ntime point can be rendered with any desired exposure. We further construct a\ndataset containing multiple dynamic scenes captured with diverse exposures for\nevaluation. All the datasets and code are available at\n\\url{https://guanjunwu.github.io/HDR-HexPlane/}.",
        "translated": ""
    },
    {
        "title": "RAVEN: Rethinking Adversarial Video Generation with Efficient Tri-plane\n  Networks",
        "url": "http://arxiv.org/abs/2401.06035v1",
        "pub_date": "2024-01-11",
        "summary": "We present a novel unconditional video generative model designed to address\nlong-term spatial and temporal dependencies. To capture these dependencies, our\napproach incorporates a hybrid explicit-implicit tri-plane representation\ninspired by 3D-aware generative frameworks developed for three-dimensional\nobject representation and employs a singular latent code to model an entire\nvideo sequence. Individual video frames are then synthesized from an\nintermediate tri-plane representation, which itself is derived from the primary\nlatent code. This novel strategy reduces computational complexity by a factor\nof $2$ as measured in FLOPs. Consequently, our approach facilitates the\nefficient and temporally coherent generation of videos. Moreover, our joint\nframe modeling approach, in contrast to autoregressive methods, mitigates the\ngeneration of visual artifacts. We further enhance the model's capabilities by\nintegrating an optical flow-based module within our Generative Adversarial\nNetwork (GAN) based generator architecture, thereby compensating for the\nconstraints imposed by a smaller generator size. As a result, our model is\ncapable of synthesizing high-fidelity video clips at a resolution of\n$256\\times256$ pixels, with durations extending to more than $5$ seconds at a\nframe rate of 30 fps. The efficacy and versatility of our approach are\nempirically validated through qualitative and quantitative assessments across\nthree different datasets comprising both synthetic and real video clips.",
        "translated": ""
    },
    {
        "title": "Seeing the roads through the trees: A benchmark for modeling spatial\n  dependencies with aerial imagery",
        "url": "http://arxiv.org/abs/2401.06762v1",
        "pub_date": "2024-01-12",
        "summary": "Fully understanding a complex high-resolution satellite or aerial imagery\nscene often requires spatial reasoning over a broad relevant context. The human\nobject recognition system is able to understand object in a scene over a\nlong-range relevant context. For example, if a human observes an aerial scene\nthat shows sections of road broken up by tree canopy, then they will be\nunlikely to conclude that the road has actually been broken up into disjoint\npieces by trees and instead think that the canopy of nearby trees is occluding\nthe road. However, there is limited research being conducted to understand\nlong-range context understanding of modern machine learning models. In this\nwork we propose a road segmentation benchmark dataset, Chesapeake Roads Spatial\nContext (RSC), for evaluating the spatial long-range context understanding of\ngeospatial machine learning models and show how commonly used semantic\nsegmentation models can fail at this task. For example, we show that a U-Net\ntrained to segment roads from background in aerial imagery achieves an 84%\nrecall on unoccluded roads, but just 63.5% recall on roads covered by tree\ncanopy despite being trained to model both the same way. We further analyze how\nthe performance of models changes as the relevant context for a decision\n(unoccluded roads in our case) varies in distance. We release the code to\nreproduce our experiments and dataset of imagery and masks to encourage future\nresearch in this direction -- https://github.com/isaaccorley/ChesapeakeRSC.",
        "translated": ""
    },
    {
        "title": "Synthetic Data Generation Framework, Dataset, and Efficient Deep Model\n  for Pedestrian Intention Prediction",
        "url": "http://arxiv.org/abs/2401.06757v1",
        "pub_date": "2024-01-12",
        "summary": "Pedestrian intention prediction is crucial for autonomous driving. In\nparticular, knowing if pedestrians are going to cross in front of the\nego-vehicle is core to performing safe and comfortable maneuvers. Creating\naccurate and fast models that predict such intentions from sequential images is\nchallenging. A factor contributing to this is the lack of datasets with diverse\ncrossing and non-crossing (C/NC) scenarios. We address this scarceness by\nintroducing a framework, named ARCANE, which allows programmatically generating\nsynthetic datasets consisting of C/NC video clip samples. As an example, we use\nARCANE to generate a large and diverse dataset named PedSynth. We will show how\nPedSynth complements widely used real-world datasets such as JAAD and PIE, so\nenabling more accurate models for C/NC prediction. Considering the onboard\ndeployment of C/NC prediction models, we also propose a deep model named\nPedGNN, which is fast and has a very low memory footprint. PedGNN is based on a\nGNN-GRU architecture that takes a sequence of pedestrian skeletons as input to\npredict crossing intentions.",
        "translated": ""
    },
    {
        "title": "Scalable 3D Panoptic Segmentation With Superpoint Graph Clustering",
        "url": "http://arxiv.org/abs/2401.06704v1",
        "pub_date": "2024-01-12",
        "summary": "We introduce a highly efficient method for panoptic segmentation of large 3D\npoint clouds by redefining this task as a scalable graph clustering problem.\nThis approach can be trained using only local auxiliary tasks, thereby\neliminating the resource-intensive instance-matching step during training.\nMoreover, our formulation can easily be adapted to the superpoint paradigm,\nfurther increasing its efficiency. This allows our model to process scenes with\nmillions of points and thousands of objects in a single inference. Our method,\ncalled SuperCluster, achieves a new state-of-the-art panoptic segmentation\nperformance for two indoor scanning datasets: $50.1$ PQ ($+7.8$) for S3DIS\nArea~5, and $58.7$ PQ ($+25.2$) for ScanNetV2. We also set the first\nstate-of-the-art for two large-scale mobile mapping benchmarks: KITTI-360 and\nDALES. With only $209$k parameters, our model is over $30$ times smaller than\nthe best-competing method and trains up to $15$ times faster. Our code and\npretrained models are available at\nhttps://github.com/drprojects/superpoint_transformer.",
        "translated": ""
    },
    {
        "title": "Embedded Planogram Compliance Control System",
        "url": "http://arxiv.org/abs/2401.06690v1",
        "pub_date": "2024-01-12",
        "summary": "The retail sector presents several open and challenging problems that could\nbenefit from advanced pattern recognition and computer vision techniques. One\nsuch critical challenge is planogram compliance control. In this study, we\npropose a complete embedded system to tackle this issue. Our system consists of\nfour key components as image acquisition and transfer via stand-alone embedded\ncamera module, object detection via computer vision and deep learning methods\nworking on single board computers, planogram compliance control method again\nworking on single board computers, and energy harvesting and power management\nblock to accompany the embedded camera modules. The image acquisition and\ntransfer block is implemented on the ESP-EYE camera module. The object\ndetection block is based on YOLOv5 as the deep learning method and local\nfeature extraction. We implement these methods on Raspberry Pi 4, NVIDIA Jetson\nOrin Nano, and NVIDIA Jetson AGX Orin as single board computers. The planogram\ncompliance control block utilizes sequence alignment through a modified\nNeedleman-Wunsch algorithm. This block is also working along with the object\ndetection block on the same single board computers. The energy harvesting and\npower management block consists of solar and RF energy harvesting modules with\nsuitable battery pack for operation. We tested the proposed embedded planogram\ncompliance control system on two different datasets to provide valuable\ninsights on its strengths and weaknesses. The results show that our method\nachieves F1 scores of 0.997 and 1.0 in object detection and planogram\ncompliance control blocks, respectively. Furthermore, we calculated that the\ncomplete embedded system can work in stand-alone form up to two years based on\nbattery. This duration can be further extended with the integration of the\nproposed solar and RF energy harvesting options.",
        "translated": ""
    },
    {
        "title": "Decoupling Pixel Flipping and Occlusion Strategy for Consistent XAI\n  Benchmarks",
        "url": "http://arxiv.org/abs/2401.06654v1",
        "pub_date": "2024-01-12",
        "summary": "Feature removal is a central building block for eXplainable AI (XAI), both\nfor occlusion-based explanations (Shapley values) as well as their evaluation\n(pixel flipping, PF). However, occlusion strategies can vary significantly from\nsimple mean replacement up to inpainting with state-of-the-art diffusion\nmodels. This ambiguity limits the usefulness of occlusion-based approaches. For\nexample, PF benchmarks lead to contradicting rankings. This is amplified by\ncompeting PF measures: Features are either removed starting with most\ninfluential first (MIF) or least influential first (LIF). This study proposes\ntwo complementary perspectives to resolve this disagreement problem. Firstly,\nwe address the common criticism of occlusion-based XAI, that artificial samples\nlead to unreliable model evaluations. We propose to measure the reliability by\nthe R(eference)-Out-of-Model-Scope (OMS) score. The R-OMS score enables a\nsystematic comparison of occlusion strategies and resolves the disagreement\nproblem by grouping consistent PF rankings. Secondly, we show that the\ninsightfulness of MIF and LIF is conversely dependent on the R-OMS score. To\nleverage this, we combine the MIF and LIF measures into the symmetric relevance\ngain (SRG) measure. This breaks the inherent connection to the underlying\nocclusion strategy and leads to consistent rankings. This resolves the\ndisagreement problem, which we verify for a set of 40 different occlusion\nstrategies.",
        "translated": ""
    },
    {
        "title": "Adversarial Examples are Misaligned in Diffusion Model Manifolds",
        "url": "http://arxiv.org/abs/2401.06637v1",
        "pub_date": "2024-01-12",
        "summary": "In recent years, diffusion models (DMs) have drawn significant attention for\ntheir success in approximating data distributions, yielding state-of-the-art\ngenerative results. Nevertheless, the versatility of these models extends\nbeyond their generative capabilities to encompass various vision applications,\nsuch as image inpainting, segmentation, adversarial robustness, among others.\nThis study is dedicated to the investigation of adversarial attacks through the\nlens of diffusion models. However, our objective does not involve enhancing the\nadversarial robustness of image classifiers. Instead, our focus lies in\nutilizing the diffusion model to detect and analyze the anomalies introduced by\nthese attacks on images. To that end, we systematically examine the alignment\nof the distributions of adversarial examples when subjected to the process of\ntransformation using diffusion models. The efficacy of this approach is\nassessed across CIFAR-10 and ImageNet datasets, including varying image sizes\nin the latter. The results demonstrate a notable capacity to discriminate\neffectively between benign and attacked images, providing compelling evidence\nthat adversarial instances do not align with the learned manifold of the DMs.",
        "translated": ""
    },
    {
        "title": "Motion2VecSets: 4D Latent Vector Set Diffusion for Non-rigid Shape\n  Reconstruction and Tracking",
        "url": "http://arxiv.org/abs/2401.06614v1",
        "pub_date": "2024-01-12",
        "summary": "We introduce Motion2VecSets, a 4D diffusion model for dynamic surface\nreconstruction from point cloud sequences. While existing state-of-the-art\nmethods have demonstrated success in reconstructing non-rigid objects using\nneural field representations, conventional feed-forward networks encounter\nchallenges with ambiguous observations from noisy, partial, or sparse point\nclouds. To address these challenges, we introduce a diffusion model that\nexplicitly learns the shape and motion distribution of non-rigid objects\nthrough an iterative denoising process of compressed latent representations.\nThe diffusion-based prior enables more plausible and probabilistic\nreconstructions when handling ambiguous inputs. We parameterize 4D dynamics\nwith latent vector sets instead of using a global latent. This novel 4D\nrepresentation allows us to learn local surface shape and deformation patterns,\nleading to more accurate non-linear motion capture and significantly improving\ngeneralizability to unseen motions and identities. For more temporal-coherent\nobject tracking, we synchronously denoise deformation latent sets and exchange\ninformation across multiple frames. To avoid the computational overhead, we\ndesign an interleaved space and time attention block to alternately aggregate\ndeformation latents along spatial and temporal domains. Extensive comparisons\nagainst the state-of-the-art methods demonstrate the superiority of our\nMotion2VecSets in 4D reconstruction from various imperfect observations,\nnotably achieving a 19% improvement in Intersection over Union (IoU) compared\nto CaDex for reconstructing unseen individuals from sparse point clouds on the\nDeformingThings4D-Animals dataset. More detailed information can be found at\nhttps://vveicao.github.io/projects/Motion2VecSets/.",
        "translated": ""
    },
    {
        "title": "Dynamic Behaviour of Connectionist Speech Recognition with Strong\n  Latency Constraints",
        "url": "http://arxiv.org/abs/2401.06588v1",
        "pub_date": "2024-01-12",
        "summary": "This paper describes the use of connectionist techniques in phonetic speech\nrecognition with strong latency constraints. The constraints are imposed by the\ntask of deriving the lip movements of a synthetic face in real time from the\nspeech signal, by feeding the phonetic string into an articulatory synthesiser.\nParticular attention has been paid to analysing the interaction between the\ntime evolution model learnt by the multi-layer perceptrons and the transition\nmodel imposed by the Viterbi decoder, in different latency conditions. Two\nexperiments were conducted in which the time dependencies in the language model\n(LM) were controlled by a parameter. The results show a strong interaction\nbetween the three factors involved, namely the neural network topology, the\nlength of time dependencies in the LM and the decoder latency.",
        "translated": ""
    },
    {
        "title": "360DVD: Controllable Panorama Video Generation with 360-Degree Video\n  Diffusion Model",
        "url": "http://arxiv.org/abs/2401.06578v1",
        "pub_date": "2024-01-12",
        "summary": "360-degree panoramic videos recently attract more interest in both studies\nand applications, courtesy of the heightened immersive experiences they\nengender. Due to the expensive cost of capturing 360-degree panoramic videos,\ngenerating desirable panoramic videos by given prompts is urgently required.\nRecently, the emerging text-to-video (T2V) diffusion methods demonstrate\nnotable effectiveness in standard video generation. However, due to the\nsignificant gap in content and motion patterns between panoramic and standard\nvideos, these methods encounter challenges in yielding satisfactory 360-degree\npanoramic videos. In this paper, we propose a controllable panorama video\ngeneration pipeline named 360-Degree Video Diffusion model (360DVD) for\ngenerating panoramic videos based on the given prompts and motion conditions.\nConcretely, we introduce a lightweight module dubbed 360-Adapter and assisted\n360 Enhancement Techniques to transform pre-trained T2V models for 360-degree\nvideo generation. We further propose a new panorama dataset named WEB360\nconsisting of 360-degree video-text pairs for training 360DVD, addressing the\nabsence of captioned panoramic video datasets. Extensive experiments\ndemonstrate the superiority and effectiveness of 360DVD for panorama video\ngeneration. The code and dataset will be released soon.",
        "translated": ""
    },
    {
        "title": "Resource-Efficient Gesture Recognition using Low-Resolution Thermal\n  Camera via Spiking Neural Networks and Sparse Segmentation",
        "url": "http://arxiv.org/abs/2401.06563v1",
        "pub_date": "2024-01-12",
        "summary": "This work proposes a novel approach for hand gesture recognition using an\ninexpensive, low-resolution (24 x 32) thermal sensor processed by a Spiking\nNeural Network (SNN) followed by Sparse Segmentation and feature-based gesture\nclassification via Robust Principal Component Analysis (R-PCA). Compared to the\nuse of standard RGB cameras, the proposed system is insensitive to lighting\nvariations while being significantly less expensive compared to high-frequency\nradars, time-of-flight cameras and high-resolution thermal sensors previously\nused in literature. Crucially, this paper shows that the innovative use of the\nrecently proposed Monostable Multivibrator (MMV) neural networks as a new class\nof SNN achieves more than one order of magnitude smaller memory and compute\ncomplexity compared to deep learning approaches, while reaching a top gesture\nrecognition accuracy of 93.9% using a 5-class thermal camera dataset acquired\nin a car cabin, within an automotive context. Our dataset is released for\nhelping future research.",
        "translated": ""
    },
    {
        "title": "SAMF: Small-Area-Aware Multi-focus Image Fusion for Object Detection",
        "url": "http://arxiv.org/abs/2401.08357v1",
        "pub_date": "2024-01-16",
        "summary": "Existing multi-focus image fusion (MFIF) methods often fail to preserve the\nuncertain transition region and detect small focus areas within large defocused\nregions accurately. To address this issue, this study proposes a new\nsmall-area-aware MFIF algorithm for enhancing object detection capability.\nFirst, we enhance the pixel attributes within the small focus and boundary\nregions, which are subsequently combined with visual saliency detection to\nobtain the pre-fusion results used to discriminate the distribution of focused\npixels. To accurately ensure pixel focus, we consider the source image as a\ncombination of focused, defocused, and uncertain regions and propose a\nthree-region segmentation strategy. Finally, we design an effective pixel\nselection rule to generate segmentation decision maps and obtain the final\nfusion results. Experiments demonstrated that the proposed method can\naccurately detect small and smooth focus areas while improving object detection\nperformance, outperforming existing methods in both subjective and objective\nevaluations. The source code is available at https://github.com/ixilai/SAMF.",
        "translated": ""
    },
    {
        "title": "Multi-view Distillation based on Multi-modal Fusion for Few-shot Action\n  Recognition(CLIP-$\\mathrm{M^2}$DF)",
        "url": "http://arxiv.org/abs/2401.08345v1",
        "pub_date": "2024-01-16",
        "summary": "In recent years, few-shot action recognition has attracted increasing\nattention. It generally adopts the paradigm of meta-learning. In this field,\novercoming the overlapping distribution of classes and outliers is still a\nchallenging problem based on limited samples. We believe the combination of\nMulti-modal and Multi-view can improve this issue depending on information\ncomplementarity. Therefore, we propose a method of Multi-view Distillation\nbased on Multi-modal Fusion. Firstly, a Probability Prompt Selector for the\nquery is constructed to generate probability prompt embedding based on the\ncomparison score between the prompt embeddings of the support and the visual\nembedding of the query. Secondly, we establish a Multi-view. In each view, we\nfuse the prompt embedding as consistent information with visual and the global\nor local temporal context to overcome the overlapping distribution of classes\nand outliers. Thirdly, we perform the distance fusion for the Multi-view and\nthe mutual distillation of matching ability from one to another, enabling the\nmodel to be more robust to the distribution bias. Our code is available at the\nURL: \\url{https://github.com/cofly2014/MDMF}.",
        "translated": ""
    },
    {
        "title": "Generative Denoise Distillation: Simple Stochastic Noises Induce\n  Efficient Knowledge Transfer for Dense Prediction",
        "url": "http://arxiv.org/abs/2401.08332v1",
        "pub_date": "2024-01-16",
        "summary": "Knowledge distillation is the process of transferring knowledge from a more\npowerful large model (teacher) to a simpler counterpart (student). Numerous\ncurrent approaches involve the student imitating the knowledge of the teacher\ndirectly. However, redundancy still exists in the learned representations\nthrough these prevalent methods, which tend to learn each spatial location's\nfeatures indiscriminately. To derive a more compact representation (concept\nfeature) from the teacher, inspired by human cognition, we suggest an\ninnovative method, termed Generative Denoise Distillation (GDD), where\nstochastic noises are added to the concept feature of the student to embed them\ninto the generated instance feature from a shallow network. Then, the generated\ninstance feature is aligned with the knowledge of the instance from the\nteacher. We extensively experiment with object detection, instance\nsegmentation, and semantic segmentation to demonstrate the versatility and\neffectiveness of our method. Notably, GDD achieves new state-of-the-art\nperformance in the tasks mentioned above. We have achieved substantial\nimprovements in semantic segmentation by enhancing PspNet and DeepLabV3, both\nof which are based on ResNet-18, resulting in mIoU scores of 74.67 and 77.69,\nrespectively, surpassing their previous scores of 69.85 and 73.20 on the\nCityscapes dataset of 20 categories. The source code of GDD is available at\nhttps://github.com/ZhgLiu/GDD.",
        "translated": ""
    },
    {
        "title": "Un-Mixing Test-Time Normalization Statistics: Combatting Label Temporal\n  Correlation",
        "url": "http://arxiv.org/abs/2401.08328v1",
        "pub_date": "2024-01-16",
        "summary": "In an era where test-time adaptation methods increasingly rely on the nuanced\nmanipulation of batch normalization (BN) parameters, one critical assumption\noften goes overlooked: that of independently and identically distributed\n(i.i.d.) test batches with respect to unknown labels. This assumption\nculminates in biased estimates of BN statistics and jeopardizes system\nstability under non-i.i.d. conditions. This paper pioneers a departure from the\ni.i.d. paradigm by introducing a groundbreaking strategy termed \"Un-Mixing\nTest-Time Normalization Statistics\" (UnMix-TNS). UnMix-TNS re-calibrates the\ninstance-wise statistics used to normalize each instance in a batch by mixing\nit with multiple unmixed statistics components, thus inherently simulating the\ni.i.d. environment. The key lies in our innovative online unmixing procedure,\nwhich persistently refines these statistics components by drawing upon the\nclosest instances from an incoming test batch. Remarkably generic in its\ndesign, UnMix-TNS seamlessly integrates with an array of state-of-the-art\ntest-time adaptation methods and pre-trained architectures equipped with BN\nlayers. Empirical evaluations corroborate the robustness of UnMix-TNS under\nvaried scenarios ranging from single to continual and mixed domain shifts.\nUnMix-TNS stands out when handling test data streams with temporal correlation,\nincluding those with corrupted real-world non-i.i.d. streams, sustaining its\nefficacy even with minimal batch sizes and individual samples. Our results set\na new standard for test-time adaptation, demonstrating significant improvements\nin both stability and performance across multiple benchmarks.",
        "translated": ""
    },
    {
        "title": "The Faiss library",
        "url": "http://arxiv.org/abs/2401.08281v1",
        "pub_date": "2024-01-16",
        "summary": "Vector databases manage large collections of embedding vectors. As AI\napplications are growing rapidly, so are the number of embeddings that need to\nbe stored and indexed. The Faiss library is dedicated to vector similarity\nsearch, a core functionality of vector databases. Faiss is a toolkit of\nindexing methods and related primitives used to search, cluster, compress and\ntransform vectors. This paper first describes the tradeoff space of vector\nsearch, then the design principles of Faiss in terms of structure, approach to\noptimization and interfacing. We benchmark key features of the library and\ndiscuss a few selected applications to highlight its broad applicability.",
        "translated": ""
    },
    {
        "title": "AesBench: An Expert Benchmark for Multimodal Large Language Models on\n  Image Aesthetics Perception",
        "url": "http://arxiv.org/abs/2401.08276v1",
        "pub_date": "2024-01-16",
        "summary": "With collective endeavors, multimodal large language models (MLLMs) are\nundergoing a flourishing development. However, their performances on image\naesthetics perception remain indeterminate, which is highly desired in\nreal-world applications. An obvious obstacle lies in the absence of a specific\nbenchmark to evaluate the effectiveness of MLLMs on aesthetic perception. This\nblind groping may impede the further development of more advanced MLLMs with\naesthetic perception capacity. To address this dilemma, we propose AesBench, an\nexpert benchmark aiming to comprehensively evaluate the aesthetic perception\ncapacities of MLLMs through elaborate design across dual facets. (1) We\nconstruct an Expert-labeled Aesthetics Perception Database (EAPD), which\nfeatures diversified image contents and high-quality annotations provided by\nprofessional aesthetic experts. (2) We propose a set of integrative criteria to\nmeasure the aesthetic perception abilities of MLLMs from four perspectives,\nincluding Perception (AesP), Empathy (AesE), Assessment (AesA) and\nInterpretation (AesI). Extensive experimental results underscore that the\ncurrent MLLMs only possess rudimentary aesthetic perception ability, and there\nis still a significant gap between MLLMs and humans. We hope this work can\ninspire the community to engage in deeper explorations on the aesthetic\npotentials of MLLMs. Source data will be available at\nhttps://github.com/yipoh/AesBench.",
        "translated": ""
    },
    {
        "title": "Modeling Spoof Noise by De-spoofing Diffusion and its Application in\n  Face Anti-spoofing",
        "url": "http://arxiv.org/abs/2401.08275v1",
        "pub_date": "2024-01-16",
        "summary": "Face anti-spoofing is crucial for ensuring the security and reliability of\nface recognition systems. Several existing face anti-spoofing methods utilize\nGAN-like networks to detect presentation attacks by estimating the noise\npattern of a spoof image and recovering the corresponding genuine image. But\nGAN's limited face appearance space results in the denoised faces cannot cover\nthe full data distribution of genuine faces, thereby undermining the\ngeneralization performance of such methods. In this work, we present a\npioneering attempt to employ diffusion models to denoise a spoof image and\nrestore the genuine image. The difference between these two images is\nconsidered as the spoof noise, which can serve as a discriminative cue for face\nanti-spoofing. We evaluate our proposed method on several intra-testing and\ninter-testing protocols, where the experimental results showcase the\neffectiveness of our method in achieving competitive performance in terms of\nboth accuracy and generalization.",
        "translated": ""
    },
    {
        "title": "Siamese Content-based Search Engine for a More Transparent Skin and\n  Breast Cancer Diagnosis through Histological Imaging",
        "url": "http://arxiv.org/abs/2401.08272v1",
        "pub_date": "2024-01-16",
        "summary": "Computer Aid Diagnosis (CAD) has developed digital pathology with Deep\nLearning (DL)-based tools to assist pathologists in decision-making.\nContent-Based Histopathological Image Retrieval (CBHIR) is a novel tool to seek\nhighly correlated patches in terms of similarity in histopathological features.\nIn this work, we proposed two CBHIR approaches on breast (Breast-twins) and\nskin cancer (Skin-twins) data sets for robust and accurate patch-level\nretrieval, integrating a custom-built Siamese network as a feature extractor.\nThe proposed Siamese network is able to generalize for unseen images by\nfocusing on the similar histopathological features of the input pairs. The\nproposed CBHIR approaches are evaluated on the Breast (public) and Skin\n(private) data sets with top K accuracy. Finding the optimum amount of K is\nchallenging, but also, as much as K increases, the dissimilarity between the\nquery and the returned images increases which might mislead the pathologists.\nTo the best of the author's belief, this paper is tackling this issue for the\nfirst time on histopathological images by evaluating the top first retrieved\nimages. The Breast-twins model achieves 70% of the F1score at the top first,\nwhich exceeds the other state-of-the-art methods at a higher amount of K such\nas 5 and 400. Skin-twins overpasses the recently proposed Convolutional Auto\nEncoder (CAE) by 67%, increasing the precision. Besides, the Skin-twins model\ntackles the challenges of Spitzoid Tumors of Uncertain Malignant Potential\n(STUMP) to assist pathologists with retrieving top K images and their\ncorresponding labels. So, this approach can offer a more explainable CAD tool\nto pathologists in terms of transparency, trustworthiness, or reliability among\nother characteristics.",
        "translated": ""
    },
    {
        "title": "Multi-Technique Sequential Information Consistency For Dynamic Visual\n  Place Recognition In Changing Environments",
        "url": "http://arxiv.org/abs/2401.08263v1",
        "pub_date": "2024-01-16",
        "summary": "Visual place recognition (VPR) is an essential component of robot navigation\nand localization systems that allows them to identify a place using only image\ndata. VPR is challenging due to the significant changes in a place's appearance\ndriven by different daily illumination, seasonal weather variations and diverse\nviewpoints. Currently, no single VPR technique excels in every environmental\ncondition, each exhibiting unique benefits and shortcomings, and therefore\ncombining multiple techniques can achieve more reliable VPR performance.\nPresent multi-method approaches either rely on online ground-truth information,\nwhich is often not available, or on brute-force technique combination,\npotentially lowering performance with high variance technique sets. Addressing\nthese shortcomings, we propose a VPR system dubbed Multi-Sequential Information\nConsistency (MuSIC) which leverages sequential information to select the most\ncohesive technique on an online per-frame basis. For each technique in a set,\nMuSIC computes their respective sequential consistencies by analysing the\nframe-to-frame continuity of their top match candidates, which are then\ndirectly compared to select the optimal technique for the current query image.\nThe use of sequential information to select between VPR methods results in an\noverall VPR performance increase across different benchmark datasets, while\navoiding the need for extra ground-truth of the runtime environment.",
        "translated": ""
    },
    {
        "title": "Multitask Learning in Minimally Invasive Surgical Vision: A Review",
        "url": "http://arxiv.org/abs/2401.08256v1",
        "pub_date": "2024-01-16",
        "summary": "Minimally invasive surgery (MIS) has revolutionized many procedures and led\nto reduced recovery time and risk of patient injury. However, MIS poses\nadditional complexity and burden on surgical teams. Data-driven surgical vision\nalgorithms are thought to be key building blocks in the development of future\nMIS systems with improved autonomy. Recent advancements in machine learning and\ncomputer vision have led to successful applications in analyzing videos\nobtained from MIS with the promise of alleviating challenges in MIS videos.\nSurgical scene and action understanding encompasses multiple related tasks\nthat, when solved individually, can be memory-intensive, inefficient, and fail\nto capture task relationships. Multitask learning (MTL), a learning paradigm\nthat leverages information from multiple related tasks to improve performance\nand aid generalization, is wellsuited for fine-grained and high-level\nunderstanding of MIS data. This review provides an overview of the current\nstate-of-the-art MTL systems that leverage videos obtained from MIS. Beyond\nlisting published approaches, we discuss the benefits and limitations of these\nMTL systems. Moreover, this manuscript presents an analysis of the literature\nfor various application fields of MTL in MIS, including those with large\nmodels, highlighting notable trends, new directions of research, and\ndevelopments.",
        "translated": ""
    },
    {
        "title": "GARField: Group Anything with Radiance Fields",
        "url": "http://arxiv.org/abs/2401.09419v1",
        "pub_date": "2024-01-17",
        "summary": "Grouping is inherently ambiguous due to the multiple levels of granularity in\nwhich one can decompose a scene -- should the wheels of an excavator be\nconsidered separate or part of the whole? We present Group Anything with\nRadiance Fields (GARField), an approach for decomposing 3D scenes into a\nhierarchy of semantically meaningful groups from posed image inputs. To do this\nwe embrace group ambiguity through physical scale: by optimizing a\nscale-conditioned 3D affinity feature field, a point in the world can belong to\ndifferent groups of different sizes. We optimize this field from a set of 2D\nmasks provided by Segment Anything (SAM) in a way that respects coarse-to-fine\nhierarchy, using scale to consistently fuse conflicting masks from different\nviewpoints. From this field we can derive a hierarchy of possible groupings via\nautomatic tree construction or user interaction. We evaluate GARField on a\nvariety of in-the-wild scenes and find it effectively extracts groups at many\nlevels: clusters of objects, objects, and various subparts. GARField inherently\nrepresents multi-view consistent groupings and produces higher fidelity groups\nthan the input SAM masks. GARField's hierarchical grouping could have exciting\ndownstream applications such as 3D asset extraction or dynamic scene\nunderstanding. See the project website at https://www.garfield.studio/",
        "translated": ""
    },
    {
        "title": "Vision Mamba: Efficient Visual Representation Learning with\n  Bidirectional State Space Model",
        "url": "http://arxiv.org/abs/2401.09417v1",
        "pub_date": "2024-01-17",
        "summary": "Recently the state space models (SSMs) with efficient hardware-aware designs,\ni.e., Mamba, have shown great potential for long sequence modeling. Building\nefficient and generic vision backbones purely upon SSMs is an appealing\ndirection. However, representing visual data is challenging for SSMs due to the\nposition-sensitivity of visual data and the requirement of global context for\nvisual understanding. In this paper, we show that the reliance of visual\nrepresentation learning on self-attention is not necessary and propose a new\ngeneric vision backbone with bidirectional Mamba blocks (Vim), which marks the\nimage sequences with position embeddings and compresses the visual\nrepresentation with bidirectional state space models. On ImageNet\nclassification, COCO object detection, and ADE20k semantic segmentation tasks,\nVim achieves higher performance compared to well-established vision\ntransformers like DeiT, while also demonstrating significantly improved\ncomputation &amp; memory efficiency. For example, Vim is 2.8$\\times$ faster than\nDeiT and saves 86.8% GPU memory when performing batch inference to extract\nfeatures on images with a resolution of 1248$\\times$1248. The results\ndemonstrate that Vim is capable of overcoming the computation &amp; memory\nconstraints on performing Transformer-style understanding for high-resolution\nimages and it has great potential to become the next-generation backbone for\nvision foundation models. Code is available at https://github.com/hustvl/Vim.",
        "translated": ""
    },
    {
        "title": "TextureDreamer: Image-guided Texture Synthesis through Geometry-aware\n  Diffusion",
        "url": "http://arxiv.org/abs/2401.09416v1",
        "pub_date": "2024-01-17",
        "summary": "We present TextureDreamer, a novel image-guided texture synthesis method to\ntransfer relightable textures from a small number of input images (3 to 5) to\ntarget 3D shapes across arbitrary categories. Texture creation is a pivotal\nchallenge in vision and graphics. Industrial companies hire experienced artists\nto manually craft textures for 3D assets. Classical methods require densely\nsampled views and accurately aligned geometry, while learning-based methods are\nconfined to category-specific shapes within the dataset. In contrast,\nTextureDreamer can transfer highly detailed, intricate textures from real-world\nenvironments to arbitrary objects with only a few casually captured images,\npotentially significantly democratizing texture creation. Our core idea,\npersonalized geometry-aware score distillation (PGSD), draws inspiration from\nrecent advancements in diffuse models, including personalized modeling for\ntexture information extraction, variational score distillation for detailed\nappearance synthesis, and explicit geometry guidance with ControlNet. Our\nintegration and several essential modifications substantially improve the\ntexture quality. Experiments on real images spanning different categories show\nthat TextureDreamer can successfully transfer highly realistic, semantic\nmeaningful texture to arbitrary objects, surpassing the visual quality of\nprevious state-of-the-art.",
        "translated": ""
    },
    {
        "title": "Vlogger: Make Your Dream A Vlog",
        "url": "http://arxiv.org/abs/2401.09414v1",
        "pub_date": "2024-01-17",
        "summary": "In this work, we present Vlogger, a generic AI system for generating a\nminute-level video blog (i.e., vlog) of user descriptions. Different from short\nvideos with a few seconds, vlog often contains a complex storyline with\ndiversified scenes, which is challenging for most existing video generation\napproaches. To break through this bottleneck, our Vlogger smartly leverages\nLarge Language Model (LLM) as Director and decomposes a long video generation\ntask of vlog into four key stages, where we invoke various foundation models to\nplay the critical roles of vlog professionals, including (1) Script, (2) Actor,\n(3) ShowMaker, and (4) Voicer. With such a design of mimicking human beings,\nour Vlogger can generate vlogs through explainable cooperation of top-down\nplanning and bottom-up shooting. Moreover, we introduce a novel video diffusion\nmodel, ShowMaker, which serves as a videographer in our Vlogger for generating\nthe video snippet of each shooting scene. By incorporating Script and Actor\nattentively as textual and visual prompts, it can effectively enhance\nspatial-temporal coherence in the snippet. Besides, we design a concise mixed\ntraining paradigm for ShowMaker, boosting its capacity for both T2V generation\nand prediction. Finally, the extensive experiments show that our method\nachieves state-of-the-art performance on zero-shot T2V generation and\nprediction tasks. More importantly, Vlogger can generate over 5-minute vlogs\nfrom open-world descriptions, without loss of video coherence on script and\nactor. The code and model is all available at\nhttps://github.com/zhuangshaobin/Vlogger.",
        "translated": ""
    },
    {
        "title": "POP-3D: Open-Vocabulary 3D Occupancy Prediction from Images",
        "url": "http://arxiv.org/abs/2401.09413v1",
        "pub_date": "2024-01-17",
        "summary": "We describe an approach to predict open-vocabulary 3D semantic voxel\noccupancy map from input 2D images with the objective of enabling 3D grounding,\nsegmentation and retrieval of free-form language queries. This is a challenging\nproblem because of the 2D-3D ambiguity and the open-vocabulary nature of the\ntarget tasks, where obtaining annotated training data in 3D is difficult. The\ncontributions of this work are three-fold. First, we design a new model\narchitecture for open-vocabulary 3D semantic occupancy prediction. The\narchitecture consists of a 2D-3D encoder together with occupancy prediction and\n3D-language heads. The output is a dense voxel map of 3D grounded language\nembeddings enabling a range of open-vocabulary tasks. Second, we develop a\ntri-modal self-supervised learning algorithm that leverages three modalities:\n(i) images, (ii) language and (iii) LiDAR point clouds, and enables training\nthe proposed architecture using a strong pre-trained vision-language model\nwithout the need for any 3D manual language annotations. Finally, we\ndemonstrate quantitatively the strengths of the proposed model on several\nopen-vocabulary tasks: Zero-shot 3D semantic segmentation using existing\ndatasets; 3D grounding and retrieval of free-form language queries, using a\nsmall dataset that we propose as an extension of nuScenes. You can find the\nproject page here https://vobecant.github.io/POP3D.",
        "translated": ""
    },
    {
        "title": "Tri$^{2}$-plane: Volumetric Avatar Reconstruction with Feature Pyramid",
        "url": "http://arxiv.org/abs/2401.09386v1",
        "pub_date": "2024-01-17",
        "summary": "Recent years have witnessed considerable achievements in facial avatar\nreconstruction with neural volume rendering. Despite notable advancements, the\nreconstruction of complex and dynamic head movements from monocular videos\nstill suffers from capturing and restoring fine-grained details. In this work,\nwe propose a novel approach, named Tri$^2$-plane, for monocular photo-realistic\nvolumetric head avatar reconstructions. Distinct from the existing works that\nrely on a single tri-plane deformation field for dynamic facial modeling, the\nproposed Tri$^2$-plane leverages the principle of feature pyramids and three\ntop-to-down lateral connections tri-planes for details improvement. It samples\nand renders facial details at multiple scales, transitioning from the entire\nface to specific local regions and then to even more refined sub-regions.\nMoreover, we incorporate a camera-based geometry-aware sliding window method as\nan augmentation in training, which improves the robustness beyond the canonical\nspace, with a particular improvement in cross-identity generation capabilities.\nExperimental outcomes indicate that the Tri$^2$-plane not only surpasses\nexisting methodologies but also achieves superior performance across both\nquantitative metrics and qualitative assessments through experiments.",
        "translated": ""
    },
    {
        "title": "Diverse Part Synthesis for 3D Shape Creation",
        "url": "http://arxiv.org/abs/2401.09384v1",
        "pub_date": "2024-01-17",
        "summary": "Methods that use neural networks for synthesizing 3D shapes in the form of a\npart-based representation have been introduced over the last few years. These\nmethods represent shapes as a graph or hierarchy of parts and enable a variety\nof applications such as shape sampling and reconstruction. However, current\nmethods do not allow easily regenerating individual shape parts according to\nuser preferences. In this paper, we investigate techniques that allow the user\nto generate multiple, diverse suggestions for individual parts. Specifically,\nwe experiment with multimodal deep generative models that allow sampling\ndiverse suggestions for shape parts and focus on models which have not been\nconsidered in previous work on shape synthesis. To provide a comparative study\nof these techniques, we introduce a method for synthesizing 3D shapes in a\npart-based representation and evaluate all the part suggestion techniques\nwithin this synthesis method. In our method, which is inspired by previous\nwork, shapes are represented as a set of parts in the form of implicit\nfunctions which are then positioned in space to form the final shape. Synthesis\nin this representation is enabled by a neural network architecture based on an\nimplicit decoder and a spatial transformer. We compare the various multimodal\ngenerative models by evaluating their performance in generating part\nsuggestions. Our contribution is to show with qualitative and quantitative\nevaluations which of the new techniques for multimodal part generation perform\nthe best and that a synthesis method based on the top-performing techniques\nallows the user to more finely control the parts that are generated in the 3D\nshapes while maintaining high shape fidelity when reconstructing shapes.",
        "translated": ""
    },
    {
        "title": "SceneVerse: Scaling 3D Vision-Language Learning for Grounded Scene\n  Understanding",
        "url": "http://arxiv.org/abs/2401.09340v1",
        "pub_date": "2024-01-17",
        "summary": "3D vision-language grounding, which focuses on aligning language with the 3D\nphysical environment, stands as a cornerstone in the development of embodied\nagents. In comparison to recent advancements in the 2D domain, grounding\nlanguage in 3D scenes faces several significant challenges: (i) the inherent\ncomplexity of 3D scenes due to the diverse object configurations, their rich\nattributes, and intricate relationships; (ii) the scarcity of paired 3D\nvision-language data to support grounded learning; and (iii) the absence of a\nunified learning framework to distill knowledge from grounded 3D data. In this\nwork, we aim to address these three major challenges in 3D vision-language by\nexamining the potential of systematically upscaling 3D vision-language learning\nin indoor environments. We introduce the first million-scale 3D vision-language\ndataset, SceneVerse, encompassing about 68K 3D indoor scenes and comprising\n2.5M vision-language pairs derived from both human annotations and our scalable\nscene-graph-based generation approach. We demonstrate that this scaling allows\nfor a unified pre-training framework, Grounded Pre-training for Scenes (GPS),\nfor 3D vision-language learning. Through extensive experiments, we showcase the\neffectiveness of GPS by achieving state-of-the-art performance on all existing\n3D visual grounding benchmarks. The vast potential of SceneVerse and GPS is\nunveiled through zero-shot transfer experiments in the challenging 3D\nvision-language tasks. Project website: https://scene-verse.github.io .",
        "translated": ""
    },
    {
        "title": "To deform or not: treatment-aware longitudinal registration for breast\n  DCE-MRI during neoadjuvant chemotherapy via unsupervised keypoints detection",
        "url": "http://arxiv.org/abs/2401.09336v1",
        "pub_date": "2024-01-17",
        "summary": "Clinicians compare breast DCE-MRI after neoadjuvant chemotherapy (NAC) with\npre-treatment scans to evaluate the response to NAC. Clinical evidence supports\nthat accurate longitudinal deformable registration without deforming treated\ntumor regions is key to quantifying tumor changes. We propose a conditional\npyramid registration network based on unsupervised keypoint detection and\nselective volume-preserving to quantify changes over time. In this approach, we\nextract the structural and the abnormal keypoints from DCE-MRI, apply the\nstructural keypoints for the registration algorithm to restrict large\ndeformation, and employ volume-preserving loss based on abnormal keypoints to\nkeep the volume of the tumor unchanged after registration. We use a clinical\ndataset with 1630 MRI scans from 314 patients treated with NAC. The results\ndemonstrate that our method registers with better performance and better volume\npreservation of the tumors. Furthermore, a local-global-combining biomarker\nbased on the proposed method achieves high accuracy in pathological complete\nresponse (pCR) prediction, indicating that predictive information exists\noutside tumor regions. The biomarkers could potentially be used to avoid\nunnecessary surgeries for certain patients. It may be valuable for clinicians\nand/or computer systems to conduct follow-up tumor segmentation and response\nprediction on images registered by our method. Our code is available on\n\\url{https://github.com/fiy2W/Treatment-aware-Longitudinal-Registration}.",
        "translated": ""
    },
    {
        "title": "Event-Based Visual Odometry on Non-Holonomic Ground Vehicles",
        "url": "http://arxiv.org/abs/2401.09331v1",
        "pub_date": "2024-01-17",
        "summary": "Despite the promise of superior performance under challenging conditions,\nevent-based motion estimation remains a hard problem owing to the difficulty of\nextracting and tracking stable features from event streams. In order to\nrobustify the estimation, it is generally believed that fusion with other\nsensors is a requirement. In this work, we demonstrate reliable, purely\nevent-based visual odometry on planar ground vehicles by employing the\nconstrained non-holonomic motion model of Ackermann steering platforms. We\nextend single feature n-linearities for regular frame-based cameras to the case\nof quasi time-continuous event-tracks, and achieve a polynomial form via\nvariable degree Taylor expansions. Robust averaging over multiple event tracks\nis simply achieved via histogram voting. As demonstrated on both simulated and\nreal data, our algorithm achieves accurate and robust estimates of the\nvehicle's instantaneous rotational velocity, and thus results that are\ncomparable to the delta rotations obtained by frame-based sensors under normal\nconditions. We furthermore significantly outperform the more traditional\nalternatives in challenging illumination scenarios. The code is available at\n\\url{https://github.com/gowanting/NHEVO}.",
        "translated": ""
    },
    {
        "title": "ParaHome: Parameterizing Everyday Home Activities Towards 3D Generative\n  Modeling of Human-Object Interactions",
        "url": "http://arxiv.org/abs/2401.10232v1",
        "pub_date": "2024-01-18",
        "summary": "To enable machines to learn how humans interact with the physical world in\nour daily activities, it is crucial to provide rich data that encompasses the\n3D motion of humans as well as the motion of objects in a learnable 3D\nrepresentation. Ideally, this data should be collected in a natural setup,\ncapturing the authentic dynamic 3D signals during human-object interactions. To\naddress this challenge, we introduce the ParaHome system, designed to capture\nand parameterize dynamic 3D movements of humans and objects within a common\nhome environment. Our system consists of a multi-view setup with 70\nsynchronized RGB cameras, as well as wearable motion capture devices equipped\nwith an IMU-based body suit and hand motion capture gloves. By leveraging the\nParaHome system, we collect a novel large-scale dataset of human-object\ninteraction. Notably, our dataset offers key advancement over existing datasets\nin three main aspects: (1) capturing 3D body and dexterous hand manipulation\nmotion alongside 3D object movement within a contextual home environment during\nnatural activities; (2) encompassing human interaction with multiple objects in\nvarious episodic scenarios with corresponding descriptions in texts; (3)\nincluding articulated objects with multiple parts expressed with parameterized\narticulations. Building upon our dataset, we introduce new research tasks aimed\nat building a generative model for learning and synthesizing human-object\ninteractions in a real-world room setting.",
        "translated": ""
    },
    {
        "title": "OMG-Seg: Is One Model Good Enough For All Segmentation?",
        "url": "http://arxiv.org/abs/2401.10229v1",
        "pub_date": "2024-01-18",
        "summary": "In this work, we address various segmentation tasks, each traditionally\ntackled by distinct or partially unified models. We propose OMG-Seg, One Model\nthat is Good enough to efficiently and effectively handle all the segmentation\ntasks, including image semantic, instance, and panoptic segmentation, as well\nas their video counterparts, open vocabulary settings, prompt-driven,\ninteractive segmentation like SAM, and video object segmentation. To our\nknowledge, this is the first model to handle all these tasks in one model and\nachieve satisfactory performance. We show that OMG-Seg, a transformer-based\nencoder-decoder architecture with task-specific queries and outputs, can\nsupport over ten distinct segmentation tasks and yet significantly reduce\ncomputational and parameter overhead across various tasks and datasets. We\nrigorously evaluate the inter-task influences and correlations during\nco-training. Code and models are available at https://github.com/lxtGH/OMG-Seg.",
        "translated": ""
    },
    {
        "title": "RAP-SAM: Towards Real-Time All-Purpose Segment Anything",
        "url": "http://arxiv.org/abs/2401.10228v1",
        "pub_date": "2024-01-18",
        "summary": "Advanced by transformer architecture, vision foundation models (VFMs) achieve\nremarkable progress in performance and generalization ability. Segment Anything\nModel (SAM) is one remarkable model that can achieve generalized segmentation.\nHowever, most VFMs cannot run in realtime, which makes it difficult to transfer\nthem into several products. On the other hand, current real-time segmentation\nmainly has one purpose, such as semantic segmentation on the driving scene. We\nargue that diverse outputs are needed for real applications. Thus, this work\nexplores a new real-time segmentation setting, named all-purpose segmentation\nin real-time, to transfer VFMs in real-time deployment. It contains three\ndifferent tasks, including interactive segmentation, panoptic segmentation, and\nvideo segmentation. We aim to use one model to achieve the above tasks in\nreal-time. We first benchmark several strong baselines. Then, we present\nReal-Time All Purpose SAM (RAP-SAM). It contains an efficient encoder and an\nefficient decoupled decoder to perform prompt-driven decoding. Moreover, we\nfurther explore different training strategies and tuning methods to boost\nco-training performance further. Our code and model are available at\nhttps://github.com/xushilin1/RAP-SAM/.",
        "translated": ""
    },
    {
        "title": "A Simple Latent Diffusion Approach for Panoptic Segmentation and Mask\n  Inpainting",
        "url": "http://arxiv.org/abs/2401.10227v1",
        "pub_date": "2024-01-18",
        "summary": "Panoptic and instance segmentation networks are often trained with\nspecialized object detection modules, complex loss functions, and ad-hoc\npost-processing steps to handle the permutation-invariance of the instance\nmasks. This work builds upon Stable Diffusion and proposes a latent diffusion\napproach for panoptic segmentation, resulting in a simple architecture which\nomits these complexities. Our training process consists of two steps: (1)\ntraining a shallow autoencoder to project the segmentation masks to latent\nspace; (2) training a diffusion model to allow image-conditioned sampling in\nlatent space. The use of a generative model unlocks the exploration of mask\ncompletion or inpainting, which has applications in interactive segmentation.\nThe experimental validation yields promising results for both panoptic\nsegmentation and mask inpainting. While not setting a new state-of-the-art, our\nmodel's simplicity, generality, and mask completion capability are desirable\nproperties.",
        "translated": ""
    },
    {
        "title": "Towards Language-Driven Video Inpainting via Multimodal Large Language\n  Models",
        "url": "http://arxiv.org/abs/2401.10226v1",
        "pub_date": "2024-01-18",
        "summary": "We introduce a new task -- language-driven video inpainting, which uses\nnatural language instructions to guide the inpainting process. This approach\novercomes the limitations of traditional video inpainting methods that depend\non manually labeled binary masks, a process often tedious and labor-intensive.\nWe present the Remove Objects from Videos by Instructions (ROVI) dataset,\ncontaining 5,650 videos and 9,091 inpainting results, to support training and\nevaluation for this task. We also propose a novel diffusion-based\nlanguage-driven video inpainting framework, the first end-to-end baseline for\nthis task, integrating Multimodal Large Language Models to understand and\nexecute complex language-based inpainting requests effectively. Our\ncomprehensive results showcase the dataset's versatility and the model's\neffectiveness in various language-instructed inpainting scenarios. We will make\ndatasets, code, and models publicly available.",
        "translated": ""
    },
    {
        "title": "The Manga Whisperer: Automatically Generating Transcriptions for Comics",
        "url": "http://arxiv.org/abs/2401.10224v1",
        "pub_date": "2024-01-18",
        "summary": "In the past few decades, Japanese comics, commonly referred to as Manga, have\ntranscended both cultural and linguistic boundaries to become a true worldwide\nsensation. Yet, the inherent reliance on visual cues and illustration within\nmanga renders it largely inaccessible to individuals with visual impairments.\nIn this work, we seek to address this substantial barrier, with the aim of\nensuring that manga can be appreciated and actively engaged by everyone.\nSpecifically, we tackle the problem of diarisation i.e. generating a\ntranscription of who said what and when, in a fully automatic way.\n  To this end, we make the following contributions: (1) we present a unified\nmodel, Magi, that is able to (a) detect panels, text boxes and character boxes,\n(b) cluster characters by identity (without knowing the number of clusters\napriori), and (c) associate dialogues to their speakers; (2) we propose a novel\napproach that is able to sort the detected text boxes in their reading order\nand generate a dialogue transcript; (3) we annotate an evaluation benchmark for\nthis task using publicly available [English] manga pages. The code, evaluation\ndatasets and the pre-trained model can be found at:\nhttps://github.com/ragavsachdeva/magi.",
        "translated": ""
    },
    {
        "title": "Supervised Fine-tuning in turn Improves Visual Foundation Models",
        "url": "http://arxiv.org/abs/2401.10222v1",
        "pub_date": "2024-01-18",
        "summary": "Image-text training like CLIP has dominated the pretraining of vision\nfoundation models in recent years. Subsequent efforts have been made to\nintroduce region-level visual learning into CLIP's pretraining but face\nscalability challenges due to the lack of large-scale region-level datasets.\nDrawing inspiration from supervised fine-tuning (SFT) in natural language\nprocessing such as instruction tuning, we explore the potential of fine-grained\nSFT in enhancing the generation of vision foundation models after their\npretraining. Thus a two-stage method ViSFT (Vision SFT) is proposed to unleash\nthe fine-grained knowledge of vision foundation models. In ViSFT, the vision\nfoundation model is enhanced by performing visual joint learning on some\nin-domain tasks and then tested on out-of-domain benchmarks. With updating\nusing ViSFT on 8 V100 GPUs in less than 2 days, a vision transformer with over\n4.4B parameters shows improvements across various out-of-domain benchmarks\nincluding vision and vision-linguistic scenarios.",
        "translated": ""
    },
    {
        "title": "AutoFT: Robust Fine-Tuning by Optimizing Hyperparameters on OOD Data",
        "url": "http://arxiv.org/abs/2401.10220v1",
        "pub_date": "2024-01-18",
        "summary": "Foundation models encode rich representations that can be adapted to a\ndesired task by fine-tuning on task-specific data. However, fine-tuning a model\non one particular data distribution often compromises the model's original\nperformance on other distributions. Current methods for robust fine-tuning\nutilize hand-crafted regularization techniques to constrain the fine-tuning\nprocess towards the base foundation model. Yet, it is hard to precisely specify\nwhat characteristics of the foundation model to retain during fine-tuning, as\nthis depends on how the pre-training, fine-tuning, and evaluation data\ndistributions relate to each other. We propose AutoFT, a data-driven approach\nfor guiding foundation model fine-tuning. AutoFT optimizes fine-tuning\nhyperparameters to maximize performance on a small out-of-distribution (OOD)\nvalidation set. To guide fine-tuning in a granular way, AutoFT searches a\nhighly expressive hyperparameter space that includes weight coefficients for\nmany different losses, in addition to learning rate and weight decay values. We\nevaluate AutoFT on nine natural distribution shifts which include domain shifts\nand subpopulation shifts. Our experiments show that AutoFT significantly\nimproves generalization to new OOD data, outperforming existing robust\nfine-tuning methods. Notably, AutoFT achieves new state-of-the-art performance\non the WILDS-iWildCam and WILDS-FMoW benchmarks, outperforming the previous\nbest methods by $6.0\\%$ and $1.5\\%$, respectively.",
        "translated": ""
    },
    {
        "title": "Edit One for All: Interactive Batch Image Editing",
        "url": "http://arxiv.org/abs/2401.10219v1",
        "pub_date": "2024-01-18",
        "summary": "In recent years, image editing has advanced remarkably. With increased human\ncontrol, it is now possible to edit an image in a plethora of ways; from\nspecifying in text what we want to change, to straight up dragging the contents\nof the image in an interactive point-based manner. However, most of the focus\nhas remained on editing single images at a time. Whether and how we can\nsimultaneously edit large batches of images has remained understudied. With the\ngoal of minimizing human supervision in the editing process, this paper\npresents a novel method for interactive batch image editing using StyleGAN as\nthe medium. Given an edit specified by users in an example image (e.g., make\nthe face frontal), our method can automatically transfer that edit to other\ntest images, so that regardless of their initial state (pose), they all arrive\nat the same final state (e.g., all facing front). Extensive experiments\ndemonstrate that edits performed using our method have similar visual quality\nto existing single-image-editing methods, while having more visual consistency\nand saving significant time and human effort.",
        "translated": ""
    },
    {
        "title": "Explaining the Implicit Neural Canvas: Connecting Pixels to Neurons by\n  Tracing their Contributions",
        "url": "http://arxiv.org/abs/2401.10217v1",
        "pub_date": "2024-01-18",
        "summary": "The many variations of Implicit Neural Representations (INRs), where a neural\nnetwork is trained as a continuous representation of a signal, have tremendous\npractical utility for downstream tasks including novel view synthesis, video\ncompression, and image superresolution. Unfortunately, the inner workings of\nthese networks are seriously under-studied. Our work, eXplaining the Implicit\nNeural Canvas (XINC), is a unified framework for explaining properties of INRs\nby examining the strength of each neuron's contribution to each output pixel.\nWe call the aggregate of these contribution maps the Implicit Neural Canvas and\nwe use this concept to demonstrate that the INRs which we study learn to\n''see'' the frames they represent in surprising ways. For example, INRs tend to\nhave highly distributed representations. While lacking high-level object\nsemantics, they have a significant bias for color and edges, and are almost\nentirely space-agnostic. We arrive at our conclusions by examining how objects\nare represented across time in video INRs, using clustering to visualize\nsimilar neurons across layers and architectures, and show that this is\ndominated by motion. These insights demonstrate the general usefulness of our\nanalysis framework. Our project page is available at\nhttps://namithap10.github.io/xinc.",
        "translated": ""
    },
    {
        "title": "Depth Anything: Unleashing the Power of Large-Scale Unlabeled Data",
        "url": "http://arxiv.org/abs/2401.10891v1",
        "pub_date": "2024-01-19",
        "summary": "This work presents Depth Anything, a highly practical solution for robust\nmonocular depth estimation. Without pursuing novel technical modules, we aim to\nbuild a simple yet powerful foundation model dealing with any images under any\ncircumstances. To this end, we scale up the dataset by designing a data engine\nto collect and automatically annotate large-scale unlabeled data (~62M), which\nsignificantly enlarges the data coverage and thus is able to reduce the\ngeneralization error. We investigate two simple yet effective strategies that\nmake data scaling-up promising. First, a more challenging optimization target\nis created by leveraging data augmentation tools. It compels the model to\nactively seek extra visual knowledge and acquire robust representations.\nSecond, an auxiliary supervision is developed to enforce the model to inherit\nrich semantic priors from pre-trained encoders. We evaluate its zero-shot\ncapabilities extensively, including six public datasets and randomly captured\nphotos. It demonstrates impressive generalization ability. Further, through\nfine-tuning it with metric depth information from NYUv2 and KITTI, new SOTAs\nare set. Our better depth model also results in a better depth-conditioned\nControlNet. Our models are released at\nhttps://github.com/LiheYoung/Depth-Anything.",
        "translated": ""
    },
    {
        "title": "Event detection from novel data sources: Leveraging satellite imagery\n  alongside GPS traces",
        "url": "http://arxiv.org/abs/2401.10890v1",
        "pub_date": "2024-01-19",
        "summary": "Rapid identification and response to breaking events, particularly those that\npose a threat to human life such as natural disasters or conflicts, is of\nparamount importance. The prevalence of mobile devices and the ubiquity of\nnetwork connectivity has generated a massive amount of temporally- and\nspatially-stamped data. Numerous studies have used mobile data to derive\nindividual human mobility patterns for various applications. Similarly, the\nincreasing number of orbital satellites has made it easier to gather\nhigh-resolution images capturing a snapshot of a geographical area in sub-daily\ntemporal frequency. We propose a novel data fusion methodology integrating\nsatellite imagery with privacy-enhanced mobile data to augment the event\ninference task, whether in real-time or historical. In the absence of boots on\nthe ground, mobile data is able to give an approximation of human mobility,\nproximity to one another, and the built environment. On the other hand,\nsatellite imagery can provide visual information on physical changes to the\nbuilt and natural environment. The expected use cases for our methodology\ninclude small-scale disaster detection (i.e., tornadoes, wildfires, and floods)\nin rural regions, search and rescue operation augmentation for lost hikers in\nremote wilderness areas, and identification of active conflict areas and\npopulation displacement in war-torn states. Our implementation is open-source\non GitHub: https://github.com/ekinugurel/SatMobFusion.",
        "translated": ""
    },
    {
        "title": "Synthesizing Moving People with 3D Control",
        "url": "http://arxiv.org/abs/2401.10889v1",
        "pub_date": "2024-01-19",
        "summary": "In this paper, we present a diffusion model-based framework for animating\npeople from a single image for a given target 3D motion sequence. Our approach\nhas two core components: a) learning priors about invisible parts of the human\nbody and clothing, and b) rendering novel body poses with proper clothing and\ntexture. For the first part, we learn an in-filling diffusion model to\nhallucinate unseen parts of a person given a single image. We train this model\non texture map space, which makes it more sample-efficient since it is\ninvariant to pose and viewpoint. Second, we develop a diffusion-based rendering\npipeline, which is controlled by 3D human poses. This produces realistic\nrenderings of novel poses of the person, including clothing, hair, and\nplausible in-filling of unseen regions. This disentangled approach allows our\nmethod to generate a sequence of images that are faithful to the target motion\nin the 3D pose and, to the input image in terms of visual similarity. In\naddition to that, the 3D control allows various synthetic camera trajectories\nto render a person. Our experiments show that our method is resilient in\ngenerating prolonged motions and varied challenging and complex poses compared\nto prior methods. Please check our website for more details:\nhttps://boyiliee.github.io/3DHM.github.io/.",
        "translated": ""
    },
    {
        "title": "SCENES: Subpixel Correspondence Estimation With Epipolar Supervision",
        "url": "http://arxiv.org/abs/2401.10886v1",
        "pub_date": "2024-01-19",
        "summary": "Extracting point correspondences from two or more views of a scene is a\nfundamental computer vision problem with particular importance for relative\ncamera pose estimation and structure-from-motion. Existing local feature\nmatching approaches, trained with correspondence supervision on large-scale\ndatasets, obtain highly-accurate matches on the test sets. However, they do not\ngeneralise well to new datasets with different characteristics to those they\nwere trained on, unlike classic feature extractors. Instead, they require\nfinetuning, which assumes that ground-truth correspondences or ground-truth\ncamera poses and 3D structure are available. We relax this assumption by\nremoving the requirement of 3D structure, e.g., depth maps or point clouds, and\nonly require camera pose information, which can be obtained from odometry. We\ndo so by replacing correspondence losses with epipolar losses, which encourage\nputative matches to lie on the associated epipolar line. While weaker than\ncorrespondence supervision, we observe that this cue is sufficient for\nfinetuning existing models on new data. We then further relax the assumption of\nknown camera poses by using pose estimates in a novel bootstrapping approach.\nWe evaluate on highly challenging datasets, including an indoor drone dataset\nand an outdoor smartphone camera dataset, and obtain state-of-the-art results\nwithout strong supervision.",
        "translated": ""
    },
    {
        "title": "The Cadaver in the Machine: The Social Practices of Measurement and\n  Validation in Motion Capture Technology",
        "url": "http://arxiv.org/abs/2401.10877v1",
        "pub_date": "2024-01-19",
        "summary": "Motion capture systems, used across various domains, make body\nrepresentations concrete through technical processes. We argue that the\nmeasurement of bodies and the validation of measurements for motion capture\nsystems can be understood as social practices. By analyzing the findings of a\nsystematic literature review (N=278) through the lens of social practice\ntheory, we show how these practices, and their varying attention to errors,\nbecome ingrained in motion capture design and innovation over time. Moreover,\nwe show how contemporary motion capture systems perpetuate assumptions about\nhuman bodies and their movements. We suggest that social practices of\nmeasurement and validation are ubiquitous in the development of data- and\nsensor-driven systems more broadly, and provide this work as a basis for\ninvestigating hidden design assumptions and their potential negative\nconsequences in human-computer interaction.",
        "translated": ""
    },
    {
        "title": "Motion Consistency Loss for Monocular Visual Odometry with\n  Attention-Based Deep Learning",
        "url": "http://arxiv.org/abs/2401.10857v1",
        "pub_date": "2024-01-19",
        "summary": "Deep learning algorithms have driven expressive progress in many complex\ntasks. The loss function is a core component of deep learning techniques,\nguiding the learning process of neural networks. This paper contributes by\nintroducing a consistency loss for visual odometry with deep learning-based\napproaches. The motion consistency loss explores repeated motions that appear\nin consecutive overlapped video clips. Experimental results show that our\napproach increased the performance of a model on the KITTI odometry benchmark.",
        "translated": ""
    },
    {
        "title": "Source-Free and Image-Only Unsupervised Domain Adaptation for Category\n  Level Object Pose Estimation",
        "url": "http://arxiv.org/abs/2401.10848v1",
        "pub_date": "2024-01-19",
        "summary": "We consider the problem of source-free unsupervised category-level pose\nestimation from only RGB images to a target domain without any access to source\ndomain data or 3D annotations during adaptation. Collecting and annotating\nreal-world 3D data and corresponding images is laborious, expensive, yet\nunavoidable process, since even 3D pose domain adaptation methods require 3D\ndata in the target domain. We introduce 3DUDA, a method capable of adapting to\na nuisance-ridden target domain without 3D or depth data. Our key insight stems\nfrom the observation that specific object subparts remain stable across\nout-of-domain (OOD) scenarios, enabling strategic utilization of these\ninvariant subcomponents for effective model updates. We represent object\ncategories as simple cuboid meshes, and harness a generative model of neural\nfeature activations modeled at each mesh vertex learnt using differential\nrendering. We focus on individual locally robust mesh vertex features and\niteratively update them based on their proximity to corresponding features in\nthe target domain even when the global pose is not correct. Our model is then\ntrained in an EM fashion, alternating between updating the vertex features and\nthe feature extractor. We show that our method simulates fine-tuning on a\nglobal pseudo-labeled dataset under mild assumptions, which converges to the\ntarget domain asymptotically. Through extensive empirical validation, including\na complex extreme UDA setup which combines real nuisances, synthetic noise, and\nocclusion, we demonstrate the potency of our simple approach in addressing the\ndomain shift challenge and significantly improving pose estimation accuracy.",
        "translated": ""
    },
    {
        "title": "Understanding Video Transformers via Universal Concept Discovery",
        "url": "http://arxiv.org/abs/2401.10831v1",
        "pub_date": "2024-01-19",
        "summary": "This paper studies the problem of concept-based interpretability of\ntransformer representations for videos. Concretely, we seek to explain the\ndecision-making process of video transformers based on high-level,\nspatiotemporal concepts that are automatically discovered. Prior research on\nconcept-based interpretability has concentrated solely on image-level tasks.\nComparatively, video models deal with the added temporal dimension, increasing\ncomplexity and posing challenges in identifying dynamic concepts over time. In\nthis work, we systematically address these challenges by introducing the first\nVideo Transformer Concept Discovery (VTCD) algorithm. To this end, we propose\nan efficient approach for unsupervised identification of units of video\ntransformer representations - concepts, and ranking their importance to the\noutput of a model. The resulting concepts are highly interpretable, revealing\nspatio-temporal reasoning mechanisms and object-centric representations in\nunstructured video models. Performing this analysis jointly over a diverse set\nof supervised and self-supervised representations, we discover that some of\nthese mechanism are universal in video transformers. Finally, we demonstrate\nthat VTCDcan be used to improve model performance for fine-grained tasks.",
        "translated": ""
    },
    {
        "title": "ActAnywhere: Subject-Aware Video Background Generation",
        "url": "http://arxiv.org/abs/2401.10822v1",
        "pub_date": "2024-01-19",
        "summary": "Generating video background that tailors to foreground subject motion is an\nimportant problem for the movie industry and visual effects community. This\ntask involves synthesizing background that aligns with the motion and\nappearance of the foreground subject, while also complies with the artist's\ncreative intention. We introduce ActAnywhere, a generative model that automates\nthis process which traditionally requires tedious manual efforts. Our model\nleverages the power of large-scale video diffusion models, and is specifically\ntailored for this task. ActAnywhere takes a sequence of foreground subject\nsegmentation as input and an image that describes the desired scene as\ncondition, to produce a coherent video with realistic foreground-background\ninteractions while adhering to the condition frame. We train our model on a\nlarge-scale dataset of human-scene interaction videos. Extensive evaluations\ndemonstrate the superior performance of our model, significantly outperforming\nbaselines. Moreover, we show that ActAnywhere generalizes to diverse\nout-of-distribution samples, including non-human subjects. Please visit our\nproject webpage at https://actanywhere.github.io.",
        "translated": ""
    },
    {
        "title": "RAD-DINO: Exploring Scalable Medical Image Encoders Beyond Text\n  Supervision",
        "url": "http://arxiv.org/abs/2401.10815v1",
        "pub_date": "2024-01-19",
        "summary": "Language-supervised pre-training has proven to be a valuable method for\nextracting semantically meaningful features from images, serving as a\nfoundational element in multimodal systems within the computer vision and\nmedical imaging domains. However, resulting features are limited by the\ninformation contained within the text. This is particularly problematic in\nmedical imaging, where radiologists' written findings focus on specific\nobservations; a challenge compounded by the scarcity of paired imaging-text\ndata due to concerns over leakage of personal health information. In this work,\nwe fundamentally challenge the prevailing reliance on language supervision for\nlearning general purpose biomedical imaging encoders. We introduce RAD-DINO, a\nbiomedical image encoder pre-trained solely on unimodal biomedical imaging data\nthat obtains similar or greater performance than state-of-the-art biomedical\nlanguage supervised models on a diverse range of benchmarks. Specifically, the\nquality of learned representations is evaluated on standard imaging tasks\n(classification and semantic segmentation), and a vision-language alignment\ntask (text report generation from images). To further demonstrate the drawback\nof language supervision, we show that features from RAD-DINO correlate with\nother medical records (e.g., sex or age) better than language-supervised\nmodels, which are generally not mentioned in radiology reports. Finally, we\nconduct a series of ablations determining the factors in RAD-DINO's\nperformance; notably, we observe that RAD-DINO's downstream performance scales\nwell with the quantity and diversity of training data, demonstrating that\nimage-only supervision is a scalable approach for training a foundational\nbiomedical image encoder.",
        "translated": ""
    },
    {
        "title": "Exploring Simple Open-Vocabulary Semantic Segmentation",
        "url": "http://arxiv.org/abs/2401.12217v1",
        "pub_date": "2024-01-22",
        "summary": "Open-vocabulary semantic segmentation models aim to accurately assign a\nsemantic label to each pixel in an image from a set of arbitrary\nopen-vocabulary texts. In order to learn such pixel-level alignment, current\napproaches typically rely on a combination of (i) image-level VL model (e.g.\nCLIP), (ii) ground truth masks, and (iii) custom grouping encoders. In this\npaper, we introduce S-Seg, a novel model that can achieve surprisingly strong\nperformance without depending on any of the above elements. S-Seg leverages\npseudo-mask and language to train a MaskFormer, and can be easily trained from\npublicly available image-text datasets. Contrary to prior works, our model\ndirectly trains for pixel-level features and language alignment. Once trained,\nS-Seg generalizes well to multiple testing datasets without requiring\nfine-tuning. In addition, S-Seg has the extra benefits of scalability with data\nand consistently improvement when augmented with self-training. We believe that\nour simple yet effective approach will serve as a solid baseline for future\nresearch.",
        "translated": ""
    },
    {
        "title": "Less Could Be Better: Parameter-efficient Fine-tuning Advances Medical\n  Vision Foundation Models",
        "url": "http://arxiv.org/abs/2401.12215v1",
        "pub_date": "2024-01-22",
        "summary": "Parameter-efficient fine-tuning (PEFT) that was initially developed for\nexploiting pre-trained large language models has recently emerged as an\neffective approach to perform transfer learning on computer vision tasks.\nHowever, the effectiveness of PEFT on medical vision foundation models is still\nunclear and remains to be explored. As a proof of concept, we conducted a\ndetailed empirical study on applying PEFT to chest radiography foundation\nmodels. Specifically, we delved into LoRA, a representative PEFT method, and\ncompared it against full-parameter fine-tuning (FFT) on two self-supervised\nradiography foundation models across three well-established chest radiograph\ndatasets. Our results showed that LoRA outperformed FFT in 13 out of 18\ntransfer learning tasks by at most 2.9% using fewer than 1% tunable parameters.\nCombining LoRA with foundation models, we set up new state-of-the-art on a\nrange of data-efficient learning tasks, such as an AUROC score of 80.6% using\n1% labeled data on NIH ChestX-ray14. We hope this study can evoke more\nattention from the community in the use of PEFT for transfer learning on\nmedical imaging tasks. Code and models are available at\nhttps://github.com/RL4M/MED-PEFT.",
        "translated": ""
    },
    {
        "title": "Connecting the Dots: Leveraging Spatio-Temporal Graph Neural Networks\n  for Accurate Bangla Sign Language Recognition",
        "url": "http://arxiv.org/abs/2401.12210v1",
        "pub_date": "2024-01-22",
        "summary": "Recent advances in Deep Learning and Computer Vision have been successfully\nleveraged to serve marginalized communities in various contexts. One such area\nis Sign Language - a primary means of communication for the deaf community.\nHowever, so far, the bulk of research efforts and investments have gone into\nAmerican Sign Language, and research activity into low-resource sign languages\n- especially Bangla Sign Language - has lagged significantly. In this research\npaper, we present a new word-level Bangla Sign Language dataset - BdSL40 -\nconsisting of 611 videos over 40 words, along with two different approaches:\none with a 3D Convolutional Neural Network model and another with a novel Graph\nNeural Network approach for the classification of BdSL40 dataset. This is the\nfirst study on word-level BdSL recognition, and the dataset was transcribed\nfrom Indian Sign Language (ISL) using the Bangla Sign Language Dictionary\n(1997). The proposed GNN model achieved an F1 score of 89%. The study\nhighlights the significant lexical and semantic similarity between BdSL, West\nBengal Sign Language, and ISL, and the lack of word-level datasets for BdSL in\nthe literature. We release the dataset and source code to stimulate further\nresearch.",
        "translated": ""
    },
    {
        "title": "CheXagent: Towards a Foundation Model for Chest X-Ray Interpretation",
        "url": "http://arxiv.org/abs/2401.12208v1",
        "pub_date": "2024-01-22",
        "summary": "Chest X-rays (CXRs) are the most frequently performed imaging test in\nclinical practice. Recent advances in the development of vision-language\nfoundation models (FMs) give rise to the possibility of performing automated\nCXR interpretation, which can assist physicians with clinical decision-making\nand improve patient outcomes. However, developing FMs that can accurately\ninterpret CXRs is challenging due to the (1) limited availability of\nlarge-scale vision-language datasets in the medical image domain, (2) lack of\nvision and language encoders that can capture the complexities of medical data,\nand (3) absence of evaluation frameworks for benchmarking the abilities of FMs\non CXR interpretation. In this work, we address these challenges by first\nintroducing \\emph{CheXinstruct} - a large-scale instruction-tuning dataset\ncurated from 28 publicly-available datasets. We then present \\emph{CheXagent} -\nan instruction-tuned FM capable of analyzing and summarizing CXRs. To build\nCheXagent, we design a clinical large language model (LLM) for parsing\nradiology reports, a vision encoder for representing CXR images, and a network\nto bridge the vision and language modalities. Finally, we introduce\n\\emph{CheXbench} - a novel benchmark designed to systematically evaluate FMs\nacross 8 clinically-relevant CXR interpretation tasks. Extensive quantitative\nevaluations and qualitative reviews with five expert radiologists demonstrate\nthat CheXagent outperforms previously-developed general- and medical-domain FMs\non CheXbench tasks. Furthermore, in an effort to improve model transparency, we\nperform a fairness evaluation across factors of sex, race and age to highlight\npotential performance disparities. Our project is at\n\\url{https://stanford-aimi.github.io/chexagent.html}.",
        "translated": ""
    },
    {
        "title": "OK-Robot: What Really Matters in Integrating Open-Knowledge Models for\n  Robotics",
        "url": "http://arxiv.org/abs/2401.12202v1",
        "pub_date": "2024-01-22",
        "summary": "Remarkable progress has been made in recent years in the fields of vision,\nlanguage, and robotics. We now have vision models capable of recognizing\nobjects based on language queries, navigation systems that can effectively\ncontrol mobile systems, and grasping models that can handle a wide range of\nobjects. Despite these advancements, general-purpose applications of robotics\nstill lag behind, even though they rely on these fundamental capabilities of\nrecognition, navigation, and grasping. In this paper, we adopt a systems-first\napproach to develop a new Open Knowledge-based robotics framework called\nOK-Robot. By combining Vision-Language Models (VLMs) for object detection,\nnavigation primitives for movement, and grasping primitives for object\nmanipulation, OK-Robot offers a integrated solution for pick-and-drop\noperations without requiring any training. To evaluate its performance, we run\nOK-Robot in 10 real-world home environments. The results demonstrate that\nOK-Robot achieves a 58.5% success rate in open-ended pick-and-drop tasks,\nrepresenting a new state-of-the-art in Open Vocabulary Mobile Manipulation\n(OVMM) with nearly 1.8x the performance of prior work. On cleaner, uncluttered\nenvironments, OK-Robot's performance increases to 82%. However, the most\nimportant insight gained from OK-Robot is the critical role of nuanced details\nwhen combining Open Knowledge systems like VLMs with robotic modules. Videos of\nour experiments are available on our website: https://ok-robot.github.io",
        "translated": ""
    },
    {
        "title": "LONEStar: The Lunar Flashlight Optical Navigation Experiment",
        "url": "http://arxiv.org/abs/2401.12198v1",
        "pub_date": "2024-01-22",
        "summary": "This paper documents the results from the highly successful Lunar flashlight\nOptical Navigation Experiment with a Star tracker (LONEStar). Launched in\nDecember 2022, Lunar Flashlight (LF) was a NASA-funded technology demonstration\nmission. After a propulsion system anomaly prevented capture in lunar orbit, LF\nwas ejected from the Earth-Moon system and into heliocentric space. NASA\nsubsequently transferred ownership of LF to Georgia Tech to conduct an unfunded\nextended mission to demonstrate further advanced technology objectives,\nincluding LONEStar. From August-December 2023, the LONEStar team performed\non-orbit calibration of the optical instrument and a number of different OPNAV\nexperiments. This campaign included the processing of nearly 400 images of star\nfields, Earth and Moon, and four other planets (Mercury, Mars, Jupiter, and\nSaturn). LONEStar provided the first on-orbit demonstrations of heliocentric\nnavigation using only optical observations of planets. Of special note is the\nsuccessful in-flight demonstration of (1) instantaneous triangulation with\nsimultaneous sightings of two planets with the LOST algorithm and (2) dynamic\ntriangulation with sequential sightings of multiple planets.",
        "translated": ""
    },
    {
        "title": "Broiler-Net: A Deep Convolutional Framework for Broiler Behavior\n  Analysis in Poultry Houses",
        "url": "http://arxiv.org/abs/2401.12176v1",
        "pub_date": "2024-01-22",
        "summary": "Detecting anomalies in poultry houses is crucial for maintaining optimal\nchicken health conditions, minimizing economic losses and bolstering\nprofitability. This paper presents a novel real-time framework for analyzing\nchicken behavior in cage-free poultry houses to detect abnormal behaviors.\nSpecifically, two significant abnormalities, namely inactive broiler and\nhuddling behavior, are investigated in this study. The proposed framework\ncomprises three key steps: (1) chicken detection utilizing a state-of-the-art\ndeep learning model, (2) tracking individual chickens across consecutive frames\nwith a fast tracker module, and (3) detecting abnormal behaviors within the\nvideo stream. Experimental studies are conducted to evaluate the efficacy of\nthe proposed algorithm in accurately assessing chicken behavior. The results\nillustrate that our framework provides a precise and efficient solution for\nreal-time anomaly detection, facilitating timely interventions to maintain\nchicken health and enhance overall productivity on poultry farms. Github:\nhttps://github.com/TaherehZarratEhsan/Chicken-Behavior-Analysis",
        "translated": ""
    },
    {
        "title": "Single-View 3D Human Digitalization with Large Reconstruction Models",
        "url": "http://arxiv.org/abs/2401.12175v1",
        "pub_date": "2024-01-22",
        "summary": "In this paper, we introduce Human-LRM, a single-stage feed-forward Large\nReconstruction Model designed to predict human Neural Radiance Fields (NeRF)\nfrom a single image. Our approach demonstrates remarkable adaptability in\ntraining using extensive datasets containing 3D scans and multi-view capture.\nFurthermore, to enhance the model's applicability for in-the-wild scenarios\nespecially with occlusions, we propose a novel strategy that distills\nmulti-view reconstruction into single-view via a conditional triplane diffusion\nmodel. This generative extension addresses the inherent variations in human\nbody shapes when observed from a single view, and makes it possible to\nreconstruct the full body human from an occluded image. Through extensive\nexperiments, we show that Human-LRM surpasses previous methods by a significant\nmargin on several benchmarks.",
        "translated": ""
    },
    {
        "title": "SpatialVLM: Endowing Vision-Language Models with Spatial Reasoning\n  Capabilities",
        "url": "http://arxiv.org/abs/2401.12168v1",
        "pub_date": "2024-01-22",
        "summary": "Understanding and reasoning about spatial relationships is a fundamental\ncapability for Visual Question Answering (VQA) and robotics. While Vision\nLanguage Models (VLM) have demonstrated remarkable performance in certain VQA\nbenchmarks, they still lack capabilities in 3D spatial reasoning, such as\nrecognizing quantitative relationships of physical objects like distances or\nsize differences. We hypothesize that VLMs' limited spatial reasoning\ncapability is due to the lack of 3D spatial knowledge in training data and aim\nto solve this problem by training VLMs with Internet-scale spatial reasoning\ndata. To this end, we present a system to facilitate this approach. We first\ndevelop an automatic 3D spatial VQA data generation framework that scales up to\n2 billion VQA examples on 10 million real-world images. We then investigate\nvarious factors in the training recipe, including data quality, training\npipeline, and VLM architecture. Our work features the first internet-scale 3D\nspatial reasoning dataset in metric space. By training a VLM on such data, we\nsignificantly enhance its ability on both qualitative and quantitative spatial\nVQA. Finally, we demonstrate that this VLM unlocks novel downstream\napplications in chain-of-thought spatial reasoning and robotics due to its\nquantitative estimation capability. Project website:\nhttps://spatial-vlm.github.io/",
        "translated": ""
    },
    {
        "title": "Semi-supervised segmentation of land cover images using nonlinear\n  canonical correlation analysis with multiple features and t-SNE",
        "url": "http://arxiv.org/abs/2401.12164v1",
        "pub_date": "2024-01-22",
        "summary": "Image segmentation is a clustering task whereby each pixel is assigned a\ncluster label. Remote sensing data usually consists of multiple bands of\nspectral images in which there exist semantically meaningful land cover\nsubregions, co-registered with other source data such as LIDAR (LIght Detection\nAnd Ranging) data, where available. This suggests that, in order to account for\nspatial correlation between pixels, a feature vector associated with each pixel\nmay be a vectorized tensor representing the multiple bands and a local patch as\nappropriate. Similarly, multiple types of texture features based on a pixel's\nlocal patch would also be beneficial for encoding locally statistical\ninformation and spatial variations, without necessarily labelling pixel-wise a\nlarge amount of ground truth, then training a supervised model, which is\nsometimes impractical. In this work, by resorting to label only a small\nquantity of pixels, a new semi-supervised segmentation approach is proposed.\nInitially, over all pixels, an image data matrix is created in high dimensional\nfeature space. Then, t-SNE projects the high dimensional data onto 3D\nembedding. By using radial basis functions as input features, which use the\nlabelled data samples as centres, to pair with the output class labels, a\nmodified canonical correlation analysis algorithm, referred to as RBF-CCA, is\nintroduced which learns the associated projection matrix via the small labelled\ndata set. The associated canonical variables, obtained for the full image, are\napplied by k-means clustering algorithm. The proposed semi-supervised RBF-CCA\nalgorithm has been implemented on several remotely sensed multispectral images,\ndemonstrating excellent segmentation results.",
        "translated": ""
    },
    {
        "title": "Zero-Shot Learning for the Primitives of 3D Affordance in General\n  Objects",
        "url": "http://arxiv.org/abs/2401.12978v1",
        "pub_date": "2024-01-23",
        "summary": "One of the major challenges in AI is teaching machines to precisely respond\nand utilize environmental functionalities, thereby achieving the affordance\nawareness that humans possess. Despite its importance, the field has been\nlagging in terms of learning, especially in 3D, as annotating affordance\naccompanies a laborious process due to the numerous variations of human-object\ninteraction. The low availability of affordance data limits the learning in\nterms of generalization for object categories, and also simplifies the\nrepresentation of affordance, capturing only a fraction of the affordance. To\novercome these challenges, we propose a novel, self-supervised method to\ngenerate the 3D affordance examples given only a 3D object, without any manual\nannotations. The method starts by capturing the 3D object into images and\ncreating 2D affordance images by inserting humans into the image via inpainting\ndiffusion models, where we present the Adaptive Mask algorithm to enable human\ninsertion without altering the original details of the object. The method\nconsequently lifts inserted humans back to 3D to create 3D human-object pairs,\nwhere the depth ambiguity is resolved within a depth optimization framework\nthat utilizes pre-generated human postures from multiple viewpoints. We also\nprovide a novel affordance representation defined on relative orientations and\nproximity between dense human and object points, that can be easily aggregated\nfrom any 3D HOI datasets. The proposed representation serves as a primitive\nthat can be manifested to conventional affordance representations via simple\ntransformations, ranging from physically exerted affordances to nonphysical\nones. We demonstrate the efficacy of our method and representation by\ngenerating the 3D affordance samples and deriving high-quality affordance\nexamples from the representation, including contact, orientation, and spatial\noccupancies.",
        "translated": ""
    },
    {
        "title": "GALA: Generating Animatable Layered Assets from a Single Scan",
        "url": "http://arxiv.org/abs/2401.12979v1",
        "pub_date": "2024-01-23",
        "summary": "We present GALA, a framework that takes as input a single-layer clothed 3D\nhuman mesh and decomposes it into complete multi-layered 3D assets. The outputs\ncan then be combined with other assets to create novel clothed human avatars\nwith any pose. Existing reconstruction approaches often treat clothed humans as\na single-layer of geometry and overlook the inherent compositionality of humans\nwith hairstyles, clothing, and accessories, thereby limiting the utility of the\nmeshes for downstream applications. Decomposing a single-layer mesh into\nseparate layers is a challenging task because it requires the synthesis of\nplausible geometry and texture for the severely occluded regions. Moreover,\neven with successful decomposition, meshes are not normalized in terms of poses\nand body shapes, failing coherent composition with novel identities and poses.\nTo address these challenges, we propose to leverage the general knowledge of a\npretrained 2D diffusion model as geometry and appearance prior for humans and\nother assets. We first separate the input mesh using the 3D surface\nsegmentation extracted from multi-view 2D segmentations. Then we synthesize the\nmissing geometry of different layers in both posed and canonical spaces using a\nnovel pose-guided Score Distillation Sampling (SDS) loss. Once we complete\ninpainting high-fidelity 3D geometry, we also apply the same SDS loss to its\ntexture to obtain the complete appearance including the initially occluded\nregions. Through a series of decomposition steps, we obtain multiple layers of\n3D assets in a shared canonical space normalized in terms of poses and human\nshapes, hence supporting effortless composition to novel identities and\nreanimation with novel poses. Our experiments demonstrate the effectiveness of\nour approach for decomposition, canonicalization, and composition tasks\ncompared to existing solutions.",
        "translated": ""
    },
    {
        "title": "IRIS: Inverse Rendering of Indoor Scenes from Low Dynamic Range Images",
        "url": "http://arxiv.org/abs/2401.12977v1",
        "pub_date": "2024-01-23",
        "summary": "While numerous 3D reconstruction and novel-view synthesis methods allow for\nphotorealistic rendering of a scene from multi-view images easily captured with\nconsumer cameras, they bake illumination in their representations and fall\nshort of supporting advanced applications like material editing, relighting,\nand virtual object insertion. The reconstruction of physically based material\nproperties and lighting via inverse rendering promises to enable such\napplications.\n  However, most inverse rendering techniques require high dynamic range (HDR)\nimages as input, a setting that is inaccessible to most users. We present a\nmethod that recovers the physically based material properties and\nspatially-varying HDR lighting of a scene from multi-view, low-dynamic-range\n(LDR) images. We model the LDR image formation process in our inverse rendering\npipeline and propose a novel optimization strategy for material, lighting, and\na camera response model. We evaluate our approach with synthetic and real\nscenes compared to the state-of-the-art inverse rendering methods that take\neither LDR or HDR input. Our method outperforms existing methods taking LDR\nimages as input, and allows for highly realistic relighting and object\ninsertion.",
        "translated": ""
    },
    {
        "title": "HAZARD Challenge: Embodied Decision Making in Dynamically Changing\n  Environments",
        "url": "http://arxiv.org/abs/2401.12975v1",
        "pub_date": "2024-01-23",
        "summary": "Recent advances in high-fidelity virtual environments serve as one of the\nmajor driving forces for building intelligent embodied agents to perceive,\nreason and interact with the physical world. Typically, these environments\nremain unchanged unless agents interact with them. However, in real-world\nscenarios, agents might also face dynamically changing environments\ncharacterized by unexpected events and need to rapidly take action accordingly.\nTo remedy this gap, we propose a new simulated embodied benchmark, called\nHAZARD, specifically designed to assess the decision-making abilities of\nembodied agents in dynamic situations. HAZARD consists of three unexpected\ndisaster scenarios, including fire, flood, and wind, and specifically supports\nthe utilization of large language models (LLMs) to assist common sense\nreasoning and decision-making. This benchmark enables us to evaluate autonomous\nagents' decision-making capabilities across various pipelines, including\nreinforcement learning (RL), rule-based, and search-based methods in\ndynamically changing environments. As a first step toward addressing this\nchallenge using large language models, we further develop an LLM-based agent\nand perform an in-depth analysis of its promise and challenge of solving these\nchallenging tasks. HAZARD is available at https://vis-www.cs.umass.edu/hazard/.",
        "translated": ""
    },
    {
        "title": "SegmentAnyBone: A Universal Model that Segments Any Bone at Any Location\n  on MRI",
        "url": "http://arxiv.org/abs/2401.12974v1",
        "pub_date": "2024-01-23",
        "summary": "Magnetic Resonance Imaging (MRI) is pivotal in radiology, offering\nnon-invasive and high-quality insights into the human body. Precise\nsegmentation of MRIs into different organs and tissues would be highly\nbeneficial since it would allow for a higher level of understanding of the\nimage content and enable important measurements, which are essential for\naccurate diagnosis and effective treatment planning. Specifically, segmenting\nbones in MRI would allow for more quantitative assessments of musculoskeletal\nconditions, while such assessments are largely absent in current radiological\npractice. The difficulty of bone MRI segmentation is illustrated by the fact\nthat limited algorithms are publicly available for use, and those contained in\nthe literature typically address a specific anatomic area. In our study, we\npropose a versatile, publicly available deep-learning model for bone\nsegmentation in MRI across multiple standard MRI locations. The proposed model\ncan operate in two modes: fully automated segmentation and prompt-based\nsegmentation. Our contributions include (1) collecting and annotating a new MRI\ndataset across various MRI protocols, encompassing over 300 annotated volumes\nand 8485 annotated slices across diverse anatomic regions; (2) investigating\nseveral standard network architectures and strategies for automated\nsegmentation; (3) introducing SegmentAnyBone, an innovative foundational\nmodel-based approach that extends Segment Anything Model (SAM); (4) comparative\nanalysis of our algorithm and previous approaches; and (5) generalization\nanalysis of our algorithm across different anatomical locations and MRI\nsequences, as well as an external dataset. We publicly release our model at\nhttps://github.com/mazurowski-lab/SegmentAnyBone.",
        "translated": ""
    },
    {
        "title": "On the Efficacy of Text-Based Input Modalities for Action Anticipation",
        "url": "http://arxiv.org/abs/2401.12972v1",
        "pub_date": "2024-01-23",
        "summary": "Although the task of anticipating future actions is highly uncertain,\ninformation from additional modalities help to narrow down plausible action\nchoices. Each modality provides different environmental context for the model\nto learn from. While previous multi-modal methods leverage information from\nmodalities such as video and audio, we primarily explore how text inputs for\nactions and objects can also enable more accurate action anticipation.\nTherefore, we propose a Multi-modal Anticipative Transformer (MAT), an\nattention-based video transformer architecture that jointly learns from\nmulti-modal features and text captions. We train our model in two-stages, where\nthe model first learns to predict actions in the video clip by aligning with\ncaptions, and during the second stage, we fine-tune the model to predict future\nactions. Compared to existing methods, MAT has the advantage of learning\nadditional environmental context from two kinds of text inputs: action\ndescriptions during the pre-training stage, and the text inputs for detected\nobjects and actions during modality feature fusion. Through extensive\nexperiments, we evaluate the effectiveness of the pre-training stage, and show\nthat our model outperforms previous methods on all datasets. In addition, we\nexamine the impact of object and action information obtained via text and\nperform extensive ablations. We evaluate the performance on on three datasets:\nEpicKitchens-100, EpicKitchens-55 and EGTEA GAZE+; and show that text\ndescriptions do indeed aid in more effective action anticipation.",
        "translated": ""
    },
    {
        "title": "AutoRT: Embodied Foundation Models for Large Scale Orchestration of\n  Robotic Agents",
        "url": "http://arxiv.org/abs/2401.12963v1",
        "pub_date": "2024-01-23",
        "summary": "Foundation models that incorporate language, vision, and more recently\nactions have revolutionized the ability to harness internet scale data to\nreason about useful tasks. However, one of the key challenges of training\nembodied foundation models is the lack of data grounded in the physical world.\nIn this paper, we propose AutoRT, a system that leverages existing foundation\nmodels to scale up the deployment of operational robots in completely unseen\nscenarios with minimal human supervision. AutoRT leverages vision-language\nmodels (VLMs) for scene understanding and grounding, and further uses large\nlanguage models (LLMs) for proposing diverse and novel instructions to be\nperformed by a fleet of robots. Guiding data collection by tapping into the\nknowledge of foundation models enables AutoRT to effectively reason about\nautonomy tradeoffs and safety while significantly scaling up data collection\nfor robot learning. We demonstrate AutoRT proposing instructions to over 20\nrobots across multiple buildings and collecting 77k real robot episodes via\nboth teleoperation and autonomous robot policies. We experimentally show that\nsuch \"in-the-wild\" data collected by AutoRT is significantly more diverse, and\nthat AutoRT's use of LLMs allows for instruction following data collection\nrobots that can align to human preferences.",
        "translated": ""
    },
    {
        "title": "Coverage Axis++: Efficient Inner Point Selection for 3D Shape\n  Skeletonization",
        "url": "http://arxiv.org/abs/2401.12946v1",
        "pub_date": "2024-01-23",
        "summary": "We introduce Coverage Axis++, a novel and efficient approach to 3D shape\nskeletonization. The current state-of-the-art approaches for this task often\nrely on the watertightness of the input or suffer from substantial\ncomputational costs, thereby limiting their practicality. To address this\nchallenge, Coverage Axis++ proposes a heuristic algorithm to select skeletal\npoints, offering a high-accuracy approximation of the Medial Axis Transform\n(MAT) while significantly mitigating computational intensity for various shape\nrepresentations. We introduce a simple yet effective strategy that considers\nboth shape coverage and uniformity to derive skeletal points. The selection\nprocedure enforces consistency with the shape structure while favoring the\ndominant medial balls, which thus introduces a compact underlying shape\nrepresentation in terms of MAT. As a result, Coverage Axis++ allows for\nskeletonization for various shape representations (e.g., water-tight meshes,\ntriangle soups, point clouds), specification of the number of skeletal points,\nfew hyperparameters, and highly efficient computation with improved\nreconstruction accuracy. Extensive experiments across a wide range of 3D shapes\nvalidate the efficiency and effectiveness of Coverage Axis++. The code will be\npublicly available once the paper is published.",
        "translated": ""
    },
    {
        "title": "Lumiere: A Space-Time Diffusion Model for Video Generation",
        "url": "http://arxiv.org/abs/2401.12945v1",
        "pub_date": "2024-01-23",
        "summary": "We introduce Lumiere -- a text-to-video diffusion model designed for\nsynthesizing videos that portray realistic, diverse and coherent motion -- a\npivotal challenge in video synthesis. To this end, we introduce a Space-Time\nU-Net architecture that generates the entire temporal duration of the video at\nonce, through a single pass in the model. This is in contrast to existing video\nmodels which synthesize distant keyframes followed by temporal super-resolution\n-- an approach that inherently makes global temporal consistency difficult to\nachieve. By deploying both spatial and (importantly) temporal down- and\nup-sampling and leveraging a pre-trained text-to-image diffusion model, our\nmodel learns to directly generate a full-frame-rate, low-resolution video by\nprocessing it in multiple space-time scales. We demonstrate state-of-the-art\ntext-to-video generation results, and show that our design easily facilitates a\nwide range of content creation tasks and video editing applications, including\nimage-to-video, video inpainting, and stylized generation.",
        "translated": ""
    },
    {
        "title": "Neural deformation fields for template-based reconstruction of cortical\n  surfaces from MRI",
        "url": "http://arxiv.org/abs/2401.12938v1",
        "pub_date": "2024-01-23",
        "summary": "The reconstruction of cortical surfaces is a prerequisite for quantitative\nanalyses of the cerebral cortex in magnetic resonance imaging (MRI). Existing\nsegmentation-based methods separate the surface registration from the surface\nextraction, which is computationally inefficient and prone to distortions. We\nintroduce Vox2Cortex-Flow (V2C-Flow), a deep mesh-deformation technique that\nlearns a deformation field from a brain template to the cortical surfaces of an\nMRI scan. To this end, we present a geometric neural network that models the\ndeformation-describing ordinary differential equation in a continuous manner.\nThe network architecture comprises convolutional and graph-convolutional\nlayers, which allows it to work with images and meshes at the same time.\nV2C-Flow is not only very fast, requiring less than two seconds to infer all\nfour cortical surfaces, but also establishes vertex-wise correspondences to the\ntemplate during reconstruction. In addition, V2C-Flow is the first approach for\ncortex reconstruction that models white matter and pial surfaces jointly,\ntherefore avoiding intersections between them. Our comprehensive experiments on\ninternal and external test data demonstrate that V2C-Flow results in cortical\nsurfaces that are state-of-the-art in terms of accuracy. Moreover, we show that\nthe established correspondences are more consistent than in FreeSurfer and that\nthey can directly be utilized for cortex parcellation and group analyses of\ncortical thickness.",
        "translated": ""
    },
    {
        "title": "Algebraic methods for solving recognition problems with non-crossing\n  classes",
        "url": "http://arxiv.org/abs/2401.13666v1",
        "pub_date": "2024-01-24",
        "summary": "In this paper, we propose to consider various models of pattern recognition.\nAt the same time, it is proposed to consider models in the form of two\noperators: a recognizing operator and a decision rule. Algebraic operations are\nintroduced on recognizing operators, and based on the application of these\noperators, a family of recognizing algorithms is created. An upper estimate is\nconstructed for the model, which guarantees the completeness of the extension.",
        "translated": ""
    },
    {
        "title": "Tyche: Stochastic In-Context Learning for Medical Image Segmentation",
        "url": "http://arxiv.org/abs/2401.13650v1",
        "pub_date": "2024-01-24",
        "summary": "Existing learning-based solutions to medical image segmentation have two\nimportant shortcomings. First, for most new segmentation task, a new model has\nto be trained or fine-tuned. This requires extensive resources and machine\nlearning expertise, and is therefore often infeasible for medical researchers\nand clinicians. Second, most existing segmentation methods produce a single\ndeterministic segmentation mask for a given image. In practice however, there\nis often considerable uncertainty about what constitutes the correct\nsegmentation, and different expert annotators will often segment the same image\ndifferently. We tackle both of these problems with Tyche, a model that uses a\ncontext set to generate stochastic predictions for previously unseen tasks\nwithout the need to retrain. Tyche differs from other in-context segmentation\nmethods in two important ways. (1) We introduce a novel convolution block\narchitecture that enables interactions among predictions. (2) We introduce\nin-context test-time augmentation, a new mechanism to provide prediction\nstochasticity. When combined with appropriate model design and loss functions,\nTyche can predict a set of plausible diverse segmentation candidates for new or\nunseen medical images and segmentation tasks without the need to retrain.",
        "translated": ""
    },
    {
        "title": "VisualWebArena: Evaluating Multimodal Agents on Realistic Visual Web\n  Tasks",
        "url": "http://arxiv.org/abs/2401.13649v1",
        "pub_date": "2024-01-24",
        "summary": "Autonomous agents capable of planning, reasoning, and executing actions on\nthe web offer a promising avenue for automating computer tasks. However, the\nmajority of existing benchmarks primarily focus on text-based agents,\nneglecting many natural tasks that require visual information to effectively\nsolve. Given that most computer interfaces cater to human perception, visual\ninformation often augments textual data in ways that text-only models struggle\nto harness effectively. To bridge this gap, we introduce VisualWebArena, a\nbenchmark designed to assess the performance of multimodal web agents on\nrealistic \\textit{visually grounded tasks}. VisualWebArena comprises of a set\nof diverse and complex web-based tasks that evaluate various capabilities of\nautonomous multimodal agents. To perform on this benchmark, agents need to\naccurately process image-text inputs, interpret natural language instructions,\nand execute actions on websites to accomplish user-defined objectives. We\nconduct an extensive evaluation of state-of-the-art LLM-based autonomous\nagents, including several multimodal models. Through extensive quantitative and\nqualitative analysis, we identify several limitations of text-only LLM agents,\nand reveal gaps in the capabilities of state-of-the-art multimodal language\nagents. VisualWebArena provides a framework for evaluating multimodal\nautonomous language agents, and offers insights towards building stronger\nautonomous agents for the web. Our code, baseline models, and data is publicly\navailable at https://jykoh.com/vwa.",
        "translated": ""
    },
    {
        "title": "How Good is ChatGPT at Face Biometrics? A First Look into Recognition,\n  Soft Biometrics, and Explainability",
        "url": "http://arxiv.org/abs/2401.13641v1",
        "pub_date": "2024-01-24",
        "summary": "Large Language Models (LLMs) such as GPT developed by OpenAI, have already\nshown astonishing results, introducing quick changes in our society. This has\nbeen intensified by the release of ChatGPT which allows anyone to interact in a\nsimple conversational way with LLMs, without any experience in the field\nneeded. As a result, ChatGPT has been rapidly applied to many different tasks\nsuch as code- and song-writer, education, virtual assistants, etc., showing\nimpressive results for tasks for which it was not trained (zero-shot learning).\n  The present study aims to explore the ability of ChatGPT, based on the recent\nGPT-4 multimodal LLM, for the task of face biometrics. In particular, we\nanalyze the ability of ChatGPT to perform tasks such as face verification,\nsoft-biometrics estimation, and explainability of the results. ChatGPT could be\nvery valuable to further increase the explainability and transparency of the\nautomatic decisions in human scenarios. Experiments are carried out in order to\nevaluate the performance and robustness of ChatGPT, using popular public\nbenchmarks and comparing the results with state-of-the-art methods in the\nfield. The results achieved in this study show the potential of LLMs such as\nChatGPT for face biometrics, especially to enhance explainability. For\nreproducibility reasons, we release all the code in GitHub.",
        "translated": ""
    },
    {
        "title": "Scaling Up to Excellence: Practicing Model Scaling for Photo-Realistic\n  Image Restoration In the Wild",
        "url": "http://arxiv.org/abs/2401.13627v1",
        "pub_date": "2024-01-24",
        "summary": "We introduce SUPIR (Scaling-UP Image Restoration), a groundbreaking image\nrestoration method that harnesses generative prior and the power of model\nscaling up. Leveraging multi-modal techniques and advanced generative prior,\nSUPIR marks a significant advance in intelligent and realistic image\nrestoration. As a pivotal catalyst within SUPIR, model scaling dramatically\nenhances its capabilities and demonstrates new potential for image restoration.\nWe collect a dataset comprising 20 million high-resolution, high-quality images\nfor model training, each enriched with descriptive text annotations. SUPIR\nprovides the capability to restore images guided by textual prompts, broadening\nits application scope and potential. Moreover, we introduce negative-quality\nprompts to further improve perceptual quality. We also develop a\nrestoration-guided sampling method to suppress the fidelity issue encountered\nin generative-based restoration. Experiments demonstrate SUPIR's exceptional\nrestoration effects and its novel capacity to manipulate restoration through\ntextual prompts.",
        "translated": ""
    },
    {
        "title": "FLLIC: Functionally Lossless Image Compression",
        "url": "http://arxiv.org/abs/2401.13616v1",
        "pub_date": "2024-01-24",
        "summary": "Recently, DNN models for lossless image coding have surpassed their\ntraditional counterparts in compression performance, reducing the bit rate by\nabout ten percent for natural color images. But even with these advances,\nmathematically lossless image compression (MLLIC) ratios for natural images\nstill fall short of the bandwidth and cost-effectiveness requirements of most\npractical imaging and vision systems at present and beyond. To break the\nbottleneck of MLLIC in compression performance, we question the necessity of\nMLLIC, as almost all digital sensors inherently introduce acquisition noises,\nmaking mathematically lossless compression counterproductive. Therefore, in\ncontrast to MLLIC, we propose a new paradigm of joint denoising and compression\ncalled functionally lossless image compression (FLLIC), which performs lossless\ncompression of optimally denoised images (the optimality may be task-specific).\nAlthough not literally lossless with respect to the noisy input, FLLIC aims to\nachieve the best possible reconstruction of the latent noise-free original\nimage. Extensive experiments show that FLLIC achieves state-of-the-art\nperformance in joint denoising and compression of noisy images and does so at a\nlower computational cost.",
        "translated": ""
    },
    {
        "title": "Enhancing Image Retrieval : A Comprehensive Study on Photo Search using\n  the CLIP Mode",
        "url": "http://arxiv.org/abs/2401.13613v1",
        "pub_date": "2024-01-24",
        "summary": "Photo search, the task of retrieving images based on textual queries, has\nwitnessed significant advancements with the introduction of CLIP (Contrastive\nLanguage-Image Pretraining) model. CLIP leverages a vision-language pre\ntraining approach, wherein it learns a shared representation space for images\nand text, enabling cross-modal understanding. This model demonstrates the\ncapability to understand the semantic relationships between diverse image and\ntext pairs, allowing for efficient and accurate retrieval of images based on\nnatural language queries. By training on a large-scale dataset containing\nimages and their associated textual descriptions, CLIP achieves remarkable\ngeneralization, providing a powerful tool for tasks such as zero-shot learning\nand few-shot classification. This abstract summarizes the foundational\nprinciples of CLIP and highlights its potential impact on advancing the field\nof photo search, fostering a seamless integration of natural language\nunderstanding and computer vision for improved information retrieval in\nmultimedia applications",
        "translated": ""
    },
    {
        "title": "PLATE: A perception-latency aware estimator,",
        "url": "http://arxiv.org/abs/2401.13596v1",
        "pub_date": "2024-01-24",
        "summary": "Target tracking is a popular problem with many potential applications. There\nhas been a lot of effort on improving the quality of the detection of targets\nusing cameras through different techniques. In general, with higher\ncomputational effort applied, i.e., a longer perception-latency, a better\ndetection accuracy is obtained. However, it is not always useful to apply the\nlongest perception-latency allowed, particularly when the environment doesn't\nrequire to and when the computational resources are shared between other tasks.\nIn this work, we propose a new Perception-LATency aware Estimator (PLATE),\nwhich uses different perception configurations in different moments of time in\norder to optimize a certain performance measure. This measure takes into\naccount a perception-latency and accuracy trade-off aiming for a good\ncompromise between quality and resource usage. Compared to other heuristic\nframe-skipping techniques, PLATE comes with a formal complexity and optimality\nanalysis. The advantages of PLATE are verified by several experiments including\nan evaluation over a standard benchmark with real data and using state of the\nart deep learning object detection methods for the perception stage.",
        "translated": ""
    },
    {
        "title": "Towards Efficient and Effective Deep Clustering with Dynamic Grouping\n  and Prototype Aggregation",
        "url": "http://arxiv.org/abs/2401.13581v1",
        "pub_date": "2024-01-24",
        "summary": "Previous contrastive deep clustering methods mostly focus on instance-level\ninformation while overlooking the member relationship within groups/clusters,\nwhich may significantly undermine their representation learning and clustering\ncapability. Recently, some group-contrastive methods have been developed,\nwhich, however, typically rely on the samples of the entire dataset to obtain\npseudo labels and lack the ability to efficiently update the group assignments\nin a batch-wise manner. To tackle these critical issues, we present a novel\nend-to-end deep clustering framework with dynamic grouping and prototype\naggregation, termed as DigPro. Specifically, the proposed dynamic grouping\nextends contrastive learning from instance-level to group-level, which is\neffective and efficient for timely updating groups. Meanwhile, we perform\ncontrastive learning on prototypes in a spherical feature space, termed as\nprototype aggregation, which aims to maximize the inter-cluster distance.\nNotably, with an expectation-maximization framework, DigPro simultaneously\ntakes advantage of compact intra-cluster connections, well-separated clusters,\nand efficient group updating during the self-supervised training. Extensive\nexperiments on six image benchmarks demonstrate the superior performance of our\napproach over the state-of-the-art. Code is available at\nhttps://github.com/Regan-Zhang/DigPro.",
        "translated": ""
    },
    {
        "title": "SegMamba: Long-range Sequential Modeling Mamba For 3D Medical Image\n  Segmentation",
        "url": "http://arxiv.org/abs/2401.13560v1",
        "pub_date": "2024-01-24",
        "summary": "The Transformer architecture has shown a remarkable ability in modeling\nglobal relationships. However, it poses a significant computational challenge\nwhen processing high-dimensional medical images. This hinders its development\nand widespread adoption in this task. Mamba, as a State Space Model (SSM),\nrecently emerged as a notable manner for long-range dependencies in sequential\nmodeling, excelling in natural language processing filed with its remarkable\nmemory efficiency and computational speed. Inspired by its success, we\nintroduce SegMamba, a novel 3D medical image \\textbf{Seg}mentation\n\\textbf{Mamba} model, designed to effectively capture long-range dependencies\nwithin whole volume features at every scale. Our SegMamba, in contrast to\nTransformer-based methods, excels in whole volume feature modeling from a state\nspace model standpoint, maintaining superior processing speed, even with volume\nfeatures at a resolution of {$64\\times 64\\times 64$}. Comprehensive experiments\non the BraTS2023 dataset demonstrate the effectiveness and efficiency of our\nSegMamba. The code for SegMamba is available at:\nhttps://github.com/ge-xing/SegMamba",
        "translated": ""
    },
    {
        "title": "Multimodal Pathway: Improve Transformers with Irrelevant Data from Other\n  Modalities",
        "url": "http://arxiv.org/abs/2401.14405v1",
        "pub_date": "2024-01-25",
        "summary": "We propose to improve transformers of a specific modality with irrelevant\ndata from other modalities, e.g., improve an ImageNet model with audio or point\ncloud datasets. We would like to highlight that the data samples of the target\nmodality are irrelevant to the other modalities, which distinguishes our method\nfrom other works utilizing paired (e.g., CLIP) or interleaved data of different\nmodalities. We propose a methodology named Multimodal Pathway - given a target\nmodality and a transformer designed for it, we use an auxiliary transformer\ntrained with data of another modality and construct pathways to connect\ncomponents of the two models so that data of the target modality can be\nprocessed by both models. In this way, we utilize the universal\nsequence-to-sequence modeling abilities of transformers obtained from two\nmodalities. As a concrete implementation, we use a modality-specific tokenizer\nand task-specific head as usual but utilize the transformer blocks of the\nauxiliary model via a proposed method named Cross-Modal Re-parameterization,\nwhich exploits the auxiliary weights without any inference costs. On the image,\npoint cloud, video, and audio recognition tasks, we observe significant and\nconsistent performance improvements with irrelevant data from other modalities.\nThe code and models are available at https://github.com/AILab-CVC/M2PT.",
        "translated": ""
    },
    {
        "title": "Deconstructing Denoising Diffusion Models for Self-Supervised Learning",
        "url": "http://arxiv.org/abs/2401.14404v1",
        "pub_date": "2024-01-25",
        "summary": "In this study, we examine the representation learning abilities of Denoising\nDiffusion Models (DDM) that were originally purposed for image generation. Our\nphilosophy is to deconstruct a DDM, gradually transforming it into a classical\nDenoising Autoencoder (DAE). This deconstructive procedure allows us to explore\nhow various components of modern DDMs influence self-supervised representation\nlearning. We observe that only a very few modern components are critical for\nlearning good representations, while many others are nonessential. Our study\nultimately arrives at an approach that is highly simplified and to a large\nextent resembles a classical DAE. We hope our study will rekindle interest in a\nfamily of classical methods within the realm of modern self-supervised\nlearning.",
        "translated": ""
    },
    {
        "title": "Adaptive Mobile Manipulation for Articulated Objects In the Open World",
        "url": "http://arxiv.org/abs/2401.14403v1",
        "pub_date": "2024-01-25",
        "summary": "Deploying robots in open-ended unstructured environments such as homes has\nbeen a long-standing research problem. However, robots are often studied only\nin closed-off lab settings, and prior mobile manipulation work is restricted to\npick-move-place, which is arguably just the tip of the iceberg in this area. In\nthis paper, we introduce Open-World Mobile Manipulation System, a full-stack\napproach to tackle realistic articulated object operation, e.g. real-world\ndoors, cabinets, drawers, and refrigerators in open-ended unstructured\nenvironments. The robot utilizes an adaptive learning framework to initially\nlearns from a small set of data through behavior cloning, followed by learning\nfrom online practice on novel objects that fall outside the training\ndistribution. We also develop a low-cost mobile manipulation hardware platform\ncapable of safe and autonomous online adaptation in unstructured environments\nwith a cost of around 20,000 USD. In our experiments we utilize 20 articulate\nobjects across 4 buildings in the CMU campus. With less than an hour of online\nlearning for each object, the system is able to increase success rate from 50%\nof BC pre-training to 95% using online adaptation. Video results at\nhttps://open-world-mobilemanip.github.io/",
        "translated": ""
    },
    {
        "title": "Range-Agnostic Multi-View Depth Estimation With Keyframe Selection",
        "url": "http://arxiv.org/abs/2401.14401v1",
        "pub_date": "2024-01-25",
        "summary": "Methods for 3D reconstruction from posed frames require prior knowledge about\nthe scene metric range, usually to recover matching cues along the epipolar\nlines and narrow the search range. However, such prior might not be directly\navailable or estimated inaccurately in real scenarios -- e.g., outdoor 3D\nreconstruction from video sequences -- therefore heavily hampering performance.\nIn this paper, we focus on multi-view depth estimation without requiring prior\nknowledge about the metric range of the scene by proposing RAMDepth, an\nefficient and purely 2D framework that reverses the depth estimation and\nmatching steps order. Moreover, we demonstrate the capability of our framework\nto provide rich insights about the quality of the views used for prediction.\nAdditional material can be found on our project page\nhttps://andreaconti.github.io/projects/range_agnostic_multi_view_depth.",
        "translated": ""
    },
    {
        "title": "pix2gestalt: Amodal Segmentation by Synthesizing Wholes",
        "url": "http://arxiv.org/abs/2401.14398v1",
        "pub_date": "2024-01-25",
        "summary": "We introduce pix2gestalt, a framework for zero-shot amodal segmentation,\nwhich learns to estimate the shape and appearance of whole objects that are\nonly partially visible behind occlusions. By capitalizing on large-scale\ndiffusion models and transferring their representations to this task, we learn\na conditional diffusion model for reconstructing whole objects in challenging\nzero-shot cases, including examples that break natural and physical priors,\nsuch as art. As training data, we use a synthetically curated dataset\ncontaining occluded objects paired with their whole counterparts. Experiments\nshow that our approach outperforms supervised baselines on established\nbenchmarks. Our model can furthermore be used to significantly improve the\nperformance of existing object recognition and 3D reconstruction methods in the\npresence of occlusions.",
        "translated": ""
    },
    {
        "title": "Rethinking Patch Dependence for Masked Autoencoders",
        "url": "http://arxiv.org/abs/2401.14391v1",
        "pub_date": "2024-01-25",
        "summary": "In this work, we re-examine inter-patch dependencies in the decoding\nmechanism of masked autoencoders (MAE). We decompose this decoding mechanism\nfor masked patch reconstruction in MAE into self-attention and cross-attention.\nOur investigations suggest that self-attention between mask patches is not\nessential for learning good representations. To this end, we propose a novel\npretraining framework: Cross-Attention Masked Autoencoders (CrossMAE).\nCrossMAE's decoder leverages only cross-attention between masked and visible\ntokens, with no degradation in downstream performance. This design also enables\ndecoding only a small subset of mask tokens, boosting efficiency. Furthermore,\neach decoder block can now leverage different encoder features, resulting in\nimproved representation learning. CrossMAE matches MAE in performance with 2.5\nto 3.7$\\times$ less decoding compute. It also surpasses MAE on ImageNet\nclassification and COCO instance segmentation under the same compute. Code and\nmodels: https://crossmae.github.io",
        "translated": ""
    },
    {
        "title": "Inconsistency Masks: Removing the Uncertainty from Input-Pseudo-Label\n  Pairs",
        "url": "http://arxiv.org/abs/2401.14387v1",
        "pub_date": "2024-01-25",
        "summary": "Generating sufficient labeled data is a significant hurdle in the efficient\nexecution of deep learning projects, especially in uncharted territories of\nimage segmentation where labeling demands extensive time, unlike classification\ntasks. Our study confronts this challenge, operating in an environment\nconstrained by limited hardware resources and the lack of extensive datasets or\npre-trained models. We introduce the novel use of Inconsistency Masks (IM) to\neffectively filter uncertainty in image-pseudo-label pairs, substantially\nelevating segmentation quality beyond traditional semi-supervised learning\ntechniques. By integrating IM with other methods, we demonstrate remarkable\nbinary segmentation performance on the ISIC 2018 dataset, starting with just\n10% labeled data. Notably, three of our hybrid models outperform those trained\non the fully labeled dataset. Our approach consistently achieves exceptional\nresults across three additional datasets and shows further improvement when\ncombined with other techniques. For comprehensive and robust evaluation, this\npaper includes an extensive analysis of prevalent semi-supervised learning\nstrategies, all trained under identical starting conditions. The full code is\navailable at: https://github.com/MichaelVorndran/InconsistencyMasks",
        "translated": ""
    },
    {
        "title": "UrbanGenAI: Reconstructing Urban Landscapes using Panoptic Segmentation\n  and Diffusion Models",
        "url": "http://arxiv.org/abs/2401.14379v1",
        "pub_date": "2024-01-25",
        "summary": "In contemporary design practices, the integration of computer vision and\ngenerative artificial intelligence (genAI) represents a transformative shift\ntowards more interactive and inclusive processes. These technologies offer new\ndimensions of image analysis and generation, which are particularly relevant in\nthe context of urban landscape reconstruction. This paper presents a novel\nworkflow encapsulated within a prototype application, designed to leverage the\nsynergies between advanced image segmentation and diffusion models for a\ncomprehensive approach to urban design. Our methodology encompasses the\nOneFormer model for detailed image segmentation and the Stable Diffusion XL\n(SDXL) diffusion model, implemented through ControlNet, for generating images\nfrom textual descriptions. Validation results indicated a high degree of\nperformance by the prototype application, showcasing significant accuracy in\nboth object detection and text-to-image generation. This was evidenced by\nsuperior Intersection over Union (IoU) and CLIP scores across iterative\nevaluations for various categories of urban landscape features. Preliminary\ntesting included utilising UrbanGenAI as an educational tool enhancing the\nlearning experience in design pedagogy, and as a participatory instrument\nfacilitating community-driven urban planning. Early results suggested that\nUrbanGenAI not only advances the technical frontiers of urban landscape\nreconstruction but also provides significant pedagogical and participatory\nplanning benefits. The ongoing development of UrbanGenAI aims to further\nvalidate its effectiveness across broader contexts and integrate additional\nfeatures such as real-time feedback mechanisms and 3D modelling capabilities.\nKeywords: generative AI; panoptic image segmentation; diffusion models; urban\nlandscape design; design pedagogy; co-design",
        "translated": ""
    },
    {
        "title": "Learning Robust Generalizable Radiance Field with Visibility and Feature\n  Augmented Point Representation",
        "url": "http://arxiv.org/abs/2401.14354v1",
        "pub_date": "2024-01-25",
        "summary": "This paper introduces a novel paradigm for the generalizable neural radiance\nfield (NeRF). Previous generic NeRF methods combine multiview stereo techniques\nwith image-based neural rendering for generalization, yielding impressive\nresults, while suffering from three issues. First, occlusions often result in\ninconsistent feature matching. Then, they deliver distortions and artifacts in\ngeometric discontinuities and locally sharp shapes due to their individual\nprocess of sampled points and rough feature aggregation. Third, their\nimage-based representations experience severe degradations when source views\nare not near enough to the target view. To address challenges, we propose the\nfirst paradigm that constructs the generalizable neural field based on\npoint-based rather than image-based rendering, which we call the Generalizable\nneural Point Field (GPF). Our approach explicitly models visibilities by\ngeometric priors and augments them with neural features. We propose a novel\nnonuniform log sampling strategy to improve both rendering speed and\nreconstruction quality. Moreover, we present a learnable kernel spatially\naugmented with features for feature aggregations, mitigating distortions at\nplaces with drastically varying geometries. Besides, our representation can be\neasily manipulated. Experiments show that our model can deliver better\ngeometries, view consistencies, and rendering quality than all counterparts and\nbenchmarks on three datasets in both generalization and finetuning settings,\npreliminarily proving the potential of the new paradigm for generalizable NeRF.",
        "translated": ""
    },
    {
        "title": "Learning to navigate efficiently and precisely in real environments",
        "url": "http://arxiv.org/abs/2401.14349v1",
        "pub_date": "2024-01-25",
        "summary": "In the context of autonomous navigation of terrestrial robots, the creation\nof realistic models for agent dynamics and sensing is a widespread habit in the\nrobotics literature and in commercial applications, where they are used for\nmodel based control and/or for localization and mapping. The more recent\nEmbodied AI literature, on the other hand, focuses on modular or end-to-end\nagents trained in simulators like Habitat or AI-Thor, where the emphasis is put\non photo-realistic rendering and scene diversity, but high-fidelity robot\nmotion is assigned a less privileged role. The resulting sim2real gap\nsignificantly impacts transfer of the trained models to real robotic platforms.\nIn this work we explore end-to-end training of agents in simulation in settings\nwhich minimize the sim2real gap both, in sensing and in actuation. Our agent\ndirectly predicts (discretized) velocity commands, which are maintained through\nclosed-loop control in the real robot. The behavior of the real robot\n(including the underlying low-level controller) is identified and simulated in\na modified Habitat simulator. Noise models for odometry and localization\nfurther contribute in lowering the sim2real gap. We evaluate on real navigation\nscenarios, explore different localization and point goal calculation methods\nand report significant gains in performance and robustness compared to prior\nwork.",
        "translated": ""
    },
    {
        "title": "Annotated Hands for Generative Models",
        "url": "http://arxiv.org/abs/2401.15075v1",
        "pub_date": "2024-01-26",
        "summary": "Generative models such as GANs and diffusion models have demonstrated\nimpressive image generation capabilities. Despite these successes, these\nsystems are surprisingly poor at creating images with hands. We propose a novel\ntraining framework for generative models that substantially improves the\nability of such systems to create hand images. Our approach is to augment the\ntraining images with three additional channels that provide annotations to\nhands in the image. These annotations provide additional structure that coax\nthe generative model to produce higher quality hand images. We demonstrate this\napproach on two different generative models: a generative adversarial network\nand a diffusion model. We demonstrate our method both on a new synthetic\ndataset of hand images and also on real photographs that contain hands. We\nmeasure the improved quality of the generated hands through higher confidence\nin finger joint identification using an off-the-shelf hand detector.",
        "translated": ""
    },
    {
        "title": "From GPT-4 to Gemini and Beyond: Assessing the Landscape of MLLMs on\n  Generalizability, Trustworthiness and Causality through Four Modalities",
        "url": "http://arxiv.org/abs/2401.15071v1",
        "pub_date": "2024-01-26",
        "summary": "Multi-modal Large Language Models (MLLMs) have shown impressive abilities in\ngenerating reasonable responses with respect to multi-modal contents. However,\nthere is still a wide gap between the performance of recent MLLM-based\napplications and the expectation of the broad public, even though the most\npowerful OpenAI's GPT-4 and Google's Gemini have been deployed. This paper\nstrives to enhance understanding of the gap through the lens of a qualitative\nstudy on the generalizability, trustworthiness, and causal reasoning\ncapabilities of recent proprietary and open-source MLLMs across four\nmodalities: ie, text, code, image, and video, ultimately aiming to improve the\ntransparency of MLLMs. We believe these properties are several representative\nfactors that define the reliability of MLLMs, in supporting various downstream\napplications. To be specific, we evaluate the closed-source GPT-4 and Gemini\nand 6 open-source LLMs and MLLMs. Overall we evaluate 230 manually designed\ncases, where the qualitative results are then summarized into 12 scores (ie, 4\nmodalities times 3 properties). In total, we uncover 14 empirical findings that\nare useful to understand the capabilities and limitations of both proprietary\nand open-source MLLMs, towards more reliable downstream multi-modal\napplications.",
        "translated": ""
    },
    {
        "title": "Deep learning-based approach for tomato classification in complex scenes",
        "url": "http://arxiv.org/abs/2401.15055v1",
        "pub_date": "2024-01-26",
        "summary": "Tracking ripening tomatoes is time consuming and labor intensive. Artificial\nintelligence technologies combined with those of computer vision can help users\noptimize the process of monitoring the ripening status of plants. To this end,\nwe have proposed a tomato ripening monitoring approach based on deep learning\nin complex scenes. The objective is to detect mature tomatoes and harvest them\nin a timely manner. The proposed approach is declined in two parts. Firstly,\nthe images of the scene are transmitted to the pre-processing layer. This\nprocess allows the detection of areas of interest (area of the image containing\ntomatoes). Then, these images are used as input to the maturity detection\nlayer. This layer, based on a deep neural network learning algorithm,\nclassifies the tomato thumbnails provided to it in one of the following five\ncategories: green, brittle, pink, pale red, mature red. The experiments are\nbased on images collected from the internet gathered through searches using\ntomato state across diverse languages including English, German, French, and\nSpanish. The experimental results of the maturity detection layer on a dataset\ncomposed of images of tomatoes taken under the extreme conditions, gave a good\nclassification rate.",
        "translated": ""
    },
    {
        "title": "Unrecognizable Yet Identifiable: Image Distortion with Preserved\n  Embeddings",
        "url": "http://arxiv.org/abs/2401.15048v1",
        "pub_date": "2024-01-26",
        "summary": "In the realm of security applications, biometric authentication systems play\na crucial role, yet one often encounters challenges concerning privacy and\nsecurity while developing one. One of the most fundamental challenges lies in\navoiding storing biometrics directly in the storage but still achieving\ndecently high accuracy. Addressing this issue, we contribute to both artificial\nintelligence and engineering fields. We introduce an innovative image\ndistortion technique that effectively renders facial images unrecognizable to\nthe eye while maintaining their identifiability by neural network models. From\nthe theoretical perspective, we explore how reliable state-of-the-art\nbiometrics recognition neural networks are by checking the maximal degree of\nimage distortion, which leaves the predicted identity unchanged. On the other\nhand, applying this technique demonstrates a practical solution to the\nengineering challenge of balancing security, precision, and performance in\nbiometric authentication systems. Through experimenting on the widely used\ndatasets, we assess the effectiveness of our method in preserving AI feature\nrepresentation and distorting relative to conventional metrics. We also compare\nour method with previously used approaches.",
        "translated": ""
    },
    {
        "title": "Learning Neural Radiance Fields of Forest Structure for Scalable and\n  Fine Monitoring",
        "url": "http://arxiv.org/abs/2401.15029v1",
        "pub_date": "2024-01-26",
        "summary": "This work leverages neural radiance fields and remote sensing for forestry\napplications. Here, we show neural radiance fields offer a wide range of\npossibilities to improve upon existing remote sensing methods in forest\nmonitoring. We present experiments that demonstrate their potential to: (1)\nexpress fine features of forest 3D structure, (2) fuse available remote sensing\nmodalities and (3), improve upon 3D structure derived forest metrics.\nAltogether, these properties make neural fields an attractive computational\ntool with great potential to further advance the scalability and accuracy of\nforest monitoring programs.",
        "translated": ""
    },
    {
        "title": "Machine learning-based analysis of glioma tissue sections: a review",
        "url": "http://arxiv.org/abs/2401.15022v1",
        "pub_date": "2024-01-26",
        "summary": "In recent years, the diagnosis of gliomas has become increasingly complex.\nHistological assessment of glioma tissue using modern machine learning\ntechniques offers new opportunities to support diagnosis and outcome\nprediction. To give an overview of the current state of research, this review\nexamines 70 publicly available research studies on machine learning-based\nanalysis of stained human glioma tissue sections, covering the diagnostic tasks\nof subtyping (16/70), grading (23/70), molecular marker prediction (13/70), and\nsurvival prediction (27/70). All studies were reviewed with regard to\nmethodological aspects as well as clinical applicability. It was found that the\nfocus of current research is the assessment of hematoxylin and eosin-stained\ntissue sections of adult-type diffuse gliomas. The majority of studies (49/70)\nare based on the publicly available glioblastoma and low-grade glioma datasets\nfrom The Cancer Genome Atlas (TCGA) and only a few studies employed other\ndatasets in isolation (10/70) or in addition to the TCGA datasets (11/70).\nCurrent approaches mostly rely on convolutional neural networks (53/70) for\nanalyzing tissue at 20x magnification (30/70). A new field of research is the\nintegration of clinical data, omics data, or magnetic resonance imaging\n(27/70). So far, machine learning-based methods have achieved promising\nresults, but are not yet used in real clinical settings. Future work should\nfocus on the independent validation of methods on larger, multi-site datasets\nwith high-quality and up-to-date clinical and molecular pathology annotations\nto demonstrate routine applicability.",
        "translated": ""
    },
    {
        "title": "BackdoorBench: A Comprehensive Benchmark and Analysis of Backdoor\n  Learning",
        "url": "http://arxiv.org/abs/2401.15002v1",
        "pub_date": "2024-01-26",
        "summary": "As an emerging and vital topic for studying deep neural networks'\nvulnerability (DNNs), backdoor learning has attracted increasing interest in\nrecent years, and many seminal backdoor attack and defense algorithms are being\ndeveloped successively or concurrently, in the status of a rapid arms race.\nHowever, mainly due to the diverse settings, and the difficulties of\nimplementation and reproducibility of existing works, there is a lack of a\nunified and standardized benchmark of backdoor learning, causing unfair\ncomparisons, and unreliable conclusions (e.g., misleading, biased or even false\nconclusions). Consequently, it is difficult to evaluate the current progress\nand design the future development roadmap of this literature. To alleviate this\ndilemma, we build a comprehensive benchmark of backdoor learning called\nBackdoorBench. Our benchmark makes three valuable contributions to the research\ncommunity. 1) We provide an integrated implementation of state-of-the-art\n(SOTA) backdoor learning algorithms (currently including 16 attack and 27\ndefense algorithms), based on an extensible modular-based codebase. 2) We\nconduct comprehensive evaluations of 12 attacks against 16 defenses, with 5\npoisoning ratios, based on 4 models and 4 datasets, thus 11,492 pairs of\nevaluations in total. 3) Based on above evaluations, we present abundant\nanalysis from 8 perspectives via 18 useful analysis tools, and provide several\ninspiring insights about backdoor learning. We hope that our efforts could\nbuild a solid foundation of backdoor learning to facilitate researchers to\ninvestigate existing algorithms, develop more innovative algorithms, and\nexplore the intrinsic mechanism of backdoor learning. Finally, we have created\na user-friendly website at http://backdoorbench.com, which collects all\nimportant information of BackdoorBench, including codebase, docs, leaderboard,\nand model Zoo.",
        "translated": ""
    },
    {
        "title": "Masked Pre-trained Model Enables Universal Zero-shot Denoiser",
        "url": "http://arxiv.org/abs/2401.14966v1",
        "pub_date": "2024-01-26",
        "summary": "In this work, we observe that the model, which is trained on vast general\nimages using masking strategy, has been naturally embedded with the\ndistribution knowledge regarding natural images, and thus spontaneously attains\nthe underlying potential for strong image denoising. Based on this observation,\nwe propose a novel zero-shot denoising paradigm, i.e., Masked Pre-train then\nIterative fill (MPI). MPI pre-trains a model with masking and fine-tunes it for\ndenoising of a single image with unseen noise degradation. Concretely, the\nproposed MPI comprises two key procedures: 1) Masked Pre-training involves\ntraining a model on multiple natural images with random masks to gather\ngeneralizable representations, allowing for practical applications in varying\nnoise degradation and even in distinct image types. 2) Iterative filling is\ndevised to efficiently fuse pre-trained knowledge for denoising. Similar to but\ndistinct from pre-training, random masking is retained to bridge the gap, but\nonly the predicted parts covered by masks are assembled for efficiency, which\nenables high-quality denoising within a limited number of iterations.\nComprehensive experiments across various noisy scenarios underscore the notable\nadvances of proposed MPI over previous approaches with a marked reduction in\ninference time. Code is available at https://github.com/krennic999/MPI.git.",
        "translated": ""
    },
    {
        "title": "Conserve-Update-Revise to Cure Generalization and Robustness Trade-off\n  in Adversarial Training",
        "url": "http://arxiv.org/abs/2401.14948v1",
        "pub_date": "2024-01-26",
        "summary": "Adversarial training improves the robustness of neural networks against\nadversarial attacks, albeit at the expense of the trade-off between standard\nand robust generalization. To unveil the underlying factors driving this\nphenomenon, we examine the layer-wise learning capabilities of neural networks\nduring the transition from a standard to an adversarial setting. Our empirical\nfindings demonstrate that selectively updating specific layers while preserving\nothers can substantially enhance the network's learning capacity. We therefore\npropose CURE, a novel training framework that leverages a gradient prominence\ncriterion to perform selective conservation, updating, and revision of weights.\nImportantly, CURE is designed to be dataset- and architecture-agnostic,\nensuring its applicability across various scenarios. It effectively tackles\nboth memorization and overfitting issues, thus enhancing the trade-off between\nrobustness and generalization and additionally, this training approach also\naids in mitigating \"robust overfitting\". Furthermore, our study provides\nvaluable insights into the mechanisms of selective adversarial training and\noffers a promising avenue for future research.",
        "translated": ""
    },
    {
        "title": "DAM: Diffusion Activation Maximization for 3D Global Explanations",
        "url": "http://arxiv.org/abs/2401.14938v1",
        "pub_date": "2024-01-26",
        "summary": "In recent years, the performance of point cloud models has been rapidly\nimproved. However, due to the limited amount of relevant explainability\nstudies, the unreliability and opacity of these black-box models may lead to\npotential risks in applications where human lives are at stake, e.g. autonomous\ndriving or healthcare. This work proposes a DDPM-based point cloud global\nexplainability method (DAM) that leverages Point Diffusion Transformer (PDT), a\nnovel point-wise symmetric model, with dual-classifier guidance to generate\nhigh-quality global explanations. In addition, an adapted path gradient\nintegration method for DAM is proposed, which not only provides a global\noverview of the saliency maps for point cloud categories, but also sheds light\non how the attributions of the explanations vary during the generation process.\nExtensive experiments indicate that our method outperforms existing ones in\nterms of perceptibility, representativeness, and diversity, with a significant\nreduction in generation time. Our code is available at:\nhttps://github.com/Explain3D/DAM",
        "translated": ""
    },
    {
        "title": "Computer Vision for Primate Behavior Analysis in the Wild",
        "url": "http://arxiv.org/abs/2401.16424v1",
        "pub_date": "2024-01-29",
        "summary": "Advances in computer vision as well as increasingly widespread video-based\nbehavioral monitoring have great potential for transforming how we study animal\ncognition and behavior. However, there is still a fairly large gap between the\nexciting prospects and what can actually be achieved in practice today,\nespecially in videos from the wild. With this perspective paper, we want to\ncontribute towards closing this gap, by guiding behavioral scientists in what\ncan be expected from current methods and steering computer vision researchers\ntowards problems that are relevant to advance research in animal behavior. We\nstart with a survey of the state-of-the-art methods for computer vision\nproblems that are directly relevant to the video-based study of animal\nbehavior, including object detection, multi-individual tracking, (inter)action\nrecognition and individual identification. We then review methods for\neffort-efficient learning, which is one of the biggest challenges from a\npractical perspective. Finally, we close with an outlook into the future of the\nemerging field of computer vision for animal behavior, where we argue that the\nfield should move fast beyond the common frame-by-frame processing and treat\nvideo as a first-class citizen.",
        "translated": ""
    },
    {
        "title": "Synchformer: Efficient Synchronization from Sparse Cues",
        "url": "http://arxiv.org/abs/2401.16423v1",
        "pub_date": "2024-01-29",
        "summary": "Our objective is audio-visual synchronization with a focus on 'in-the-wild'\nvideos, such as those on YouTube, where synchronization cues can be sparse. Our\ncontributions include a novel audio-visual synchronization model, and training\nthat decouples feature extraction from synchronization modelling through\nmulti-modal segment-level contrastive pre-training. This approach achieves\nstate-of-the-art performance in both dense and sparse settings. We also extend\nsynchronization model training to AudioSet a million-scale 'in-the-wild'\ndataset, investigate evidence attribution techniques for interpretability, and\nexplore a new capability for synchronization models: audio-visual\nsynchronizability.",
        "translated": ""
    },
    {
        "title": "InternLM-XComposer2: Mastering Free-form Text-Image Composition and\n  Comprehension in Vision-Language Large Model",
        "url": "http://arxiv.org/abs/2401.16420v1",
        "pub_date": "2024-01-29",
        "summary": "We introduce InternLM-XComposer2, a cutting-edge vision-language model\nexcelling in free-form text-image composition and comprehension. This model\ngoes beyond conventional vision-language understanding, adeptly crafting\ninterleaved text-image content from diverse inputs like outlines, detailed\ntextual specifications, and reference images, enabling highly customizable\ncontent creation. InternLM-XComposer2 proposes a Partial LoRA (PLoRA) approach\nthat applies additional LoRA parameters exclusively to image tokens to preserve\nthe integrity of pre-trained language knowledge, striking a balance between\nprecise vision understanding and text composition with literary talent.\nExperimental results demonstrate the superiority of InternLM-XComposer2 based\non InternLM2-7B in producing high-quality long-text multi-modal content and its\nexceptional vision-language understanding performance across various\nbenchmarks, where it not only significantly outperforms existing multimodal\nmodels but also matches or even surpasses GPT-4V and Gemini Pro in certain\nassessments. This highlights its remarkable proficiency in the realm of\nmultimodal understanding. The InternLM-XComposer2 model series with 7B\nparameters are publicly available at\nhttps://github.com/InternLM/InternLM-XComposer.",
        "translated": ""
    },
    {
        "title": "Endo-4DGS: Distilling Depth Ranking for Endoscopic Monocular Scene\n  Reconstruction with 4D Gaussian Splatting",
        "url": "http://arxiv.org/abs/2401.16416v1",
        "pub_date": "2024-01-29",
        "summary": "In the realm of robot-assisted minimally invasive surgery, dynamic scene\nreconstruction can significantly enhance downstream tasks and improve surgical\noutcomes. Neural Radiance Fields (NeRF)-based methods have recently risen to\nprominence for their exceptional ability to reconstruct scenes. Nonetheless,\nthese methods are hampered by slow inference, prolonged training, and\nsubstantial computational demands. Additionally, some rely on stereo depth\nestimation, which is often infeasible due to the high costs and logistical\nchallenges associated with stereo cameras. Moreover, the monocular\nreconstruction quality for deformable scenes is currently inadequate. To\novercome these obstacles, we present Endo-4DGS, an innovative, real-time\nendoscopic dynamic reconstruction approach that utilizes 4D Gaussian Splatting\n(GS) and requires no ground truth depth data. This method extends 3D GS by\nincorporating a temporal component and leverages a lightweight MLP to capture\ntemporal Gaussian deformations. This effectively facilitates the reconstruction\nof dynamic surgical scenes with variable conditions. We also integrate\nDepth-Anything to generate pseudo-depth maps from monocular views, enhancing\nthe depth-guided reconstruction process. Our approach has been validated on two\nsurgical datasets, where it has proven to render in real-time, compute\nefficiently, and reconstruct with remarkable accuracy. These results underline\nthe vast potential of Endo-4DGS to improve surgical assistance.",
        "translated": ""
    },
    {
        "title": "A Survey on Visual Anomaly Detection: Challenge, Approach, and Prospect",
        "url": "http://arxiv.org/abs/2401.16402v1",
        "pub_date": "2024-01-29",
        "summary": "Visual Anomaly Detection (VAD) endeavors to pinpoint deviations from the\nconcept of normality in visual data, widely applied across diverse domains,\ne.g., industrial defect inspection, and medical lesion detection. This survey\ncomprehensively examines recent advancements in VAD by identifying three\nprimary challenges: 1) scarcity of training data, 2) diversity of visual\nmodalities, and 3) complexity of hierarchical anomalies. Starting with a brief\noverview of the VAD background and its generic concept definitions, we\nprogressively categorize, emphasize, and discuss the latest VAD progress from\nthe perspective of sample number, data modality, and anomaly hierarchy. Through\nan in-depth analysis of the VAD field, we finally summarize future developments\nfor VAD and conclude the key findings and contributions of this survey.",
        "translated": ""
    },
    {
        "title": "Amazon's 2023 Drought: Sentinel-1 Reveals Extreme Rio Negro River\n  Contraction",
        "url": "http://arxiv.org/abs/2401.16393v1",
        "pub_date": "2024-01-29",
        "summary": "The Amazon, the world's largest rainforest, faces a severe historic drought.\nThe Rio Negro River, one of the major Amazon River tributaries, reaches its\nlowest level in a century in October 2023. Here, we used a U-net deep learning\nmodel to map water surfaces in the Rio Negro River basin every 12 days in 2022\nand 2023 using 10 m spatial resolution Sentinel-1 satellite radar images. The\naccuracy of the water surface model was high with an F1-score of 0.93. The 12\ndays mosaic time series of water surface was generated from the Sentinel-1\nprediction. The water surface mask demonstrated relatively consistent agreement\nwith the Global Surface Water (GSW) product from Joint Research Centre\n(F1-score: 0.708) and with the Brazilian Mapbiomas Water initiative (F1-score:\n0.686). The main errors of the map were omission errors in flooded woodland, in\nflooded shrub and because of clouds. Rio Negro water surfaces reached their\nlowest level around the 25th of November 2023 and were reduced to 68.1\\%\n(9,559.9 km$^2$) of the maximum water surfaces observed in the period 2022-2023\n(14,036.3 km$^2$). Synthetic Aperture Radar (SAR) data, in conjunction with\ndeep learning techniques, can significantly improve near real-time mapping of\nwater surface in tropical regions.",
        "translated": ""
    },
    {
        "title": "Continual Learning with Pre-Trained Models: A Survey",
        "url": "http://arxiv.org/abs/2401.16386v1",
        "pub_date": "2024-01-29",
        "summary": "Nowadays, real-world applications often face streaming data, which requires\nthe learning system to absorb new knowledge as data evolves. Continual Learning\n(CL) aims to achieve this goal and meanwhile overcome the catastrophic\nforgetting of former knowledge when learning new ones. Typical CL methods build\nthe model from scratch to grow with incoming data. However, the advent of the\npre-trained model (PTM) era has sparked immense research interest, particularly\nin leveraging PTMs' robust representational capabilities. This paper presents a\ncomprehensive survey of the latest advancements in PTM-based CL. We categorize\nexisting methodologies into three distinct groups, providing a comparative\nanalysis of their similarities, differences, and respective advantages and\ndisadvantages. Additionally, we offer an empirical study contrasting various\nstate-of-the-art methods to highlight concerns regarding fairness in\ncomparisons. The source code to reproduce these evaluations is available at:\nhttps://github.com/sun-hailong/LAMDA-PILOT",
        "translated": ""
    },
    {
        "title": "Spot the Error: Non-autoregressive Graphic Layout Generation with\n  Wireframe Locator",
        "url": "http://arxiv.org/abs/2401.16375v1",
        "pub_date": "2024-01-29",
        "summary": "Layout generation is a critical step in graphic design to achieve meaningful\ncompositions of elements. Most previous works view it as a sequence generation\nproblem by concatenating element attribute tokens (i.e., category, size,\nposition). So far the autoregressive approach (AR) has achieved promising\nresults, but is still limited in global context modeling and suffers from error\npropagation since it can only attend to the previously generated tokens. Recent\nnon-autoregressive attempts (NAR) have shown competitive results, which\nprovides a wider context range and the flexibility to refine with iterative\ndecoding. However, current works only use simple heuristics to recognize\nerroneous tokens for refinement which is inaccurate. This paper first conducts\nan in-depth analysis to better understand the difference between the AR and NAR\nframework. Furthermore, based on our observation that pixel space is more\nsensitive in capturing spatial patterns of graphic layouts (e.g., overlap,\nalignment), we propose a learning-based locator to detect erroneous tokens\nwhich takes the wireframe image rendered from the generated layout sequence as\ninput. We show that it serves as a complementary modality to the element\nsequence in object space and contributes greatly to the overall performance.\nExperiments on two public datasets show that our approach outperforms both AR\nand NAR baselines. Extensive studies further prove the effectiveness of\ndifferent modules with interesting findings. Our code will be available at\nhttps://github.com/ffffatgoose/SpotError.",
        "translated": ""
    },
    {
        "title": "Evaluation of pseudo-healthy image reconstruction for anomaly detection\n  with deep generative models: Application to brain FDG PET",
        "url": "http://arxiv.org/abs/2401.16363v1",
        "pub_date": "2024-01-29",
        "summary": "Over the past years, pseudo-healthy reconstruction for unsupervised anomaly\ndetection has gained in popularity. This approach has the great advantage of\nnot requiring tedious pixel-wise data annotation and offers possibility to\ngeneralize to any kind of anomalies, including that corresponding to rare\ndiseases. By training a deep generative model with only images from healthy\nsubjects, the model will learn to reconstruct pseudo-healthy images. This\npseudo-healthy reconstruction is then compared to the input to detect and\nlocalize anomalies. The evaluation of such methods often relies on a ground\ntruth lesion mask that is available for test data, which may not exist\ndepending on the application.\n  We propose an evaluation procedure based on the simulation of realistic\nabnormal images to validate pseudo-healthy reconstruction methods when no\nground truth is available. This allows us to extensively test generative models\non different kinds of anomalies and measuring their performance using the pair\nof normal and abnormal images corresponding to the same subject. It can be used\nas a preliminary automatic step to validate the capacity of a generative model\nto reconstruct pseudo-healthy images, before a more advanced validation step\nthat would require clinician's expertise. We apply this framework to the\nreconstruction of 3D brain FDG PET using a convolutional variational\nautoencoder with the aim to detect as early as possible the neurodegeneration\nmarkers that are specific to dementia such as Alzheimer's disease.",
        "translated": ""
    },
    {
        "title": "PathMMU: A Massive Multimodal Expert-Level Benchmark for Understanding\n  and Reasoning in Pathology",
        "url": "http://arxiv.org/abs/2401.16355v1",
        "pub_date": "2024-01-29",
        "summary": "The emergence of large multimodal models has unlocked remarkable potential in\nAI, particularly in pathology. However, the lack of specialized, high-quality\nbenchmark impeded their development and precise evaluation. To address this, we\nintroduce PathMMU, the largest and highest-quality expert-validated pathology\nbenchmark for LMMs. It comprises 33,573 multimodal multi-choice questions and\n21,599 images from various sources, and an explanation for the correct answer\naccompanies each question. The construction of PathMMU capitalizes on the\nrobust capabilities of GPT-4V, utilizing approximately 30,000 gathered\nimage-caption pairs to generate Q\\&amp;As. Significantly, to maximize PathMMU's\nauthority, we invite six pathologists to scrutinize each question under strict\nstandards in PathMMU's validation and test sets, while simultaneously setting\nan expert-level performance benchmark for PathMMU. We conduct extensive\nevaluations, including zero-shot assessments of 14 open-sourced and three\nclosed-sourced LMMs and their robustness to image corruption. We also fine-tune\nrepresentative LMMs to assess their adaptability to PathMMU. The empirical\nfindings indicate that advanced LMMs struggle with the challenging PathMMU\nbenchmark, with the top-performing LMM, GPT-4V, achieving only a 51.7\\%\nzero-shot performance, significantly lower than the 71.4\\% demonstrated by\nhuman pathologists. After fine-tuning, even open-sourced LMMs can surpass\nGPT-4V with a performance of over 60\\%, but still fall short of the expertise\nshown by pathologists. We hope that the PathMMU will offer valuable insights\nand foster the development of more specialized, next-generation LLMs for\npathology.",
        "translated": ""
    },
    {
        "title": "A simple, strong baseline for building damage detection on the xBD\n  dataset",
        "url": "http://arxiv.org/abs/2401.17271v1",
        "pub_date": "2024-01-30",
        "summary": "We construct a strong baseline method for building damage detection by\nstarting with the highly-engineered winning solution of the xView2 competition,\nand gradually stripping away components. This way, we obtain a much simpler\nmethod, while retaining adequate performance. We expect the simplified solution\nto be more widely and easily applicable. This expectation is based on the\nreduced complexity, as well as the fact that we choose hyperparameters based on\nsimple heuristics, that transfer to other datasets. We then re-arrange the\nxView2 dataset splits such that the test locations are not seen during\ntraining, contrary to the competition setup. In this setting, we find that both\nthe complex and the simplified model fail to generalize to unseen locations.\nAnalyzing the dataset indicates that this failure to generalize is not only a\nmodel-based problem, but that the difficulty might also be influenced by the\nunequal class distributions between events.\n  Code, including the baseline model, is available under\nhttps://github.com/PaulBorneP/Xview2_Strong_Baseline",
        "translated": ""
    },
    {
        "title": "YOLO-World: Real-Time Open-Vocabulary Object Detection",
        "url": "http://arxiv.org/abs/2401.17270v1",
        "pub_date": "2024-01-30",
        "summary": "The You Only Look Once (YOLO) series of detectors have established themselves\nas efficient and practical tools. However, their reliance on predefined and\ntrained object categories limits their applicability in open scenarios.\nAddressing this limitation, we introduce YOLO-World, an innovative approach\nthat enhances YOLO with open-vocabulary detection capabilities through\nvision-language modeling and pre-training on large-scale datasets.\nSpecifically, we propose a new Re-parameterizable Vision-Language Path\nAggregation Network (RepVL-PAN) and region-text contrastive loss to facilitate\nthe interaction between visual and linguistic information. Our method excels in\ndetecting a wide range of objects in a zero-shot manner with high efficiency.\nOn the challenging LVIS dataset, YOLO-World achieves 35.4 AP with 52.0 FPS on\nV100, which outperforms many state-of-the-art methods in terms of both accuracy\nand speed. Furthermore, the fine-tuned YOLO-World achieves remarkable\nperformance on several downstream tasks, including object detection and\nopen-vocabulary instance segmentation.",
        "translated": ""
    },
    {
        "title": "Robust Prompt Optimization for Defending Language Models Against\n  Jailbreaking Attacks",
        "url": "http://arxiv.org/abs/2401.17263v1",
        "pub_date": "2024-01-30",
        "summary": "Despite advances in AI alignment, language models (LM) remain vulnerable to\nadversarial attacks or jailbreaking, in which adversaries modify input prompts\nto induce harmful behavior. While some defenses have been proposed, they focus\non narrow threat models and fall short of a strong defense, which we posit\nshould be effective, universal, and practical. To achieve this, we propose the\nfirst adversarial objective for defending LMs against jailbreaking attacks and\nan algorithm, robust prompt optimization (RPO), that uses gradient-based token\noptimization to enforce harmless outputs. This results in an easily accessible\nsuffix that significantly improves robustness to both jailbreaks seen during\noptimization and unknown, held-out jailbreaks, reducing the attack success rate\non Starling-7B from 84% to 8.66% across 20 jailbreaks. In addition, we find\nthat RPO has a minor effect on normal LM use, is successful under adaptive\nattacks, and can transfer to black-box models, reducing the success rate of the\nstrongest attack on GPT-4 from 92% to 6%.",
        "translated": ""
    },
    {
        "title": "You Only Need One Step: Fast Super-Resolution with Stable Diffusion via\n  Scale Distillation",
        "url": "http://arxiv.org/abs/2401.17258v1",
        "pub_date": "2024-01-30",
        "summary": "In this paper, we introduce YONOS-SR, a novel stable diffusion-based approach\nfor image super-resolution that yields state-of-the-art results using only a\nsingle DDIM step. We propose a novel scale distillation approach to train our\nSR model. Instead of directly training our SR model on the scale factor of\ninterest, we start by training a teacher model on a smaller magnification\nscale, thereby making the SR problem simpler for the teacher. We then train a\nstudent model for a higher magnification scale, using the predictions of the\nteacher as a target during the training. This process is repeated iteratively\nuntil we reach the target scale factor of the final model. The rationale behind\nour scale distillation is that the teacher aids the student diffusion model\ntraining by i) providing a target adapted to the current noise level rather\nthan using the same target coming from ground truth data for all noise levels\nand ii) providing an accurate target as the teacher has a simpler task to\nsolve. We empirically show that the distilled model significantly outperforms\nthe model trained for high scales directly, specifically with few steps during\ninference. Having a strong diffusion model that requires only one step allows\nus to freeze the U-Net and fine-tune the decoder on top of it. We show that the\ncombination of spatially distilled U-Net and fine-tuned decoder outperforms\nstate-of-the-art methods requiring 200 steps with only one single step.",
        "translated": ""
    },
    {
        "title": "SLIC: A Learned Image Codec Using Structure and Color",
        "url": "http://arxiv.org/abs/2401.17246v1",
        "pub_date": "2024-01-30",
        "summary": "We propose the structure and color based learned image codec (SLIC) in which\nthe task of compression is split into that of luminance and chrominance. The\ndeep learning model is built with a novel multi-scale architecture for Y and UV\nchannels in the encoder, where the features from various stages are combined to\nobtain the latent representation. An autoregressive context model is employed\nfor backward adaptation and a hyperprior block for forward adaptation. Various\nexperiments are carried out to study and analyze the performance of the\nproposed model, and to compare it with other image codecs. We also illustrate\nthe advantages of our method through the visualization of channel impulse\nresponses, latent channels and various ablation studies. The model achieves\nBj{\\o}ntegaard delta bitrate gains of 7.5% and 4.66% in terms of MS-SSIM and\nCIEDE2000 metrics with respect to other state-of-the-art reference codecs.",
        "translated": ""
    },
    {
        "title": "ReAlnet: Achieving More Human Brain-Like Vision via Human Neural\n  Representational Alignment",
        "url": "http://arxiv.org/abs/2401.17231v1",
        "pub_date": "2024-01-30",
        "summary": "Despite the remarkable strides made in artificial intelligence, current\nobject recognition models still lag behind in emulating the mechanism of visual\ninformation processing in human brains. Recent studies have highlighted the\npotential of using neural data to mimic brain processing; however, these often\nreply on invasive neural recordings from non-human subjects, leaving a critical\ngap in our understanding of human visual perception and the development of more\nhuman brain-like vision models. Addressing this gap, we present, for the first\ntime, \"Re(presentational)Al(ignment)net\", a vision model aligned with human\nbrain activity based on non-invasive EEG recordings, demonstrating a\nsignificantly higher similarity to human brain representations. Our innovative\nimage-to-brain multi-layer encoding alignment framework not only optimizes\nmultiple layers of the model, marking a substantial leap in neural alignment,\nbut also enables the model to efficiently learn and mimic human brain's visual\nrepresentational patterns across object categories and different neural data\nmodalities. Furthermore, we discover that alignment with human brain\nrepresentations improves the model's adversarial robustness. Our findings\nsuggest that ReAlnet sets a new precedent in the field, bridging the gap\nbetween artificial and human vision, and paving the way for more brain-like\nartificial intelligence systems.",
        "translated": ""
    },
    {
        "title": "MouSi: Poly-Visual-Expert Vision-Language Models",
        "url": "http://arxiv.org/abs/2401.17221v1",
        "pub_date": "2024-01-30",
        "summary": "Current large vision-language models (VLMs) often encounter challenges such\nas insufficient capabilities of a single visual component and excessively long\nvisual tokens. These issues can limit the model's effectiveness in accurately\ninterpreting complex visual information and over-lengthy contextual\ninformation. Addressing these challenges is crucial for enhancing the\nperformance and applicability of VLMs. This paper proposes the use of ensemble\nexperts technique to synergizes the capabilities of individual visual encoders,\nincluding those skilled in image-text matching, OCR, image segmentation, etc.\nThis technique introduces a fusion network to unify the processing of outputs\nfrom different visual experts, while bridging the gap between image encoders\nand pre-trained LLMs. In addition, we explore different positional encoding\nschemes to alleviate the waste of positional encoding caused by lengthy image\nfeature sequences, effectively addressing the issue of position overflow and\nlength limitations. For instance, in our implementation, this technique\nsignificantly reduces the positional occupancy in models like SAM, from a\nsubstantial 4096 to a more efficient and manageable 64 or even down to 1.\nExperimental results demonstrate that VLMs with multiple experts exhibit\nconsistently superior performance over isolated visual encoders and mark a\nsignificant performance boost as more experts are integrated. We have\nopen-sourced the training code used in this report. All of these resources can\nbe found on our project website.",
        "translated": ""
    },
    {
        "title": "GazeGPT: Augmenting Human Capabilities using Gaze-contingent Contextual\n  AI for Smart Eyewear",
        "url": "http://arxiv.org/abs/2401.17217v1",
        "pub_date": "2024-01-30",
        "summary": "Multimodal large language models (LMMs) excel in world knowledge and\nproblem-solving abilities. Through the use of a world-facing camera and\ncontextual AI, emerging smart accessories aim to provide a seamless interface\nbetween humans and LMMs. Yet, these wearable computing systems lack an\nunderstanding of the user's attention. We introduce GazeGPT as a new user\ninteraction paradigm for contextual AI. GazeGPT uses eye tracking to help the\nLMM understand which object in the world-facing camera view a user is paying\nattention to. Using extensive user evaluations, we show that this\ngaze-contingent mechanism is a faster and more accurate pointing mechanism than\nalternatives; that it augments human capabilities by significantly improving\ntheir accuracy in a dog-breed classification task; and that it is consistently\nranked as more natural than head- or body-driven selection mechanisms for\ncontextual AI. Moreover, we prototype a variety of application scenarios that\nsuggest GazeGPT could be of significant value to users as part of future\nAI-driven personal assistants.",
        "translated": ""
    },
    {
        "title": "ContactGen: Contact-Guided Interactive 3D Human Generation for Partners",
        "url": "http://arxiv.org/abs/2401.17212v1",
        "pub_date": "2024-01-30",
        "summary": "Among various interactions between humans, such as eye contact and gestures,\nphysical interactions by contact can act as an essential moment in\nunderstanding human behaviors. Inspired by this fact, given a 3D partner human\nwith the desired interaction label, we introduce a new task of 3D human\ngeneration in terms of physical contact. Unlike previous works of interacting\nwith static objects or scenes, a given partner human can have diverse poses and\ndifferent contact regions according to the type of interaction. To handle this\nchallenge, we propose a novel method of generating interactive 3D humans for a\ngiven partner human based on a guided diffusion framework. Specifically, we\nnewly present a contact prediction module that adaptively estimates potential\ncontact regions between two input humans according to the interaction label.\nUsing the estimated potential contact regions as complementary guidances, we\ndynamically enforce ContactGen to generate interactive 3D humans for a given\npartner human within a guided diffusion model. We demonstrate ContactGen on the\nCHI3D dataset, where our method generates physically plausible and diverse\nposes compared to comparison methods.",
        "translated": ""
    },
    {
        "title": "Self-Supervised Representation Learning for Nerve Fiber Distribution\n  Patterns in 3D-PLI",
        "url": "http://arxiv.org/abs/2401.17207v1",
        "pub_date": "2024-01-30",
        "summary": "A comprehensive understanding of the organizational principles in the human\nbrain requires, among other factors, well-quantifiable descriptors of nerve\nfiber architecture. Three-dimensional polarized light imaging (3D-PLI) is a\nmicroscopic imaging technique that enables insights into the fine-grained\norganization of myelinated nerve fibers with high resolution. Descriptors\ncharacterizing the fiber architecture observed in 3D-PLI would enable\ndownstream analysis tasks such as multimodal correlation studies, clustering,\nand mapping. However, best practices for observer-independent characterization\nof fiber architecture in 3D-PLI are not yet available. To this end, we propose\nthe application of a fully data-driven approach to characterize nerve fiber\narchitecture in 3D-PLI images using self-supervised representation learning. We\nintroduce a 3D-Context Contrastive Learning (CL-3D) objective that utilizes the\nspatial neighborhood of texture examples across histological brain sections of\na 3D reconstructed volume to sample positive pairs for contrastive learning. We\ncombine this sampling strategy with specifically designed image augmentations\nto gain robustness to typical variations in 3D-PLI parameter maps. The approach\nis demonstrated for the 3D reconstructed occipital lobe of a vervet monkey\nbrain. We show that extracted features are highly sensitive to different\nconfigurations of nerve fibers, yet robust to variations between consecutive\nbrain sections arising from histological processing. We demonstrate their\npractical applicability for retrieving clusters of homogeneous fiber\narchitecture and performing data mining for interactively selected templates of\nspecific components of fiber architecture such as U-fibers.",
        "translated": ""
    },
    {
        "title": "Motion Guidance: Diffusion-Based Image Editing with Differentiable\n  Motion Estimators",
        "url": "http://arxiv.org/abs/2401.18085v1",
        "pub_date": "2024-01-31",
        "summary": "Diffusion models are capable of generating impressive images conditioned on\ntext descriptions, and extensions of these models allow users to edit images at\na relatively coarse scale. However, the ability to precisely edit the layout,\nposition, pose, and shape of objects in images with diffusion models is still\ndifficult. To this end, we propose motion guidance, a zero-shot technique that\nallows a user to specify dense, complex motion fields that indicate where each\npixel in an image should move. Motion guidance works by steering the diffusion\nsampling process with the gradients through an off-the-shelf optical flow\nnetwork. Specifically, we design a guidance loss that encourages the sample to\nhave the desired motion, as estimated by a flow network, while also being\nvisually similar to the source image. By simultaneously sampling from a\ndiffusion model and guiding the sample to have low guidance loss, we can obtain\na motion-edited image. We demonstrate that our technique works on complex\nmotions and produces high quality edits of real and generated images.",
        "translated": ""
    },
    {
        "title": "Binding Touch to Everything: Learning Unified Multimodal Tactile\n  Representations",
        "url": "http://arxiv.org/abs/2401.18084v1",
        "pub_date": "2024-01-31",
        "summary": "The ability to associate touch with other modalities has huge implications\nfor humans and computational systems. However, multimodal learning with touch\nremains challenging due to the expensive data collection process and\nnon-standardized sensor outputs. We introduce UniTouch, a unified tactile model\nfor vision-based touch sensors connected to multiple modalities, including\nvision, language, and sound. We achieve this by aligning our UniTouch\nembeddings to pretrained image embeddings already associated with a variety of\nother modalities. We further propose learnable sensor-specific tokens, allowing\nthe model to learn from a set of heterogeneous tactile sensors, all at the same\ntime. UniTouch is capable of conducting various touch sensing tasks in the\nzero-shot setting, from robot grasping prediction to touch image question\nanswering. To the best of our knowledge, UniTouch is the first to demonstrate\nsuch capabilities. Project page: https://cfeng16.github.io/UniTouch/",
        "translated": ""
    },
    {
        "title": "Improved Scene Landmark Detection for Camera Localization",
        "url": "http://arxiv.org/abs/2401.18083v1",
        "pub_date": "2024-01-31",
        "summary": "Camera localization methods based on retrieval, local feature matching, and\n3D structure-based pose estimation are accurate but require high storage, are\nslow, and are not privacy-preserving. A method based on scene landmark\ndetection (SLD) was recently proposed to address these limitations. It involves\ntraining a convolutional neural network (CNN) to detect a few predetermined,\nsalient, scene-specific 3D points or landmarks and computing camera pose from\nthe associated 2D-3D correspondences. Although SLD outperformed existing\nlearning-based approaches, it was notably less accurate than 3D structure-based\nmethods. In this paper, we show that the accuracy gap was due to insufficient\nmodel capacity and noisy labels during training. To mitigate the capacity\nissue, we propose to split the landmarks into subgroups and train a separate\nnetwork for each subgroup. To generate better training labels, we propose using\ndense reconstructions to estimate visibility of scene landmarks. Finally, we\npresent a compact architecture to improve memory efficiency. Accuracy wise, our\napproach is on par with state of the art structure based methods on the\nINDOOR-6 dataset but runs significantly faster and uses less storage. Code and\nmodels can be found at https://github.com/microsoft/SceneLandmarkLocalization.",
        "translated": ""
    },
    {
        "title": "CARFF: Conditional Auto-encoded Radiance Field for 3D Scene Forecasting",
        "url": "http://arxiv.org/abs/2401.18075v1",
        "pub_date": "2024-01-31",
        "summary": "We propose CARFF: Conditional Auto-encoded Radiance Field for 3D Scene\nForecasting, a method for predicting future 3D scenes given past observations,\nsuch as 2D ego-centric images. Our method maps an image to a distribution over\nplausible 3D latent scene configurations using a probabilistic encoder, and\npredicts the evolution of the hypothesized scenes through time. Our latent\nscene representation conditions a global Neural Radiance Field (NeRF) to\nrepresent a 3D scene model, which enables explainable predictions and\nstraightforward downstream applications. This approach extends beyond previous\nneural rendering work by considering complex scenarios of uncertainty in\nenvironmental states and dynamics. We employ a two-stage training of\nPose-Conditional-VAE and NeRF to learn 3D representations. Additionally, we\nauto-regressively predict latent scene representations as a partially\nobservable Markov decision process, utilizing a mixture density network. We\ndemonstrate the utility of our method in realistic scenarios using the CARLA\ndriving simulator, where CARFF can be used to enable efficient trajectory and\ncontingency planning in complex multi-agent autonomous driving scenarios\ninvolving visual occlusions.",
        "translated": ""
    },
    {
        "title": "Benchmarking Sensitivity of Continual Graph Learning for Skeleton-Based\n  Action Recognition",
        "url": "http://arxiv.org/abs/2401.18054v1",
        "pub_date": "2024-01-31",
        "summary": "Continual learning (CL) is the research field that aims to build machine\nlearning models that can accumulate knowledge continuously over different tasks\nwithout retraining from scratch. Previous studies have shown that pre-training\ngraph neural networks (GNN) may lead to negative transfer (Hu et al., 2020)\nafter fine-tuning, a setting which is closely related to CL. Thus, we focus on\nstudying GNN in the continual graph learning (CGL) setting. We propose the\nfirst continual graph learning benchmark for spatio-temporal graphs and use it\nto benchmark well-known CGL methods in this novel setting. The benchmark is\nbased on the N-UCLA and NTU-RGB+D datasets for skeleton-based action\nrecognition. Beyond benchmarking for standard performance metrics, we study the\nclass and task-order sensitivity of CGL methods, i.e., the impact of learning\norder on each class/task's performance, and the architectural sensitivity of\nCGL methods with backbone GNN at various widths and depths. We reveal that\ntask-order robust methods can still be class-order sensitive and observe\nresults that contradict previous empirical observations on architectural\nsensitivity in CL.",
        "translated": ""
    },
    {
        "title": "DROP: Decouple Re-Identification and Human Parsing with Task-specific\n  Features for Occluded Person Re-identification",
        "url": "http://arxiv.org/abs/2401.18032v1",
        "pub_date": "2024-01-31",
        "summary": "The paper introduces the Decouple Re-identificatiOn and human Parsing (DROP)\nmethod for occluded person re-identification (ReID). Unlike mainstream\napproaches using global features for simultaneous multi-task learning of ReID\nand human parsing, or relying on semantic information for attention guidance,\nDROP argues that the inferior performance of the former is due to distinct\ngranularity requirements for ReID and human parsing features. ReID focuses on\ninstance part-level differences between pedestrian parts, while human parsing\ncenters on semantic spatial context, reflecting the internal structure of the\nhuman body. To address this, DROP decouples features for ReID and human\nparsing, proposing detail-preserving upsampling to combine varying resolution\nfeature maps. Parsing-specific features for human parsing are decoupled, and\nhuman position information is exclusively added to the human parsing branch. In\nthe ReID branch, a part-aware compactness loss is introduced to enhance\ninstance-level part differences. Experimental results highlight the efficacy of\nDROP, especially achieving a Rank-1 accuracy of 76.8% on Occluded-Duke,\nsurpassing two mainstream methods. The codebase is accessible at\nhttps://github.com/shuguang-52/DROP.",
        "translated": ""
    },
    {
        "title": "Multilinear Operator Networks",
        "url": "http://arxiv.org/abs/2401.17992v1",
        "pub_date": "2024-01-31",
        "summary": "Despite the remarkable capabilities of deep neural networks in image\nrecognition, the dependence on activation functions remains a largely\nunexplored area and has yet to be eliminated. On the other hand, Polynomial\nNetworks is a class of models that does not require activation functions, but\nhave yet to perform on par with modern architectures. In this work, we aim\nclose this gap and propose MONet, which relies solely on multilinear operators.\nThe core layer of MONet, called Mu-Layer, captures multiplicative interactions\nof the elements of the input token. MONet captures high-degree interactions of\nthe input elements and we demonstrate the efficacy of our approach on a series\nof image recognition and scientific computing benchmarks. The proposed model\noutperforms prior polynomial networks and performs on par with modern\narchitectures. We believe that MONet can inspire further research on models\nthat use entirely multilinear operations.",
        "translated": ""
    },
    {
        "title": "Shrub of a thousand faces: an individual segmentation from satellite\n  images using deep learning",
        "url": "http://arxiv.org/abs/2401.17985v1",
        "pub_date": "2024-01-31",
        "summary": "Monitoring the distribution and size structure of long-living shrubs, such as\nJuniperus communis, can be used to estimate the long-term effects of climate\nchange on high-mountain and high latitude ecosystems. Historical aerial\nvery-high resolution imagery offers a retrospective tool to monitor shrub\ngrowth and distribution at high precision. Currently, deep learning models\nprovide impressive results for detecting and delineating the contour of objects\nwith defined shapes. However, adapting these models to detect natural objects\nthat express complex growth patterns, such as junipers, is still a challenging\ntask.\n  This research presents a novel approach that leverages remotely sensed RGB\nimagery in conjunction with Mask R-CNN-based instance segmentation models to\nindividually delineate Juniperus shrubs above the treeline in Sierra Nevada\n(Spain). In this study, we propose a new data construction design that consists\nin using photo interpreted (PI) and field work (FW) data to respectively\ndevelop and externally validate the model. We also propose a new shrub-tailored\nevaluation algorithm based on a new metric called Multiple Intersections over\nGround Truth Area (MIoGTA) to assess and optimize the model shrub delineation\nperformance. Finally, we deploy the developed model for the first time to\ngenerate a wall-to-wall map of Juniperus individuals.\n  The experimental results demonstrate the efficiency of our dual data\nconstruction approach in overcoming the limitations associated with traditional\nfield survey methods. They also highlight the robustness of MIoGTA metric in\nevaluating instance segmentation models on species with complex growth patterns\nshowing more resilience against data annotation uncertainty. Furthermore, they\nshow the effectiveness of employing Mask R-CNN with ResNet101-C4 backbone in\ndelineating PI and FW shrubs, achieving an F1-score of 87,87% and 76.86%,\nrespectively.",
        "translated": ""
    },
    {
        "title": "Enhancing Multimodal Large Language Models with Vision Detection Models:\n  An Empirical Study",
        "url": "http://arxiv.org/abs/2401.17981v1",
        "pub_date": "2024-01-31",
        "summary": "Despite the impressive capabilities of Multimodal Large Language Models\n(MLLMs) in integrating text and image modalities, challenges remain in\naccurately interpreting detailed visual elements. This paper presents an\nempirical study on enhancing MLLMs with state-of-the-art (SOTA) object\ndetection and Optical Character Recognition models to improve fine-grained\nimage understanding and reduce hallucination in responses. Our research\ninvestigates the embedding-based infusion of detection information, the impact\nof such infusion on the MLLMs' original abilities, and the interchangeability\nof detection models. We conduct systematic experiments with models such as\nLLaVA-1.5, DINO, and PaddleOCRv2, revealing that our approach not only refines\nMLLMs' performance in specific visual tasks but also maintains their original\nstrengths. The resulting enhanced MLLMs outperform SOTA models on 9 out of 10\nbenchmarks, achieving an improvement of up to 12.99% on the normalized average\nscore, marking a notable advancement in multimodal understanding. We release\nour codes to facilitate further exploration into the fine-grained multimodal\ndialogue capabilities of MLLMs.",
        "translated": ""
    },
    {
        "title": "MelNet: A Real-Time Deep Learning Algorithm for Object Detection",
        "url": "http://arxiv.org/abs/2401.17972v1",
        "pub_date": "2024-01-31",
        "summary": "In this study, a novel deep learning algorithm for object detection, named\nMelNet, was introduced. MelNet underwent training utilizing the KITTI dataset\nfor object detection. Following 300 training epochs, MelNet attained an mAP\n(mean average precision) score of 0.732. Additionally, three alternative models\n-YOLOv5, EfficientDet, and Faster-RCNN-MobileNetv3- were trained on the KITTI\ndataset and juxtaposed with MelNet for object detection.\n  The outcomes underscore the efficacy of employing transfer learning in\ncertain instances. Notably, preexisting models trained on prominent datasets\n(e.g., ImageNet, COCO, and Pascal VOC) yield superior results. Another finding\nunderscores the viability of creating a new model tailored to a specific\nscenario and training it on a specific dataset. This investigation demonstrates\nthat training MelNet exclusively on the KITTI dataset also surpasses\nEfficientDet after 150 epochs. Consequently, post-training, MelNet's\nperformance closely aligns with that of other pre-trained models.",
        "translated": ""
    },
    {
        "title": "AToM: Amortized Text-to-Mesh using 2D Diffusion",
        "url": "http://arxiv.org/abs/2402.00867v1",
        "pub_date": "2024-02-01",
        "summary": "We introduce Amortized Text-to-Mesh (AToM), a feed-forward text-to-mesh\nframework optimized across multiple text prompts simultaneously. In contrast to\nexisting text-to-3D methods that often entail time-consuming per-prompt\noptimization and commonly output representations other than polygonal meshes,\nAToM directly generates high-quality textured meshes in less than 1 second with\naround 10 times reduction in the training cost, and generalizes to unseen\nprompts. Our key idea is a novel triplane-based text-to-mesh architecture with\na two-stage amortized optimization strategy that ensures stable training and\nenables scalability. Through extensive experiments on various prompt\nbenchmarks, AToM significantly outperforms state-of-the-art amortized\napproaches with over 4 times higher accuracy (in DF415 dataset) and produces\nmore distinguishable and higher-quality 3D outputs. AToM demonstrates strong\ngeneralizability, offering finegrained 3D assets for unseen interpolated\nprompts without further optimization during inference, unlike per-prompt\nsolutions.",
        "translated": ""
    },
    {
        "title": "We're Not Using Videos Effectively: An Updated Domain Adaptive Video\n  Segmentation Baseline",
        "url": "http://arxiv.org/abs/2402.00868v1",
        "pub_date": "2024-02-01",
        "summary": "There has been abundant work in unsupervised domain adaptation for semantic\nsegmentation (DAS) seeking to adapt a model trained on images from a labeled\nsource domain to an unlabeled target domain. While the vast majority of prior\nwork has studied this as a frame-level Image-DAS problem, a few Video-DAS works\nhave sought to additionally leverage the temporal signal present in adjacent\nframes. However, Video-DAS works have historically studied a distinct set of\nbenchmarks from Image-DAS, with minimal cross-benchmarking. In this work, we\naddress this gap. Surprisingly, we find that (1) even after carefully\ncontrolling for data and model architecture, state-of-the-art Image-DAS methods\n(HRDA and HRDA+MIC)} outperform Video-DAS methods on established Video-DAS\nbenchmarks (+14.5 mIoU on Viper$\\rightarrow$CityscapesSeq, +19.0 mIoU on\nSynthia$\\rightarrow$CityscapesSeq), and (2) naive combinations of Image-DAS and\nVideo-DAS techniques only lead to marginal improvements across datasets. To\navoid siloed progress between Image-DAS and Video-DAS, we open-source our\ncodebase with support for a comprehensive set of Video-DAS and Image-DAS\nmethods on a common benchmark. Code available at\nhttps://github.com/SimarKareer/UnifiedVideoDA",
        "translated": ""
    },
    {
        "title": "Towards Optimal Feature-Shaping Methods for Out-of-Distribution\n  Detection",
        "url": "http://arxiv.org/abs/2402.00865v1",
        "pub_date": "2024-02-01",
        "summary": "Feature shaping refers to a family of methods that exhibit state-of-the-art\nperformance for out-of-distribution (OOD) detection. These approaches\nmanipulate the feature representation, typically from the penultimate layer of\na pre-trained deep learning model, so as to better differentiate between\nin-distribution (ID) and OOD samples. However, existing feature-shaping methods\nusually employ rules manually designed for specific model architectures and OOD\ndatasets, which consequently limit their generalization ability. To address\nthis gap, we first formulate an abstract optimization framework for studying\nfeature-shaping methods. We then propose a concrete reduction of the framework\nwith a simple piecewise constant shaping function and show that existing\nfeature-shaping methods approximate the optimal solution to the concrete\noptimization problem. Further, assuming that OOD data is inaccessible, we\npropose a formulation that yields a closed-form solution for the piecewise\nconstant shaping function, utilizing solely the ID data. Through extensive\nexperiments, we show that the feature-shaping function optimized by our method\nimproves the generalization ability of OOD detection across a large variety of\ndatasets and model architectures.",
        "translated": ""
    },
    {
        "title": "ViCA-NeRF: View-Consistency-Aware 3D Editing of Neural Radiance Fields",
        "url": "http://arxiv.org/abs/2402.00864v1",
        "pub_date": "2024-02-01",
        "summary": "We introduce ViCA-NeRF, the first view-consistency-aware method for 3D\nediting with text instructions. In addition to the implicit neural radiance\nfield (NeRF) modeling, our key insight is to exploit two sources of\nregularization that explicitly propagate the editing information across\ndifferent views, thus ensuring multi-view consistency. For geometric\nregularization, we leverage the depth information derived from NeRF to\nestablish image correspondences between different views. For learned\nregularization, we align the latent codes in the 2D diffusion model between\nedited and unedited images, enabling us to edit key views and propagate the\nupdate throughout the entire scene. Incorporating these two strategies, our\nViCA-NeRF operates in two stages. In the initial stage, we blend edits from\ndifferent views to create a preliminary 3D edit. This is followed by a second\nstage of NeRF training, dedicated to further refining the scene's appearance.\nExperimental results demonstrate that ViCA-NeRF provides more flexible,\nefficient (3 times faster) editing with higher levels of consistency and\ndetails, compared with the state of the art. Our code is publicly available.",
        "translated": ""
    },
    {
        "title": "Geometry Transfer for Stylizing Radiance Fields",
        "url": "http://arxiv.org/abs/2402.00863v1",
        "pub_date": "2024-02-01",
        "summary": "Shape and geometric patterns are essential in defining stylistic identity.\nHowever, current 3D style transfer methods predominantly focus on transferring\ncolors and textures, often overlooking geometric aspects. In this paper, we\nintroduce Geometry Transfer, a novel method that leverages geometric\ndeformation for 3D style transfer. This technique employs depth maps to extract\na style guide, subsequently applied to stylize the geometry of radiance fields.\nMoreover, we propose new techniques that utilize geometric cues from the 3D\nscene, thereby enhancing aesthetic expressiveness and more accurately\nreflecting intended styles. Our extensive experiments show that Geometry\nTransfer enables a broader and more expressive range of stylizations, thereby\nsignificantly expanding the scope of 3D style transfer.",
        "translated": ""
    },
    {
        "title": "BootsTAP: Bootstrapped Training for Tracking-Any-Point",
        "url": "http://arxiv.org/abs/2402.00847v1",
        "pub_date": "2024-02-01",
        "summary": "To endow models with greater understanding of physics and motion, it is\nuseful to enable them to perceive how solid surfaces move and deform in real\nscenes. This can be formalized as Tracking-Any-Point (TAP), which requires the\nalgorithm to be able to track any point corresponding to a solid surface in a\nvideo, potentially densely in space and time. Large-scale ground-truth training\ndata for TAP is only available in simulation, which currently has limited\nvariety of objects and motion. In this work, we demonstrate how large-scale,\nunlabeled, uncurated real-world data can improve a TAP model with minimal\narchitectural changes, using a self-supervised student-teacher setup. We\ndemonstrate state-of-the-art performance on the TAP-Vid benchmark surpassing\nprevious results by a wide margin: for example, TAP-Vid-DAVIS performance\nimproves from 61.3% to 66.4%, and TAP-Vid-Kinetics from 57.2% to 61.5%.",
        "translated": ""
    },
    {
        "title": "Emo-Avatar: Efficient Monocular Video Style Avatar through Texture\n  Rendering",
        "url": "http://arxiv.org/abs/2402.00827v1",
        "pub_date": "2024-02-01",
        "summary": "Artistic video portrait generation is a significant and sought-after task in\nthe fields of computer graphics and vision. While various methods have been\ndeveloped that integrate NeRFs or StyleGANs with instructional editing models\nfor creating and editing drivable portraits, these approaches face several\nchallenges. They often rely heavily on large datasets, require extensive\ncustomization processes, and frequently result in reduced image quality. To\naddress the above problems, we propose the Efficient Monotonic Video Style\nAvatar (Emo-Avatar) through deferred neural rendering that enhances StyleGAN's\ncapacity for producing dynamic, drivable portrait videos. We proposed a\ntwo-stage deferred neural rendering pipeline. In the first stage, we utilize\nfew-shot PTI initialization to initialize the StyleGAN generator through\nseveral extreme poses sampled from the video to capture the consistent\nrepresentation of aligned faces from the target portrait. In the second stage,\nwe propose a Laplacian pyramid for high-frequency texture sampling from UV maps\ndeformed by dynamic flow of expression for motion-aware texture prior\nintegration to provide torso features to enhance StyleGAN's ability to generate\ncomplete and upper body for portrait video rendering. Emo-Avatar reduces style\ncustomization time from hours to merely 5 minutes compared with existing\nmethods. In addition, Emo-Avatar requires only a single reference image for\nediting and employs region-aware contrastive learning with semantic invariant\nCLIP guidance, ensuring consistent high-resolution output and identity\npreservation. Through both quantitative and qualitative assessments, Emo-Avatar\ndemonstrates superior performance over existing methods in terms of training\nefficiency, rendering quality and editability in self- and cross-reenactment.",
        "translated": ""
    },
    {
        "title": "AnimateLCM: Accelerating the Animation of Personalized Diffusion Models\n  and Adapters with Decoupled Consistency Learning",
        "url": "http://arxiv.org/abs/2402.00769v1",
        "pub_date": "2024-02-01",
        "summary": "Video diffusion models has been gaining increasing attention for its ability\nto produce videos that are both coherent and of high fidelity. However, the\niterative denoising process makes it computationally intensive and\ntime-consuming, thus limiting its applications. Inspired by the Consistency\nModel (CM) that distills pretrained image diffusion models to accelerate the\nsampling with minimal steps and its successful extension Latent Consistency\nModel (LCM) on conditional image generation, we propose AnimateLCM, allowing\nfor high-fidelity video generation within minimal steps. Instead of directly\nconducting consistency learning on the raw video dataset, we propose a\ndecoupled consistency learning strategy that decouples the distillation of\nimage generation priors and motion generation priors, which improves the\ntraining efficiency and enhance the generation visual quality. Additionally, to\nenable the combination of plug-and-play adapters in stable diffusion community\nto achieve various functions (e.g., ControlNet for controllable generation). we\npropose an efficient strategy to adapt existing adapters to our distilled\ntext-conditioned video consistency model or train adapters from scratch without\nharming the sampling speed. We validate the proposed strategy in\nimage-conditioned video generation and layout-conditioned video generation, all\nachieving top-performing results. Experimental results validate the\neffectiveness of our proposed method. Code and weights will be made public.\nMore details are available at https://github.com/G-U-N/AnimateLCM.",
        "translated": ""
    },
    {
        "title": "360-GS: Layout-guided Panoramic Gaussian Splatting For Indoor Roaming",
        "url": "http://arxiv.org/abs/2402.00763v1",
        "pub_date": "2024-02-01",
        "summary": "3D Gaussian Splatting (3D-GS) has recently attracted great attention with\nreal-time and photo-realistic renderings. This technique typically takes\nperspective images as input and optimizes a set of 3D elliptical Gaussians by\nsplatting them onto the image planes, resulting in 2D Gaussians. However,\napplying 3D-GS to panoramic inputs presents challenges in effectively modeling\nthe projection onto the spherical surface of ${360^\\circ}$ images using 2D\nGaussians. In practical applications, input panoramas are often sparse, leading\nto unreliable initialization of 3D Gaussians and subsequent degradation of\n3D-GS quality. In addition, due to the under-constrained geometry of\ntexture-less planes (e.g., walls and floors), 3D-GS struggles to model these\nflat regions with elliptical Gaussians, resulting in significant floaters in\nnovel views. To address these issues, we propose 360-GS, a novel $360^{\\circ}$\nGaussian splatting for a limited set of panoramic inputs. Instead of splatting\n3D Gaussians directly onto the spherical surface, 360-GS projects them onto the\ntangent plane of the unit sphere and then maps them to the spherical\nprojections. This adaptation enables the representation of the projection using\nGaussians. We guide the optimization of 360-GS by exploiting layout priors\nwithin panoramas, which are simple to obtain and contain strong structural\ninformation about the indoor scene. Our experimental results demonstrate that\n360-GS allows panoramic rendering and outperforms state-of-the-art methods with\nfewer artifacts in novel view synthesis, thus providing immersive roaming in\nindoor scenarios.",
        "translated": ""
    },
    {
        "title": "GS++: Error Analyzing and Optimal Gaussian Splatting",
        "url": "http://arxiv.org/abs/2402.00752v1",
        "pub_date": "2024-02-01",
        "summary": "3D Gaussian Splatting has garnered extensive attention and application in\nreal-time neural rendering. Concurrently, concerns have been raised about the\nlimitations of this technology in aspects such as point cloud storage,\nperformance , and robustness in sparse viewpoints , leading to various\nimprovements. However, there has been a notable lack of attention to the\nprojection errors introduced by the local affine approximation inherent in the\nsplatting itself, and the consequential impact of these errors on the quality\nof photo-realistic rendering. This paper addresses the projection error\nfunction of 3D Gaussian Splatting, commencing with the residual error from the\nfirst-order Taylor expansion of the projection function $\\phi$. The analysis\nestablishes a correlation between the error and the Gaussian mean position.\nSubsequently, leveraging function optimization theory, this paper analyzes the\nfunction's minima to provide an optimal projection strategy for Gaussian\nSplatting referred to Optimal Gaussian Splatting. Experimental validation\nfurther confirms that this projection methodology reduces artifacts, resulting\nin a more convincingly realistic rendering.",
        "translated": ""
    },
    {
        "title": "Immersive Video Compression using Implicit Neural Representations",
        "url": "http://arxiv.org/abs/2402.01596v1",
        "pub_date": "2024-02-02",
        "summary": "Recent work on implicit neural representations (INRs) has evidenced their\npotential for efficiently representing and encoding conventional video content.\nIn this paper we, for the first time, extend their application to immersive\n(multi-view) videos, by proposing MV-HiNeRV, a new INR-based immersive video\ncodec. MV-HiNeRV is an enhanced version of a state-of-the-art INR-based video\ncodec, HiNeRV, which was developed for single-view video compression. We have\nmodified the model to learn a different group of feature grids for each view,\nand share the learnt network parameters among all views. This enables the model\nto effectively exploit the spatio-temporal and the inter-view redundancy that\nexists within multi-view videos. The proposed codec was used to compress\nmulti-view texture and depth video sequences in the MPEG Immersive Video (MIV)\nCommon Test Conditions, and tested against the MIV Test model (TMIV) that uses\nthe VVenC video codec. The results demonstrate the superior performance of\nMV-HiNeRV, with significant coding gains (up to 72.33%) over TMIV. The\nimplementation of MV-HiNeRV will be published for further development and\nevaluation.",
        "translated": ""
    },
    {
        "title": "NeuroCine: Decoding Vivid Video Sequences from Human Brain Activties",
        "url": "http://arxiv.org/abs/2402.01590v1",
        "pub_date": "2024-02-02",
        "summary": "In the pursuit to understand the intricacies of human brain's visual\nprocessing, reconstructing dynamic visual experiences from brain activities\nemerges as a challenging yet fascinating endeavor. While recent advancements\nhave achieved success in reconstructing static images from non-invasive brain\nrecordings, the domain of translating continuous brain activities into video\nformat remains underexplored. In this work, we introduce NeuroCine, a novel\ndual-phase framework to targeting the inherent challenges of decoding fMRI\ndata, such as noises, spatial redundancy and temporal lags. This framework\nproposes spatial masking and temporal interpolation-based augmentation for\ncontrastive learning fMRI representations and a diffusion model enhanced by\ndependent prior noise for video generation. Tested on a publicly available fMRI\ndataset, our method shows promising results, outperforming the previous\nstate-of-the-art models by a notable margin of ${20.97\\%}$, ${31.00\\%}$ and\n${12.30\\%}$ respectively on decoding the brain activities of three subjects in\nthe fMRI dataset, as measured by SSIM. Additionally, our attention analysis\nsuggests that the model aligns with existing brain structures and functions,\nindicating its biological plausibility and interpretability.",
        "translated": ""
    },
    {
        "title": "Boximator: Generating Rich and Controllable Motions for Video Synthesis",
        "url": "http://arxiv.org/abs/2402.01566v1",
        "pub_date": "2024-02-02",
        "summary": "Generating rich and controllable motion is a pivotal challenge in video\nsynthesis. We propose Boximator, a new approach for fine-grained motion\ncontrol. Boximator introduces two constraint types: hard box and soft box.\nUsers select objects in the conditional frame using hard boxes and then use\neither type of boxes to roughly or rigorously define the object's position,\nshape, or motion path in future frames. Boximator functions as a plug-in for\nexisting video diffusion models. Its training process preserves the base\nmodel's knowledge by freezing the original weights and training only the\ncontrol module. To address training challenges, we introduce a novel\nself-tracking technique that greatly simplifies the learning of box-object\ncorrelations. Empirically, Boximator achieves state-of-the-art video quality\n(FVD) scores, improving on two base models, and further enhanced after\nincorporating box constraints. Its robust motion controllability is validated\nby drastic increases in the bounding box alignment metric. Human evaluation\nalso shows that users favor Boximator generation results over the base model.",
        "translated": ""
    },
    {
        "title": "Deep Continuous Networks",
        "url": "http://arxiv.org/abs/2402.01557v1",
        "pub_date": "2024-02-02",
        "summary": "CNNs and computational models of biological vision share some fundamental\nprinciples, which opened new avenues of research. However, fruitful cross-field\nresearch is hampered by conventional CNN architectures being based on spatially\nand depthwise discrete representations, which cannot accommodate certain\naspects of biological complexity such as continuously varying receptive field\nsizes and dynamics of neuronal responses. Here we propose deep continuous\nnetworks (DCNs), which combine spatially continuous filters, with the\ncontinuous depth framework of neural ODEs. This allows us to learn the spatial\nsupport of the filters during training, as well as model the continuous\nevolution of feature maps, linking DCNs closely to biological models. We show\nthat DCNs are versatile and highly applicable to standard image classification\nand reconstruction problems, where they improve parameter and data efficiency,\nand allow for meta-parametrization. We illustrate the biological plausibility\nof the scale distributions learned by DCNs and explore their performance in a\nneuroscientifically inspired pattern completion task. Finally, we investigate\nan efficient implementation of DCNs by changing input contrast.",
        "translated": ""
    },
    {
        "title": "SLYKLatent, a Learning Framework for Facial Features Estimation",
        "url": "http://arxiv.org/abs/2402.01555v1",
        "pub_date": "2024-02-02",
        "summary": "In this research, we present SLYKLatent, a novel approach for enhancing gaze\nestimation by addressing appearance instability challenges in datasets due to\naleatoric uncertainties, covariant shifts, and test domain generalization.\nSLYKLatent utilizes Self-Supervised Learning for initial training with facial\nexpression datasets, followed by refinement with a patch-based tri-branch\nnetwork and an inverse explained variance-weighted training loss function. Our\nevaluation on benchmark datasets achieves an 8.7% improvement on Gaze360,\nrivals top MPIIFaceGaze results, and leads on a subset of ETH-XGaze by 13%,\nsurpassing existing methods by significant margins. Adaptability tests on\nRAF-DB and Affectnet show 86.4% and 60.9% accuracies, respectively. Ablation\nstudies confirm the effectiveness of SLYKLatent's novel components. This\napproach has strong potential in human-robot interaction.",
        "translated": ""
    },
    {
        "title": "Closing the Gap in Human Behavior Analysis: A Pipeline for Synthesizing\n  Trimodal Data",
        "url": "http://arxiv.org/abs/2402.01537v1",
        "pub_date": "2024-02-02",
        "summary": "In pervasive machine learning, especially in Human Behavior Analysis (HBA),\nRGB has been the primary modality due to its accessibility and richness of\ninformation. However, linked with its benefits are challenges, including\nsensitivity to lighting conditions and privacy concerns. One possibility to\novercome these vulnerabilities is to resort to different modalities. For\ninstance, thermal is particularly adept at accentuating human forms, while\ndepth adds crucial contextual layers. Despite their known benefits, only a few\nHBA-specific datasets that integrate these modalities exist. To address this\nshortage, our research introduces a novel generative technique for creating\ntrimodal, i.e., RGB, thermal, and depth, human-focused datasets. This technique\ncapitalizes on human segmentation masks derived from RGB images, combined with\nthermal and depth backgrounds that are sourced automatically. With these two\ningredients, we synthesize depth and thermal counterparts from existing RGB\ndata utilizing conditional image-to-image translation. By employing this\napproach, we generate trimodal data that can be leveraged to train models for\nsettings with limited data, bad lightning conditions, or privacy-sensitive\nareas.",
        "translated": ""
    },
    {
        "title": "HyperPlanes: Hypernetwork Approach to Rapid NeRF Adaptation",
        "url": "http://arxiv.org/abs/2402.01524v1",
        "pub_date": "2024-02-02",
        "summary": "Neural radiance fields (NeRFs) are a widely accepted standard for\nsynthesizing new 3D object views from a small number of base images. However,\nNeRFs have limited generalization properties, which means that we need to use\nsignificant computational resources to train individual architectures for each\nitem we want to represent. To address this issue, we propose a few-shot\nlearning approach based on the hypernetwork paradigm that does not require\ngradient optimization during inference. The hypernetwork gathers information\nfrom the training data and generates an update for universal weights. As a\nresult, we have developed an efficient method for generating a high-quality 3D\nobject representation from a small number of images in a single step. This has\nbeen confirmed by direct comparison with the state-of-the-art solutions and a\ncomprehensive ablation study.",
        "translated": ""
    },
    {
        "title": "Cross-view Masked Diffusion Transformers for Person Image Synthesis",
        "url": "http://arxiv.org/abs/2402.01516v1",
        "pub_date": "2024-02-02",
        "summary": "We present X-MDPT (Cross-view Masked Diffusion Prediction Transformers), a\nnovel diffusion model designed for pose-guided human image generation. X-MDPT\ndistinguishes itself by employing masked diffusion transformers that operate on\nlatent patches, a departure from the commonly-used Unet structures in existing\nworks. The model comprises three key modules: 1) a denoising diffusion\nTransformer, 2) an aggregation network that consolidates conditions into a\nsingle vector for the diffusion process, and 3) a mask cross-prediction module\nthat enhances representation learning with semantic information from the\nreference image. X-MDPT demonstrates scalability, improving FID, SSIM, and\nLPIPS with larger models. Despite its simple design, our model outperforms\nstate-of-the-art approaches on the DeepFashion dataset while exhibiting\nefficiency in terms of training parameters, training time, and inference speed.\nOur compact 33MB model achieves an FID of 7.42, surpassing a prior Unet latent\ndiffusion approach (FID 8.07) using only $11\\times$ fewer parameters. Our best\nmodel surpasses the pixel-based diffusion with $\\frac{2}{3}$ of the parameters\nand achieves $5.43 \\times$ faster inference.",
        "translated": ""
    },
    {
        "title": "Advancing Brain Tumor Inpainting with Generative Models",
        "url": "http://arxiv.org/abs/2402.01509v1",
        "pub_date": "2024-02-02",
        "summary": "Synthesizing healthy brain scans from diseased brain scans offers a potential\nsolution to address the limitations of general-purpose algorithms, such as\ntissue segmentation and brain extraction algorithms, which may not effectively\nhandle diseased images. We consider this a 3D inpainting task and investigate\nthe adaptation of 2D inpainting methods to meet the requirements of 3D magnetic\nresonance imaging(MRI) data. Our contributions encompass potential\nmodifications tailored to MRI-specific needs, and we conducted evaluations of\nmultiple inpainting techniques using the BraTS2023 Inpainting datasets to\nassess their efficacy and limitations.",
        "translated": ""
    },
    {
        "title": "Self-Attention through Kernel-Eigen Pair Sparse Variational Gaussian\n  Processes",
        "url": "http://arxiv.org/abs/2402.01476v1",
        "pub_date": "2024-02-02",
        "summary": "While the great capability of Transformers significantly boosts prediction\naccuracy, it could also yield overconfident predictions and require calibrated\nuncertainty estimation, which can be commonly tackled by Gaussian processes\n(GPs). Existing works apply GPs with symmetric kernels under variational\ninference to the attention kernel; however, omitting the fact that attention\nkernels are in essence asymmetric. Moreover, the complexity of deriving the GP\nposteriors remains high for large-scale data. In this work, we propose\nKernel-Eigen Pair Sparse Variational Gaussian Processes (KEP-SVGP) for building\nuncertainty-aware self-attention where the asymmetry of attention kernels is\ntackled by Kernel SVD (KSVD) and a reduced complexity is acquired. Through\nKEP-SVGP, i) the SVGP pair induced by the two sets of singular vectors from\nKSVD w.r.t. the attention kernel fully characterizes the asymmetry; ii) using\nonly a small set of adjoint eigenfunctions from KSVD, the derivation of SVGP\nposteriors can be based on the inversion of a diagonal matrix containing\nsingular values, contributing to a reduction in time complexity; iii) an\nevidence lower bound is derived so that variational parameters can be optimized\ntowards this objective. Experiments verify our excellent performances and\nefficiency on in-distribution, distribution-shift and out-of-distribution\nbenchmarks.",
        "translated": ""
    },
    {
        "title": "Test-Time Adaptation for Depth Completion",
        "url": "http://arxiv.org/abs/2402.03312v1",
        "pub_date": "2024-02-05",
        "summary": "It is common to observe performance degradation when transferring models\ntrained on some (source) datasets to target testing data due to a domain gap\nbetween them. Existing methods for bridging this gap, such as domain adaptation\n(DA), may require the source data on which the model was trained (often not\navailable), while others, i.e., source-free DA, require many passes through the\ntesting data. We propose an online test-time adaptation method for depth\ncompletion, the task of inferring a dense depth map from a single image and\nassociated sparse depth map, that closes the performance gap in a single pass.\nWe first present a study on how the domain shift in each data modality affects\nmodel performance. Based on our observations that the sparse depth modality\nexhibits a much smaller covariate shift than the image, we design an embedding\nmodule trained in the source domain that preserves a mapping from features\nencoding only sparse depth to those encoding image and sparse depth. During\ntest time, sparse depth features are projected using this map as a proxy for\nsource domain features and are used as guidance to train a set of auxiliary\nparameters (i.e., adaptation layer) to align image and sparse depth features\nfrom the target test domain to that of the source domain. We evaluate our\nmethod on indoor and outdoor scenarios and show that it improves over baselines\nby an average of 21.1%.",
        "translated": ""
    },
    {
        "title": "HASSOD: Hierarchical Adaptive Self-Supervised Object Detection",
        "url": "http://arxiv.org/abs/2402.03311v1",
        "pub_date": "2024-02-05",
        "summary": "The human visual perception system demonstrates exceptional capabilities in\nlearning without explicit supervision and understanding the part-to-whole\ncomposition of objects. Drawing inspiration from these two abilities, we\npropose Hierarchical Adaptive Self-Supervised Object Detection (HASSOD), a\nnovel approach that learns to detect objects and understand their compositions\nwithout human supervision. HASSOD employs a hierarchical adaptive clustering\nstrategy to group regions into object masks based on self-supervised visual\nrepresentations, adaptively determining the number of objects per image.\nFurthermore, HASSOD identifies the hierarchical levels of objects in terms of\ncomposition, by analyzing coverage relations between masks and constructing\ntree structures. This additional self-supervised learning task leads to\nimproved detection performance and enhanced interpretability. Lastly, we\nabandon the inefficient multi-round self-training process utilized in prior\nmethods and instead adapt the Mean Teacher framework from semi-supervised\nlearning, which leads to a smoother and more efficient training process.\nThrough extensive experiments on prevalent image datasets, we demonstrate the\nsuperiority of HASSOD over existing methods, thereby advancing the state of the\nart in self-supervised object detection. Notably, we improve Mask AR from 20.2\nto 22.5 on LVIS, and from 17.0 to 26.0 on SA-1B. Project page:\nhttps://HASSOD-NeurIPS23.github.io.",
        "translated": ""
    },
    {
        "title": "V-IRL: Grounding Virtual Intelligence in Real Life",
        "url": "http://arxiv.org/abs/2402.03310v1",
        "pub_date": "2024-02-05",
        "summary": "There is a sensory gulf between the Earth that humans inhabit and the digital\nrealms in which modern AI agents are created. To develop AI agents that can\nsense, think, and act as flexibly as humans in real-world settings, it is\nimperative to bridge the realism gap between the digital and physical worlds.\nHow can we embody agents in an environment as rich and diverse as the one we\ninhabit, without the constraints imposed by real hardware and control? Towards\nthis end, we introduce V-IRL: a platform that enables agents to scalably\ninteract with the real world in a virtual yet realistic environment. Our\nplatform serves as a playground for developing agents that can accomplish\nvarious practical tasks and as a vast testbed for measuring progress in\ncapabilities spanning perception, decision-making, and interaction with\nreal-world data across the entire globe.",
        "translated": ""
    },
    {
        "title": "AONeuS: A Neural Rendering Framework for Acoustic-Optical Sensor Fusion",
        "url": "http://arxiv.org/abs/2402.03309v1",
        "pub_date": "2024-02-05",
        "summary": "Underwater perception and 3D surface reconstruction are challenging problems\nwith broad applications in construction, security, marine archaeology, and\nenvironmental monitoring. Treacherous operating conditions, fragile\nsurroundings, and limited navigation control often dictate that submersibles\nrestrict their range of motion and, thus, the baseline over which they can\ncapture measurements. In the context of 3D scene reconstruction, it is\nwell-known that smaller baselines make reconstruction more challenging. Our\nwork develops a physics-based multimodal acoustic-optical neural surface\nreconstruction framework (AONeuS) capable of effectively integrating\nhigh-resolution RGB measurements with low-resolution depth-resolved imaging\nsonar measurements. By fusing these complementary modalities, our framework can\nreconstruct accurate high-resolution 3D surfaces from measurements captured\nover heavily-restricted baselines. Through extensive simulations and in-lab\nexperiments, we demonstrate that AONeuS dramatically outperforms recent\nRGB-only and sonar-only inverse-differentiable-rendering--based surface\nreconstruction methods. A website visualizing the results of our paper is\nlocated at this address: https://aoneus.github.io/",
        "translated": ""
    },
    {
        "title": "4D Gaussian Splatting: Towards Efficient Novel View Synthesis for\n  Dynamic Scenes",
        "url": "http://arxiv.org/abs/2402.03307v1",
        "pub_date": "2024-02-05",
        "summary": "We consider the problem of novel view synthesis (NVS) for dynamic scenes.\nRecent neural approaches have accomplished exceptional NVS results for static\n3D scenes, but extensions to 4D time-varying scenes remain non-trivial. Prior\nefforts often encode dynamics by learning a canonical space plus implicit or\nexplicit deformation fields, which struggle in challenging scenarios like\nsudden movements or capturing high-fidelity renderings. In this paper, we\nintroduce 4D Gaussian Splatting (4DGS), a novel method that represents dynamic\nscenes with anisotropic 4D XYZT Gaussians, inspired by the success of 3D\nGaussian Splatting in static scenes. We model dynamics at each timestamp by\ntemporally slicing the 4D Gaussians, which naturally compose dynamic 3D\nGaussians and can be seamlessly projected into images. As an explicit\nspatial-temporal representation, 4DGS demonstrates powerful capabilities for\nmodeling complicated dynamics and fine details, especially for scenes with\nabrupt motions. We further implement our temporal slicing and splatting\ntechniques in a highly optimized CUDA acceleration framework, achieving\nreal-time inference rendering speeds of up to 277 FPS on an RTX 3090 GPU and\n583 FPS on an RTX 4090 GPU. Rigorous evaluations on scenes with diverse motions\nshowcase the superior efficiency and effectiveness of 4DGS, which consistently\noutperforms existing methods both quantitatively and qualitatively.",
        "translated": ""
    },
    {
        "title": "Do Diffusion Models Learn Semantically Meaningful and Efficient\n  Representations?",
        "url": "http://arxiv.org/abs/2402.03305v1",
        "pub_date": "2024-02-05",
        "summary": "Diffusion models are capable of impressive feats of image generation with\nuncommon juxtapositions such as astronauts riding horses on the moon with\nproperly placed shadows. These outputs indicate the ability to perform\ncompositional generalization, but how do the models do so? We perform\ncontrolled experiments on conditional DDPMs learning to generate 2D spherical\nGaussian bumps centered at specified $x$- and $y$-positions. Our results show\nthat the emergence of semantically meaningful latent representations is key to\nachieving high performance. En route to successful performance over learning,\nthe model traverses three distinct phases of latent representations: (phase A)\nno latent structure, (phase B) a 2D manifold of disordered states, and (phase\nC) a 2D ordered manifold. Corresponding to each of these phases, we identify\nqualitatively different generation behaviors: 1) multiple bumps are generated,\n2) one bump is generated but at inaccurate $x$ and $y$ locations, 3) a bump is\ngenerated at the correct $x$ and y location. Furthermore, we show that even\nunder imbalanced datasets where features ($x$- versus $y$-positions) are\nrepresented with skewed frequencies, the learning process for $x$ and $y$ is\ncoupled rather than factorized, demonstrating that simple vanilla-flavored\ndiffusion models cannot learn efficient representations in which localization\nin $x$ and $y$ are factorized into separate 1D tasks. These findings suggest\nthe need for future work to find inductive biases that will push generative\nmodels to discover and exploit factorizable independent structures in their\ninputs, which will be required to vault these models into more data-efficient\nregimes.",
        "translated": ""
    },
    {
        "title": "Swin-UMamba: Mamba-based UNet with ImageNet-based pretraining",
        "url": "http://arxiv.org/abs/2402.03302v1",
        "pub_date": "2024-02-05",
        "summary": "Accurate medical image segmentation demands the integration of multi-scale\ninformation, spanning from local features to global dependencies. However, it\nis challenging for existing methods to model long-range global information,\nwhere convolutional neural networks (CNNs) are constrained by their local\nreceptive fields, and vision transformers (ViTs) suffer from high quadratic\ncomplexity of their attention mechanism. Recently, Mamba-based models have\ngained great attention for their impressive ability in long sequence modeling.\nSeveral studies have demonstrated that these models can outperform popular\nvision models in various tasks, offering higher accuracy, lower memory\nconsumption, and less computational burden. However, existing Mamba-based\nmodels are mostly trained from scratch and do not explore the power of\npretraining, which has been proven to be quite effective for data-efficient\nmedical image analysis. This paper introduces a novel Mamba-based model,\nSwin-UMamba, designed specifically for medical image segmentation tasks,\nleveraging the advantages of ImageNet-based pretraining. Our experimental\nresults reveal the vital role of ImageNet-based training in enhancing the\nperformance of Mamba-based models. Swin-UMamba demonstrates superior\nperformance with a large margin compared to CNNs, ViTs, and latest Mamba-based\nmodels. Notably, on AbdomenMRI, Encoscopy, and Microscopy datasets, Swin-UMamba\noutperforms its closest counterpart U-Mamba by an average score of 3.58%. The\ncode and models of Swin-UMamba are publicly available at:\nhttps://github.com/JiarunLiu/Swin-UMamba",
        "translated": ""
    },
    {
        "title": "GUARD: Role-playing to Generate Natural-language Jailbreakings to Test\n  Guideline Adherence of Large Language Models",
        "url": "http://arxiv.org/abs/2402.03299v1",
        "pub_date": "2024-02-05",
        "summary": "The discovery of \"jailbreaks\" to bypass safety filters of Large Language\nModels (LLMs) and harmful responses have encouraged the community to implement\nsafety measures. One major safety measure is to proactively test the LLMs with\njailbreaks prior to the release. Therefore, such testing will require a method\nthat can generate jailbreaks massively and efficiently. In this paper, we\nfollow a novel yet intuitive strategy to generate jailbreaks in the style of\nthe human generation. We propose a role-playing system that assigns four\ndifferent roles to the user LLMs to collaborate on new jailbreaks. Furthermore,\nwe collect existing jailbreaks and split them into different independent\ncharacteristics using clustering frequency and semantic patterns sentence by\nsentence. We organize these characteristics into a knowledge graph, making them\nmore accessible and easier to retrieve. Our system of different roles will\nleverage this knowledge graph to generate new jailbreaks, which have proved\neffective in inducing LLMs to generate unethical or guideline-violating\nresponses. In addition, we also pioneer a setting in our system that will\nautomatically follow the government-issued guidelines to generate jailbreaks to\ntest whether LLMs follow the guidelines accordingly. We refer to our system as\nGUARD (Guideline Upholding through Adaptive Role-play Diagnostics). We have\nempirically validated the effectiveness of GUARD on three cutting-edge\nopen-sourced LLMs (Vicuna-13B, LongChat-7B, and Llama-2-7B), as well as a\nwidely-utilized commercial LLM (ChatGPT). Moreover, our work extends to the\nrealm of vision language models (MiniGPT-v2 and Gemini Vision Pro), showcasing\nGUARD's versatility and contributing valuable insights for the development of\nsafer, more reliable LLM-based applications across diverse modalities.",
        "translated": ""
    },
    {
        "title": "Zero-shot Object-Level OOD Detection with Context-Aware Inpainting",
        "url": "http://arxiv.org/abs/2402.03292v1",
        "pub_date": "2024-02-05",
        "summary": "Machine learning algorithms are increasingly provided as black-box cloud\nservices or pre-trained models, without access to their training data. This\nmotivates the problem of zero-shot out-of-distribution (OOD) detection.\nConcretely, we aim to detect OOD objects that do not belong to the classifier's\nlabel set but are erroneously classified as in-distribution (ID) objects. Our\napproach, RONIN, uses an off-the-shelf diffusion model to replace detected\nobjects with inpainting. RONIN conditions the inpainting process with the\npredicted ID label, drawing the input object closer to the in-distribution\ndomain. As a result, the reconstructed object is very close to the original in\nthe ID cases and far in the OOD cases, allowing RONIN to effectively\ndistinguish ID and OOD samples. Throughout extensive experiments, we\ndemonstrate that RONIN achieves competitive results compared to previous\napproaches across several datasets, both in zero-shot and non-zero-shot\nsettings.",
        "translated": ""
    },
    {
        "title": "InstanceDiffusion: Instance-level Control for Image Generation",
        "url": "http://arxiv.org/abs/2402.03290v1",
        "pub_date": "2024-02-05",
        "summary": "Text-to-image diffusion models produce high quality images but do not offer\ncontrol over individual instances in the image. We introduce InstanceDiffusion\nthat adds precise instance-level control to text-to-image diffusion models.\nInstanceDiffusion supports free-form language conditions per instance and\nallows flexible ways to specify instance locations such as simple single\npoints, scribbles, bounding boxes or intricate instance segmentation masks, and\ncombinations thereof. We propose three major changes to text-to-image models\nthat enable precise instance-level control. Our UniFusion block enables\ninstance-level conditions for text-to-image models, the ScaleU block improves\nimage fidelity, and our Multi-instance Sampler improves generations for\nmultiple instances. InstanceDiffusion significantly surpasses specialized\nstate-of-the-art models for each location condition. Notably, on the COCO\ndataset, we outperform previous state-of-the-art by 20.4% AP$_{50}^\\text{box}$\nfor box inputs, and 25.4% IoU for mask inputs.",
        "translated": ""
    },
    {
        "title": "EVA-CLIP-18B: Scaling CLIP to 18 Billion Parameters",
        "url": "http://arxiv.org/abs/2402.04252v1",
        "pub_date": "2024-02-06",
        "summary": "Scaling up contrastive language-image pretraining (CLIP) is critical for\nempowering both vision and multimodal models. We present EVA-CLIP-18B, the\nlargest and most powerful open-source CLIP model to date, with 18-billion\nparameters. With only 6-billion training samples seen, EVA-CLIP-18B achieves an\nexceptional 80.7% zero-shot top-1 accuracy averaged across 27 widely recognized\nimage classification benchmarks, outperforming its forerunner EVA-CLIP\n(5-billion parameters) and other open-source CLIP models by a large margin.\nRemarkably, we observe a consistent performance improvement with the model size\nscaling of EVA-CLIP, despite maintaining a constant training dataset of\n2-billion image-text pairs from LAION-2B and COYO-700M. This dataset is openly\navailable and much smaller than the in-house datasets (e.g., DFN-5B, WebLI-10B)\nemployed in other state-of-the-art CLIP models. EVA-CLIP-18B demonstrates the\npotential of EVA-style weak-to-strong visual model scaling. With our model\nweights made publicly available, we hope to facilitate future research in\nvision and multimodal foundation models.",
        "translated": ""
    },
    {
        "title": "HarmBench: A Standardized Evaluation Framework for Automated Red Teaming\n  and Robust Refusal",
        "url": "http://arxiv.org/abs/2402.04249v1",
        "pub_date": "2024-02-06",
        "summary": "Automated red teaming holds substantial promise for uncovering and mitigating\nthe risks associated with the malicious use of large language models (LLMs),\nyet the field lacks a standardized evaluation framework to rigorously assess\nnew methods. To address this issue, we introduce HarmBench, a standardized\nevaluation framework for automated red teaming. We identify several desirable\nproperties previously unaccounted for in red teaming evaluations and\nsystematically design HarmBench to meet these criteria. Using HarmBench, we\nconduct a large-scale comparison of 18 red teaming methods and 33 target LLMs\nand defenses, yielding novel insights. We also introduce a highly efficient\nadversarial training method that greatly enhances LLM robustness across a wide\nrange of attacks, demonstrating how HarmBench enables codevelopment of attacks\nand defenses. We open source HarmBench at\nhttps://github.com/centerforaisafety/HarmBench.",
        "translated": ""
    },
    {
        "title": "CogCoM: Train Large Vision-Language Models Diving into Details through\n  Chain of Manipulations",
        "url": "http://arxiv.org/abs/2402.04236v1",
        "pub_date": "2024-02-06",
        "summary": "Vision-Language Models (VLMs) have demonstrated their widespread viability\nthanks to extensive training in aligning visual instructions to answers.\nHowever, this conclusive alignment leads models to ignore critical visual\nreasoning, and further result in failures on meticulous visual problems and\nunfaithful responses. In this paper, we propose Chain of Manipulations, a\nmechanism that enables VLMs to solve problems with a series of manipulations,\nwhere each manipulation refers to an operation on the visual input, either from\nintrinsic abilities (e.g., grounding) acquired through prior training or from\nimitating human-like behaviors (e.g., zoom in). This mechanism encourages VLMs\nto generate faithful responses with evidential visual reasoning, and permits\nusers to trace error causes in the interpretable paths. We thus train CogCoM, a\ngeneral 17B VLM with a memory-based compatible architecture endowed this\nreasoning mechanism. Experiments show that our model achieves the\nstate-of-the-art performance across 8 benchmarks from 3 categories, and a\nlimited number of training steps with the data swiftly gains a competitive\nperformance. The code and data are publicly available at\nhttps://github.com/THUDM/CogCoM.",
        "translated": ""
    },
    {
        "title": "Instance by Instance: An Iterative Framework for Multi-instance 3D\n  Registration",
        "url": "http://arxiv.org/abs/2402.04195v1",
        "pub_date": "2024-02-06",
        "summary": "Multi-instance registration is a challenging problem in computer vision and\nrobotics, where multiple instances of an object need to be registered in a\nstandard coordinate system. In this work, we propose the first iterative\nframework called instance-by-instance (IBI) for multi-instance 3D registration\n(MI-3DReg). It successively registers all instances in a given scenario,\nstarting from the easiest and progressing to more challenging ones. Throughout\nthe iterative process, outliers are eliminated continuously, leading to an\nincreasing inlier rate for the remaining and more challenging instances. Under\nthe IBI framework, we further propose a sparse-to-dense-correspondence-based\nmulti-instance registration method (IBI-S2DC) to achieve robust MI-3DReg.\nExperiments on the synthetic and real datasets have demonstrated the\neffectiveness of IBI and suggested the new state-of-the-art performance of\nIBI-S2DC, e.g., our MHF1 is 12.02%/12.35% higher than the existing\nstate-of-the-art method ECC on the synthetic/real datasets.",
        "translated": ""
    },
    {
        "title": "SHIELD : An Evaluation Benchmark for Face Spoofing and Forgery Detection\n  with Multimodal Large Language Models",
        "url": "http://arxiv.org/abs/2402.04178v1",
        "pub_date": "2024-02-06",
        "summary": "Multimodal large language models (MLLMs) have demonstrated remarkable\nproblem-solving capabilities in various vision fields (e.g., generic object\nrecognition and grounding) based on strong visual semantic representation and\nlanguage reasoning ability. However, whether MLLMs are sensitive to subtle\nvisual spoof/forged clues and how they perform in the domain of face attack\ndetection (e.g., face spoofing and forgery detection) is still unexplored. In\nthis paper, we introduce a new benchmark, namely SHIELD, to evaluate the\nability of MLLMs on face spoofing and forgery detection. Specifically, we\ndesign true/false and multiple-choice questions to evaluate multimodal face\ndata in these two face security tasks. For the face anti-spoofing task, we\nevaluate three different modalities (i.e., RGB, infrared, depth) under four\ntypes of presentation attacks (i.e., print attack, replay attack, rigid mask,\npaper mask). For the face forgery detection task, we evaluate GAN-based and\ndiffusion-based data with both visual and acoustic modalities. Each question is\nsubjected to both zero-shot and few-shot tests under standard and chain of\nthought (COT) settings. The results indicate that MLLMs hold substantial\npotential in the face security domain, offering advantages over traditional\nspecific models in terms of interpretability, multimodal flexible reasoning,\nand joint face spoof and forgery detection. Additionally, we develop a novel\nMulti-Attribute Chain of Thought (MA-COT) paradigm for describing and judging\nvarious task-specific and task-irrelevant attributes of face images, which\nprovides rich task-related knowledge for subtle spoof/forged clue mining.\nExtensive experiments in separate face anti-spoofing, separate face forgery\ndetection, and joint detection tasks demonstrate the effectiveness of the\nproposed MA-COT. The project is available at\nhttps$:$//github.com/laiyingxin2/SHIELD",
        "translated": ""
    },
    {
        "title": "3D Volumetric Super-Resolution in Radiology Using 3D RRDB-GAN",
        "url": "http://arxiv.org/abs/2402.04171v1",
        "pub_date": "2024-02-06",
        "summary": "This study introduces the 3D Residual-in-Residual Dense Block GAN (3D\nRRDB-GAN) for 3D super-resolution for radiology imagery. A key aspect of 3D\nRRDB-GAN is the integration of a 2.5D perceptual loss function, which\ncontributes to improved volumetric image quality and realism. The effectiveness\nof our model was evaluated through 4x super-resolution experiments across\ndiverse datasets, including Mice Brain MRH, OASIS, HCP1200, and MSD-Task-6.\nThese evaluations, encompassing both quantitative metrics like LPIPS and FID\nand qualitative assessments through sample visualizations, demonstrate the\nmodels effectiveness in detailed image analysis. The 3D RRDB-GAN offers a\nsignificant contribution to medical imaging, particularly by enriching the\ndepth, clarity, and volumetric detail of medical images. Its application shows\npromise in enhancing the interpretation and analysis of complex medical imagery\nfrom a comprehensive 3D perspective.",
        "translated": ""
    },
    {
        "title": "Informed Reinforcement Learning for Situation-Aware Traffic Rule\n  Exceptions",
        "url": "http://arxiv.org/abs/2402.04168v1",
        "pub_date": "2024-02-06",
        "summary": "Reinforcement Learning is a highly active research field with promising\nadvancements. In the field of autonomous driving, however, often very simple\nscenarios are being examined. Common approaches use non-interpretable control\ncommands as the action space and unstructured reward designs which lack\nstructure. In this work, we introduce Informed Reinforcement Learning, where a\nstructured rulebook is integrated as a knowledge source. We learn trajectories\nand asses them with a situation-aware reward design, leading to a dynamic\nreward which allows the agent to learn situations which require controlled\ntraffic rule exceptions. Our method is applicable to arbitrary RL models. We\nsuccessfully demonstrate high completion rates of complex scenarios with recent\nmodel-based agents.",
        "translated": ""
    },
    {
        "title": "U-shaped Vision Mamba for Single Image Dehazing",
        "url": "http://arxiv.org/abs/2402.04139v1",
        "pub_date": "2024-02-06",
        "summary": "Currently, Transformer is the most popular architecture for image dehazing,\nbut due to its large computational complexity, its ability to handle long-range\ndependency is limited on resource-constrained devices. To tackle this\nchallenge, we introduce the U-shaped Vision Mamba (UVM-Net), an efficient\nsingle-image dehazing network. Inspired by the State Space Sequence Models\n(SSMs), a new deep sequence model known for its power to handle long sequences,\nwe design a Bi-SSM block that integrates the local feature extraction ability\nof the convolutional layer with the ability of the SSM to capture long-range\ndependencies. Extensive experimental results demonstrate the effectiveness of\nour method. Our method provides a more highly efficient idea of long-range\ndependency modeling for image dehazing as well as other image restoration\ntasks. The URL of the code is \\url{https://github.com/zzr-idam}.",
        "translated": ""
    },
    {
        "title": "OVOR: OnePrompt with Virtual Outlier Regularization for Rehearsal-Free\n  Class-Incremental Learning",
        "url": "http://arxiv.org/abs/2402.04129v1",
        "pub_date": "2024-02-06",
        "summary": "Recent works have shown that by using large pre-trained models along with\nlearnable prompts, rehearsal-free methods for class-incremental learning (CIL)\nsettings can achieve superior performance to prominent rehearsal-based ones.\nRehearsal-free CIL methods struggle with distinguishing classes from different\ntasks, as those are not trained together. In this work we propose a\nregularization method based on virtual outliers to tighten decision boundaries\nof the classifier, such that confusion of classes among different tasks is\nmitigated. Recent prompt-based methods often require a pool of task-specific\nprompts, in order to prevent overwriting knowledge of previous tasks with that\nof the new task, leading to extra computation in querying and composing an\nappropriate prompt from the pool. This additional cost can be eliminated,\nwithout sacrificing accuracy, as we reveal in the paper. We illustrate that a\nsimplified prompt-based method can achieve results comparable to previous\nstate-of-the-art (SOTA) methods equipped with a prompt pool, using much less\nlearnable parameters and lower inference cost. Our regularization method has\ndemonstrated its compatibility with different prompt-based methods, boosting\nthose previous SOTA rehearsal-free CIL methods' accuracy on the ImageNet-R and\nCIFAR-100 benchmarks. Our source code is available at\nhttps://github.com/jpmorganchase/ovor.",
        "translated": ""
    },
    {
        "title": "VRMM: A Volumetric Relightable Morphable Head Model",
        "url": "http://arxiv.org/abs/2402.04101v1",
        "pub_date": "2024-02-06",
        "summary": "In this paper, we introduce the Volumetric Relightable Morphable Model\n(VRMM), a novel volumetric and parametric facial prior for 3D face modeling.\nWhile recent volumetric prior models offer improvements over traditional\nmethods like 3D Morphable Models (3DMMs), they face challenges in model\nlearning and personalized reconstructions. Our VRMM overcomes these by\nemploying a novel training framework that efficiently disentangles and encodes\nlatent spaces of identity, expression, and lighting into low-dimensional\nrepresentations. This framework, designed with self-supervised learning,\nsignificantly reduces the constraints for training data, making it more\nfeasible in practice. The learned VRMM offers relighting capabilities and\nencompasses a comprehensive range of expressions. We demonstrate the\nversatility and effectiveness of VRMM through various applications like avatar\ngeneration, facial reconstruction, and animation. Additionally, we address the\ncommon issue of overfitting in generative volumetric models with a novel\nprior-preserving personalization framework based on VRMM. Such an approach\nenables accurate 3D face reconstruction from even a single portrait input. Our\nexperiments showcase the potential of VRMM to significantly enhance the field\nof 3D face modeling.",
        "translated": ""
    },
    {
        "title": "Image captioning for Brazilian Portuguese using GRIT model",
        "url": "http://arxiv.org/abs/2402.05106v1",
        "pub_date": "2024-02-07",
        "summary": "This work presents the early development of a model of image captioning for\nthe Brazilian Portuguese language. We used the GRIT (Grid - and Region-based\nImage captioning Transformer) model to accomplish this work. GRIT is a\nTransformer-only neural architecture that effectively utilizes two visual\nfeatures to generate better captions. The GRIT method emerged as a proposal to\nbe a more efficient way to generate image captioning. In this work, we adapt\nthe GRIT model to be trained in a Brazilian Portuguese dataset to have an image\ncaptioning method for the Brazilian Portuguese Language.",
        "translated": ""
    },
    {
        "title": "Language-Based Augmentation to Address Shortcut Learning in Object Goal\n  Navigation",
        "url": "http://arxiv.org/abs/2402.05090v1",
        "pub_date": "2024-02-07",
        "summary": "Deep Reinforcement Learning (DRL) has shown great potential in enabling\nrobots to find certain objects (e.g., `find a fridge') in environments like\nhomes or schools. This task is known as Object-Goal Navigation (ObjectNav). DRL\nmethods are predominantly trained and evaluated using environment simulators.\nAlthough DRL has shown impressive results, the simulators may be biased or\nlimited. This creates a risk of shortcut learning, i.e., learning a policy\ntailored to specific visual details of training environments. We aim to deepen\nour understanding of shortcut learning in ObjectNav, its implications and\npropose a solution. We design an experiment for inserting a shortcut bias in\nthe appearance of training environments. As a proof-of-concept, we associate\nroom types to specific wall colors (e.g., bedrooms with green walls), and\nobserve poor generalization of a state-of-the-art (SOTA) ObjectNav method to\nenvironments where this is not the case (e.g., bedrooms with blue walls). We\nfind that shortcut learning is the root cause: the agent learns to navigate to\ntarget objects, by simply searching for the associated wall color of the target\nobject's room. To solve this, we propose Language-Based (L-B) augmentation. Our\nkey insight is that we can leverage the multimodal feature space of a\nVision-Language Model (VLM) to augment visual representations directly at the\nfeature-level, requiring no changes to the simulator, and only an addition of\none layer to the model. Where the SOTA ObjectNav method's success rate drops\n69%, our proposal has only a drop of 23%.",
        "translated": ""
    },
    {
        "title": "Mamba-UNet: UNet-Like Pure Visual Mamba for Medical Image Segmentation",
        "url": "http://arxiv.org/abs/2402.05079v1",
        "pub_date": "2024-02-07",
        "summary": "In recent advancements in medical image analysis, Convolutional Neural\nNetworks (CNN) and Vision Transformers (ViT) have set significant benchmarks.\nWhile the former excels in capturing local features through its convolution\noperations, the latter achieves remarkable global context understanding by\nleveraging self-attention mechanisms. However, both architectures exhibit\nlimitations in efficiently modeling long-range dependencies within medical\nimages, which is a critical aspect for precise segmentation. Inspired by the\nMamba architecture, known for its proficiency in handling long sequences and\nglobal contextual information with enhanced computational efficiency as a State\nSpace Model (SSM), we propose Mamba-UNet, a novel architecture that synergizes\nthe U-Net in medical image segmentation with Mamba's capability. Mamba-UNet\nadopts a pure Visual Mamba (VMamba)-based encoder-decoder structure, infused\nwith skip connections to preserve spatial information across different scales\nof the network. This design facilitates a comprehensive feature learning\nprocess, capturing intricate details and broader semantic contexts within\nmedical images. We introduce a novel integration mechanism within the VMamba\nblocks to ensure seamless connectivity and information flow between the encoder\nand decoder paths, enhancing the segmentation performance. We conducted\nexperiments on publicly available MRI cardiac multi-structures segmentation\ndataset. The results show that Mamba-UNet outperforms UNet, Swin-UNet in\nmedical image segmentation under the same hyper-parameter setting. The source\ncode and baseline implementations are available.",
        "translated": ""
    },
    {
        "title": "LGM: Large Multi-View Gaussian Model for High-Resolution 3D Content\n  Creation",
        "url": "http://arxiv.org/abs/2402.05054v1",
        "pub_date": "2024-02-07",
        "summary": "3D content creation has achieved significant progress in terms of both\nquality and speed. Although current feed-forward models can produce 3D objects\nin seconds, their resolution is constrained by the intensive computation\nrequired during training. In this paper, we introduce Large Multi-View Gaussian\nModel (LGM), a novel framework designed to generate high-resolution 3D models\nfrom text prompts or single-view images. Our key insights are two-fold: 1) 3D\nRepresentation: We propose multi-view Gaussian features as an efficient yet\npowerful representation, which can then be fused together for differentiable\nrendering. 2) 3D Backbone: We present an asymmetric U-Net as a high-throughput\nbackbone operating on multi-view images, which can be produced from text or\nsingle-view image input by leveraging multi-view diffusion models. Extensive\nexperiments demonstrate the high fidelity and efficiency of our approach.\nNotably, we maintain the fast speed to generate 3D objects within 5 seconds\nwhile boosting the training resolution to 512, thereby achieving\nhigh-resolution 3D content generation.",
        "translated": ""
    },
    {
        "title": "Efficient Multi-Resolution Fusion for Remote Sensing Data with Label\n  Uncertainty",
        "url": "http://arxiv.org/abs/2402.05045v1",
        "pub_date": "2024-02-07",
        "summary": "Multi-modal sensor data fusion takes advantage of complementary or\nreinforcing information from each sensor and can boost overall performance in\napplications such as scene classification and target detection. This paper\npresents a new method for fusing multi-modal and multi-resolution remote sensor\ndata without requiring pixel-level training labels, which can be difficult to\nobtain. Previously, we developed a Multiple Instance Multi-Resolution Fusion\n(MIMRF) framework that addresses label uncertainty for fusion, but it can be\nslow to train due to the large search space for the fuzzy measures used to\nintegrate sensor data sources. We propose a new method based on binary fuzzy\nmeasures, which reduces the search space and significantly improves the\nefficiency of the MIMRF framework. We present experimental results on synthetic\ndata and a real-world remote sensing detection task and show that the proposed\nMIMRF-BFM algorithm can effectively and efficiently perform multi-resolution\nfusion given remote sensing data with uncertainty.",
        "translated": ""
    },
    {
        "title": "A Survey on Domain Generalization for Medical Image Analysis",
        "url": "http://arxiv.org/abs/2402.05035v1",
        "pub_date": "2024-02-07",
        "summary": "Medical Image Analysis (MedIA) has emerged as a crucial tool in\ncomputer-aided diagnosis systems, particularly with the advancement of deep\nlearning (DL) in recent years. However, well-trained deep models often\nexperience significant performance degradation when deployed in different\nmedical sites, modalities, and sequences, known as a domain shift issue. In\nlight of this, Domain Generalization (DG) for MedIA aims to address the domain\nshift challenge by generalizing effectively and performing robustly across\nunknown data distributions. This paper presents the a comprehensive review of\nsubstantial developments in this area. First, we provide a formal definition of\ndomain shift and domain generalization in medical field, and discuss several\nrelated settings. Subsequently, we summarize the recent methods from three\nviewpoints: data manipulation level, feature representation level, and model\ntraining level, and present some algorithms in detail for each viewpoints.\nFurthermore, we introduce the commonly used datasets. Finally, we summarize\nexisting literature and present some potential research topics for the future.\nFor this survey, we also created a GitHub project by collecting the supporting\nresources, at the link: https://github.com/Ziwei-Niu/DG_for_MedIA",
        "translated": ""
    },
    {
        "title": "EfficientViT-SAM: Accelerated Segment Anything Model Without Performance\n  Loss",
        "url": "http://arxiv.org/abs/2402.05008v1",
        "pub_date": "2024-02-07",
        "summary": "We present EfficientViT-SAM, a new family of accelerated segment anything\nmodels. We retain SAM's lightweight prompt encoder and mask decoder while\nreplacing the heavy image encoder with EfficientViT. For the training, we begin\nwith the knowledge distillation from the SAM-ViT-H image encoder to\nEfficientViT. Subsequently, we conduct end-to-end training on the SA-1B\ndataset. Benefiting from EfficientViT's efficiency and capacity,\nEfficientViT-SAM delivers 48.9x measured TensorRT speedup on A100 GPU over\nSAM-ViT-H without sacrificing performance. Our code and pre-trained models are\nreleased at https://github.com/mit-han-lab/efficientvit.",
        "translated": ""
    },
    {
        "title": "Detection and Pose Estimation of flat, Texture-less Industry Objects on\n  HoloLens using synthetic Training",
        "url": "http://arxiv.org/abs/2402.04979v1",
        "pub_date": "2024-02-07",
        "summary": "Current state-of-the-art 6d pose estimation is too compute intensive to be\ndeployed on edge devices, such as Microsoft HoloLens (2) or Apple iPad, both\nused for an increasing number of augmented reality applications. The quality of\nAR is greatly dependent on its capabilities to detect and overlay geometry\nwithin the scene. We propose a synthetically trained client-server-based\naugmented reality application, demonstrating state-of-the-art object pose\nestimation of metallic and texture-less industry objects on edge devices.\nSynthetic data enables training without real photographs, i.e. for\nyet-to-be-manufactured objects. Our qualitative evaluation on an AR-assisted\nsorting task, and quantitative evaluation on both renderings, as well as\nreal-world data recorded on HoloLens 2, sheds light on its real-world\napplicability.",
        "translated": ""
    },
    {
        "title": "Text or Image? What is More Important in Cross-Domain Generalization\n  Capabilities of Hate Meme Detection Models?",
        "url": "http://arxiv.org/abs/2402.04967v1",
        "pub_date": "2024-02-07",
        "summary": "This paper delves into the formidable challenge of cross-domain\ngeneralization in multimodal hate meme detection, presenting compelling\nfindings. We provide enough pieces of evidence supporting the hypothesis that\nonly the textual component of hateful memes enables the existing multimodal\nclassifier to generalize across different domains, while the image component\nproves highly sensitive to a specific training dataset. The evidence includes\ndemonstrations showing that hate-text classifiers perform similarly to\nhate-meme classifiers in a zero-shot setting. Simultaneously, the introduction\nof captions generated from images of memes to the hate-meme classifier worsens\nperformance by an average F1 of 0.02. Through blackbox explanations, we\nidentify a substantial contribution of the text modality (average of 83%),\nwhich diminishes with the introduction of meme's image captions (52%).\nAdditionally, our evaluation on a newly created confounder dataset reveals\nhigher performance on text confounders as compared to image confounders with an\naverage $\\Delta$F1 of 0.18.",
        "translated": ""
    },
    {
        "title": "ConvLoRA and AdaBN based Domain Adaptation via Self-Training",
        "url": "http://arxiv.org/abs/2402.04964v1",
        "pub_date": "2024-02-07",
        "summary": "Existing domain adaptation (DA) methods often involve pre-training on the\nsource domain and fine-tuning on the target domain. For multi-target domain\nadaptation, having a dedicated/separate fine-tuned network for each target\ndomain, that retain all the pre-trained model parameters, is prohibitively\nexpensive. To address this limitation, we propose Convolutional Low-Rank\nAdaptation (ConvLoRA). ConvLoRA freezes pre-trained model weights, adds\ntrainable low-rank decomposition matrices to convolutional layers, and\nbackpropagates the gradient through these matrices thus greatly reducing the\nnumber of trainable parameters. To further boost adaptation, we utilize\nAdaptive Batch Normalization (AdaBN) which computes target-specific running\nstatistics and use it along with ConvLoRA. Our method has fewer trainable\nparameters and performs better or on-par with large independent fine-tuned\nnetworks (with less than 0.9% trainable parameters of the total base model)\nwhen tested on the segmentation of Calgary-Campinas dataset containing brain\nMRI images. Our approach is simple, yet effective and can be applied to any\ndeep learning-based architecture which uses convolutional and batch\nnormalization layers. Code is available at:\nhttps://github.com/aleemsidra/ConvLoRA.",
        "translated": ""
    },
    {
        "title": "InstaGen: Enhancing Object Detection by Training on Synthetic Dataset",
        "url": "http://arxiv.org/abs/2402.05937v1",
        "pub_date": "2024-02-08",
        "summary": "In this paper, we introduce a novel paradigm to enhance the ability of object\ndetector, e.g., expanding categories or improving detection performance, by\ntraining on synthetic dataset generated from diffusion models. Specifically, we\nintegrate an instance-level grounding head into a pre-trained, generative\ndiffusion model, to augment it with the ability of localising arbitrary\ninstances in the generated images. The grounding head is trained to align the\ntext embedding of category names with the regional visual feature of the\ndiffusion model, using supervision from an off-the-shelf object detector, and a\nnovel self-training scheme on (novel) categories not covered by the detector.\nThis enhanced version of diffusion model, termed as InstaGen, can serve as a\ndata synthesizer for object detection. We conduct thorough experiments to show\nthat, object detector can be enhanced while training on the synthetic dataset\nfrom InstaGen, demonstrating superior performance over existing\nstate-of-the-art methods in open-vocabulary (+4.5 AP) and data-sparse (+1.2 to\n5.2 AP) scenarios.",
        "translated": ""
    },
    {
        "title": "SPHINX-X: Scaling Data and Parameters for a Family of Multi-modal Large\n  Language Models",
        "url": "http://arxiv.org/abs/2402.05935v1",
        "pub_date": "2024-02-08",
        "summary": "We propose SPHINX-X, an extensive Multimodality Large Language Model (MLLM)\nseries developed upon SPHINX. To improve the architecture and training\nefficiency, we modify the SPHINX framework by removing redundant visual\nencoders, bypassing fully-padded sub-images with skip tokens, and simplifying\nmulti-stage training into a one-stage all-in-one paradigm. To fully unleash the\npotential of MLLMs, we assemble a comprehensive multi-domain and multimodal\ndataset covering publicly available resources in language, vision, and\nvision-language tasks. We further enrich this collection with our curated OCR\nintensive and Set-of-Mark datasets, extending the diversity and generality. By\ntraining over different base LLMs including TinyLlama1.1B, InternLM2-7B,\nLLaMA2-13B, and Mixtral8x7B, we obtain a spectrum of MLLMs that vary in\nparameter size and multilingual capabilities. Comprehensive benchmarking\nreveals a strong correlation between the multi-modal performance with the data\nand parameter scales. Code and models are released at\nhttps://github.com/Alpha-VLLM/LLaMA2-Accessory",
        "translated": ""
    },
    {
        "title": "WebLINX: Real-World Website Navigation with Multi-Turn Dialogue",
        "url": "http://arxiv.org/abs/2402.05930v1",
        "pub_date": "2024-02-08",
        "summary": "We propose the problem of conversational web navigation, where a digital\nagent controls a web browser and follows user instructions to solve real-world\ntasks in a multi-turn dialogue fashion. To support this problem, we introduce\nWEBLINX - a large-scale benchmark of 100K interactions across 2300 expert\ndemonstrations of conversational web navigation. Our benchmark covers a broad\nrange of patterns on over 150 real-world websites and can be used to train and\nevaluate agents in diverse scenarios. Due to the magnitude of information\npresent, Large Language Models (LLMs) cannot process entire web pages in\nreal-time. To solve this bottleneck, we design a retrieval-inspired model that\nefficiently prunes HTML pages by ranking relevant elements. We use the selected\nelements, along with screenshots and action history, to assess a variety of\nmodels for their ability to replicate human behavior when navigating the web.\nOur experiments span from small text-only to proprietary multimodal LLMs. We\nfind that smaller finetuned decoders surpass the best zero-shot LLMs (including\nGPT-4V), but also larger finetuned multimodal models which were explicitly\npretrained on screenshots. However, all finetuned models struggle to generalize\nto unseen websites. Our findings highlight the need for large multimodal models\nthat can generalize to novel settings. Our code, data and models are available\nfor research: https://mcgill-nlp.github.io/weblinx",
        "translated": ""
    },
    {
        "title": "Collaborative Control for Geometry-Conditioned PBR Image Generation",
        "url": "http://arxiv.org/abs/2402.05919v1",
        "pub_date": "2024-02-08",
        "summary": "Current 3D content generation builds on generative models that output RGB\nimages. Modern graphics pipelines, however, require physically-based rendering\n(PBR) material properties. We propose to model the PBR image distribution\ndirectly to avoid photometric inaccuracies in RGB generation and the inherent\nambiguity in extracting PBR from RGB. Existing paradigms for cross-modal\nfinetuning are not suited for PBR generation due to a lack of data and the high\ndimensionality of the output modalities: we overcome both challenges by\nretaining a frozen RGB model and tightly linking a newly trained PBR model\nusing a novel cross-network communication paradigm. As the base RGB model is\nfully frozen, the proposed method does not risk catastrophic forgetting during\nfinetuning and remains compatible with techniques such as IPAdapter pretrained\nfor the base RGB model. We validate our design choices, robustness to data\nsparsity, and compare against existing paradigms with an extensive experimental\nsection.",
        "translated": ""
    },
    {
        "title": "Point-VOS: Pointing Up Video Object Segmentation",
        "url": "http://arxiv.org/abs/2402.05917v1",
        "pub_date": "2024-02-08",
        "summary": "Current state-of-the-art Video Object Segmentation (VOS) methods rely on\ndense per-object mask annotations both during training and testing. This\nrequires time-consuming and costly video annotation mechanisms. We propose a\nnovel Point-VOS task with a spatio-temporally sparse point-wise annotation\nscheme that substantially reduces the annotation effort. We apply our\nannotation scheme to two large-scale video datasets with text descriptions and\nannotate over 19M points across 133K objects in 32K videos. Based on our\nannotations, we propose a new Point-VOS benchmark, and a corresponding\npoint-based training mechanism, which we use to establish strong baseline\nresults. We show that existing VOS methods can easily be adapted to leverage\nour point annotations during training, and can achieve results close to the\nfully-supervised performance when trained on pseudo-masks generated from these\npoints. In addition, we show that our data can be used to improve models that\nconnect vision and language, by evaluating it on the Video Narrative Grounding\n(VNG) task. We will make our code and annotations available at\nhttps://pointvos.github.io.",
        "translated": ""
    },
    {
        "title": "ClickSAM: Fine-tuning Segment Anything Model using click prompts for\n  ultrasound image segmentation",
        "url": "http://arxiv.org/abs/2402.05902v1",
        "pub_date": "2024-02-08",
        "summary": "The newly released Segment Anything Model (SAM) is a popular tool used in\nimage processing due to its superior segmentation accuracy, variety of input\nprompts, training capabilities, and efficient model design. However, its\ncurrent model is trained on a diverse dataset not tailored to medical images,\nparticularly ultrasound images. Ultrasound images tend to have a lot of noise,\nmaking it difficult to segment out important structures. In this project, we\ndeveloped ClickSAM, which fine-tunes the Segment Anything Model using click\nprompts for ultrasound images. ClickSAM has two stages of training: the first\nstage is trained on single-click prompts centered in the ground-truth contours,\nand the second stage focuses on improving the model performance through\nadditional positive and negative click prompts. By comparing the first stage\npredictions to the ground-truth masks, true positive, false positive, and false\nnegative segments are calculated. Positive clicks are generated using the true\npositive and false negative segments, and negative clicks are generated using\nthe false positive segments. The Centroidal Voronoi Tessellation algorithm is\nthen employed to collect positive and negative click prompts in each segment\nthat are used to enhance the model performance during the second stage of\ntraining. With click-train methods, ClickSAM exhibits superior performance\ncompared to other existing models for ultrasound image segmentation.",
        "translated": ""
    },
    {
        "title": "Mamba-ND: Selective State Space Modeling for Multi-Dimensional Data",
        "url": "http://arxiv.org/abs/2402.05892v1",
        "pub_date": "2024-02-08",
        "summary": "In recent years, Transformers have become the de-facto architecture for\nsequence modeling on text and a variety of multi-dimensional data, such as\nimages and video. However, the use of self-attention layers in a Transformer\nincurs prohibitive compute and memory complexity that scales quadratically\nw.r.t. the sequence length. A recent architecture, Mamba, based on state space\nmodels has been shown to achieve comparable performance for modeling text\nsequences, while scaling linearly with the sequence length. In this work, we\npresent Mamba-ND, a generalized design extending the Mamba architecture to\narbitrary multi-dimensional data. Our design alternatively unravels the input\ndata across different dimensions following row-major orderings. We provide a\nsystematic comparison of Mamba-ND with several other alternatives, based on\nprior multi-dimensional extensions such as Bi-directional LSTMs and S4ND.\nEmpirically, we show that Mamba-ND demonstrates performance competitive with\nthe state-of-the-art on a variety of multi-dimensional benchmarks, including\nImageNet-1K classification, HMDB-51 action recognition, and ERA5 weather\nforecasting.",
        "translated": ""
    },
    {
        "title": "CREMA: Multimodal Compositional Video Reasoning via Efficient Modular\n  Adaptation and Fusion",
        "url": "http://arxiv.org/abs/2402.05889v1",
        "pub_date": "2024-02-08",
        "summary": "Despite impressive advancements in multimodal compositional reasoning\napproaches, they are still limited in their flexibility and efficiency by\nprocessing fixed modality inputs while updating a lot of model parameters. This\npaper tackles these critical challenges and proposes CREMA, an efficient and\nmodular modality-fusion framework for injecting any new modality into video\nreasoning. We first augment multiple informative modalities (such as optical\nflow, 3D point cloud, audio) from given videos without extra human annotation\nby leveraging existing pre-trained models. Next, we introduce a query\ntransformer with multiple parameter-efficient modules associated with each\naccessible modality. It projects diverse modality features to the LLM token\nembedding space, allowing the model to integrate different data types for\nresponse generation. Furthermore, we propose a fusion module designed to\ncompress multimodal queries, maintaining computational efficiency in the LLM\nwhile combining additional modalities. We validate our method on video-3D,\nvideo-audio, and video-language reasoning tasks and achieve better/equivalent\nperformance against strong multimodal LLMs, including BLIP-2, 3D-LLM, and\nSeViLA while using 96% fewer trainable parameters. We provide extensive\nanalyses of CREMA, including the impact of each modality on reasoning domains,\nthe design of the fusion module, and example visualizations.",
        "translated": ""
    },
    {
        "title": "Adaptive Surface Normal Constraint for Geometric Estimation from\n  Monocular Images",
        "url": "http://arxiv.org/abs/2402.05869v1",
        "pub_date": "2024-02-08",
        "summary": "We introduce a novel approach to learn geometries such as depth and surface\nnormal from images while incorporating geometric context. The difficulty of\nreliably capturing geometric context in existing methods impedes their ability\nto accurately enforce the consistency between the different geometric\nproperties, thereby leading to a bottleneck of geometric estimation quality. We\ntherefore propose the Adaptive Surface Normal (ASN) constraint, a simple yet\nefficient method. Our approach extracts geometric context that encodes the\ngeometric variations present in the input image and correlates depth estimation\nwith geometric constraints. By dynamically determining reliable local geometry\nfrom randomly sampled candidates, we establish a surface normal constraint,\nwhere the validity of these candidates is evaluated using the geometric\ncontext. Furthermore, our normal estimation leverages the geometric context to\nprioritize regions that exhibit significant geometric variations, which makes\nthe predicted normals accurately capture intricate and detailed geometric\ninformation. Through the integration of geometric context, our method unifies\ndepth and surface normal estimations within a cohesive framework, which enables\nthe generation of high-quality 3D geometry from images. We validate the\nsuperiority of our approach over state-of-the-art methods through extensive\nevaluations and comparisons on diverse indoor and outdoor datasets, showcasing\nits efficiency and robustness.",
        "translated": ""
    },
    {
        "title": "Memory Consolidation Enables Long-Context Video Understanding",
        "url": "http://arxiv.org/abs/2402.05861v1",
        "pub_date": "2024-02-08",
        "summary": "Most transformer-based video encoders are limited to short temporal contexts\ndue to their quadratic complexity. While various attempts have been made to\nextend this context, this has often come at the cost of both conceptual and\ncomputational complexity. We propose to instead re-purpose existing pre-trained\nvideo transformers by simply fine-tuning them to attend to memories derived\nnon-parametrically from past activations. By leveraging redundancy reduction,\nour memory-consolidated vision transformer (MC-ViT) effortlessly extends its\ncontext far into the past and exhibits excellent scaling behavior when learning\nfrom longer videos. In doing so, MC-ViT sets a new state-of-the-art in\nlong-context video understanding on EgoSchema, Perception Test, and Diving48,\noutperforming methods that benefit from orders of magnitude more parameters.",
        "translated": ""
    },
    {
        "title": "Image-based Deep Learning for the time-dependent prediction of fresh\n  concrete properties",
        "url": "http://arxiv.org/abs/2402.06611v1",
        "pub_date": "2024-02-09",
        "summary": "Increasing the degree of digitisation and automation in the concrete\nproduction process can play a crucial role in reducing the CO$_2$ emissions\nthat are associated with the production of concrete. In this paper, a method is\npresented that makes it possible to predict the properties of fresh concrete\nduring the mixing process based on stereoscopic image sequences of the\nconcretes flow behaviour. A Convolutional Neural Network (CNN) is used for the\nprediction, which receives the images supported by information on the mix\ndesign as input. In addition, the network receives temporal information in the\nform of the time difference between the time at which the images are taken and\nthe time at which the reference values of the concretes are carried out. With\nthis temporal information, the network implicitly learns the time-dependent\nbehaviour of the concretes properties. The network predicts the slump flow\ndiameter, the yield stress and the plastic viscosity. The time-dependent\nprediction potentially opens up the pathway to determine the temporal\ndevelopment of the fresh concrete properties already during mixing. This\nprovides a huge advantage for the concrete industry. As a result,\ncountermeasures can be taken in a timely manner. It is shown that an approach\nbased on depth and optical flow images, supported by information of the mix\ndesign, achieves the best results.",
        "translated": ""
    },
    {
        "title": "On the Out-Of-Distribution Generalization of Multimodal Large Language\n  Models",
        "url": "http://arxiv.org/abs/2402.06599v1",
        "pub_date": "2024-02-09",
        "summary": "We investigate the generalization boundaries of current Multimodal Large\nLanguage Models (MLLMs) via comprehensive evaluation under out-of-distribution\nscenarios and domain-specific tasks. We evaluate their zero-shot generalization\nacross synthetic images, real-world distributional shifts, and specialized\ndatasets like medical and molecular imagery. Empirical results indicate that\nMLLMs struggle with generalization beyond common training domains, limiting\ntheir direct application without adaptation. To understand the cause of\nunreliable performance, we analyze three hypotheses: semantic\nmisinterpretation, visual feature extraction insufficiency, and mapping\ndeficiency. Results identify mapping deficiency as the primary hurdle. To\naddress this problem, we show that in-context learning (ICL) can significantly\nenhance MLLMs' generalization, opening new avenues for overcoming\ngeneralization barriers. We further explore the robustness of ICL under\ndistribution shifts and show its vulnerability to domain shifts, label shifts,\nand spurious correlation shifts between in-context examples and test data.",
        "translated": ""
    },
    {
        "title": "More than the Sum of Its Parts: Ensembling Backbone Networks for\n  Few-Shot Segmentation",
        "url": "http://arxiv.org/abs/2402.06581v1",
        "pub_date": "2024-02-09",
        "summary": "Semantic segmentation is a key prerequisite to robust image understanding for\napplications in \\acrlong{ai} and Robotics. \\acrlong{fss}, in particular,\nconcerns the extension and optimization of traditional segmentation methods in\nchallenging conditions where limited training examples are available. A\npredominant approach in \\acrlong{fss} is to rely on a single backbone for\nvisual feature extraction. Choosing which backbone to leverage is a deciding\nfactor contributing to the overall performance. In this work, we interrogate on\nwhether fusing features from different backbones can improve the ability of\n\\acrlong{fss} models to capture richer visual features. To tackle this\nquestion, we propose and compare two ensembling techniques-Independent Voting\nand Feature Fusion. Among the available \\acrlong{fss} methods, we implement the\nproposed ensembling techniques on PANet. The module dedicated to predicting\nsegmentation masks from the backbone embeddings in PANet avoids trainable\nparameters, creating a controlled `in vitro' setting for isolating the impact\nof different ensembling strategies. Leveraging the complementary strengths of\ndifferent backbones, our approach outperforms the original single-backbone\nPANet across standard benchmarks even in challenging one-shot learning\nscenarios. Specifically, it achieved a performance improvement of +7.37\\% on\nPASCAL-5\\textsuperscript{i} and of +10.68\\% on COCO-20\\textsuperscript{i} in\nthe top-performing scenario where three backbones are combined. These results,\ntogether with the qualitative inspection of the predicted subject masks,\nsuggest that relying on multiple backbones in PANet leads to a more\ncomprehensive feature representation, thus expediting the successful\napplication of \\acrlong{fss} methods in challenging, data-scarce environments.",
        "translated": ""
    },
    {
        "title": "Video Annotator: A framework for efficiently building video classifiers\n  using vision-language models and active learning",
        "url": "http://arxiv.org/abs/2402.06560v1",
        "pub_date": "2024-02-09",
        "summary": "High-quality and consistent annotations are fundamental to the successful\ndevelopment of robust machine learning models. Traditional data annotation\nmethods are resource-intensive and inefficient, often leading to a reliance on\nthird-party annotators who are not the domain experts. Hard samples, which are\nusually the most informative for model training, tend to be difficult to label\naccurately and consistently without business context. These can arise\nunpredictably during the annotation process, requiring a variable number of\niterations and rounds of feedback, leading to unforeseen expenses and time\ncommitments to guarantee quality.\n  We posit that more direct involvement of domain experts, using a\nhuman-in-the-loop system, can resolve many of these practical challenges. We\npropose a novel framework we call Video Annotator (VA) for annotating,\nmanaging, and iterating on video classification datasets. Our approach offers a\nnew paradigm for an end-user-centered model development process, enhancing the\nefficiency, usability, and effectiveness of video classifiers. Uniquely, VA\nallows for a continuous annotation process, seamlessly integrating data\ncollection and model training.\n  We leverage the zero-shot capabilities of vision-language foundation models\ncombined with active learning techniques, and demonstrate that VA enables the\nefficient creation of high-quality models. VA achieves a median 6.8 point\nimprovement in Average Precision relative to the most competitive baseline\nacross a wide-ranging assortment of tasks. We release a dataset with 153k\nlabels across 56 video understanding tasks annotated by three professional\nvideo editors using VA, and also release code to replicate our experiments at:\nhttp://github.com/netflix/videoannotator.",
        "translated": ""
    },
    {
        "title": "Hybridnet for depth estimation and semantic segmentation",
        "url": "http://arxiv.org/abs/2402.06539v1",
        "pub_date": "2024-02-09",
        "summary": "Semantic segmentation and depth estimation are two important tasks in the\narea of image processing. Traditionally, these two tasks are addressed in an\nindependent manner. However, for those applications where geometric and\nsemantic information is required, such as robotics or autonomous\nnavigation,depth or semantic segmentation alone are not sufficient. In this\npaper, depth estimation and semantic segmentation are addressed together from a\nsingle input image through a hybrid convolutional network. Different from the\nstate of the art methods where features are extracted by a sole feature\nextraction network for both tasks, the proposed HybridNet improves the features\nextraction by separating the relevant features for one task from those which\nare relevant for both. Experimental results demonstrate that HybridNet results\nare comparable with the state of the art methods, as well as the single task\nmethods that HybridNet is based on.",
        "translated": ""
    },
    {
        "title": "Feature Density Estimation for Out-of-Distribution Detection via\n  Normalizing Flows",
        "url": "http://arxiv.org/abs/2402.06537v1",
        "pub_date": "2024-02-09",
        "summary": "Out-of-distribution (OOD) detection is a critical task for safe deployment of\nlearning systems in the open world setting. In this work, we investigate the\nuse of feature density estimation via normalizing flows for OOD detection and\npresent a fully unsupervised approach which requires no exposure to OOD data,\navoiding researcher bias in OOD sample selection. This is a post-hoc method\nwhich can be applied to any pretrained model, and involves training a\nlightweight auxiliary normalizing flow model to perform the out-of-distribution\ndetection via density thresholding. Experiments on OOD detection in image\nclassification show strong results for far-OOD data detection with only a\nsingle epoch of flow training, including 98.2% AUROC for ImageNet-1k vs.\nTextures, which exceeds the state of the art by 7.8%. We additionally explore\nthe connection between the feature space distribution of the pretrained model\nand the performance of our method. Finally, we provide insights into training\npitfalls that have plagued normalizing flows for use in OOD detection.",
        "translated": ""
    },
    {
        "title": "Transferring facade labels between point clouds with semantic octrees\n  while considering change detection",
        "url": "http://arxiv.org/abs/2402.06531v1",
        "pub_date": "2024-02-09",
        "summary": "Point clouds and high-resolution 3D data have become increasingly important\nin various fields, including surveying, construction, and virtual reality.\nHowever, simply having this data is not enough; to extract useful information,\nsemantic labeling is crucial. In this context, we propose a method to transfer\nannotations from a labeled to an unlabeled point cloud using an octree\nstructure. The structure also analyses changes between the point clouds. Our\nexperiments confirm that our method effectively transfers annotations while\naddressing changes. The primary contribution of this project is the development\nof the method for automatic label transfer between two different point clouds\nthat represent the same real-world object. The proposed method can be of great\nimportance for data-driven deep learning algorithms as it can also allow\ncircumventing stochastic transfer learning by deterministic label transfer\nbetween datasets depicting the same objects.",
        "translated": ""
    },
    {
        "title": "Reconstructing facade details using MLS point clouds and Bag-of-Words\n  approach",
        "url": "http://arxiv.org/abs/2402.06521v1",
        "pub_date": "2024-02-09",
        "summary": "In the reconstruction of fa\\c{c}ade elements, the identification of specific\nobject types remains challenging and is often circumvented by rectangularity\nassumptions or the use of bounding boxes. We propose a new approach for the\nreconstruction of 3D fa\\c{c}ade details. We combine MLS point clouds and a\npre-defined 3D model library using a BoW concept, which we augment by\nincorporating semi-global features. We conduct experiments on the models\nsuperimposed with random noise and on the TUM-FA\\c{C}ADE dataset. Our method\ndemonstrates promising results, improving the conventional BoW approach. It\nholds the potential to be utilized for more realistic facade reconstruction\nwithout rectangularity assumptions, which can be used in applications such as\ntesting automated driving functions or estimating fa\\c{c}ade solar potential.",
        "translated": ""
    },
    {
        "title": "Classifying point clouds at the facade-level using geometric features\n  and deep learning networks",
        "url": "http://arxiv.org/abs/2402.06506v1",
        "pub_date": "2024-02-09",
        "summary": "3D building models with facade details are playing an important role in many\napplications now. Classifying point clouds at facade-level is key to create\nsuch digital replicas of the real world. However, few studies have focused on\nsuch detailed classification with deep neural networks. We propose a method\nfusing geometric features with deep learning networks for point cloud\nclassification at facade-level. Our experiments conclude that such early-fused\nfeatures improve deep learning methods' performance. This method can be applied\nfor compensating deep learning networks' ability in capturing local geometric\ninformation and promoting the advancement of semantic segmentation.",
        "translated": ""
    },
    {
        "title": "BarlowTwins-CXR : Enhancing Chest X-Ray abnormality localization in\n  heterogeneous data with cross-domain self-supervised learning",
        "url": "http://arxiv.org/abs/2402.06499v1",
        "pub_date": "2024-02-09",
        "summary": "Background: Chest X-ray imaging-based abnormality localization, essential in\ndiagnosing various diseases, faces significant clinical challenges due to\ncomplex interpretations and the growing workload of radiologists. While recent\nadvances in deep learning offer promising solutions, there is still a critical\nissue of domain inconsistency in cross-domain transfer learning, which hampers\nthe efficiency and accuracy of diagnostic processes. This study aims to address\nthe domain inconsistency problem and improve autonomic abnormality localization\nperformance of heterogeneous chest X-ray image analysis, by developing a\nself-supervised learning strategy called \"BarlwoTwins-CXR\". Methods: We\nutilized two publicly available datasets: the NIH Chest X-ray Dataset and the\nVinDr-CXR. The BarlowTwins-CXR approach was conducted in a two-stage training\nprocess. Initially, self-supervised pre-training was performed using an\nadjusted Barlow Twins algorithm on the NIH dataset with a Resnet50 backbone\npre-trained on ImageNet. This was followed by supervised fine-tuning on the\nVinDr-CXR dataset using Faster R-CNN with Feature Pyramid Network (FPN).\nResults: Our experiments showed a significant improvement in model performance\nwith BarlowTwins-CXR. The approach achieved a 3% increase in mAP50 accuracy\ncompared to traditional ImageNet pre-trained models. In addition, the Ablation\nCAM method revealed enhanced precision in localizing chest abnormalities.\nConclusion: BarlowTwins-CXR significantly enhances the efficiency and accuracy\nof chest X-ray image-based abnormality localization, outperforming traditional\ntransfer learning methods and effectively overcoming domain inconsistency in\ncross-domain scenarios. Our experiment results demonstrate the potential of\nusing self-supervised learning to improve the generalizability of models in\nmedical settings with limited amounts of heterogeneous data.",
        "translated": ""
    },
    {
        "title": "Wavefront Randomization Improves Deconvolution",
        "url": "http://arxiv.org/abs/2402.07900v1",
        "pub_date": "2024-02-12",
        "summary": "The performance of an imaging system is limited by optical aberrations, which\ncause blurriness in the resulting image. Digital correction techniques, such as\ndeconvolution, have limited ability to correct the blur, since some spatial\nfrequencies in the scene are not measured adequately due to the aberrations\n('zeros' of the system transfer function). We prove that the addition of a\nrandom mask to an imaging system removes its dependence on aberrations,\nreducing the likelihood of zeros in the transfer function and consequently\nreducing the sensitivity to noise during deconvolution. and consequently result\nin lower sensitivity to noise during deconvolution. In simulation, we show that\nthis strategy improves image quality over a range of aberration types,\naberration strengths, and signal-to-noise ratios.",
        "translated": ""
    },
    {
        "title": "Detection of Spider Mites on Labrador Beans through Machine Learning\n  Approaches Using Custom Datasets",
        "url": "http://arxiv.org/abs/2402.07895v1",
        "pub_date": "2024-02-12",
        "summary": "Amidst growing food production demands, early plant disease detection is\nessential to safeguard crops; this study proposes a visual machine learning\napproach for plant disease detection, harnessing RGB and NIR data collected in\nreal-world conditions through a JAI FS-1600D-10GE camera to build an RGBN\ndataset. A two-stage early plant disease detection model with YOLOv8 and a\nsequential CNN was used to train on a dataset with partial labels, which showed\na 3.6% increase in mAP compared to a single-stage end-to-end segmentation\nmodel. The sequential CNN model achieved 90.62% validation accuracy utilising\nRGBN data. An average of 6.25% validation accuracy increase is found using RGBN\nin classification compared to RGB using ResNet15 and the sequential CNN models.\nFurther research and dataset improvements are needed to meet food production\ndemands.",
        "translated": ""
    },
    {
        "title": "MODIPHY: Multimodal Obscured Detection for IoT using PHantom\n  Convolution-Enabled Faster YOLO",
        "url": "http://arxiv.org/abs/2402.07894v1",
        "pub_date": "2024-02-12",
        "summary": "Low-light conditions and occluded scenarios impede object detection in\nreal-world Internet of Things (IoT) applications like autonomous vehicles and\nsecurity systems. While advanced machine learning models strive for accuracy,\ntheir computational demands clash with the limitations of resource-constrained\ndevices, hampering real-time performance. In our current research, we tackle\nthis challenge, by introducing \"YOLO Phantom\", one of the smallest YOLO models\never conceived. YOLO Phantom utilizes the novel Phantom Convolution block,\nachieving comparable accuracy to the latest YOLOv8n model while simultaneously\nreducing both parameters and model size by 43%, resulting in a significant 19%\nreduction in Giga Floating Point Operations (GFLOPs). YOLO Phantom leverages\ntransfer learning on our multimodal RGB-infrared dataset to address low-light\nand occlusion issues, equipping it with robust vision under adverse conditions.\nIts real-world efficacy is demonstrated on an IoT platform with advanced\nlow-light and RGB cameras, seamlessly connecting to an AWS-based notification\nendpoint for efficient real-time object detection. Benchmarks reveal a\nsubstantial boost of 17% and 14% in frames per second (FPS) for thermal and RGB\ndetection, respectively, compared to the baseline YOLOv8n model. For community\ncontribution, both the code and the multimodal dataset are available on GitHub.",
        "translated": ""
    },
    {
        "title": "PIVOT: Iterative Visual Prompting Elicits Actionable Knowledge for VLMs",
        "url": "http://arxiv.org/abs/2402.07872v1",
        "pub_date": "2024-02-12",
        "summary": "Vision language models (VLMs) have shown impressive capabilities across a\nvariety of tasks, from logical reasoning to visual understanding. This opens\nthe door to richer interaction with the world, for example robotic control.\nHowever, VLMs produce only textual outputs, while robotic control and other\nspatial tasks require outputting continuous coordinates, actions, or\ntrajectories. How can we enable VLMs to handle such settings without\nfine-tuning on task-specific data?\n  In this paper, we propose a novel visual prompting approach for VLMs that we\ncall Prompting with Iterative Visual Optimization (PIVOT), which casts tasks as\niterative visual question answering. In each iteration, the image is annotated\nwith a visual representation of proposals that the VLM can refer to (e.g.,\ncandidate robot actions, localizations, or trajectories). The VLM then selects\nthe best ones for the task. These proposals are iteratively refined, allowing\nthe VLM to eventually zero in on the best available answer. We investigate\nPIVOT on real-world robotic navigation, real-world manipulation from images,\ninstruction following in simulation, and additional spatial inference tasks\nsuch as localization. We find, perhaps surprisingly, that our approach enables\nzero-shot control of robotic systems without any robot training data,\nnavigation in a variety of environments, and other capabilities. Although\ncurrent performance is far from perfect, our work highlights potentials and\nlimitations of this new regime and shows a promising approach for\nInternet-Scale VLMs in robotic and spatial reasoning domains. Website:\npivot-prompt.github.io and HuggingFace:\nhttps://huggingface.co/spaces/pivot-prompt/pivot-prompt-demo.",
        "translated": ""
    },
    {
        "title": "Prismatic VLMs: Investigating the Design Space of Visually-Conditioned\n  Language Models",
        "url": "http://arxiv.org/abs/2402.07865v1",
        "pub_date": "2024-02-12",
        "summary": "Visually-conditioned language models (VLMs) have seen growing adoption in\napplications such as visual dialogue, scene understanding, and robotic task\nplanning; adoption that has fueled a wealth of new models such as LLaVa,\nInstructBLIP, and PaLI-3. Despite the volume of new releases, key design\ndecisions around image preprocessing, architecture, and optimization are\nunder-explored, making it challenging to understand what factors account for\nmodel performance $-$ a challenge further complicated by the lack of objective,\nconsistent evaluations. To address these gaps, we first compile a suite of\nstandardized evaluations spanning visual question answering, object\nlocalization from language, and targeted challenge sets that probe properties\nsuch as hallucination; evaluations that provide calibrated, fine-grained\ninsight into a VLM's capabilities. Second, we rigorously investigate VLMs along\nkey design axes, including pretrained visual representations and quantifying\nthe tradeoffs of using base vs. instruct-tuned language models, amongst others.\nWe couple our analysis with three resource contributions: (1) a unified\nframework for evaluating VLMs, (2) optimized, flexible code for VLM training,\nand (3) checkpoints for all models, including a family of VLMs at the 7-13B\nscale that strictly outperform InstructBLIP and LLaVa v1.5, the\nstate-of-the-art in open-source VLMs.",
        "translated": ""
    },
    {
        "title": "Towards Meta-Pruning via Optimal Transport",
        "url": "http://arxiv.org/abs/2402.07839v1",
        "pub_date": "2024-02-12",
        "summary": "Structural pruning of neural networks conventionally relies on identifying\nand discarding less important neurons, a practice often resulting in\nsignificant accuracy loss that necessitates subsequent fine-tuning efforts.\nThis paper introduces a novel approach named Intra-Fusion, challenging this\nprevailing pruning paradigm. Unlike existing methods that focus on designing\nmeaningful neuron importance metrics, Intra-Fusion redefines the overlying\npruning procedure. Through utilizing the concepts of model fusion and Optimal\nTransport, we leverage an agnostically given importance metric to arrive at a\nmore effective sparse model representation. Notably, our approach achieves\nsubstantial accuracy recovery without the need for resource-intensive\nfine-tuning, making it an efficient and promising tool for neural network\ncompression.\n  Additionally, we explore how fusion can be added to the pruning process to\nsignificantly decrease the training time while maintaining competitive\nperformance. We benchmark our results for various networks on commonly used\ndatasets such as CIFAR-10, CIFAR-100, and ImageNet. More broadly, we hope that\nthe proposed Intra-Fusion approach invigorates exploration into a fresh\nalternative to the predominant compression approaches. Our code is available\nhere: https://github.com/alexandertheus/Intra-Fusion.",
        "translated": ""
    },
    {
        "title": "A Benchmark Grocery Dataset of Realworld Point Clouds From Single View",
        "url": "http://arxiv.org/abs/2402.07819v1",
        "pub_date": "2024-02-12",
        "summary": "Fine-grained grocery object recognition is an important computer vision\nproblem with broad applications in automatic checkout, in-store robotic\nnavigation, and assistive technologies for the visually impaired. Existing\ndatasets on groceries are mainly 2D images. Models trained on these datasets\nare limited to learning features from the regular 2D grids. While portable 3D\nsensors such as Kinect were commonly available for mobile phones, sensors such\nas LiDAR and TrueDepth, have recently been integrated into mobile phones.\nDespite the availability of mobile 3D sensors, there are currently no dedicated\nreal-world large-scale benchmark 3D datasets for grocery. In addition, existing\n3D datasets lack fine-grained grocery categories and have limited training\nsamples. Furthermore, collecting data by going around the object versus the\ntraditional photo capture makes data collection cumbersome. Thus, we introduce\na large-scale grocery dataset called 3DGrocery100. It constitutes 100 classes,\nwith a total of 87,898 3D point clouds created from 10,755 RGB-D single-view\nimages. We benchmark our dataset on six recent state-of-the-art 3D point cloud\nclassification models. Additionally, we also benchmark the dataset on few-shot\nand continual learning point cloud classification tasks. Project Page:\nhttps://bigdatavision.org/3DGrocery100/.",
        "translated": ""
    },
    {
        "title": "PBADet: A One-Stage Anchor-Free Approach for Part-Body Association",
        "url": "http://arxiv.org/abs/2402.07814v1",
        "pub_date": "2024-02-12",
        "summary": "The detection of human parts (e.g., hands, face) and their correct\nassociation with individuals is an essential task, e.g., for ubiquitous\nhuman-machine interfaces and action recognition. Traditional methods often\nemploy multi-stage processes, rely on cumbersome anchor-based systems, or do\nnot scale well to larger part sets. This paper presents PBADet, a novel\none-stage, anchor-free approach for part-body association detection. Building\nupon the anchor-free object representation across multi-scale feature maps, we\nintroduce a singular part-to-body center offset that effectively encapsulates\nthe relationship between parts and their parent bodies. Our design is\ninherently versatile and capable of managing multiple parts-to-body\nassociations without compromising on detection accuracy or robustness.\nComprehensive experiments on various datasets underscore the efficacy of our\napproach, which not only outperforms existing state-of-the-art techniques but\nalso offers a more streamlined and efficient solution to the part-body\nassociation challenge.",
        "translated": ""
    },
    {
        "title": "Minimally Interactive Segmentation of Soft-Tissue Tumors on CT and MRI\n  using Deep Learning",
        "url": "http://arxiv.org/abs/2402.07746v1",
        "pub_date": "2024-02-12",
        "summary": "Segmentations are crucial in medical imaging to obtain morphological,\nvolumetric, and radiomics biomarkers. Manual segmentation is accurate but not\nfeasible in the radiologist's clinical workflow, while automatic segmentation\ngenerally obtains sub-par performance. We therefore developed a minimally\ninteractive deep learning-based segmentation method for soft-tissue tumors\n(STTs) on CT and MRI. The method requires the user to click six points near the\ntumor's extreme boundaries. These six points are transformed into a distance\nmap and serve, with the image, as input for a Convolutional Neural Network. For\ntraining and validation, a multicenter dataset containing 514 patients and nine\nSTT types in seven anatomical locations was used, resulting in a Dice\nSimilarity Coefficient (DSC) of 0.85$\\pm$0.11 (mean $\\pm$ standard deviation\n(SD)) for CT and 0.84$\\pm$0.12 for T1-weighted MRI, when compared to manual\nsegmentations made by expert radiologists. Next, the method was externally\nvalidated on a dataset including five unseen STT phenotypes in extremities,\nachieving 0.81$\\pm$0.08 for CT, 0.84$\\pm$0.09 for T1-weighted MRI, and\n0.88\\pm0.08 for previously unseen T2-weighted fat-saturated (FS) MRI. In\nconclusion, our minimally interactive segmentation method effectively segments\ndifferent types of STTs on CT and MRI, with robust generalization to previously\nunseen phenotypes and imaging modalities.",
        "translated": ""
    },
    {
        "title": "Asking Multimodal Clarifying Questions in Mixed-Initiative\n  Conversational Search",
        "url": "http://arxiv.org/abs/2402.07742v1",
        "pub_date": "2024-02-12",
        "summary": "In mixed-initiative conversational search systems, clarifying questions are\nused to help users who struggle to express their intentions in a single query.\nThese questions aim to uncover user's information needs and resolve query\nambiguities. We hypothesize that in scenarios where multimodal information is\npertinent, the clarification process can be improved by using non-textual\ninformation. Therefore, we propose to add images to clarifying questions and\nformulate the novel task of asking multimodal clarifying questions in\nopen-domain, mixed-initiative conversational search systems. To facilitate\nresearch into this task, we collect a dataset named Melon that contains over 4k\nmultimodal clarifying questions, enriched with over 14k images. We also propose\na multimodal query clarification model named Marto and adopt a prompt-based,\ngenerative fine-tuning strategy to perform the training of different stages\nwith different prompts. Several analyses are conducted to understand the\nimportance of multimodal contents during the query clarification phase.\nExperimental results indicate that the addition of images leads to significant\nimprovements of up to 90% in retrieval performance when selecting the relevant\nimages. Extensive analyses are also performed to show the superiority of Marto\ncompared with discriminative baselines in terms of effectiveness and\nefficiency.",
        "translated": ""
    },
    {
        "title": "IM-3D: Iterative Multiview Diffusion and Reconstruction for High-Quality\n  3D Generation",
        "url": "http://arxiv.org/abs/2402.08682v1",
        "pub_date": "2024-02-13",
        "summary": "Most text-to-3D generators build upon off-the-shelf text-to-image models\ntrained on billions of images. They use variants of Score Distillation Sampling\n(SDS), which is slow, somewhat unstable, and prone to artifacts. A mitigation\nis to fine-tune the 2D generator to be multi-view aware, which can help\ndistillation or can be combined with reconstruction networks to output 3D\nobjects directly. In this paper, we further explore the design space of\ntext-to-3D models. We significantly improve multi-view generation by\nconsidering video instead of image generators. Combined with a 3D\nreconstruction algorithm which, by using Gaussian splatting, can optimize a\nrobust image-based loss, we directly produce high-quality 3D outputs from the\ngenerated views. Our new method, IM-3D, reduces the number of evaluations of\nthe 2D generator network 10-100x, resulting in a much more efficient pipeline,\nbetter quality, fewer geometric inconsistencies, and higher yield of usable 3D\nassets.",
        "translated": ""
    },
    {
        "title": "Mitigating Object Hallucination in Large Vision-Language Models via\n  Classifier-Free Guidance",
        "url": "http://arxiv.org/abs/2402.08680v1",
        "pub_date": "2024-02-13",
        "summary": "The advancement of Large Vision-Language Models (LVLMs) has increasingly\nhighlighted the critical issue of their tendency to hallucinate non-existing\nobjects in the images. To address this issue, previous works focused on using\nspecially curated datasets or powerful LLMs (e.g., GPT-3.5) to rectify the\noutputs of LVLMs. However, these approaches require either expensive\ntraining/fine-tuning or API access to advanced LLMs to correct the model's\noutput post-generation. In this paper, we tackle this challenge by introducing\na framework called Mitigating hallucinAtion via classifieR-Free guIdaNcE\n(MARINE), which is both training-free and API-free, and can effectively and\nefficiently reduce object hallucinations during the generation process.\nSpecifically, MARINE enriches the visual context of LVLMs by integrating\nexisting open-source vision models, and employs classifier-free guidance to\nincorporate the additional object grounding features to improve the precision\nof LVLMs' generations. Through comprehensive evaluations across $6$ popular\nLVLMs with diverse evaluation metrics, we demonstrate the effectiveness of\nMARINE, which even outperforms existing fine-tuning-based methods. Remarkably,\nit not only reduces hallucinations but also improves the detailedness of LVLMs'\ngenerations, as assessed by GPT-4V.",
        "translated": ""
    },
    {
        "title": "Are Semi-Dense Detector-Free Methods Good at Matching Local Features?",
        "url": "http://arxiv.org/abs/2402.08671v1",
        "pub_date": "2024-02-13",
        "summary": "Semi-dense detector-free approaches (SDF), such as LoFTR, are currently among\nthe most popular image matching methods. While SDF methods are trained to\nestablish correspondences between two images, their performances are almost\nexclusively evaluated using relative pose estimation metrics. Thus, the link\nbetween their ability to establish correspondences and the quality of the\nresulting estimated pose has thus far received little attention. This paper is\na first attempt to study this link. We start with proposing a novel structured\nattention-based image matching architecture (SAM). It allows us to show a\ncounter-intuitive result on two datasets (MegaDepth and HPatches): on the one\nhand SAM either outperforms or is on par with SDF methods in terms of\npose/homography estimation metrics, but on the other hand SDF approaches are\nsignificantly better than SAM in terms of matching accuracy. We then propose to\nlimit the computation of the matching accuracy to textured regions, and show\nthat in this case SAM often surpasses SDF methods. Our findings highlight a\nstrong correlation between the ability to establish accurate correspondences in\ntextured regions and the accuracy of the resulting estimated pose/homography.\nOur code will be made available.",
        "translated": ""
    },
    {
        "title": "PIN: Positional Insert Unlocks Object Localisation Abilities in VLMs",
        "url": "http://arxiv.org/abs/2402.08657v1",
        "pub_date": "2024-02-13",
        "summary": "Vision-Language Models (VLMs), such as Flamingo and GPT-4V, have shown\nimmense potential by integrating large language models with vision systems.\nNevertheless, these models face challenges in the fundamental computer vision\ntask of object localisation, due to their training on multimodal data\ncontaining mostly captions without explicit spatial grounding. While it is\npossible to construct custom, supervised training pipelines with bounding box\nannotations that integrate with VLMs, these result in specialized and\nhard-to-scale models. In this paper, we aim to explore the limits of\ncaption-based VLMs and instead propose to tackle the challenge in a simpler\nmanner by i) keeping the weights of a caption-based VLM frozen and ii) not\nusing any supervised detection data. To this end, we introduce an\ninput-agnostic Positional Insert (PIN), a learnable spatial prompt, containing\na minimal set of parameters that are slid inside the frozen VLM, unlocking\nobject localisation capabilities. Our PIN module is trained with a simple\nnext-token prediction task on synthetic data without requiring the introduction\nof new output heads. Our experiments demonstrate strong zero-shot localisation\nperformances on a variety of images, including Pascal VOC, COCO, LVIS, and\ndiverse images like paintings or cartoons.",
        "translated": ""
    },
    {
        "title": "Learning Continuous 3D Words for Text-to-Image Generation",
        "url": "http://arxiv.org/abs/2402.08654v1",
        "pub_date": "2024-02-13",
        "summary": "Current controls over diffusion models (e.g., through text or ControlNet) for\nimage generation fall short in recognizing abstract, continuous attributes like\nillumination direction or non-rigid shape change. In this paper, we present an\napproach for allowing users of text-to-image models to have fine-grained\ncontrol of several attributes in an image. We do this by engineering special\nsets of input tokens that can be transformed in a continuous manner -- we call\nthem Continuous 3D Words. These attributes can, for example, be represented as\nsliders and applied jointly with text prompts for fine-grained control over\nimage generation. Given only a single mesh and a rendering engine, we show that\nour approach can be adopted to provide continuous user control over several\n3D-aware attributes, including time-of-day illumination, bird wing orientation,\ndollyzoom effect, and object poses. Our method is capable of conditioning image\ncreation with multiple Continuous 3D Words and text descriptions simultaneously\nwhile adding no overhead to the generative process. Project Page:\nhttps://ttchengab.github.io/continuous_3d_words",
        "translated": ""
    },
    {
        "title": "Peeking Behind the Curtains of Residual Learning",
        "url": "http://arxiv.org/abs/2402.08645v1",
        "pub_date": "2024-02-13",
        "summary": "The utilization of residual learning has become widespread in deep and\nscalable neural nets. However, the fundamental principles that contribute to\nthe success of residual learning remain elusive, thus hindering effective\ntraining of plain nets with depth scalability. In this paper, we peek behind\nthe curtains of residual learning by uncovering the \"dissipating inputs\"\nphenomenon that leads to convergence failure in plain neural nets: the input is\ngradually compromised through plain layers due to non-linearities, resulting in\nchallenges of learning feature representations. We theoretically demonstrate\nhow plain neural nets degenerate the input to random noise and emphasize the\nsignificance of a residual connection that maintains a better lower bound of\nsurviving neurons as a solution. With our theoretical discoveries, we propose\n\"The Plain Neural Net Hypothesis\" (PNNH) that identifies the internal path\nacross non-linear layers as the most critical part in residual learning, and\nestablishes a paradigm to support the training of deep plain neural nets devoid\nof residual connections. We thoroughly evaluate PNNH-enabled CNN architectures\nand Transformers on popular vision benchmarks, showing on-par accuracy, up to\n0.3% higher training throughput, and 2x better parameter efficiency compared to\nResNets and vision Transformers.",
        "translated": ""
    },
    {
        "title": "Learned Image Compression with Text Quality Enhancement",
        "url": "http://arxiv.org/abs/2402.08643v1",
        "pub_date": "2024-02-13",
        "summary": "Learned image compression has gained widespread popularity for their\nefficiency in achieving ultra-low bit-rates. Yet, images containing substantial\ntextual content, particularly screen-content images (SCI), often suffers from\ntext distortion at such compressed levels. To address this, we propose to\nminimize a novel text logit loss designed to quantify the disparity in text\nbetween the original and reconstructed images, thereby improving the perceptual\nquality of the reconstructed text. Through rigorous experimentation across\ndiverse datasets and employing state-of-the-art algorithms, our findings reveal\nsignificant enhancements in the quality of reconstructed text upon integration\nof the proposed loss function with appropriate weighting. Notably, we achieve a\nBjontegaard delta (BD) rate of -32.64% for Character Error Rate (CER) and\n-28.03% for Word Error Rate (WER) on average by applying the text logit loss\nfor two screenshot datasets. Additionally, we present quantitative metrics\ntailored for evaluating text quality in image compression tasks. Our findings\nunderscore the efficacy and potential applicability of our proposed text logit\nloss function across various text-aware image compression contexts.",
        "translated": ""
    },
    {
        "title": "BdSLW60: A Word-Level Bangla Sign Language Dataset",
        "url": "http://arxiv.org/abs/2402.08635v1",
        "pub_date": "2024-02-13",
        "summary": "Sign language discourse is an essential mode of daily communication for the\ndeaf and hard-of-hearing people. However, research on Bangla Sign Language\n(BdSL) faces notable limitations, primarily due to the lack of datasets.\nRecognizing wordlevel signs in BdSL (WL-BdSL) presents a multitude of\nchallenges, including the need for well-annotated datasets, capturing the\ndynamic nature of sign gestures from facial or hand landmarks, developing\nsuitable machine learning or deep learning-based models with substantial video\nsamples, and so on. In this paper, we address these challenges by creating a\ncomprehensive BdSL word-level dataset named BdSLW60 in an unconstrained and\nnatural setting, allowing positional and temporal variations and allowing sign\nusers to change hand dominance freely. The dataset encompasses 60 Bangla sign\nwords, with a significant scale of 9307 video trials provided by 18 signers\nunder the supervision of a sign language professional. The dataset was\nrigorously annotated and cross-checked by 60 annotators. We also introduced a\nunique approach of a relative quantization-based key frame encoding technique\nfor landmark based sign gesture recognition. We report the benchmarking of our\nBdSLW60 dataset using the Support Vector Machine (SVM) with testing accuracy up\nto 67.6% and an attention-based bi-LSTM with testing accuracy up to 75.1%. The\ndataset is available at https://www.kaggle.com/datasets/hasaniut/bdslw60 and\nthe code base is accessible from https://github.com/hasanssl/BdSLW60_Code.",
        "translated": ""
    },
    {
        "title": "NeRF Analogies: Example-Based Visual Attribute Transfer for NeRFs",
        "url": "http://arxiv.org/abs/2402.08622v1",
        "pub_date": "2024-02-13",
        "summary": "A Neural Radiance Field (NeRF) encodes the specific relation of 3D geometry\nand appearance of a scene. We here ask the question whether we can transfer the\nappearance from a source NeRF onto a target 3D geometry in a semantically\nmeaningful way, such that the resulting new NeRF retains the target geometry\nbut has an appearance that is an analogy to the source NeRF. To this end, we\ngeneralize classic image analogies from 2D images to NeRFs. We leverage\ncorrespondence transfer along semantic affinity that is driven by semantic\nfeatures from large, pre-trained 2D image models to achieve multi-view\nconsistent appearance transfer. Our method allows exploring the mix-and-match\nproduct space of 3D geometry and appearance. We show that our method\noutperforms traditional stylization-based methods and that a large majority of\nusers prefer our method over several typical baselines.",
        "translated": ""
    },
    {
        "title": "Latent Inversion with Timestep-aware Sampling for Training-free\n  Non-rigid Editing",
        "url": "http://arxiv.org/abs/2402.08601v1",
        "pub_date": "2024-02-13",
        "summary": "Text-guided non-rigid editing involves complex edits for input images, such\nas changing motion or compositions within their surroundings. Since it requires\nmanipulating the input structure, existing methods often struggle with\npreserving object identity and background, particularly when combined with\nStable Diffusion. In this work, we propose a training-free approach for\nnon-rigid editing with Stable Diffusion, aimed at improving the identity\npreservation quality without compromising editability. Our approach comprises\nthree stages: text optimization, latent inversion, and timestep-aware text\ninjection sampling. Inspired by the recent success of Imagic, we employ their\ntext optimization for smooth editing. Then, we introduce latent inversion to\npreserve the input image's identity without additional model fine-tuning. To\nfully utilize the input reconstruction ability of latent inversion, we suggest\ntimestep-aware text inject sampling. This effectively retains the structure of\nthe input image by injecting the source text prompt in early sampling steps and\nthen transitioning to the target prompt in subsequent sampling steps. This\nstrategic approach seamlessly harmonizes with text optimization, facilitating\ncomplex non-rigid edits to the input without losing the original identity. We\ndemonstrate the effectiveness of our method in terms of identity preservation,\neditability, and aesthetic quality through extensive experiments.",
        "translated": ""
    },
    {
        "title": "Deep Rib Fracture Instance Segmentation and Classification from CT on\n  the RibFrac Challenge",
        "url": "http://arxiv.org/abs/2402.09372v1",
        "pub_date": "2024-02-14",
        "summary": "Rib fractures are a common and potentially severe injury that can be\nchallenging and labor-intensive to detect in CT scans. While there have been\nefforts to address this field, the lack of large-scale annotated datasets and\nevaluation benchmarks has hindered the development and validation of deep\nlearning algorithms. To address this issue, the RibFrac Challenge was\nintroduced, providing a benchmark dataset of over 5,000 rib fractures from 660\nCT scans, with voxel-level instance mask annotations and diagnosis labels for\nfour clinical categories (buckle, nondisplaced, displaced, or segmental). The\nchallenge includes two tracks: a detection (instance segmentation) track\nevaluated by an FROC-style metric and a classification track evaluated by an\nF1-style metric. During the MICCAI 2020 challenge period, 243 results were\nevaluated, and seven teams were invited to participate in the challenge\nsummary. The analysis revealed that several top rib fracture detection\nsolutions achieved performance comparable or even better than human experts.\nNevertheless, the current rib fracture classification solutions are hardly\nclinically applicable, which can be an interesting area in the future. As an\nactive benchmark and research resource, the data and online evaluation of the\nRibFrac Challenge are available at the challenge website. As an independent\ncontribution, we have also extended our previous internal baseline by\nincorporating recent advancements in large-scale pretrained networks and\npoint-based rib segmentation techniques. The resulting FracNet+ demonstrates\ncompetitive performance in rib fracture detection, which lays a foundation for\nfurther research and development in AI-assisted rib fracture detection and\ndiagnosis.",
        "translated": ""
    },
    {
        "title": "Magic-Me: Identity-Specific Video Customized Diffusion",
        "url": "http://arxiv.org/abs/2402.09368v1",
        "pub_date": "2024-02-14",
        "summary": "Creating content for a specific identity (ID) has shown significant interest\nin the field of generative models. In the field of text-to-image generation\n(T2I), subject-driven content generation has achieved great progress with the\nID in the images controllable. However, extending it to video generation is not\nwell explored. In this work, we propose a simple yet effective subject identity\ncontrollable video generation framework, termed Video Custom Diffusion (VCD).\nWith a specified subject ID defined by a few images, VCD reinforces the\nidentity information extraction and injects frame-wise correlation at the\ninitialization stage for stable video outputs with identity preserved to a\nlarge extent. To achieve this, we propose three novel components that are\nessential for high-quality ID preservation: 1) an ID module trained with the\ncropped identity by prompt-to-segmentation to disentangle the ID information\nand the background noise for more accurate ID token learning; 2) a\ntext-to-video (T2V) VCD module with 3D Gaussian Noise Prior for better\ninter-frame consistency and 3) video-to-video (V2V) Face VCD and Tiled VCD\nmodules to deblur the face and upscale the video for higher resolution.\n  Despite its simplicity, we conducted extensive experiments to verify that VCD\nis able to generate stable and high-quality videos with better ID over the\nselected strong baselines. Besides, due to the transferability of the ID\nmodule, VCD is also working well with finetuned text-to-image models available\npublically, further improving its usability. The codes are available at\nhttps://github.com/Zhen-Dong/Magic-Me.",
        "translated": ""
    },
    {
        "title": "Prediction of Activated Sludge Settling Characteristics from Microscopy\n  Images with Deep Convolutional Neural Networks and Transfer Learning",
        "url": "http://arxiv.org/abs/2402.09367v1",
        "pub_date": "2024-02-14",
        "summary": "Microbial communities play a key role in biological wastewater treatment\nprocesses. Activated sludge settling characteristics, for example, are affected\nby microbial community composition, varying by changes in operating conditions\nand influent characteristics of wastewater treatment plants (WWTPs). Timely\nassessment and prediction of changes in microbial composition leading to\nsettling problems, such as filamentous bulking (FB), can prevent operational\nchallenges, reductions in treatment efficiency, and adverse environmental\nimpacts. This study presents an innovative computer vision-based approach to\nassess activated sludge-settling characteristics based on the morphological\nproperties of flocs and filaments in microscopy images. Implementing the\ntransfer learning of deep convolutional neural network (CNN) models, this\napproach aims to overcome the limitations of existing quantitative image\nanalysis techniques. The offline microscopy image dataset was collected over\ntwo years, with weekly sampling at a full-scale industrial WWTP in Belgium.\nMultiple data augmentation techniques were employed to enhance the\ngeneralizability of the CNN models. Various CNN architectures, including\nInception v3, ResNet18, ResNet152, ConvNeXt-nano, and ConvNeXt-S, were tested\nto evaluate their performance in predicting sludge settling characteristics.\nThe sludge volume index was used as the final prediction variable, but the\nmethod can easily be adjusted to predict any other settling metric of choice.\nThe results showed that the suggested CNN-based approach provides less\nlabour-intensive, objective, and consistent assessments, while transfer\nlearning notably minimises the training phase, resulting in a generalizable\nsystem that can be employed in real-time applications.",
        "translated": ""
    },
    {
        "title": "Pruning Sparse Tensor Neural Networks Enables Deep Learning for 3D\n  Ultrasound Localization Microscopy",
        "url": "http://arxiv.org/abs/2402.09359v1",
        "pub_date": "2024-02-14",
        "summary": "Ultrasound Localization Microscopy (ULM) is a non-invasive technique that\nallows for the imaging of micro-vessels in vivo, at depth and with a resolution\non the order of ten microns. ULM is based on the sub-resolution localization of\nindividual microbubbles injected in the bloodstream. Mapping the whole\nangioarchitecture requires the accumulation of microbubbles trajectories from\nthousands of frames, typically acquired over a few minutes. ULM acquisition\ntimes can be reduced by increasing the microbubble concentration, but requires\nmore advanced algorithms to detect them individually. Several deep learning\napproaches have been proposed for this task, but they remain limited to 2D\nimaging, in part due to the associated large memory requirements. Herein, we\npropose to use sparse tensor neural networks to reduce memory usage in 2D and\nto improve the scaling of the memory requirement for the extension of deep\nlearning architecture to 3D. We study several approaches to efficiently convert\nultrasound data into a sparse format and study the impact of the associated\nloss of information. When applied in 2D, the sparse formulation reduces the\nmemory requirements by a factor 2 at the cost of a small reduction of\nperformance when compared against dense networks. In 3D, the proposed approach\nreduces memory requirements by two order of magnitude while largely\noutperforming conventional ULM in high concentration settings. We show that\nSparse Tensor Neural Networks in 3D ULM allow for the same benefits as dense\ndeep learning based method in 2D ULM i.e. the use of higher concentration in\nsilico and reduced acquisition time.",
        "translated": ""
    },
    {
        "title": "DoRA: Weight-Decomposed Low-Rank Adaptation",
        "url": "http://arxiv.org/abs/2402.09353v1",
        "pub_date": "2024-02-14",
        "summary": "Among the widely used parameter-efficient finetuning (PEFT) methods, LoRA and\nits variants have gained considerable popularity because of avoiding additional\ninference costs. However, there still often exists an accuracy gap between\nthese methods and full fine-tuning (FT). In this work, we first introduce a\nnovel weight decomposition analysis to investigate the inherent differences\nbetween FT and LoRA. Aiming to resemble the learning capacity of FT from the\nfindings, we propose Weight-Decomposed LowRank Adaptation (DoRA). DoRA\ndecomposes the pre-trained weight into two components, magnitude and direction,\nfor fine-tuning, specifically employing LoRA for directional updates to\nefficiently minimize the number of trainable parameters. By employing DoRA, we\nenhance both the learning capacity and training stability of LoRA while\navoiding any additional inference overhead. DoRA consistently outperforms LoRA\non fine-tuning LLaMA, LLaVA, and VL-BART on various downstream tasks, such as\ncommonsense reasoning, visual instruction tuning, and image/video-text\nunderstanding.",
        "translated": ""
    },
    {
        "title": "Registration of Longitudinal Spine CTs for Monitoring Lesion Growth",
        "url": "http://arxiv.org/abs/2402.09341v1",
        "pub_date": "2024-02-14",
        "summary": "Accurate and reliable registration of longitudinal spine images is essential\nfor assessment of disease progression and surgical outcome. Implementing a\nfully automatic and robust registration is crucial for clinical use, however,\nit is challenging due to substantial change in shape and appearance due to\nlesions. In this paper we present a novel method to automatically align\nlongitudinal spine CTs and accurately assess lesion progression. Our method\nfollows a two-step pipeline where vertebrae are first automatically localized,\nlabeled and 3D surfaces are generated using a deep learning model, then\nlongitudinally aligned using a Gaussian mixture model surface registration. We\ntested our approach on 37 vertebrae, from 5 patients, with baseline CTs and 3,\n6, and 12 months follow-ups leading to 111 registrations. Our experiment showed\naccurate registration with an average Hausdorff distance of 0.65 mm and average\nDice score of 0.92.",
        "translated": ""
    },
    {
        "title": "YOLOv8-AM: YOLOv8 with Attention Mechanisms for Pediatric Wrist Fracture\n  Detection",
        "url": "http://arxiv.org/abs/2402.09329v1",
        "pub_date": "2024-02-14",
        "summary": "Wrist trauma and even fractures occur frequently in daily life, particularly\namong children who account for a significant proportion of fracture cases.\nBefore performing surgery, surgeons often request patients to undergo X-ray\nimaging first and prepare for it based on the analysis of the radiologist. With\nthe development of neural networks, You Only Look Once (YOLO) series models\nhave been widely used in fracture detection as computer-assisted diagnosis\n(CAD). In 2023, Ultralytics presented the latest version of the YOLO models,\nwhich has been employed for detecting fractures across various parts of the\nbody. Attention mechanism is one of the hottest methods to improve the model\nperformance. This research work proposes YOLOv8-AM, which incorporates the\nattention mechanism into the original YOLOv8 architecture. Specifically, we\nrespectively employ four attention modules, Convolutional Block Attention\nModule (CBAM), Global Attention Mechanism (GAM), Efficient Channel Attention\n(ECA), and Shuffle Attention (SA), to design the improved models and train them\non GRAZPEDWRI-DX dataset. Experimental results demonstrate that the mean\nAverage Precision at IoU 50 (mAP 50) of the YOLOv8-AM model based on ResBlock +\nCBAM (ResCBAM) increased from 63.6% to 65.8%, which achieves the\nstate-of-the-art (SOTA) performance. Conversely, YOLOv8-AM model incorporating\nGAM obtains the mAP 50 value of 64.2%, which is not a satisfactory enhancement.\nTherefore, we combine ResBlock and GAM, introducing ResGAM to design another\nnew YOLOv8-AM model, whose mAP 50 value is increased to 65.0%.",
        "translated": ""
    },
    {
        "title": "PC-NeRF: Parent-Child Neural Radiance Fields Using Sparse LiDAR Frames\n  in Autonomous Driving Environments",
        "url": "http://arxiv.org/abs/2402.09325v1",
        "pub_date": "2024-02-14",
        "summary": "Large-scale 3D scene reconstruction and novel view synthesis are vital for\nautonomous vehicles, especially utilizing temporally sparse LiDAR frames.\nHowever, conventional explicit representations remain a significant bottleneck\ntowards representing the reconstructed and synthetic scenes at unlimited\nresolution. Although the recently developed neural radiance fields (NeRF) have\nshown compelling results in implicit representations, the problem of\nlarge-scale 3D scene reconstruction and novel view synthesis using sparse LiDAR\nframes remains unexplored. To bridge this gap, we propose a 3D scene\nreconstruction and novel view synthesis framework called parent-child neural\nradiance field (PC-NeRF). Based on its two modules, parent NeRF and child NeRF,\nthe framework implements hierarchical spatial partitioning and multi-level\nscene representation, including scene, segment, and point levels. The\nmulti-level scene representation enhances the efficient utilization of sparse\nLiDAR point cloud data and enables the rapid acquisition of an approximate\nvolumetric scene representation. With extensive experiments, PC-NeRF is proven\nto achieve high-precision novel LiDAR view synthesis and 3D reconstruction in\nlarge-scale scenes. Moreover, PC-NeRF can effectively handle situations with\nsparse LiDAR frames and demonstrate high deployment efficiency with limited\ntraining epochs. Our approach implementation and the pre-trained models are\navailable at https://github.com/biter0088/pc-nerf.",
        "translated": ""
    },
    {
        "title": "Only My Model On My Data: A Privacy Preserving Approach Protecting one\n  Model and Deceiving Unauthorized Black-Box Models",
        "url": "http://arxiv.org/abs/2402.09316v1",
        "pub_date": "2024-02-14",
        "summary": "Deep neural networks are extensively applied to real-world tasks, such as\nface recognition and medical image classification, where privacy and data\nprotection are critical. Image data, if not protected, can be exploited to\ninfer personal or contextual information. Existing privacy preservation\nmethods, like encryption, generate perturbed images that are unrecognizable to\neven humans. Adversarial attack approaches prohibit automated inference even\nfor authorized stakeholders, limiting practical incentives for commercial and\nwidespread adaptation. This pioneering study tackles an unexplored practical\nprivacy preservation use case by generating human-perceivable images that\nmaintain accurate inference by an authorized model while evading other\nunauthorized black-box models of similar or dissimilar objectives, and\naddresses the previous research gaps. The datasets employed are ImageNet, for\nimage classification, Celeba-HQ dataset, for identity classification, and\nAffectNet, for emotion classification. Our results show that the generated\nimages can successfully maintain the accuracy of a protected model and degrade\nthe average accuracy of the unauthorized black-box models to 11.97%, 6.63%, and\n55.51% on ImageNet, Celeba-HQ, and AffectNet datasets, respectively.",
        "translated": ""
    },
    {
        "title": "Few-Shot Object Detection with Sparse Context Transformers",
        "url": "http://arxiv.org/abs/2402.09315v1",
        "pub_date": "2024-02-14",
        "summary": "Few-shot detection is a major task in pattern recognition which seeks to\nlocalize objects using models trained with few labeled data. One of the\nmainstream few-shot methods is transfer learning which consists in pretraining\na detection model in a source domain prior to its fine-tuning in a target\ndomain. However, it is challenging for fine-tuned models to effectively\nidentify new classes in the target domain, particularly when the underlying\nlabeled training data are scarce. In this paper, we devise a novel sparse\ncontext transformer (SCT) that effectively leverages object knowledge in the\nsource domain, and automatically learns a sparse context from only few training\nimages in the target domain. As a result, it combines different relevant clues\nin order to enhance the discrimination power of the learned detectors and\nreduce class confusion. We evaluate the proposed method on two challenging\nfew-shot object detection benchmarks, and empirical results show that the\nproposed method obtains competitive performance compared to the related\nstate-of-the-art.",
        "translated": ""
    },
    {
        "title": "Self-Play Fine-Tuning of Diffusion Models for Text-to-Image Generation",
        "url": "http://arxiv.org/abs/2402.10210v1",
        "pub_date": "2024-02-15",
        "summary": "Fine-tuning Diffusion Models remains an underexplored frontier in generative\nartificial intelligence (GenAI), especially when compared with the remarkable\nprogress made in fine-tuning Large Language Models (LLMs). While cutting-edge\ndiffusion models such as Stable Diffusion (SD) and SDXL rely on supervised\nfine-tuning, their performance inevitably plateaus after seeing a certain\nvolume of data. Recently, reinforcement learning (RL) has been employed to\nfine-tune diffusion models with human preference data, but it requires at least\ntwo images (\"winner\" and \"loser\" images) for each text prompt. In this paper,\nwe introduce an innovative technique called self-play fine-tuning for diffusion\nmodels (SPIN-Diffusion), where the diffusion model engages in competition with\nits earlier versions, facilitating an iterative self-improvement process. Our\napproach offers an alternative to conventional supervised fine-tuning and RL\nstrategies, significantly improving both model performance and alignment. Our\nexperiments on the Pick-a-Pic dataset reveal that SPIN-Diffusion outperforms\nthe existing supervised fine-tuning method in aspects of human preference\nalignment and visual appeal right from its first iteration. By the second\niteration, it exceeds the performance of RLHF-based methods across all metrics,\nachieving these results with less data.",
        "translated": ""
    },
    {
        "title": "Recovering the Pre-Fine-Tuning Weights of Generative Models",
        "url": "http://arxiv.org/abs/2402.10208v1",
        "pub_date": "2024-02-15",
        "summary": "The dominant paradigm in generative modeling consists of two steps: i)\npre-training on a large-scale but unsafe dataset, ii) aligning the pre-trained\nmodel with human values via fine-tuning. This practice is considered safe, as\nno current method can recover the unsafe, pre-fine-tuning model weights. In\nthis paper, we demonstrate that this assumption is often false. Concretely, we\npresent Spectral DeTuning, a method that can recover the weights of the\npre-fine-tuning model using a few low-rank (LoRA) fine-tuned models. In\ncontrast to previous attacks that attempt to recover pre-fine-tuning\ncapabilities, our method aims to recover the exact pre-fine-tuning weights. Our\napproach exploits this new vulnerability against large-scale models such as a\npersonalized Stable Diffusion and an aligned Mistral.",
        "translated": ""
    },
    {
        "title": "Radio-astronomical Image Reconstruction with Conditional Denoising\n  Diffusion Model",
        "url": "http://arxiv.org/abs/2402.10204v1",
        "pub_date": "2024-02-15",
        "summary": "Reconstructing sky models from dirty radio images for accurate source\nlocalization and flux estimation is crucial for studying galaxy evolution at\nhigh redshift, especially in deep fields using instruments like the Atacama\nLarge Millimetre Array (ALMA). With new projects like the Square Kilometre\nArray (SKA), there's a growing need for better source extraction methods.\nCurrent techniques, such as CLEAN and PyBDSF, often fail to detect faint\nsources, highlighting the need for more accurate methods. This study proposes\nusing stochastic neural networks to rebuild sky models directly from dirty\nimages. This method can pinpoint radio sources and measure their fluxes with\nrelated uncertainties, marking a potential improvement in radio source\ncharacterization. We tested this approach on 10164 images simulated with the\nCASA tool simalma, based on ALMA's Cycle 5.3 antenna setup. We applied\nconditional Denoising Diffusion Probabilistic Models (DDPMs) for sky models\nreconstruction, then used Photutils to determine source coordinates and fluxes,\nassessing the model's performance across different water vapor levels. Our\nmethod showed excellence in source localization, achieving more than 90%\ncompleteness at a signal-to-noise ratio (SNR) as low as 2. It also surpassed\nPyBDSF in flux estimation, accurately identifying fluxes for 96% of sources in\nthe test set, a significant improvement over CLEAN+ PyBDSF's 57%. Conditional\nDDPMs is a powerful tool for image-to-image translation, yielding accurate and\nrobust characterisation of radio sources, and outperforming existing\nmethodologies. While this study underscores its significant potential for\napplications in radio astronomy, we also acknowledge certain limitations that\naccompany its usage, suggesting directions for further refinement and research.",
        "translated": ""
    },
    {
        "title": "Is Continual Learning Ready for Real-world Challenges?",
        "url": "http://arxiv.org/abs/2402.10130v1",
        "pub_date": "2024-02-15",
        "summary": "Despite continual learning's long and well-established academic history, its\napplication in real-world scenarios remains rather limited. This paper contends\nthat this gap is attributable to a misalignment between the actual challenges\nof continual learning and the evaluation protocols in use, rendering proposed\nsolutions ineffective for addressing the complexities of real-world setups. We\nvalidate our hypothesis and assess progress to date, using a new 3D semantic\nsegmentation benchmark, OCL-3DSS. We investigate various continual learning\nschemes from the literature by utilizing more realistic protocols that\nnecessitate online and continual learning for dynamic, real-world scenarios\n(eg., in robotics and 3D vision applications). The outcomes are sobering: all\nconsidered methods perform poorly, significantly deviating from the upper bound\nof joint offline training. This raises questions about the applicability of\nexisting methods in realistic settings. Our paper aims to initiate a paradigm\nshift, advocating for the adoption of continual learning methods through new\nexperimental protocols that better emulate real-world conditions to facilitate\nbreakthroughs in the field.",
        "translated": ""
    },
    {
        "title": "GES: Generalized Exponential Splatting for Efficient Radiance Field\n  Rendering",
        "url": "http://arxiv.org/abs/2402.10128v1",
        "pub_date": "2024-02-15",
        "summary": "Advancements in 3D Gaussian Splatting have significantly accelerated 3D\nreconstruction and generation. However, it may require a large number of\nGaussians, which creates a substantial memory footprint. This paper introduces\nGES (Generalized Exponential Splatting), a novel representation that employs\nGeneralized Exponential Function (GEF) to model 3D scenes, requiring far fewer\nparticles to represent a scene and thus significantly outperforming Gaussian\nSplatting methods in efficiency with a plug-and-play replacement ability for\nGaussian-based utilities. GES is validated theoretically and empirically in\nboth principled 1D setup and realistic 3D scenes.\n  It is shown to represent signals with sharp edges more accurately, which are\ntypically challenging for Gaussians due to their inherent low-pass\ncharacteristics. Our empirical analysis demonstrates that GEF outperforms\nGaussians in fitting natural-occurring signals (e.g. squares, triangles, and\nparabolic signals), thereby reducing the need for extensive splitting\noperations that increase the memory footprint of Gaussian Splatting. With the\naid of a frequency-modulated loss, GES achieves competitive performance in\nnovel-view synthesis benchmarks while requiring less than half the memory\nstorage of Gaussian Splatting and increasing the rendering speed by up to 39%.\nThe code is available on the project website https://abdullahamdi.com/ges .",
        "translated": ""
    },
    {
        "title": "Any-Shift Prompting for Generalization over Distributions",
        "url": "http://arxiv.org/abs/2402.10099v1",
        "pub_date": "2024-02-15",
        "summary": "Image-language models with prompt learning have shown remarkable advances in\nnumerous downstream vision tasks. Nevertheless, conventional prompt learning\nmethods overfit their training distribution and lose the generalization ability\non test distributions. To improve generalization across various distribution\nshifts, we propose any-shift prompting: a general probabilistic inference\nframework that considers the relationship between training and test\ndistributions during prompt learning. We explicitly connect training and test\ndistributions in the latent space by constructing training and test prompts in\na hierarchical architecture. Within this framework, the test prompt exploits\nthe distribution relationships to guide the generalization of the CLIP\nimage-language model from training to any test distribution. To effectively\nencode the distribution information and their relationships, we further\nintroduce a transformer inference network with a pseudo-shift training\nmechanism. The network generates the tailored test prompt with both training\nand test information in a feedforward pass, avoiding extra training costs at\ntest time. Extensive experiments on twenty-three datasets demonstrate the\neffectiveness of any-shift prompting on the generalization over various\ndistribution shifts.",
        "translated": ""
    },
    {
        "title": "MIM-Refiner: A Contrastive Learning Boost from Intermediate Pre-Trained\n  Representations",
        "url": "http://arxiv.org/abs/2402.10093v1",
        "pub_date": "2024-02-15",
        "summary": "We introduce MIM (Masked Image Modeling)-Refiner, a contrastive learning\nboost for pre-trained MIM models. The motivation behind MIM-Refiner is rooted\nin the insight that optimal representations within MIM models generally reside\nin intermediate layers. Accordingly, MIM-Refiner leverages multiple contrastive\nheads that are connected to diverse intermediate layers. In each head, a\nmodified nearest neighbor objective helps to construct respective semantic\nclusters.\n  The refinement process is short but effective. Within a few epochs, we refine\nthe features of MIM models from subpar to state-of-the-art, off-the-shelf\nfeatures. Refining a ViT-H, pre-trained with data2vec 2.0 on ImageNet-1K,\nachieves new state-of-the-art results in linear probing (84.7%) and low-shot\nclassification among models that are pre-trained on ImageNet-1K. In ImageNet-1K\n1-shot classification, MIM-Refiner sets a new state-of-the-art of 64.2%,\noutperforming larger models that were trained on up to 2000x more data such as\nDINOv2-g, OpenCLIP-G and MAWS-6.5B. Project page:\nhttps://ml-jku.github.io/MIM-Refiner",
        "translated": ""
    },
    {
        "title": "NYCTALE: Neuro-Evidence Transformer for Adaptive and Personalized Lung\n  Nodule Invasiveness Prediction",
        "url": "http://arxiv.org/abs/2402.10066v1",
        "pub_date": "2024-02-15",
        "summary": "Drawing inspiration from the primate brain's intriguing evidence accumulation\nprocess, and guided by models from cognitive psychology and neuroscience, the\npaper introduces the NYCTALE framework, a neuro-inspired and evidence\naccumulation-based Transformer architecture. The proposed neuro-inspired\nNYCTALE offers a novel pathway in the domain of Personalized Medicine (PM) for\nlung cancer diagnosis. In nature, Nyctales are small owls known for their\nnocturnal behavior, hunting primarily during the darkness of night. The NYCTALE\noperates in a similarly vigilant manner, i.e., processing data in an\nevidence-based fashion and making predictions dynamically/adaptively. Distinct\nfrom conventional Computed Tomography (CT)-based Deep Learning (DL) models, the\nNYCTALE performs predictions only when sufficient amount of evidence is\naccumulated. In other words, instead of processing all or a pre-defined subset\nof CT slices, for each person, slices are provided one at a time. The NYCTALE\nframework then computes an evidence vector associated with contribution of each\nnew CT image. A decision is made once the total accumulated evidence surpasses\na specific threshold. Preliminary experimental analyses conducted using a\nchallenging in-house dataset comprising 114 subjects. The results are\nnoteworthy, suggesting that NYCTALE outperforms the benchmark accuracy even\nwith approximately 60% less training data on this demanding and small dataset.",
        "translated": ""
    },
    {
        "title": "X-maps: Direct Depth Lookup for Event-based Structured Light Systems",
        "url": "http://arxiv.org/abs/2402.10061v1",
        "pub_date": "2024-02-15",
        "summary": "We present a new approach to direct depth estimation for Spatial Augmented\nReality (SAR) applications using event cameras. These dynamic vision sensors\nare a great fit to be paired with laser projectors for depth estimation in a\nstructured light approach. Our key contributions involve a conversion of the\nprojector time map into a rectified X-map, capturing x-axis correspondences for\nincoming events and enabling direct disparity lookup without any additional\nsearch. Compared to previous implementations, this significantly simplifies\ndepth estimation, making it more efficient, while the accuracy is similar to\nthe time map-based process. Moreover, we compensate non-linear temporal\nbehavior of cheap laser projectors by a simple time map calibration, resulting\nin improved performance and increased depth estimation accuracy. Since depth\nestimation is executed by two lookups only, it can be executed almost instantly\n(less than 3 ms per frame with a Python implementation) for incoming events.\nThis allows for real-time interactivity and responsiveness, which makes our\napproach especially suitable for SAR experiences where low latency, high frame\nrates and direct feedback are crucial. We present valuable insights gained into\ndata transformed into X-maps and evaluate our depth from disparity estimation\nagainst the state of the art time map-based results. Additional results and\ncode are available on our project page: https://fraunhoferhhi.github.io/X-maps/",
        "translated": ""
    },
    {
        "title": "Robust semi-automatic vessel tracing in the human retinal image by an\n  instance segmentation neural network",
        "url": "http://arxiv.org/abs/2402.10055v1",
        "pub_date": "2024-02-15",
        "summary": "The morphology and hierarchy of the vascular systems are essential for\nperfusion in supporting metabolism. In human retina, one of the most\nenergy-demanding organs, retinal circulation nourishes the entire inner retina\nby an intricate vasculature emerging and remerging at the optic nerve head\n(ONH). Thus, tracing the vascular branching from ONH through the vascular tree\ncan illustrate vascular hierarchy and allow detailed morphological\nquantification, and yet remains a challenging task. Here, we presented a novel\napproach for a robust semi-automatic vessel tracing algorithm on human fundus\nimages by an instance segmentation neural network (InSegNN). Distinct from\nsemantic segmentation, InSegNN separates and labels different vascular trees\nindividually and therefore enable tracing each tree throughout its branching.\nWe have built-in three strategies to improve robustness and accuracy with\ntemporal learning, spatial multi-sampling, and dynamic probability map. We\nachieved 83% specificity, and 50% improvement in Symmetric Best Dice (SBD)\ncompared to literature, and outperformed baseline U-net. We have demonstrated\ntracing individual vessel trees from fundus images, and simultaneously retain\nthe vessel hierarchy information. InSegNN paves a way for any subsequent\nmorphological analysis of vascular morphology in relation to retinal diseases.",
        "translated": ""
    },
    {
        "title": "PaLM2-VAdapter: Progressively Aligned Language Model Makes a Strong\n  Vision-language Adapter",
        "url": "http://arxiv.org/abs/2402.10896v1",
        "pub_date": "2024-02-16",
        "summary": "This paper demonstrates that a progressively aligned language model can\neffectively bridge frozen vision encoders and large language models (LLMs).\nWhile the fundamental architecture and pre-training methods of vision encoders\nand LLMs have been extensively studied, the architecture and training strategy\nof vision-language adapters vary significantly across recent works. Our\nresearch undertakes a thorough exploration of the state-of-the-art perceiver\nresampler architecture and builds a strong baseline. However, we observe that\nthe vision-language alignment with perceiver resampler exhibits slow\nconvergence and limited scalability with a lack of direct supervision. To\naddress this issue, we propose PaLM2-VAdapter, employing a progressively\naligned language model as the vision-language adapter. Compared to the strong\nbaseline with perceiver resampler, our method empirically shows faster\nconvergence, higher performance, and stronger scalability. Extensive\nexperiments across various Visual Question Answering (VQA) and captioning tasks\non both images and videos demonstrate that our model exhibits state-of-the-art\nvisual understanding and multi-modal reasoning capabilities. Notably, our\nmethod achieves these advancements with 30~70% fewer parameters than the\nstate-of-the-art large vision-language models, marking a significant efficiency\nimprovement.",
        "translated": ""
    },
    {
        "title": "Fusion of Diffusion Weighted MRI and Clinical Data for Predicting\n  Functional Outcome after Acute Ischemic Stroke with Deep Contrastive Learning",
        "url": "http://arxiv.org/abs/2402.10894v1",
        "pub_date": "2024-02-16",
        "summary": "Stroke is a common disabling neurological condition that affects about\none-quarter of the adult population over age 25; more than half of patients\nstill have poor outcomes, such as permanent functional dependence or even\ndeath, after the onset of acute stroke. The aim of this study is to investigate\nthe efficacy of diffusion-weighted MRI modalities combining with structured\nhealth profile on predicting the functional outcome to facilitate early\nintervention. A deep fusion learning network is proposed with two-stage\ntraining: the first stage focuses on cross-modality representation learning and\nthe second stage on classification. Supervised contrastive learning is\nexploited to learn discriminative features that separate the two classes of\npatients from embeddings of individual modalities and from the fused multimodal\nembedding. The network takes as the input DWI and ADC images, and structured\nhealth profile data. The outcome is the prediction of the patient needing\nlong-term care at 3 months after the onset of stroke. Trained and evaluated\nwith a dataset of 3297 patients, our proposed fusion model achieves 0.87, 0.80\nand 80.45% for AUC, F1-score and accuracy, respectively, outperforming existing\nmodels that consolidate both imaging and structured data in the medical domain.\nIf trained with comprehensive clinical variables, including NIHSS and\ncomorbidities, the gain from images on making accurate prediction is not\nconsidered substantial, but significant. However, diffusion-weighted MRI can\nreplace NIHSS to achieve comparable level of accuracy combining with other\nreadily available clinical variables for better generalization.",
        "translated": ""
    },
    {
        "title": "Weak-Mamba-UNet: Visual Mamba Makes CNN and ViT Work Better for\n  Scribble-based Medical Image Segmentation",
        "url": "http://arxiv.org/abs/2402.10887v1",
        "pub_date": "2024-02-16",
        "summary": "Medical image segmentation is increasingly reliant on deep learning\ntechniques, yet the promising performance often come with high annotation\ncosts. This paper introduces Weak-Mamba-UNet, an innovative weakly-supervised\nlearning (WSL) framework that leverages the capabilities of Convolutional\nNeural Network (CNN), Vision Transformer (ViT), and the cutting-edge Visual\nMamba (VMamba) architecture for medical image segmentation, especially when\ndealing with scribble-based annotations. The proposed WSL strategy incorporates\nthree distinct architecture but same symmetrical encoder-decoder networks: a\nCNN-based UNet for detailed local feature extraction, a Swin Transformer-based\nSwinUNet for comprehensive global context understanding, and a VMamba-based\nMamba-UNet for efficient long-range dependency modeling. The key concept of\nthis framework is a collaborative and cross-supervisory mechanism that employs\npseudo labels to facilitate iterative learning and refinement across the\nnetworks. The effectiveness of Weak-Mamba-UNet is validated on a publicly\navailable MRI cardiac segmentation dataset with processed scribble annotations,\nwhere it surpasses the performance of a similar WSL framework utilizing only\nUNet or SwinUNet. This highlights its potential in scenarios with sparse or\nimprecise annotations. The source code is made publicly accessible.",
        "translated": ""
    },
    {
        "title": "3D Diffuser Actor: Policy Diffusion with 3D Scene Representations",
        "url": "http://arxiv.org/abs/2402.10885v1",
        "pub_date": "2024-02-16",
        "summary": "We marry diffusion policies and 3D scene representations for robot\nmanipulation. Diffusion policies learn the action distribution conditioned on\nthe robot and environment state using conditional diffusion models. They have\nrecently shown to outperform both deterministic and alternative\nstate-conditioned action distribution learning methods. 3D robot policies use\n3D scene feature representations aggregated from a single or multiple camera\nviews using sensed depth. They have shown to generalize better than their 2D\ncounterparts across camera viewpoints. We unify these two lines of work and\npresent 3D Diffuser Actor, a neural policy architecture that, given a language\ninstruction, builds a 3D representation of the visual scene and conditions on\nit to iteratively denoise 3D rotations and translations for the robot's\nend-effector. At each denoising iteration, our model represents end-effector\npose estimates as 3D scene tokens and predicts the 3D translation and rotation\nerror for each of them, by featurizing them using 3D relative attention to\nother 3D visual and language tokens. 3D Diffuser Actor sets a new\nstate-of-the-art on RLBench with an absolute performance gain of 16.3% over the\ncurrent SOTA on a multi-view setup and an absolute gain of 13.1% on a\nsingle-view setup. On the CALVIN benchmark, it outperforms the current SOTA in\nthe setting of zero-shot unseen scene generalization by being able to\nsuccessfully run 0.2 more tasks, a 7% relative increase. It also works in the\nreal world from a handful of demonstrations. We ablate our model's\narchitectural design choices, such as 3D scene featurization and 3D relative\nattentions, and show they all help generalization. Our results suggest that 3D\nscene representations and powerful generative modeling are keys to efficient\nrobot learning from demonstrations.",
        "translated": ""
    },
    {
        "title": "Multi-modal preference alignment remedies regression of visual\n  instruction tuning on language model",
        "url": "http://arxiv.org/abs/2402.10884v1",
        "pub_date": "2024-02-16",
        "summary": "In production, multi-modal large language models (MLLMs) are expected to\nsupport multi-turn queries of interchanging image and text modalities. However,\nthe current MLLMs trained with visual-question-answering (VQA) datasets could\nsuffer from degradation, as VQA datasets lack the diversity and complexity of\nthe original text instruction datasets which the underlying language model had\nbeen trained with. To address this challenging degradation, we first collect a\nlightweight (6k entries) VQA preference dataset where answers were annotated by\nGemini for 5 quality metrics in a granular fashion, and investigate standard\nSupervised Fine-tuning, rejection sampling, Direct Preference Optimization\n(DPO), and SteerLM. Our findings indicate that the with DPO we are able to\nsurpass instruction-following capabilities of the language model, achieving a\n6.73 score on MT-Bench, compared to Vicuna's 6.57 and LLaVA's 5.99 despite\nsmall data scale. This enhancement in textual instruction proficiency\ncorrelates with boosted visual instruction performance (+4.9\\% on MM-Vet, +6\\%\non LLaVA-Bench), with minimal alignment tax on visual knowledge benchmarks\ncompared to previous RLHF approach. In conclusion, we propose a\ndistillation-based multi-modal alignment model with fine-grained annotations on\na small dataset that reconciles the textual and visual performance of MLLMs,\nrestoring and boosting language capability after visual instruction tuning.",
        "translated": ""
    },
    {
        "title": "Universal Prompt Optimizer for Safe Text-to-Image Generation",
        "url": "http://arxiv.org/abs/2402.10882v1",
        "pub_date": "2024-02-16",
        "summary": "Text-to-Image (T2I) models have shown great performance in generating images\nbased on textual prompts. However, these models are vulnerable to unsafe input\nto generate unsafe content like sexual, harassment and illegal-activity images.\nExisting studies based on image checker, model fine-tuning and embedding\nblocking are impractical in real-world applications. Hence, \\textit{we propose\nthe first universal prompt optimizer for safe T2I generation in black-box\nscenario}. We first construct a dataset consisting of toxic-clean prompt pairs\nby GPT-3.5 Turbo. To guide the optimizer to have the ability of converting\ntoxic prompt to clean prompt while preserving semantic information, we design a\nnovel reward function measuring toxicity and text alignment of generated images\nand train the optimizer through Proximal Policy Optimization. Experiments show\nthat our approach can effectively reduce the likelihood of various T2I models\nin generating inappropriate images, with no significant impact on text\nalignment. It is also flexible to be combined with methods to achieve better\nperformance.",
        "translated": ""
    },
    {
        "title": "Multi-Model 3D Registration: Finding Multiple Moving Objects in\n  Cluttered Point Clouds",
        "url": "http://arxiv.org/abs/2402.10865v1",
        "pub_date": "2024-02-16",
        "summary": "We investigate a variation of the 3D registration problem, named multi-model\n3D registration. In the multi-model registration problem, we are given two\npoint clouds picturing a set of objects at different poses (and possibly\nincluding points belonging to the background) and we want to simultaneously\nreconstruct how all objects moved between the two point clouds. This setup\ngeneralizes standard 3D registration where one wants to reconstruct a single\npose, e.g., the motion of the sensor picturing a static scene. Moreover, it\nprovides a mathematically grounded formulation for relevant robotics\napplications, e.g., where a depth sensor onboard a robot perceives a dynamic\nscene and has the goal of estimating its own motion (from the static portion of\nthe scene) while simultaneously recovering the motion of all dynamic objects.\nWe assume a correspondence-based setup where we have putative matches between\nthe two point clouds and consider the practical case where these\ncorrespondences are plagued with outliers. We then propose a simple approach\nbased on Expectation-Maximization (EM) and establish theoretical conditions\nunder which the EM approach converges to the ground truth. We evaluate the\napproach in simulated and real datasets ranging from table-top scenes to\nself-driving scenarios and demonstrate its effectiveness when combined with\nstate-of-the-art scene flow methods to establish dense correspondences.",
        "translated": ""
    },
    {
        "title": "Control Color: Multimodal Diffusion-based Interactive Image Colorization",
        "url": "http://arxiv.org/abs/2402.10855v1",
        "pub_date": "2024-02-16",
        "summary": "Despite the existence of numerous colorization methods, several limitations\nstill exist, such as lack of user interaction, inflexibility in local\ncolorization, unnatural color rendering, insufficient color variation, and\ncolor overflow. To solve these issues, we introduce Control Color (CtrlColor),\na multi-modal colorization method that leverages the pre-trained Stable\nDiffusion (SD) model, offering promising capabilities in highly controllable\ninteractive image colorization. While several diffusion-based methods have been\nproposed, supporting colorization in multiple modalities remains non-trivial.\nIn this study, we aim to tackle both unconditional and conditional image\ncolorization (text prompts, strokes, exemplars) and address color overflow and\nincorrect color within a unified framework. Specifically, we present an\neffective way to encode user strokes to enable precise local color manipulation\nand employ a practical way to constrain the color distribution similar to\nexemplars. Apart from accepting text prompts as conditions, these designs add\nversatility to our approach. We also introduce a novel module based on\nself-attention and a content-guided deformable autoencoder to address the\nlong-standing issues of color overflow and inaccurate coloring. Extensive\ncomparisons show that our model outperforms state-of-the-art image colorization\nmethods both qualitatively and quantitatively.",
        "translated": ""
    },
    {
        "title": "HistoSegCap: Capsules for Weakly-Supervised Semantic Segmentation of\n  Histological Tissue Type in Whole Slide Images",
        "url": "http://arxiv.org/abs/2402.10851v1",
        "pub_date": "2024-02-16",
        "summary": "Digital pathology involves converting physical tissue slides into\nhigh-resolution Whole Slide Images (WSIs), which pathologists analyze for\ndisease-affected tissues. However, large histology slides with numerous\nmicroscopic fields pose challenges for visual search. To aid pathologists,\nComputer Aided Diagnosis (CAD) systems offer visual assistance in efficiently\nexamining WSIs and identifying diagnostically relevant regions. This paper\npresents a novel histopathological image analysis method employing Weakly\nSupervised Semantic Segmentation (WSSS) based on Capsule Networks, the first\nsuch application. The proposed model is evaluated using the Atlas of Digital\nPathology (ADP) dataset and its performance is compared with other\nhistopathological semantic segmentation methodologies. The findings underscore\nthe potential of Capsule Networks in enhancing the precision and efficiency of\nhistopathological image analysis. Experimental results show that the proposed\nmodel outperforms traditional methods in terms of accuracy and the mean\nIntersection-over-Union (mIoU) metric.",
        "translated": ""
    },
    {
        "title": "Enhancement-Driven Pretraining for Robust Fingerprint Representation\n  Learning",
        "url": "http://arxiv.org/abs/2402.10847v1",
        "pub_date": "2024-02-16",
        "summary": "Fingerprint recognition stands as a pivotal component of biometric\ntechnology, with diverse applications from identity verification to advanced\nsearch tools. In this paper, we propose a unique method for deriving robust\nfingerprint representations by leveraging enhancement-based pre-training.\nBuilding on the achievements of U-Net-based fingerprint enhancement, our method\nemploys a specialized encoder to derive representations from fingerprint images\nin a self-supervised manner. We further refine these representations, aiming to\nenhance the verification capabilities. Our experimental results, tested on\npublicly available fingerprint datasets, reveal a marked improvement in\nverification performance against established self-supervised training\ntechniques. Our findings not only highlight the effectiveness of our method but\nalso pave the way for potential advancements. Crucially, our research indicates\nthat it is feasible to extract meaningful fingerprint representations from\ndegraded images without relying on enhanced samples.",
        "translated": ""
    },
    {
        "title": "Binary Opacity Grids: Capturing Fine Geometric Detail for Mesh-Based\n  View Synthesis",
        "url": "http://arxiv.org/abs/2402.12377v1",
        "pub_date": "2024-02-19",
        "summary": "While surface-based view synthesis algorithms are appealing due to their low\ncomputational requirements, they often struggle to reproduce thin structures.\nIn contrast, more expensive methods that model the scene's geometry as a\nvolumetric density field (e.g. NeRF) excel at reconstructing fine geometric\ndetail. However, density fields often represent geometry in a \"fuzzy\" manner,\nwhich hinders exact localization of the surface. In this work, we modify\ndensity fields to encourage them to converge towards surfaces, without\ncompromising their ability to reconstruct thin structures. First, we employ a\ndiscrete opacity grid representation instead of a continuous density field,\nwhich allows opacity values to discontinuously transition from zero to one at\nthe surface. Second, we anti-alias by casting multiple rays per pixel, which\nallows occlusion boundaries and subpixel structures to be modelled without\nusing semi-transparent voxels. Third, we minimize the binary entropy of the\nopacity values, which facilitates the extraction of surface geometry by\nencouraging opacity values to binarize towards the end of training. Lastly, we\ndevelop a fusion-based meshing strategy followed by mesh simplification and\nappearance model fitting. The compact meshes produced by our model can be\nrendered in real-time on mobile devices and achieve significantly higher view\nsynthesis quality compared to existing mesh-based approaches.",
        "translated": ""
    },
    {
        "title": "FiT: Flexible Vision Transformer for Diffusion Model",
        "url": "http://arxiv.org/abs/2402.12376v1",
        "pub_date": "2024-02-19",
        "summary": "Nature is infinitely resolution-free. In the context of this reality,\nexisting diffusion models, such as Diffusion Transformers, often face\nchallenges when processing image resolutions outside of their trained domain.\nTo overcome this limitation, we present the Flexible Vision Transformer (FiT),\na transformer architecture specifically designed for generating images with\nunrestricted resolutions and aspect ratios. Unlike traditional methods that\nperceive images as static-resolution grids, FiT conceptualizes images as\nsequences of dynamically-sized tokens. This perspective enables a flexible\ntraining strategy that effortlessly adapts to diverse aspect ratios during both\ntraining and inference phases, thus promoting resolution generalization and\neliminating biases induced by image cropping. Enhanced by a meticulously\nadjusted network structure and the integration of training-free extrapolation\ntechniques, FiT exhibits remarkable flexibility in resolution extrapolation\ngeneration. Comprehensive experiments demonstrate the exceptional performance\nof FiT across a broad range of resolutions, showcasing its effectiveness both\nwithin and beyond its training resolution distribution. Repository available at\nhttps://github.com/whlzy/FiT.",
        "translated": ""
    },
    {
        "title": "Robust CLIP: Unsupervised Adversarial Fine-Tuning of Vision Embeddings\n  for Robust Large Vision-Language Models",
        "url": "http://arxiv.org/abs/2402.12336v1",
        "pub_date": "2024-02-19",
        "summary": "Multi-modal foundation models like OpenFlamingo, LLaVA, and GPT-4 are\nincreasingly used for various real-world tasks. Prior work has shown that these\nmodels are highly vulnerable to adversarial attacks on the vision modality.\nThese attacks can be leveraged to spread fake information or defraud users, and\nthus pose a significant risk, which makes the robustness of large multi-modal\nfoundation models a pressing problem. The CLIP model, or one of its variants,\nis used as a frozen vision encoder in many vision-language models (VLMs), e.g.\nLLaVA and OpenFlamingo. We propose an unsupervised adversarial fine-tuning\nscheme to obtain a robust CLIP vision encoder, which yields robustness on all\nvision down-stream tasks (VLMs, zero-shot classification) that rely on CLIP. In\nparticular, we show that stealth-attacks on users of VLMs by a malicious third\nparty providing manipulated images are no longer possible once one replaces the\noriginal CLIP model with our robust one. No retraining or fine-tuning of the\nVLM is required. The code and robust models are available at\nhttps://github.com/chs20/RobustVLM",
        "translated": ""
    },
    {
        "title": "Landmark Stereo Dataset for Landmark Recognition and Moving Node\n  Localization in a Non-GPS Battlefield Environment",
        "url": "http://arxiv.org/abs/2402.12320v1",
        "pub_date": "2024-02-19",
        "summary": "In this paper, we have proposed a new strategy of using the landmark anchor\nnode instead of a radio-based anchor node to obtain the virtual coordinates\n(landmarkID, DISTANCE) of moving troops or defense forces that will help in\ntracking and maneuvering the troops along a safe path within a GPS-denied\nbattlefield environment. The proposed strategy implements landmark recognition\nusing the Yolov5 model and landmark distance estimation using an efficient\nStereo Matching Algorithm. We consider that a moving node carrying a low-power\nmobile device facilitated with a calibrated stereo vision camera that captures\nstereo images of a scene containing landmarks within the battlefield region\nwhose locations are stored in an offline server residing within the device\nitself. We created a custom landmark image dataset called MSTLandmarkv1 with 34\nlandmark classes and another landmark stereo dataset of those 34 landmark\ninstances called MSTLandmarkStereov1. We trained the YOLOv5 model with\nMSTLandmarkv1 dataset and achieved 0.95 mAP @ 0.5 IoU and 0.767 mAP @ [0.5:\n0.95] IoU. We calculated the distance from a node to the landmark utilizing the\nbounding box coordinates and the depth map generated by the improved SGM\nalgorithm using MSTLandmarkStereov1. The tuple of landmark IDs obtained from\nthe detection result and the distances calculated by the SGM algorithm are\nstored as the virtual coordinates of a node. In future work, we will use these\nvirtual coordinates to obtain the location of a node using an efficient\ntrilateration algorithm and optimize the node position using the appropriate\noptimization method.",
        "translated": ""
    },
    {
        "title": "UncertaintyTrack: Exploiting Detection and Localization Uncertainty in\n  Multi-Object Tracking",
        "url": "http://arxiv.org/abs/2402.12303v1",
        "pub_date": "2024-02-19",
        "summary": "Multi-object tracking (MOT) methods have seen a significant boost in\nperformance recently, due to strong interest from the research community and\nsteadily improving object detection methods. The majority of tracking methods\nfollow the tracking-by-detection (TBD) paradigm, blindly trust the incoming\ndetections with no sense of their associated localization uncertainty. This\nlack of uncertainty awareness poses a problem in safety-critical tasks such as\nautonomous driving where passengers could be put at risk due to erroneous\ndetections that have propagated to downstream tasks, including MOT. While there\nare existing works in probabilistic object detection that predict the\nlocalization uncertainty around the boxes, no work in 2D MOT for autonomous\ndriving has studied whether these estimates are meaningful enough to be\nleveraged effectively in object tracking. We introduce UncertaintyTrack, a\ncollection of extensions that can be applied to multiple TBD trackers to\naccount for localization uncertainty estimates from probabilistic object\ndetectors. Experiments on the Berkeley Deep Drive MOT dataset show that the\ncombination of our method and informative uncertainty estimates reduces the\nnumber of ID switches by around 19\\% and improves mMOTA by 2-3%. The source\ncode is available at https://github.com/TRAILab/UncertaintyTrack",
        "translated": ""
    },
    {
        "title": "Regularization by denoising: Bayesian model and Langevin-within-split\n  Gibbs sampling",
        "url": "http://arxiv.org/abs/2402.12292v1",
        "pub_date": "2024-02-19",
        "summary": "This paper introduces a Bayesian framework for image inversion by deriving a\nprobabilistic counterpart to the regularization-by-denoising (RED) paradigm. It\nadditionally implements a Monte Carlo algorithm specifically tailored for\nsampling from the resulting posterior distribution, based on an asymptotically\nexact data augmentation (AXDA). The proposed algorithm is an approximate\ninstance of split Gibbs sampling (SGS) which embeds one Langevin Monte Carlo\nstep. The proposed method is applied to common imaging tasks such as\ndeblurring, inpainting and super-resolution, demonstrating its efficacy through\nextensive numerical experiments. These contributions advance Bayesian inference\nin imaging by leveraging data-driven regularization strategies within a\nprobabilistic framework.",
        "translated": ""
    },
    {
        "title": "DriveVLM: The Convergence of Autonomous Driving and Large\n  Vision-Language Models",
        "url": "http://arxiv.org/abs/2402.12289v1",
        "pub_date": "2024-02-19",
        "summary": "A primary hurdle of autonomous driving in urban environments is understanding\ncomplex and long-tail scenarios, such as challenging road conditions and\ndelicate human behaviors. We introduce DriveVLM, an autonomous driving system\nleveraging Vision-Language Models (VLMs) for enhanced scene understanding and\nplanning capabilities. DriveVLM integrates a unique combination of\nchain-of-thought (CoT) modules for scene description, scene analysis, and\nhierarchical planning. Furthermore, recognizing the limitations of VLMs in\nspatial reasoning and heavy computational requirements, we propose\nDriveVLM-Dual, a hybrid system that synergizes the strengths of DriveVLM with\nthe traditional autonomous driving pipeline. DriveVLM-Dual achieves robust\nspatial understanding and real-time inference speed. Extensive experiments on\nboth the nuScenes dataset and our SUP-AD dataset demonstrate the effectiveness\nof DriveVLM and the enhanced performance of DriveVLM-Dual, surpassing existing\nmethods in complex and unpredictable driving conditions.",
        "translated": ""
    },
    {
        "title": "Open3DSG: Open-Vocabulary 3D Scene Graphs from Point Clouds with\n  Queryable Objects and Open-Set Relationships",
        "url": "http://arxiv.org/abs/2402.12259v1",
        "pub_date": "2024-02-19",
        "summary": "Current approaches for 3D scene graph prediction rely on labeled datasets to\ntrain models for a fixed set of known object classes and relationship\ncategories. We present Open3DSG, an alternative approach to learn 3D scene\ngraph prediction in an open world without requiring labeled scene graph data.\nWe co-embed the features from a 3D scene graph prediction backbone with the\nfeature space of powerful open world 2D vision language foundation models. This\nenables us to predict 3D scene graphs from 3D point clouds in a zero-shot\nmanner by querying object classes from an open vocabulary and predicting the\ninter-object relationships from a grounded LLM with scene graph features and\nqueried object classes as context. Open3DSG is the first 3D point cloud method\nto predict not only explicit open-vocabulary object classes, but also open-set\nrelationships that are not limited to a predefined label set, making it\npossible to express rare as well as specific objects and relationships in the\npredicted 3D scene graph. Our experiments show that Open3DSG is effective at\npredicting arbitrary object classes as well as their complex inter-object\nrelationships describing spatial, supportive, semantic and comparative\nrelationships.",
        "translated": ""
    },
    {
        "title": "Mixed Gaussian Flow for Diverse Trajectory Prediction",
        "url": "http://arxiv.org/abs/2402.12238v1",
        "pub_date": "2024-02-19",
        "summary": "Existing trajectory prediction studies intensively leverage generative\nmodels. Normalizing flow is one of the genres with the advantage of being\ninvertible to derive the probability density of predicted trajectories.\nHowever, mapping from a standard Gaussian by a flow-based model hurts the\ncapacity to capture complicated patterns of trajectories, ignoring the\nunder-represented motion intentions in the training data. To solve the problem,\nwe propose a flow-based model to transform a mixed Gaussian prior into the\nfuture trajectory manifold. The model shows a better capacity for generating\ndiverse trajectory patterns. Also, by associating each sub-Gaussian with a\ncertain subspace of trajectories, we can generate future trajectories with\ncontrollable motion intentions. In such a fashion, the flow-based model is not\nencouraged to simply seek the most likelihood of the intended manifold anymore\nbut a family of controlled manifolds with explicit interpretability. Our\nproposed method is demonstrated to show state-of-the-art performance in the\nquantitative evaluation of sampling well-aligned trajectories in top-M\ngenerated candidates. We also demonstrate that it can generate diverse,\ncontrollable, and out-of-distribution trajectories. Code is available at\nhttps://github.com/mulplue/MGF.",
        "translated": ""
    },
    {
        "title": "AnyGPT: Unified Multimodal LLM with Discrete Sequence Modeling",
        "url": "http://arxiv.org/abs/2402.12226v1",
        "pub_date": "2024-02-19",
        "summary": "We introduce AnyGPT, an any-to-any multimodal language model that utilizes\ndiscrete representations for the unified processing of various modalities,\nincluding speech, text, images, and music. AnyGPT can be trained stably without\nany alterations to the current large language model (LLM) architecture or\ntraining paradigms. Instead, it relies exclusively on data-level preprocessing,\nfacilitating the seamless integration of new modalities into LLMs, akin to the\nincorporation of new languages. We build a multimodal text-centric dataset for\nmultimodal alignment pre-training. Utilizing generative models, we synthesize\nthe first large-scale any-to-any multimodal instruction dataset. It consists of\n108k samples of multi-turn conversations that intricately interweave various\nmodalities, thus equipping the model to handle arbitrary combinations of\nmultimodal inputs and outputs. Experimental results demonstrate that AnyGPT is\ncapable of facilitating any-to-any multimodal conversation while achieving\nperformance comparable to specialized models across all modalities, proving\nthat discrete representations can effectively and conveniently unify multiple\nmodalities within a language model. Demos are shown in\nhttps://junzhan2000.github.io/AnyGPT.github.io/",
        "translated": ""
    },
    {
        "title": "How NeRFs and 3D Gaussian Splatting are Reshaping SLAM: a Survey",
        "url": "http://arxiv.org/abs/2402.13255v1",
        "pub_date": "2024-02-20",
        "summary": "Over the past two decades, research in the field of Simultaneous Localization\nand Mapping (SLAM) has undergone a significant evolution, highlighting its\ncritical role in enabling autonomous exploration of unknown environments. This\nevolution ranges from hand-crafted methods, through the era of deep learning,\nto more recent developments focused on Neural Radiance Fields (NeRFs) and 3D\nGaussian Splatting (3DGS) representations. Recognizing the growing body of\nresearch and the absence of a comprehensive survey on the topic, this paper\naims to provide the first comprehensive overview of SLAM progress through the\nlens of the latest advancements in radiance fields. It sheds light on the\nbackground, evolutionary path, inherent strengths and limitations, and serves\nas a fundamental reference to highlight the dynamic progress and specific\nchallenges.",
        "translated": ""
    },
    {
        "title": "CounterCurate: Enhancing Physical and Semantic Visio-Linguistic\n  Compositional Reasoning via Counterfactual Examples",
        "url": "http://arxiv.org/abs/2402.13254v1",
        "pub_date": "2024-02-20",
        "summary": "We propose CounterCurate, a framework to comprehensively improve the\nvisio-linguistic compositional reasoning capability for both contrastive and\ngenerative multimodal models. In particular, we identify two under-explored\ncritical problems: the neglect of the physically grounded reasoning (counting\nand position understanding) and the potential of using highly capable text and\nimage generation models for semantic counterfactual fine-tuning. Our work\npioneers an approach that addresses these gaps. We first spotlight the\nnear-chance performance of multimodal models like CLIP and LLaVA in physically\ngrounded compositional reasoning. We then apply simple data augmentation using\na grounded image generation model, GLIGEN, to generate finetuning data,\nresulting in significant performance improvements: +33% and +37% for CLIP and\nLLaVA, respectively, on our newly curated Flickr30k-Positions benchmark.\nMoreover, we exploit the capabilities of high-performing text generation and\nimage generation models, specifically GPT-4V and DALLE-3, to curate challenging\nsemantic counterfactuals, thereby further enhancing compositional reasoning\ncapabilities on benchmarks such as SugarCrepe, where CounterCurate outperforms\nGPT-4V.",
        "translated": ""
    },
    {
        "title": "Improving Robustness for Joint Optimization of Camera Poses and\n  Decomposed Low-Rank Tensorial Radiance Fields",
        "url": "http://arxiv.org/abs/2402.13252v1",
        "pub_date": "2024-02-20",
        "summary": "In this paper, we propose an algorithm that allows joint refinement of camera\npose and scene geometry represented by decomposed low-rank tensor, using only\n2D images as supervision. First, we conduct a pilot study based on a 1D signal\nand relate our findings to 3D scenarios, where the naive joint pose\noptimization on voxel-based NeRFs can easily lead to sub-optimal solutions.\nMoreover, based on the analysis of the frequency spectrum, we propose to apply\nconvolutional Gaussian filters on 2D and 3D radiance fields for a\ncoarse-to-fine training schedule that enables joint camera pose optimization.\nLeveraging the decomposition property in decomposed low-rank tensor, our method\nachieves an equivalent effect to brute-force 3D convolution with only incurring\nlittle computational overhead. To further improve the robustness and stability\nof joint optimization, we also propose techniques of smoothed 2D supervision,\nrandomly scaled kernel parameters, and edge-guided loss mask. Extensive\nquantitative and qualitative evaluations demonstrate that our proposed\nframework achieves superior performance in novel view synthesis as well as\nrapid convergence for optimization.",
        "translated": ""
    },
    {
        "title": "FlashTex: Fast Relightable Mesh Texturing with LightControlNet",
        "url": "http://arxiv.org/abs/2402.13251v1",
        "pub_date": "2024-02-20",
        "summary": "Manually creating textures for 3D meshes is time-consuming, even for expert\nvisual content creators. We propose a fast approach for automatically texturing\nan input 3D mesh based on a user-provided text prompt. Importantly, our\napproach disentangles lighting from surface material/reflectance in the\nresulting texture so that the mesh can be properly relit and rendered in any\nlighting environment. We introduce LightControlNet, a new text-to-image model\nbased on the ControlNet architecture, which allows the specification of the\ndesired lighting as a conditioning image to the model. Our text-to-texture\npipeline then constructs the texture in two stages. The first stage produces a\nsparse set of visually consistent reference views of the mesh using\nLightControlNet. The second stage applies a texture optimization based on Score\nDistillation Sampling (SDS) that works with LightControlNet to increase the\ntexture quality while disentangling surface material from lighting. Our\npipeline is significantly faster than previous text-to-texture methods, while\nproducing high-quality and relightable textures.",
        "translated": ""
    },
    {
        "title": "Video ReCap: Recursive Captioning of Hour-Long Videos",
        "url": "http://arxiv.org/abs/2402.13250v1",
        "pub_date": "2024-02-20",
        "summary": "Most video captioning models are designed to process short video clips of few\nseconds and output text describing low-level visual concepts (e.g., objects,\nscenes, atomic actions). However, most real-world videos last for minutes or\nhours and have a complex hierarchical structure spanning different temporal\ngranularities. We propose Video ReCap, a recursive video captioning model that\ncan process video inputs of dramatically different lengths (from 1 second to 2\nhours) and output video captions at multiple hierarchy levels. The recursive\nvideo-language architecture exploits the synergy between different video\nhierarchies and can process hour-long videos efficiently. We utilize a\ncurriculum learning training scheme to learn the hierarchical structure of\nvideos, starting from clip-level captions describing atomic actions, then\nfocusing on segment-level descriptions, and concluding with generating\nsummaries for hour-long videos. Furthermore, we introduce Ego4D-HCap dataset by\naugmenting Ego4D with 8,267 manually collected long-range video summaries. Our\nrecursive model can flexibly generate captions at different hierarchy levels\nwhile also being useful for other complex video understanding tasks, such as\nVideoQA on EgoSchema. Data, code, and models are available at:\nhttps://sites.google.com/view/vidrecap",
        "translated": ""
    },
    {
        "title": "VADv2: End-to-End Vectorized Autonomous Driving via Probabilistic\n  Planning",
        "url": "http://arxiv.org/abs/2402.13243v1",
        "pub_date": "2024-02-20",
        "summary": "Learning a human-like driving policy from large-scale driving demonstrations\nis promising, but the uncertainty and non-deterministic nature of planning make\nit challenging. In this work, to cope with the uncertainty problem, we propose\nVADv2, an end-to-end driving model based on probabilistic planning. VADv2 takes\nmulti-view image sequences as input in a streaming manner, transforms sensor\ndata into environmental token embeddings, outputs the probabilistic\ndistribution of action, and samples one action to control the vehicle. Only\nwith camera sensors, VADv2 achieves state-of-the-art closed-loop performance on\nthe CARLA Town05 benchmark, significantly outperforming all existing methods.\nIt runs stably in a fully end-to-end manner, even without the rule-based\nwrapper. Closed-loop demos are presented at https://hgao-cv.github.io/VADv2.",
        "translated": ""
    },
    {
        "title": "A Touch, Vision, and Language Dataset for Multimodal Alignment",
        "url": "http://arxiv.org/abs/2402.13232v1",
        "pub_date": "2024-02-20",
        "summary": "Touch is an important sensing modality for humans, but it has not yet been\nincorporated into a multimodal generative language model. This is partially due\nto the difficulty of obtaining natural language labels for tactile data and the\ncomplexity of aligning tactile readings with both visual observations and\nlanguage descriptions. As a step towards bridging that gap, this work\nintroduces a new dataset of 44K in-the-wild vision-touch pairs, with English\nlanguage labels annotated by humans (10%) and textual pseudo-labels from GPT-4V\n(90%). We use this dataset to train a vision-language-aligned tactile encoder\nfor open-vocabulary classification and a touch-vision-language (TVL) model for\ntext generation using the trained encoder. Results suggest that by\nincorporating touch, the TVL model improves (+29% classification accuracy)\ntouch-vision-language alignment over existing models trained on any pair of\nthose modalities. Although only a small fraction of the dataset is\nhuman-labeled, the TVL model demonstrates improved visual-tactile understanding\nover GPT-4V (+12%) and open-source vision-language models (+32%) on a new\ntouch-vision understanding benchmark. Code and data:\nhttps://tactile-vlm.github.io.",
        "translated": ""
    },
    {
        "title": "How Easy is It to Fool Your Multimodal LLMs? An Empirical Analysis on\n  Deceptive Prompts",
        "url": "http://arxiv.org/abs/2402.13220v1",
        "pub_date": "2024-02-20",
        "summary": "The remarkable advancements in Multimodal Large Language Models (MLLMs) have\nnot rendered them immune to challenges, particularly in the context of handling\ndeceptive information in prompts, thus producing hallucinated responses under\nsuch conditions. To quantitatively assess this vulnerability, we present\nMAD-Bench, a carefully curated benchmark that contains 850 test samples divided\ninto 6 categories, such as non-existent objects, count of objects, spatial\nrelationship, and visual confusion. We provide a comprehensive analysis of\npopular MLLMs, ranging from GPT-4V, Gemini-Pro, to open-sourced models, such as\nLLaVA-1.5 and CogVLM. Empirically, we observe significant performance gaps\nbetween GPT-4V and other models; and previous robust instruction-tuned models,\nsuch as LRV-Instruction and LLaVA-RLHF, are not effective on this new\nbenchmark. While GPT-4V achieves 75.02% accuracy on MAD-Bench, the accuracy of\nany other model in our experiments ranges from 5% to 35%. We further propose a\nremedy that adds an additional paragraph to the deceptive prompts to encourage\nmodels to think twice before answering the question. Surprisingly, this simple\nmethod can even double the accuracy; however, the absolute numbers are still\ntoo low to be satisfactory. We hope MAD-Bench can serve as a valuable benchmark\nto stimulate further research to enhance models' resilience against deceptive\nprompts.",
        "translated": ""
    },
    {
        "title": "VideoPrism: A Foundational Visual Encoder for Video Understanding",
        "url": "http://arxiv.org/abs/2402.13217v1",
        "pub_date": "2024-02-20",
        "summary": "We introduce VideoPrism, a general-purpose video encoder that tackles diverse\nvideo understanding tasks with a single frozen model. We pretrain VideoPrism on\na heterogeneous corpus containing 36M high-quality video-caption pairs and 582M\nvideo clips with noisy parallel text (e.g., ASR transcripts). The pretraining\napproach improves upon masked autoencoding by global-local distillation of\nsemantic video embeddings and a token shuffling scheme, enabling VideoPrism to\nfocus primarily on the video modality while leveraging the invaluable text\nassociated with videos. We extensively test VideoPrism on four broad groups of\nvideo understanding tasks, from web video question answering to CV for science,\nachieving state-of-the-art performance on 30 out of 33 video understanding\nbenchmarks.",
        "translated": ""
    },
    {
        "title": "Design and Flight Demonstration of a Quadrotor for Urban Mapping and\n  Target Tracking Research",
        "url": "http://arxiv.org/abs/2402.13195v1",
        "pub_date": "2024-02-20",
        "summary": "This paper describes the hardware design and flight demonstration of a small\nquadrotor with imaging sensors for urban mapping, hazard avoidance, and target\ntracking research. The vehicle is equipped with five cameras, including two\npairs of fisheye stereo cameras that enable a nearly omnidirectional view and a\ntwo-axis gimbaled camera. An onboard NVIDIA Jetson Orin Nano computer running\nthe Robot Operating System software is used for data collection. An autonomous\ntracking behavior was implemented to coordinate the motion of the quadrotor and\ngimbaled camera to track a moving GPS coordinate. The data collection system\nwas demonstrated through a flight test that tracked a moving GPS-tagged vehicle\nthrough a series of roads and parking lots. A map of the environment was\nreconstructed from the collected images using the Direct Sparse Odometry (DSO)\nalgorithm. The performance of the quadrotor was also characterized by acoustic\nnoise, communication range, battery voltage in hover, and maximum speed tests.",
        "translated": ""
    },
    {
        "title": "Corrective Machine Unlearning",
        "url": "http://arxiv.org/abs/2402.14015v1",
        "pub_date": "2024-02-21",
        "summary": "Machine Learning models increasingly face data integrity challenges due to\nthe use of large-scale training datasets drawn from the internet. We study what\nmodel developers can do if they detect that some data was manipulated or\nincorrect. Such manipulated data can cause adverse effects like vulnerability\nto backdoored samples, systematic biases, and in general, reduced accuracy on\ncertain input domains. Often, all manipulated training samples are not known,\nand only a small, representative subset of the affected data is flagged.\n  We formalize \"Corrective Machine Unlearning\" as the problem of mitigating the\nimpact of data affected by unknown manipulations on a trained model, possibly\nknowing only a subset of impacted samples. We demonstrate that the problem of\ncorrective unlearning has significantly different requirements from traditional\nprivacy-oriented unlearning. We find most existing unlearning methods,\nincluding the gold-standard retraining-from-scratch, require most of the\nmanipulated data to be identified for effective corrective unlearning. However,\none approach, SSD, achieves limited success in unlearning adverse effects with\njust a small portion of the manipulated samples, showing the tractability of\nthis setting. We hope our work spurs research towards developing better methods\nfor corrective unlearning and offers practitioners a new strategy to handle\ndata integrity challenges arising from web-scale training.",
        "translated": ""
    },
    {
        "title": "Geometry-Informed Neural Networks",
        "url": "http://arxiv.org/abs/2402.14009v1",
        "pub_date": "2024-02-21",
        "summary": "We introduce the concept of geometry-informed neural networks (GINNs), which\nencompass (i) learning under geometric constraints, (ii) neural fields as a\nsuitable representation, and (iii) generating diverse solutions to\nunder-determined systems often encountered in geometric tasks. Notably, the\nGINN formulation does not require training data, and as such can be considered\ngenerative modeling driven purely by constraints. We add an explicit diversity\nloss to mitigate mode collapse. We consider several constraints, in particular,\nthe connectedness of components which we convert to a differentiable loss\nthrough Morse theory. Experimentally, we demonstrate the efficacy of the GINN\nlearning paradigm across a range of two and three-dimensional scenarios with\nincreasing levels of complexity.",
        "translated": ""
    },
    {
        "title": "Real-time 3D-aware Portrait Editing from a Single Image",
        "url": "http://arxiv.org/abs/2402.14000v1",
        "pub_date": "2024-02-21",
        "summary": "This work presents 3DPE, a practical tool that can efficiently edit a face\nimage following given prompts, like reference images or text descriptions, in\nthe 3D-aware manner. To this end, a lightweight module is distilled from a 3D\nportrait generator and a text-to-image model, which provide prior knowledge of\nface geometry and open-vocabulary editing capability, respectively. Such a\ndesign brings two compelling advantages over existing approaches. First, our\nsystem achieves real-time editing with a feedforward network (i.e., ~0.04s per\nimage), over 100x faster than the second competitor. Second, thanks to the\npowerful priors, our module could focus on the learning of editing-related\nvariations, such that it manages to handle various types of editing\nsimultaneously in the training phase and further supports fast adaptation to\nuser-specified novel types of editing during inference (e.g., with ~5min\nfine-tuning per case). The code, the model, and the interface will be made\npublicly available to facilitate future research.",
        "translated": ""
    },
    {
        "title": "BEE-NET: A deep neural network to identify in-the-wild Bodily Expression\n  of Emotions",
        "url": "http://arxiv.org/abs/2402.13955v1",
        "pub_date": "2024-02-21",
        "summary": "In this study, we investigate how environmental factors, specifically the\nscenes and objects involved, can affect the expression of emotions through body\nlanguage. To this end, we introduce a novel multi-stream deep convolutional\nneural network named BEE-NET. We also propose a new late fusion strategy that\nincorporates meta-information on places and objects as prior knowledge in the\nlearning process. Our proposed probabilistic pooling model leverages this\ninformation to generate a joint probability distribution of both available and\nanticipated non-available contextual information in latent space. Importantly,\nour fusion strategy is differentiable, allowing for end-to-end training and\ncapturing of hidden associations among data points without requiring further\npost-processing or regularisation. To evaluate our deep model, we use the Body\nLanguage Database (BoLD), which is currently the largest available database for\nthe Automatic Identification of the in-the-wild Bodily Expression of Emotions\n(AIBEE). Our experimental results demonstrate that our proposed approach\nsurpasses the current state-of-the-art in AIBEE by a margin of 2.07%, achieving\nan Emotional Recognition Score of 66.33%.",
        "translated": ""
    },
    {
        "title": "Distinctive Image Captioning: Leveraging Ground Truth Captions in CLIP\n  Guided Reinforcement Learning",
        "url": "http://arxiv.org/abs/2402.13936v1",
        "pub_date": "2024-02-21",
        "summary": "Training image captioning models using teacher forcing results in very\ngeneric samples, whereas more distinctive captions can be very useful in\nretrieval applications or to produce alternative texts describing images for\naccessibility. Reinforcement Learning (RL) allows to use cross-modal retrieval\nsimilarity score between the generated caption and the input image as reward to\nguide the training, leading to more distinctive captions. Recent studies show\nthat pre-trained cross-modal retrieval models can be used to provide this\nreward, completely eliminating the need for reference captions. However, we\nargue in this paper that Ground Truth (GT) captions can still be useful in this\nRL framework. We propose a new image captioning model training strategy that\nmakes use of GT captions in different ways. Firstly, they can be used to train\na simple MLP discriminator that serves as a regularization to prevent reward\nhacking and ensures the fluency of generated captions, resulting in a textual\nGAN setup extended for multimodal inputs. Secondly, they can serve as\nadditional trajectories in the RL strategy, resulting in a teacher forcing loss\nweighted by the similarity of the GT to the image. This objective acts as an\nadditional learning signal grounded to the distribution of the GT captions.\nThirdly, they can serve as strong baselines when added to the pool of captions\nused to compute the proposed contrastive reward to reduce the variance of\ngradient estimate. Experiments on MS-COCO demonstrate the interest of the\nproposed training strategy to produce highly distinctive captions while\nmaintaining high writing quality.",
        "translated": ""
    },
    {
        "title": "Tumor segmentation on whole slide images: training or prompting?",
        "url": "http://arxiv.org/abs/2402.13932v1",
        "pub_date": "2024-02-21",
        "summary": "Tumor segmentation stands as a pivotal task in cancer diagnosis. Given the\nimmense dimensions of whole slide images (WSI) in histology, deep learning\napproaches for WSI classification mainly operate at patch-wise or\nsuperpixel-wise level. However, these solutions often struggle to capture\nglobal WSI information and cannot directly generate the binary mask.\nDownsampling the WSI and performing semantic segmentation is another possible\napproach. While this method offers computational efficiency, it necessitates a\nlarge amount of annotated data since resolution reduction may lead to\ninformation loss. Visual prompting is a novel paradigm that allows the model to\nperform new tasks by making subtle modifications to the input space, rather\nthan adapting the model itself. Such approach has demonstrated promising\nresults on many computer vision tasks. In this paper, we show the efficacy of\nvisual prompting in the context of tumor segmentation for three distinct\norgans. In comparison to classical methods trained for this specific task, our\nfindings reveal that, with appropriate prompt examples, visual prompting can\nachieve comparable or better performance without extensive fine-tuning.",
        "translated": ""
    },
    {
        "title": "SDXL-Lightning: Progressive Adversarial Diffusion Distillation",
        "url": "http://arxiv.org/abs/2402.13929v1",
        "pub_date": "2024-02-21",
        "summary": "We propose a diffusion distillation method that achieves new state-of-the-art\nin one-step/few-step 1024px text-to-image generation based on SDXL. Our method\ncombines progressive and adversarial distillation to achieve a balance between\nquality and mode coverage. In this paper, we discuss the theoretical analysis,\ndiscriminator design, model formulation, and training techniques. We\nopen-source our distilled SDXL-Lightning models both as LoRA and full UNet\nweights.",
        "translated": ""
    },
    {
        "title": "BenchCloudVision: A Benchmark Analysis of Deep Learning Approaches for\n  Cloud Detection and Segmentation in Remote Sensing Imagery",
        "url": "http://arxiv.org/abs/2402.13918v1",
        "pub_date": "2024-02-21",
        "summary": "Satellites equipped with optical sensors capture high-resolution imagery,\nproviding valuable insights into various environmental phenomena. In recent\nyears, there has been a surge of research focused on addressing some challenges\nin remote sensing, ranging from water detection in diverse landscapes to the\nsegmentation of mountainous and terrains. Ongoing investigations goals to\nenhance the precision and efficiency of satellite imagery analysis. Especially,\nthere is a growing emphasis on developing methodologies for accurate water body\ndetection, snow and clouds, important for environmental monitoring, resource\nmanagement, and disaster response. Within this context, this paper focus on the\ncloud segmentation from remote sensing imagery. Accurate remote sensing data\nanalysis can be challenging due to the presence of clouds in optical\nsensor-based applications. The quality of resulting products such as\napplications and research is directly impacted by cloud detection, which plays\na key role in the remote sensing data processing pipeline. This paper examines\nseven cutting-edge semantic segmentation and detection algorithms applied to\nclouds identification, conducting a benchmark analysis to evaluate their\narchitectural approaches and identify the most performing ones. To increase the\nmodel's adaptability, critical elements including the type of imagery and the\namount of spectral bands used during training are analyzed. Additionally, this\nresearch tries to produce machine learning algorithms that can perform cloud\nsegmentation using only a few spectral bands, including RGB and RGBN-IR\ncombinations. The model's flexibility for a variety of applications and user\nscenarios is assessed by using imagery from Sentinel-2 and Landsat-8 as\ndatasets. This benchmark can be reproduced using the material from this github\nlink: \\url{https://github.com/toelt-llc/cloud\\_segmentation\\_comparative}.",
        "translated": ""
    },
    {
        "title": "Scene Prior Filtering for Depth Map Super-Resolution",
        "url": "http://arxiv.org/abs/2402.13876v1",
        "pub_date": "2024-02-21",
        "summary": "Multi-modal fusion is vital to the success of super-resolution of depth\nimages. However, commonly used fusion strategies, such as addition and\nconcatenation, fall short of effectively bridging the modal gap. As a result,\nguided image filtering methods have been introduced to mitigate this issue.\nNevertheless, it is observed that their filter kernels usually encounter\nsignificant texture interference and edge inaccuracy. To tackle these two\nchallenges, we introduce a Scene Prior Filtering network, SPFNet, which\nutilizes the priors surface normal and semantic map from large-scale models.\nSpecifically, we design an All-in-one Prior Propagation that computes the\nsimilarity between multi-modal scene priors, \\textit{i.e.}, RGB, normal,\nsemantic, and depth, to reduce the texture interference. In addition, we\npresent a One-to-one Prior Embedding that continuously embeds each single-modal\nprior into depth using Mutual Guided Filtering, further alleviating the texture\ninterference while enhancing edges. Our SPFNet has been extensively evaluated\non both real and synthetic datasets, achieving state-of-the-art performance.",
        "translated": ""
    },
    {
        "title": "VL-Trojan: Multimodal Instruction Backdoor Attacks against\n  Autoregressive Visual Language Models",
        "url": "http://arxiv.org/abs/2402.13851v1",
        "pub_date": "2024-02-21",
        "summary": "Autoregressive Visual Language Models (VLMs) showcase impressive few-shot\nlearning capabilities in a multimodal context. Recently, multimodal instruction\ntuning has been proposed to further enhance instruction-following abilities.\nHowever, we uncover the potential threat posed by backdoor attacks on\nautoregressive VLMs during instruction tuning. Adversaries can implant a\nbackdoor by injecting poisoned samples with triggers embedded in instructions\nor images, enabling malicious manipulation of the victim model's predictions\nwith predefined triggers. Nevertheless, the frozen visual encoder in\nautoregressive VLMs imposes constraints on the learning of conventional image\ntriggers. Additionally, adversaries may encounter restrictions in accessing the\nparameters and architectures of the victim model. To address these challenges,\nwe propose a multimodal instruction backdoor attack, namely VL-Trojan. Our\napproach facilitates image trigger learning through an isolating and clustering\nstrategy and enhance black-box-attack efficacy via an iterative character-level\ntext trigger generation method. Our attack successfully induces target outputs\nduring inference, significantly surpassing baselines (+62.52\\%) in ASR.\nMoreover, it demonstrates robustness across various model scales and few-shot\nin-context reasoning scenarios.",
        "translated": ""
    },
    {
        "title": "PALO: A Polyglot Large Multimodal Model for 5B People",
        "url": "http://arxiv.org/abs/2402.14818v1",
        "pub_date": "2024-02-22",
        "summary": "In pursuit of more inclusive Vision-Language Models (VLMs), this study\nintroduces a Large Multilingual Multimodal Model called \\textsc{Palo}.\n\\textsc{Palo} offers visual reasoning capabilities in 10 major languages,\nincluding English, Chinese, Hindi, Spanish, French, Arabic, Bengali, Russian,\nUrdu, and Japanese, that span a total of $\\sim$5B people (65\\% of the world\npopulation). Our approach involves a semi-automated translation approach to\nadapt the multimodal instruction dataset from English to the target languages\nusing a fine-tuned Large Language Model, thereby ensuring high linguistic\nfidelity while allowing scalability due to minimal manual effort. The\nincorporation of diverse instruction sets helps us boost overall performance\nacross multiple languages especially those that are underrepresented like\nHindi, Arabic, Bengali, and Urdu. The resulting models are trained across three\nscales (1.7B, 7B and 13B parameters) to show the generalization and scalability\nwhere we observe substantial improvements compared to strong baselines. We also\npropose the first multilingual multimodal benchmark for the forthcoming\napproaches to evaluate their vision-language reasoning capabilities across\nlanguages. Code: https://github.com/mbzuai-oryx/PALO.",
        "translated": ""
    },
    {
        "title": "Cameras as Rays: Pose Estimation via Ray Diffusion",
        "url": "http://arxiv.org/abs/2402.14817v1",
        "pub_date": "2024-02-22",
        "summary": "Estimating camera poses is a fundamental task for 3D reconstruction and\nremains challenging given sparse views (&lt;10). In contrast to existing\napproaches that pursue top-down prediction of global parametrizations of camera\nextrinsics, we propose a distributed representation of camera pose that treats\na camera as a bundle of rays. This representation allows for a tight coupling\nwith spatial image features improving pose precision. We observe that this\nrepresentation is naturally suited for set-level level transformers and develop\na regression-based approach that maps image patches to corresponding rays. To\ncapture the inherent uncertainties in sparse-view pose inference, we adapt this\napproach to learn a denoising diffusion model which allows us to sample\nplausible modes while improving performance. Our proposed methods, both\nregression- and diffusion-based, demonstrate state-of-the-art performance on\ncamera pose estimation on CO3D while generalizing to unseen object categories\nand in-the-wild captures.",
        "translated": ""
    },
    {
        "title": "Demographic Bias of Expert-Level Vision-Language Foundation Models in\n  Medical Imaging",
        "url": "http://arxiv.org/abs/2402.14815v1",
        "pub_date": "2024-02-22",
        "summary": "Advances in artificial intelligence (AI) have achieved expert-level\nperformance in medical imaging applications. Notably, self-supervised\nvision-language foundation models can detect a broad spectrum of pathologies\nwithout relying on explicit training annotations. However, it is crucial to\nensure that these AI models do not mirror or amplify human biases, thereby\ndisadvantaging historically marginalized groups such as females or Black\npatients. The manifestation of such biases could systematically delay essential\nmedical care for certain patient subgroups. In this study, we investigate the\nalgorithmic fairness of state-of-the-art vision-language foundation models in\nchest X-ray diagnosis across five globally-sourced datasets. Our findings\nreveal that compared to board-certified radiologists, these foundation models\nconsistently underdiagnose marginalized groups, with even higher rates seen in\nintersectional subgroups, such as Black female patients. Such demographic\nbiases present over a wide range of pathologies and demographic attributes.\nFurther analysis of the model embedding uncovers its significant encoding of\ndemographic information. Deploying AI systems with these biases in medical\nimaging can intensify pre-existing care disparities, posing potential\nchallenges to equitable healthcare access and raising ethical questions about\ntheir clinical application.",
        "translated": ""
    },
    {
        "title": "WeakSAM: Segment Anything Meets Weakly-supervised Instance-level\n  Recognition",
        "url": "http://arxiv.org/abs/2402.14812v1",
        "pub_date": "2024-02-22",
        "summary": "Weakly supervised visual recognition using inexact supervision is a critical\nyet challenging learning problem. It significantly reduces human labeling costs\nand traditionally relies on multi-instance learning and pseudo-labeling. This\npaper introduces WeakSAM and solves the weakly-supervised object detection\n(WSOD) and segmentation by utilizing the pre-learned world knowledge contained\nin a vision foundation model, i.e., the Segment Anything Model (SAM). WeakSAM\naddresses two critical limitations in traditional WSOD retraining, i.e., pseudo\nground truth (PGT) incompleteness and noisy PGT instances, through adaptive PGT\ngeneration and Region of Interest (RoI) drop regularization. It also addresses\nthe SAM's problems of requiring prompts and category unawareness for automatic\nobject detection and segmentation. Our results indicate that WeakSAM\nsignificantly surpasses previous state-of-the-art methods in WSOD and WSIS\nbenchmarks with large margins, i.e. average improvements of 7.4% and 8.5%,\nrespectively. The code is available at \\url{https://github.com/hustvl/WeakSAM}.",
        "translated": ""
    },
    {
        "title": "GeneOH Diffusion: Towards Generalizable Hand-Object Interaction\n  Denoising via Denoising Diffusion",
        "url": "http://arxiv.org/abs/2402.14810v1",
        "pub_date": "2024-02-22",
        "summary": "In this work, we tackle the challenging problem of denoising hand-object\ninteractions (HOI). Given an erroneous interaction sequence, the objective is\nto refine the incorrect hand trajectory to remove interaction artifacts for a\nperceptually realistic sequence. This challenge involves intricate interaction\nnoise, including unnatural hand poses and incorrect hand-object relations,\nalongside the necessity for robust generalization to new interactions and\ndiverse noise patterns. We tackle those challenges through a novel approach,\nGeneOH Diffusion, incorporating two key designs: an innovative contact-centric\nHOI representation named GeneOH and a new domain-generalizable denoising\nscheme. The contact-centric representation GeneOH informatively parameterizes\nthe HOI process, facilitating enhanced generalization across various HOI\nscenarios. The new denoising scheme consists of a canonical denoising model\ntrained to project noisy data samples from a whitened noise space to a clean\ndata manifold and a \"denoising via diffusion\" strategy which can handle input\ntrajectories with various noise patterns by first diffusing them to align with\nthe whitened noise space and cleaning via the canonical denoiser. Extensive\nexperiments on four benchmarks with significant domain variations demonstrate\nthe superior effectiveness of our method. GeneOH Diffusion also shows promise\nfor various downstream applications. Project website:\nhttps://meowuu7.github.io/GeneOH-Diffusion/.",
        "translated": ""
    },
    {
        "title": "Measuring Multimodal Mathematical Reasoning with MATH-Vision Dataset",
        "url": "http://arxiv.org/abs/2402.14804v1",
        "pub_date": "2024-02-22",
        "summary": "Recent advancements in Large Multimodal Models (LMMs) have shown promising\nresults in mathematical reasoning within visual contexts, with models\napproaching human-level performance on existing benchmarks such as MathVista.\nHowever, we observe significant limitations in the diversity of questions and\nbreadth of subjects covered by these benchmarks. To address this issue, we\npresent the MATH-Vision (MATH-V) dataset, a meticulously curated collection of\n3,040 high-quality mathematical problems with visual contexts sourced from real\nmath competitions. Spanning 16 distinct mathematical disciplines and graded\nacross 5 levels of difficulty, our dataset provides a comprehensive and diverse\nset of challenges for evaluating the mathematical reasoning abilities of LMMs.\nThrough extensive experimentation, we unveil a notable performance gap between\ncurrent LMMs and human performance on MATH-V, underscoring the imperative for\nfurther advancements in LMMs. Moreover, our detailed categorization allows for\na thorough error analysis of LMMs, offering valuable insights to guide future\nresearch and development. The project is available at\nhttps://mathvision-cuhk.github.io",
        "translated": ""
    },
    {
        "title": "Snap Video: Scaled Spatiotemporal Transformers for Text-to-Video\n  Synthesis",
        "url": "http://arxiv.org/abs/2402.14797v1",
        "pub_date": "2024-02-22",
        "summary": "Contemporary models for generating images show remarkable quality and\nversatility. Swayed by these advantages, the research community repurposes them\nto generate videos. Since video content is highly redundant, we argue that\nnaively bringing advances of image models to the video generation domain\nreduces motion fidelity, visual quality and impairs scalability. In this work,\nwe build Snap Video, a video-first model that systematically addresses these\nchallenges. To do that, we first extend the EDM framework to take into account\nspatially and temporally redundant pixels and naturally support video\ngeneration. Second, we show that a U-Net - a workhorse behind image generation\n- scales poorly when generating videos, requiring significant computational\noverhead. Hence, we propose a new transformer-based architecture that trains\n3.31 times faster than U-Nets (and is ~4.5 faster at inference). This allows us\nto efficiently train a text-to-video model with billions of parameters for the\nfirst time, reach state-of-the-art results on a number of benchmarks, and\ngenerate videos with substantially higher quality, temporal consistency, and\nmotion complexity. The user studies showed that our model was favored by a\nlarge margin over the most recent methods. See our website at\nhttps://snap-research.github.io/snapvideo/.",
        "translated": ""
    },
    {
        "title": "CyberDemo: Augmenting Simulated Human Demonstration for Real-World\n  Dexterous Manipulation",
        "url": "http://arxiv.org/abs/2402.14795v1",
        "pub_date": "2024-02-22",
        "summary": "We introduce CyberDemo, a novel approach to robotic imitation learning that\nleverages simulated human demonstrations for real-world tasks. By incorporating\nextensive data augmentation in a simulated environment, CyberDemo outperforms\ntraditional in-domain real-world demonstrations when transferred to the real\nworld, handling diverse physical and visual conditions. Regardless of its\naffordability and convenience in data collection, CyberDemo outperforms\nbaseline methods in terms of success rates across various tasks and exhibits\ngeneralizability with previously unseen objects. For example, it can rotate\nnovel tetra-valve and penta-valve, despite human demonstrations only involving\ntri-valves. Our research demonstrates the significant potential of simulated\nhuman demonstrations for real-world dexterous manipulation tasks. More details\ncan be found at https://cyber-demo.github.io",
        "translated": ""
    },
    {
        "title": "Consolidating Attention Features for Multi-view Image Editing",
        "url": "http://arxiv.org/abs/2402.14792v1",
        "pub_date": "2024-02-22",
        "summary": "Large-scale text-to-image models enable a wide range of image editing\ntechniques, using text prompts or even spatial controls. However, applying\nthese editing methods to multi-view images depicting a single scene leads to\n3D-inconsistent results. In this work, we focus on spatial control-based\ngeometric manipulations and introduce a method to consolidate the editing\nprocess across various views. We build on two insights: (1) maintaining\nconsistent features throughout the generative process helps attain consistency\nin multi-view editing, and (2) the queries in self-attention layers\nsignificantly influence the image structure. Hence, we propose to improve the\ngeometric consistency of the edited images by enforcing the consistency of the\nqueries. To do so, we introduce QNeRF, a neural radiance field trained on the\ninternal query features of the edited images. Once trained, QNeRF can render\n3D-consistent queries, which are then softly injected back into the\nself-attention layers during generation, greatly improving multi-view\nconsistency. We refine the process through a progressive, iterative method that\nbetter consolidates queries across the diffusion timesteps. We compare our\nmethod to a range of existing techniques and demonstrate that it can achieve\nbetter multi-view consistency and higher fidelity to the input scene. These\nadvantages allow us to train NeRFs with fewer visual artifacts, that are better\naligned with the target geometry.",
        "translated": ""
    },
    {
        "title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models",
        "url": "http://arxiv.org/abs/2402.14780v1",
        "pub_date": "2024-02-22",
        "summary": "Image customization has been extensively studied in text-to-image (T2I)\ndiffusion models, leading to impressive outcomes and applications. With the\nemergence of text-to-video (T2V) diffusion models, its temporal counterpart,\nmotion customization, has not yet been well investigated. To address the\nchallenge of one-shot motion customization, we propose Customize-A-Video that\nmodels the motion from a single reference video and adapting it to new subjects\nand scenes with both spatial and temporal varieties. It leverages low-rank\nadaptation (LoRA) on temporal attention layers to tailor the pre-trained T2V\ndiffusion model for specific motion modeling from the reference videos. To\ndisentangle the spatial and temporal information during the training pipeline,\nwe introduce a novel concept of appearance absorbers that detach the original\nappearance from the single reference video prior to motion learning. Our\nproposed method can be easily extended to various downstream tasks, including\ncustom video generation and editing, video appearance customization, and\nmultiple motion combination, in a plug-and-play fashion. Our project page can\nbe found at https://anonymous-314.github.io.",
        "translated": ""
    },
    {
        "title": "Seamless Human Motion Composition with Blended Positional Encodings",
        "url": "http://arxiv.org/abs/2402.15509v1",
        "pub_date": "2024-02-23",
        "summary": "Conditional human motion generation is an important topic with many\napplications in virtual reality, gaming, and robotics. While prior works have\nfocused on generating motion guided by text, music, or scenes, these typically\nresult in isolated motions confined to short durations. Instead, we address the\ngeneration of long, continuous sequences guided by a series of varying textual\ndescriptions. In this context, we introduce FlowMDM, the first diffusion-based\nmodel that generates seamless Human Motion Compositions (HMC) without any\npostprocessing or redundant denoising steps. For this, we introduce the Blended\nPositional Encodings, a technique that leverages both absolute and relative\npositional encodings in the denoising chain. More specifically, global motion\ncoherence is recovered at the absolute stage, whereas smooth and realistic\ntransitions are built at the relative stage. As a result, we achieve\nstate-of-the-art results in terms of accuracy, realism, and smoothness on the\nBabel and HumanML3D datasets. FlowMDM excels when trained with only a single\ndescription per motion sequence thanks to its Pose-Centric Cross-ATtention,\nwhich makes it robust against varying text descriptions at inference time.\nFinally, to address the limitations of existing HMC metrics, we propose two new\nmetrics: the Peak Jerk and the Area Under the Jerk, to detect abrupt\ntransitions.",
        "translated": ""
    },
    {
        "title": "Co-Supervised Learning: Improving Weak-to-Strong Generalization with\n  Hierarchical Mixture of Experts",
        "url": "http://arxiv.org/abs/2402.15505v1",
        "pub_date": "2024-02-23",
        "summary": "Steering the behavior of a strong model pre-trained on internet-scale data\ncan be difficult due to the scarcity of competent supervisors. Recent studies\nreveal that, despite supervisory noises, a strong student model may surpass its\nweak teacher when fine-tuned on specific objectives. Yet, the effectiveness of\nsuch weak-to-strong generalization remains limited, especially in the presence\nof large capability gaps. In this paper, we propose to address this challenge\nby harnessing a diverse set of specialized teachers, instead of a single\ngeneralist one, that collectively supervises the strong student. Our approach\nresembles the classical hierarchical mixture of experts, with two components\ntailored for co-supervision: (i) we progressively alternate student training\nand teacher assignment, leveraging the growth of the strong student to identify\nplausible supervisions; (ii) we conservatively enforce teacher-student and\nlocal-global consistency, leveraging their dependencies to reject potential\nannotation noises. We validate the proposed method through visual recognition\ntasks on the OpenAI weak-to-strong benchmark and additional multi-domain\ndatasets. Our code is available at \\url{https://github.com/yuejiangliu/csl}.",
        "translated": ""
    },
    {
        "title": "Gen4Gen: Generative Data Pipeline for Generative Multi-Concept\n  Composition",
        "url": "http://arxiv.org/abs/2402.15504v1",
        "pub_date": "2024-02-23",
        "summary": "Recent text-to-image diffusion models are able to learn and synthesize images\ncontaining novel, personalized concepts (e.g., their own pets or specific\nitems) with just a few examples for training. This paper tackles two\ninterconnected issues within this realm of personalizing text-to-image\ndiffusion models. First, current personalization techniques fail to reliably\nextend to multiple concepts -- we hypothesize this to be due to the mismatch\nbetween complex scenes and simple text descriptions in the pre-training dataset\n(e.g., LAION). Second, given an image containing multiple personalized\nconcepts, there lacks a holistic metric that evaluates performance on not just\nthe degree of resemblance of personalized concepts, but also whether all\nconcepts are present in the image and whether the image accurately reflects the\noverall text description. To address these issues, we introduce Gen4Gen, a\nsemi-automated dataset creation pipeline utilizing generative models to combine\npersonalized concepts into complex compositions along with text-descriptions.\nUsing this, we create a dataset called MyCanvas, that can be used to benchmark\nthe task of multi-concept personalization. In addition, we design a\ncomprehensive metric comprising two scores (CP-CLIP and TI-CLIP) for better\nquantifying the performance of multi-concept, personalized text-to-image\ndiffusion methods. We provide a simple baseline built on top of Custom\nDiffusion with empirical prompting strategies for future researchers to\nevaluate on MyCanvas. We show that by improving data quality and prompting\nstrategies, we can significantly increase multi-concept personalized image\ngeneration quality, without requiring any modifications to model architecture\nor training algorithms.",
        "translated": ""
    },
    {
        "title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation",
        "url": "http://arxiv.org/abs/2402.15487v1",
        "pub_date": "2024-02-23",
        "summary": "Robots need to explore their surroundings to adapt to and tackle tasks in\nunknown environments. Prior work has proposed building scene graphs of the\nenvironment but typically assumes that the environment is static, omitting\nregions that require active interactions. This severely limits their ability to\nhandle more complex tasks in household and office environments: before setting\nup a table, robots must explore drawers and cabinets to locate all utensils and\ncondiments. In this work, we introduce the novel task of interactive scene\nexploration, wherein robots autonomously explore environments and produce an\naction-conditioned scene graph (ACSG) that captures the structure of the\nunderlying environment. The ACSG accounts for both low-level information, such\nas geometry and semantics, and high-level information, such as the\naction-conditioned relationships between different entities in the scene. To\nthis end, we present the Robotic Exploration (RoboEXP) system, which\nincorporates the Large Multimodal Model (LMM) and an explicit memory design to\nenhance our system's capabilities. The robot reasons about what and how to\nexplore an object, accumulating new information through the interaction process\nand incrementally constructing the ACSG. We apply our system across various\nreal-world settings in a zero-shot manner, demonstrating its effectiveness in\nexploring and modeling environments it has never seen before. Leveraging the\nconstructed ACSG, we illustrate the effectiveness and efficiency of our RoboEXP\nsystem in facilitating a wide range of real-world manipulation tasks involving\nrigid, articulated objects, nested objects like Matryoshka dolls, and\ndeformable objects like cloth.",
        "translated": ""
    },
    {
        "title": "Retinotopic Mapping Enhances the Robustness of Convolutional Neural\n  Networks",
        "url": "http://arxiv.org/abs/2402.15480v1",
        "pub_date": "2024-02-23",
        "summary": "Foveated vision, a trait shared by many animals, including humans, has not\nbeen fully utilized in machine learning applications, despite its significant\ncontributions to biological visual function. This study investigates whether\nretinotopic mapping, a critical component of foveated vision, can enhance image\ncategorization and localization performance when integrated into deep\nconvolutional neural networks (CNNs). Retinotopic mapping was integrated into\nthe inputs of standard off-the-shelf convolutional neural networks (CNNs),\nwhich were then retrained on the ImageNet task. As expected, the\nlogarithmic-polar mapping improved the network's ability to handle arbitrary\nimage zooms and rotations, particularly for isolated objects. Surprisingly, the\nretinotopically mapped network achieved comparable performance in\nclassification. Furthermore, the network demonstrated improved classification\nlocalization when the foveated center of the transform was shifted. This\nreplicates a crucial ability of the human visual system that is absent in\ntypical convolutional neural networks (CNNs). These findings suggest that\nretinotopic mapping may be fundamental to significant preattentive visual\nprocesses.",
        "translated": ""
    },
    {
        "title": "Benchmarking the Robustness of Panoptic Segmentation for Automated\n  Driving",
        "url": "http://arxiv.org/abs/2402.15469v1",
        "pub_date": "2024-02-23",
        "summary": "Precise situational awareness is required for the safe decision-making of\nassisted and automated driving (AAD) functions. Panoptic segmentation is a\npromising perception technique to identify and categorise objects, impending\nhazards, and driveable space at a pixel level. While segmentation quality is\ngenerally associated with the quality of the camera data, a comprehensive\nunderstanding and modelling of this relationship are paramount for AAD system\ndesigners. Motivated by such a need, this work proposes a unifying pipeline to\nassess the robustness of panoptic segmentation models for AAD, correlating it\nwith traditional image quality. The first step of the proposed pipeline\ninvolves generating degraded camera data that reflects real-world noise\nfactors. To this end, 19 noise factors have been identified and implemented\nwith 3 severity levels. Of these factors, this work proposes novel models for\nunfavourable light and snow. After applying the degradation models, three\nstate-of-the-art CNN- and vision transformers (ViT)-based panoptic segmentation\nnetworks are used to analyse their robustness. The variations of the\nsegmentation performance are then correlated to 8 selected image quality\nmetrics. This research reveals that: 1) certain specific noise factors produce\nthe highest impact on panoptic segmentation, i.e. droplets on lens and Gaussian\nnoise; 2) the ViT-based panoptic segmentation backbones show better robustness\nto the considered noise factors; 3) some image quality metrics (i.e. LPIPS and\nCW-SSIM) correlate strongly with panoptic segmentation performance and\ntherefore they can be used as predictive metrics for network performance.",
        "translated": ""
    },
    {
        "title": "CLIPPER+: A Fast Maximal Clique Algorithm for Robust Global Registration",
        "url": "http://arxiv.org/abs/2402.15464v1",
        "pub_date": "2024-02-23",
        "summary": "We present CLIPPER+, an algorithm for finding maximal cliques in unweighted\ngraphs for outlier-robust global registration. The registration problem can be\nformulated as a graph and solved by finding its maximum clique. This\nformulation leads to extreme robustness to outliers; however, finding the\nmaximum clique is an NP-hard problem, and therefore approximation is required\nin practice for large-size problems. The performance of an approximation\nalgorithm is evaluated by its computational complexity (the lower the runtime,\nthe better) and solution accuracy (how close the solution is to the maximum\nclique). Accordingly, the main contribution of CLIPPER+ is outperforming the\nstate-of-the-art in accuracy while maintaining a relatively low runtime.\nCLIPPER+ builds on prior work (CLIPPER [1] and PMC [2]) and prunes the graph by\nremoving vertices that have a small core number and cannot be a part of the\nmaximum clique. This will result in a smaller graph, on which the maximum\nclique can be estimated considerably faster. We evaluate the performance of\nCLIPPER+ on standard graph benchmarks, as well as synthetic and real-world\npoint cloud registration problems. These evaluations demonstrate that CLIPPER+\nhas the highest accuracy and can register point clouds in scenarios where over\n$99\\%$ of associations are outliers. Our code and evaluation benchmarks are\nreleased at https://github.com/ariarobotics/clipperp.",
        "translated": ""
    },
    {
        "title": "Computer Vision for Multimedia Geolocation in Human Trafficking\n  Investigation: A Systematic Literature Review",
        "url": "http://arxiv.org/abs/2402.15448v1",
        "pub_date": "2024-02-23",
        "summary": "The task of multimedia geolocation is becoming an increasingly essential\ncomponent of the digital forensics toolkit to effectively combat human\ntrafficking, child sexual exploitation, and other illegal acts. Typically,\nmetadata-based geolocation information is stripped when multimedia content is\nshared via instant messaging and social media. The intricacy of geolocating,\ngeotagging, or finding geographical clues in this content is often overly\nburdensome for investigators. Recent research has shown that contemporary\nadvancements in artificial intelligence, specifically computer vision and deep\nlearning, show significant promise towards expediting the multimedia\ngeolocation task. This systematic literature review thoroughly examines the\nstate-of-the-art leveraging computer vision techniques for multimedia\ngeolocation and assesses their potential to expedite human trafficking\ninvestigation. This includes a comprehensive overview of the application of\ncomputer vision-based approaches to multimedia geolocation, identifies their\napplicability in combating human trafficking, and highlights the potential\nimplications of enhanced multimedia geolocation for prosecuting human\ntrafficking. 123 articles inform this systematic literature review. The\nfindings suggest numerous potential paths for future impactful research on the\nsubject.",
        "translated": ""
    },
    {
        "title": "Hierarchical Invariance for Robust and Interpretable Vision Tasks at\n  Larger Scales",
        "url": "http://arxiv.org/abs/2402.15430v1",
        "pub_date": "2024-02-23",
        "summary": "Developing robust and interpretable vision systems is a crucial step towards\ntrustworthy artificial intelligence. In this regard, a promising paradigm\nconsiders embedding task-required invariant structures, e.g., geometric\ninvariance, in the fundamental image representation. However, such invariant\nrepresentations typically exhibit limited discriminability, limiting their\napplications in larger-scale trustworthy vision tasks. For this open problem,\nwe conduct a systematic investigation of hierarchical invariance, exploring\nthis topic from theoretical, practical, and application perspectives. At the\ntheoretical level, we show how to construct over-complete invariants with a\nConvolutional Neural Networks (CNN)-like hierarchical architecture yet in a\nfully interpretable manner. The general blueprint, specific definitions,\ninvariant properties, and numerical implementations are provided. At the\npractical level, we discuss how to customize this theoretical framework into a\ngiven task. With the over-completeness, discriminative features w.r.t. the task\ncan be adaptively formed in a Neural Architecture Search (NAS)-like manner. We\ndemonstrate the above arguments with accuracy, invariance, and efficiency\nresults on texture, digit, and parasite classification experiments.\nFurthermore, at the application level, our representations are explored in\nreal-world forensics tasks on adversarial perturbations and Artificial\nIntelligence Generated Content (AIGC). Such applications reveal that the\nproposed strategy not only realizes the theoretically promised invariance, but\nalso exhibits competitive discriminability even in the era of deep learning.\nFor robust and interpretable vision tasks at larger scales, hierarchical\ninvariant representation can be considered as an effective alternative to\ntraditional CNN and invariants.",
        "translated": ""
    },
    {
        "title": "ProTIP: Probabilistic Robustness Verification on Text-to-Image Diffusion\n  Models against Stochastic Perturbation",
        "url": "http://arxiv.org/abs/2402.15429v1",
        "pub_date": "2024-02-23",
        "summary": "Text-to-Image (T2I) Diffusion Models (DMs) have shown impressive abilities in\ngenerating high-quality images based on simple text descriptions. However, as\nis common with many Deep Learning (DL) models, DMs are subject to a lack of\nrobustness. While there are attempts to evaluate the robustness of T2I DMs as a\nbinary or worst-case problem, they cannot answer how robust in general the\nmodel is whenever an adversarial example (AE) can be found. In this study, we\nfirst introduce a probabilistic notion of T2I DMs' robustness; and then\nestablish an efficient framework, ProTIP, to evaluate it with statistical\nguarantees. The main challenges stem from: i) the high computational cost of\nthe generation process; and ii) determining if a perturbed input is an AE\ninvolves comparing two output distributions, which is fundamentally harder\ncompared to other DL tasks like classification where an AE is identified upon\nmisprediction of labels. To tackle the challenges, we employ sequential\nanalysis with efficacy and futility early stopping rules in the statistical\ntesting for identifying AEs, and adaptive concentration inequalities to\ndynamically determine the \"just-right\" number of stochastic perturbations\nwhenever the verification target is met. Empirical experiments validate the\neffectiveness and efficiency of ProTIP over common T2I DMs. Finally, we\ndemonstrate an application of ProTIP to rank commonly used defence methods.",
        "translated": ""
    },
    {
        "title": "Enhancement of 3D Camera Synthetic Training Data with Noise Models",
        "url": "http://arxiv.org/abs/2402.16514v1",
        "pub_date": "2024-02-26",
        "summary": "The goal of this paper is to assess the impact of noise in 3D camera-captured\ndata by modeling the noise of the imaging process and applying it on synthetic\ntraining data. We compiled a dataset of specifically constructed scenes to\nobtain a noise model. We specifically model lateral noise, affecting the\nposition of captured points in the image plane, and axial noise, affecting the\nposition along the axis perpendicular to the image plane. The estimated models\ncan be used to emulate noise in synthetic training data. The added benefit of\nadding artificial noise is evaluated in an experiment with rendered data for\nobject segmentation. We train a series of neural networks with varying levels\nof noise in the data and measure their ability to generalize on real data. The\nresults show that using too little or too much noise can hurt the networks'\nperformance indicating that obtaining a model of noise from real scanners is\nbeneficial for synthetic data generation.",
        "translated": ""
    },
    {
        "title": "Stochastic Conditional Diffusion Models for Semantic Image Synthesis",
        "url": "http://arxiv.org/abs/2402.16506v1",
        "pub_date": "2024-02-26",
        "summary": "Semantic image synthesis (SIS) is a task to generate realistic images\ncorresponding to semantic maps (labels). It can be applied to diverse\nreal-world practices such as photo editing or content creation. However, in\nreal-world applications, SIS often encounters noisy user inputs. To address\nthis, we propose Stochastic Conditional Diffusion Model (SCDM), which is a\nrobust conditional diffusion model that features novel forward and generation\nprocesses tailored for SIS with noisy labels. It enhances robustness by\nstochastically perturbing the semantic label maps through Label Diffusion,\nwhich diffuses the labels with discrete diffusion. Through the diffusion of\nlabels, the noisy and clean semantic maps become similar as the timestep\nincreases, eventually becoming identical at $t=T$. This facilitates the\ngeneration of an image close to a clean image, enabling robust generation.\nFurthermore, we propose a class-wise noise schedule to differentially diffuse\nthe labels depending on the class. We demonstrate that the proposed method\ngenerates high-quality samples through extensive experiments and analyses on\nbenchmark datasets, including a novel experimental setup simulating human\nerrors during real-world applications.",
        "translated": ""
    },
    {
        "title": "Intelligent Known and Novel Aircraft Recognition -- A Shift from\n  Classification to Similarity Learning for Combat Identification",
        "url": "http://arxiv.org/abs/2402.16486v1",
        "pub_date": "2024-02-26",
        "summary": "Precise aircraft recognition in low-resolution remote sensing imagery is a\nchallenging yet crucial task in aviation, especially combat identification.\nThis research addresses this problem with a novel, scalable, and AI-driven\nsolution. The primary hurdle in combat identification in remote sensing imagery\nis the accurate recognition of Novel/Unknown types of aircraft in addition to\nKnown types. Traditional methods, human expert-driven combat identification and\nimage classification, fall short in identifying Novel classes. Our methodology\nemploys similarity learning to discern features of a broad spectrum of military\nand civilian aircraft. It discerns both Known and Novel aircraft types,\nleveraging metric learning for the identification and supervised few-shot\nlearning for aircraft type classification. To counter the challenge of limited\nlow-resolution remote sensing data, we propose an end-to-end framework that\nadapts to the diverse and versatile process of military aircraft recognition by\ntraining a generalized embedder in fully supervised manner. Comparative\nanalysis with earlier aircraft image classification methods shows that our\napproach is effective for aircraft image classification (F1-score Aircraft Type\nof 0.861) and pioneering for quantifying the identification of Novel types\n(F1-score Bipartitioning of 0.936). The proposed methodology effectively\naddresses inherent challenges in remote sensing data, thereby setting new\nstandards in dataset quality. The research opens new avenues for domain experts\nand demonstrates unique capabilities in distinguishing various aircraft types,\ncontributing to a more robust, domain-adapted potential for real-time aircraft\nrecognition.",
        "translated": ""
    },
    {
        "title": "Edge Detectors Can Make Deep Convolutional Neural Networks More Robust",
        "url": "http://arxiv.org/abs/2402.16479v1",
        "pub_date": "2024-02-26",
        "summary": "Deep convolutional neural networks (DCNN for short) are vulnerable to\nexamples with small perturbations. Improving DCNN's robustness is of great\nsignificance to the safety-critical applications, such as autonomous driving\nand industry automation. Inspired by the principal way that human eyes\nrecognize objects, i.e., largely relying on the shape features, this paper\nfirst employs the edge detectors as layer kernels and designs a binary edge\nfeature branch (BEFB for short) to learn the binary edge features, which can be\neasily integrated into any popular backbone. The four edge detectors can learn\nthe horizontal, vertical, positive diagonal, and negative diagonal edge\nfeatures, respectively, and the branch is stacked by multiple Sobel layers\n(using edge detectors as kernels) and one threshold layer. The binary edge\nfeatures learned by the branch, concatenated with the texture features learned\nby the backbone, are fed into the fully connected layers for classification. We\nintegrate the proposed branch into VGG16 and ResNet34, respectively, and\nconduct experiments on multiple datasets. Experimental results demonstrate the\nBEFB is lightweight and has no side effects on training. And the accuracy of\nthe BEFB integrated models is better than the original ones on all datasets\nwhen facing FGSM, PGD, and C\\&amp;W attacks. Besides, BEFB integrated models\nequipped with the robustness enhancing techniques can achieve better\nclassification accuracy compared to the original models. The work in this paper\nfor the first time shows it is feasible to enhance the robustness of DCNNs\nthrough combining both shape-like features and texture features.",
        "translated": ""
    },
    {
        "title": "DCVSMNet: Double Cost Volume Stereo Matching Network",
        "url": "http://arxiv.org/abs/2402.16473v1",
        "pub_date": "2024-02-26",
        "summary": "We introduce Double Cost Volume Stereo Matching Network(DCVSMNet) which is a\nnovel architecture characterised by by two small upper (group-wise) and lower\n(norm correlation) cost volumes. Each cost volume is processed separately, and\na coupling module is proposed to fuse the geometry information extracted from\nthe upper and lower cost volumes. DCVSMNet is a fast stereo matching network\nwith a 67 ms inference time and strong generalization ability which can produce\ncompetitive results compared to state-of-the-art methods. The results on\nseveral bench mark datasets show that DCVSMNet achieves better accuracy than\nmethods such as CGI-Stereo and BGNet at the cost of greater inference time.",
        "translated": ""
    },
    {
        "title": "On Distributed Larger-Than-Memory Subset Selection With Pairwise\n  Submodular Functions",
        "url": "http://arxiv.org/abs/2402.16442v1",
        "pub_date": "2024-02-26",
        "summary": "Many learning problems hinge on the fundamental problem of subset selection,\ni.e., identifying a subset of important and representative points. For example,\nselecting the most significant samples in ML training cannot only reduce\ntraining costs but also enhance model quality. Submodularity, a discrete\nanalogue of convexity, is commonly used for solving subset selection problems.\nHowever, existing algorithms for optimizing submodular functions are\nsequential, and the prior distributed methods require at least one central\nmachine to fit the target subset. In this paper, we relax the requirement of\nhaving a central machine for the target subset by proposing a novel distributed\nbounding algorithm with provable approximation guarantees. The algorithm\niteratively bounds the minimum and maximum utility values to select high\nquality points and discard the unimportant ones. When bounding does not find\nthe complete subset, we use a multi-round, partition-based distributed greedy\nalgorithm to identify the remaining subset. We show that these algorithms find\nhigh quality subsets on CIFAR-100 and ImageNet with marginal or no loss in\nquality compared to centralized methods, and scale to a dataset with 13 billion\npoints.",
        "translated": ""
    },
    {
        "title": "COMAE: COMprehensive Attribute Exploration for Zero-shot Hashing",
        "url": "http://arxiv.org/abs/2402.16424v1",
        "pub_date": "2024-02-26",
        "summary": "Zero-shot hashing (ZSH) has shown excellent success owing to its efficiency\nand generalization in large-scale retrieval scenarios. While considerable\nsuccess has been achieved, there still exist urgent limitations. Existing works\nignore the locality relationships of representations and attributes, which have\neffective transferability between seeable classes and unseeable classes. Also,\nthe continuous-value attributes are not fully harnessed. In response, we\nconduct a COMprehensive Attribute Exploration for ZSH, named COMAE, which\ndepicts the relationships from seen classes to unseen ones through three\nmeticulously designed explorations, i.e., point-wise, pair-wise and class-wise\nconsistency constraints. By regressing attributes from the proposed attribute\nprototype network, COMAE learns the local features that are relevant to the\nvisual attributes. Then COMAE utilizes contrastive learning to comprehensively\ndepict the context of attributes, rather than instance-independent\noptimization. Finally, the class-wise constraint is designed to cohesively\nlearn the hash code, image representation, and visual attributes more\neffectively. Experimental results on the popular ZSH datasets demonstrate that\nCOMAE outperforms state-of-the-art hashing techniques, especially in scenarios\nwith a larger number of unseen label classes.",
        "translated": ""
    },
    {
        "title": "Outline-Guided Object Inpainting with Diffusion Models",
        "url": "http://arxiv.org/abs/2402.16421v1",
        "pub_date": "2024-02-26",
        "summary": "Instance segmentation datasets play a crucial role in training accurate and\nrobust computer vision models. However, obtaining accurate mask annotations to\nproduce high-quality segmentation datasets is a costly and labor-intensive\nprocess. In this work, we show how this issue can be mitigated by starting with\nsmall annotated instance segmentation datasets and augmenting them to\neffectively obtain a sizeable annotated dataset. We achieve that by creating\nvariations of the available annotated object instances in a way that preserves\nthe provided mask annotations, thereby resulting in new image-mask pairs to be\nadded to the set of annotated images. Specifically, we generate new images\nusing a diffusion-based inpainting model to fill out the masked area with a\ndesired object class by guiding the diffusion through the object outline. We\nshow that the object outline provides a simple, but also reliable and\nconvenient training-free guidance signal for the underlying inpainting model\nthat is often sufficient to fill out the mask with an object of the correct\nclass without further text guidance and preserve the correspondence between\ngenerated images and the mask annotations with high precision. Our experimental\nresults reveal that our method successfully generates realistic variations of\nobject instances, preserving their shape characteristics while introducing\ndiversity within the augmented area. We also show that the proposed method can\nnaturally be combined with text guidance and other image augmentation\ntechniques.",
        "translated": ""
    },
    {
        "title": "CMC: Few-shot Novel View Synthesis via Cross-view Multiplane Consistency",
        "url": "http://arxiv.org/abs/2402.16407v1",
        "pub_date": "2024-02-26",
        "summary": "Neural Radiance Field (NeRF) has shown impressive results in novel view\nsynthesis, particularly in Virtual Reality (VR) and Augmented Reality (AR),\nthanks to its ability to represent scenes continuously. However, when just a\nfew input view images are available, NeRF tends to overfit the given views and\nthus make the estimated depths of pixels share almost the same value. Unlike\nprevious methods that conduct regularization by introducing complex priors or\nadditional supervisions, we propose a simple yet effective method that\nexplicitly builds depth-aware consistency across input views to tackle this\nchallenge. Our key insight is that by forcing the same spatial points to be\nsampled repeatedly in different input views, we are able to strengthen the\ninteractions between views and therefore alleviate the overfitting problem. To\nachieve this, we build the neural networks on layered representations\n(\\textit{i.e.}, multiplane images), and the sampling point can thus be\nresampled on multiple discrete planes. Furthermore, to regularize the unseen\ntarget views, we constrain the rendered colors and depths from different input\nviews to be the same. Although simple, extensive experiments demonstrate that\nour proposed method can achieve better synthesis quality over state-of-the-art\nmethods.",
        "translated": ""
    },
    {
        "title": "Analysis of Embeddings Learned by End-to-End Machine Learning Eye\n  Movement-driven Biometrics Pipeline",
        "url": "http://arxiv.org/abs/2402.16399v1",
        "pub_date": "2024-02-26",
        "summary": "This paper expands on the foundational concept of temporal persistence in\nbiometric systems, specifically focusing on the domain of eye movement\nbiometrics facilitated by machine learning. Unlike previous studies that\nprimarily focused on developing biometric authentication systems, our research\ndelves into the embeddings learned by these systems, particularly examining\ntheir temporal persistence, reliability, and biometric efficacy in response to\nvarying input data. Utilizing two publicly available eye-movement datasets, we\nemployed the state-of-the-art Eye Know You Too machine learning pipeline for\nour analysis. We aim to validate whether the machine learning-derived\nembeddings in eye movement biometrics mirror the temporal persistence observed\nin traditional biometrics. Our methodology involved conducting extensive\nexperiments to assess how different lengths and qualities of input data\ninfluence the performance of eye movement biometrics more specifically how it\nimpacts the learned embeddings. We also explored the reliability and\nconsistency of the embeddings under varying data conditions. Three key metrics\n(kendall's coefficient of concordance, intercorrelations, and equal error rate)\nwere employed to quantitatively evaluate our findings. The results reveal while\ndata length significantly impacts the stability of the learned embeddings,\nhowever, the intercorrelations among embeddings show minimal effect.",
        "translated": ""
    },
    {
        "title": "Diffusion Meets DAgger: Supercharging Eye-in-hand Imitation Learning",
        "url": "http://arxiv.org/abs/2402.17768v1",
        "pub_date": "2024-02-27",
        "summary": "A common failure mode for policies trained with imitation is compounding\nexecution errors at test time. When the learned policy encounters states that\nwere not present in the expert demonstrations, the policy fails, leading to\ndegenerate behavior. The Dataset Aggregation, or DAgger approach to this\nproblem simply collects more data to cover these failure states. However, in\npractice, this is often prohibitively expensive. In this work, we propose\nDiffusion Meets DAgger (DMD), a method to reap the benefits of DAgger without\nthe cost for eye-in-hand imitation learning problems. Instead of collecting new\nsamples to cover out-of-distribution states, DMD uses recent advances in\ndiffusion models to create these samples with diffusion models. This leads to\nrobust performance from few demonstrations. In experiments conducted for\nnon-prehensile pushing on a Franka Research 3, we show that DMD can achieve a\nsuccess rate of 80% with as few as 8 expert demonstrations, where naive\nbehavior cloning reaches only 20%. DMD also outperform competing NeRF-based\naugmentation schemes by 50%.",
        "translated": ""
    },
    {
        "title": "Opening Cabinets and Drawers in the Real World using a Commodity Mobile\n  Manipulator",
        "url": "http://arxiv.org/abs/2402.17767v1",
        "pub_date": "2024-02-27",
        "summary": "Pulling open cabinets and drawers presents many difficult technical\nchallenges in perception (inferring articulation parameters for objects from\nonboard sensors), planning (producing motion plans that conform to tight task\nconstraints), and control (making and maintaining contact while applying forces\non the environment). In this work, we build an end-to-end system that enables a\ncommodity mobile manipulator (Stretch RE2) to pull open cabinets and drawers in\ndiverse previously unseen real world environments. We conduct 4 days of real\nworld testing of this system spanning 31 different objects from across 13\ndifferent real world environments. Our system achieves a success rate of 61% on\nopening novel cabinets and drawers in unseen environments zero-shot. An\nanalysis of the failure modes suggests that errors in perception are the most\nsignificant challenge for our system. We will open source code and models for\nothers to replicate and build upon our system.",
        "translated": ""
    },
    {
        "title": "ShapeLLM: Universal 3D Object Understanding for Embodied Interaction",
        "url": "http://arxiv.org/abs/2402.17766v1",
        "pub_date": "2024-02-27",
        "summary": "This paper presents ShapeLLM, the first 3D Multimodal Large Language Model\n(LLM) designed for embodied interaction, exploring a universal 3D object\nunderstanding with 3D point clouds and languages. ShapeLLM is built upon an\nimproved 3D encoder by extending ReCon to ReCon++ that benefits from multi-view\nimage distillation for enhanced geometry understanding. By utilizing ReCon++ as\nthe 3D point cloud input encoder for LLMs, ShapeLLM is trained on constructed\ninstruction-following data and tested on our newly human-curated evaluation\nbenchmark, 3D MM-Vet. ReCon++ and ShapeLLM achieve state-of-the-art performance\nin 3D geometry understanding and language-unified 3D interaction tasks, such as\nembodied visual grounding.",
        "translated": ""
    },
    {
        "title": "ADL4D: Towards A Contextually Rich Dataset for 4D Activities of Daily\n  Living",
        "url": "http://arxiv.org/abs/2402.17758v1",
        "pub_date": "2024-02-27",
        "summary": "Hand-Object Interactions (HOIs) are conditioned on spatial and temporal\ncontexts like surrounding objects, pre- vious actions, and future intents (for\nexample, grasping and handover actions vary greatly based on objects proximity\nand trajectory obstruction). However, existing datasets for 4D HOI (3D HOI over\ntime) are limited to one subject inter- acting with one object only. This\nrestricts the generalization of learning-based HOI methods trained on those\ndatasets. We introduce ADL4D, a dataset of up to two subjects inter- acting\nwith different sets of objects performing Activities of Daily Living (ADL) like\nbreakfast or lunch preparation ac- tivities. The transition between multiple\nobjects to complete a certain task over time introduces a unique context\nlacking in existing datasets. Our dataset consists of 75 sequences with a total\nof 1.1M RGB-D frames, hand and object poses, and per-hand fine-grained action\nannotations. We develop an automatic system for multi-view multi-hand 3D pose\nan- notation capable of tracking hand poses over time. We inte- grate and test\nit against publicly available datasets. Finally, we evaluate our dataset on the\ntasks of Hand Mesh Recov- ery (HMR) and Hand Action Segmentation (HAS).",
        "translated": ""
    },
    {
        "title": "LoDIP: Low light phase retrieval with deep image prior",
        "url": "http://arxiv.org/abs/2402.17745v1",
        "pub_date": "2024-02-27",
        "summary": "Phase retrieval (PR) is a fundamental challenge in scientific imaging,\nenabling nanoscale techniques like coherent diffractive imaging (CDI). Imaging\nat low radiation doses becomes important in applications where samples are\nsusceptible to radiation damage. However, most PR methods struggle in low dose\nscenario due to the presence of very high shot noise. Advancements in the\noptical data acquisition setup, exemplified by in-situ CDI, have shown\npotential for low-dose imaging. But these depend on a time series of\nmeasurements, rendering them unsuitable for single-image applications.\nSimilarly, on the computational front, data-driven phase retrieval techniques\nare not readily adaptable to the single-image context. Deep learning based\nsingle-image methods, such as deep image prior, have been effective for various\nimaging tasks but have exhibited limited success when applied to PR. In this\nwork, we propose LoDIP which combines the in-situ CDI setup with the power of\nimplicit neural priors to tackle the problem of single-image low-dose phase\nretrieval. Quantitative evaluations demonstrate the superior performance of\nLoDIP on this task as well as applicability to real experimental scenarios.",
        "translated": ""
    },
    {
        "title": "Analyzing Regional Organization of the Human Hippocampus in 3D-PLI Using\n  Contrastive Learning and Geometric Unfolding",
        "url": "http://arxiv.org/abs/2402.17744v1",
        "pub_date": "2024-02-27",
        "summary": "Understanding the cortical organization of the human brain requires\ninterpretable descriptors for distinct structural and functional imaging data.\n3D polarized light imaging (3D-PLI) is an imaging modality for visualizing\nfiber architecture in postmortem brains with high resolution that also captures\nthe presence of cell bodies, for example, to identify hippocampal subfields.\nThe rich texture in 3D-PLI images, however, makes this modality particularly\ndifficult to analyze and best practices for characterizing architectonic\npatterns still need to be established. In this work, we demonstrate a novel\nmethod to analyze the regional organization of the human hippocampus in 3D-PLI\nby combining recent advances in unfolding methods with deep texture features\nobtained using a self-supervised contrastive learning approach. We identify\nclusters in the representations that correspond well with classical\ndescriptions of hippocampal subfields, lending validity to the developed\nmethodology.",
        "translated": ""
    },
    {
        "title": "Towards Fairness-Aware Adversarial Learning",
        "url": "http://arxiv.org/abs/2402.17729v1",
        "pub_date": "2024-02-27",
        "summary": "Although adversarial training (AT) has proven effective in enhancing the\nmodel's robustness, the recently revealed issue of fairness in robustness has\nnot been well addressed, i.e. the robust accuracy varies significantly among\ndifferent categories. In this paper, instead of uniformly evaluating the\nmodel's average class performance, we delve into the issue of robust fairness,\nby considering the worst-case distribution across various classes. We propose a\nnovel learning paradigm, named Fairness-Aware Adversarial Learning (FAAL). As a\ngeneralization of conventional AT, we re-define the problem of adversarial\ntraining as a min-max-max framework, to ensure both robustness and fairness of\nthe trained model. Specifically, by taking advantage of distributional robust\noptimization, our method aims to find the worst distribution among different\ncategories, and the solution is guaranteed to obtain the upper bound\nperformance with high probability. In particular, FAAL can fine-tune an unfair\nrobust model to be fair within only two epochs, without compromising the\noverall clean and robust accuracies. Extensive experiments on various image\ndatasets validate the superior performance and efficiency of the proposed FAAL\ncompared to other state-of-the-art methods.",
        "translated": ""
    },
    {
        "title": "VRP-SAM: SAM with Visual Reference Prompt",
        "url": "http://arxiv.org/abs/2402.17726v1",
        "pub_date": "2024-02-27",
        "summary": "In this paper, we propose a novel Visual Reference Prompt (VRP) encoder that\nempowers the Segment Anything Model (SAM) to utilize annotated reference images\nas prompts for segmentation, creating the VRP-SAM model. In essence, VRP-SAM\ncan utilize annotated reference images to comprehend specific objects and\nperform segmentation of specific objects in target image. It is note that the\nVRP encoder can support a variety of annotation formats for reference images,\nincluding \\textbf{point}, \\textbf{box}, \\textbf{scribble}, and \\textbf{mask}.\nVRP-SAM achieves a breakthrough within the SAM framework by extending its\nversatility and applicability while preserving SAM's inherent strengths, thus\nenhancing user-friendliness. To enhance the generalization ability of VRP-SAM,\nthe VRP encoder adopts a meta-learning strategy. To validate the effectiveness\nof VRP-SAM, we conducted extensive empirical studies on the Pascal and COCO\ndatasets. Remarkably, VRP-SAM achieved state-of-the-art performance in visual\nreference segmentation with minimal learnable parameters. Furthermore, VRP-SAM\ndemonstrates strong generalization capabilities, allowing it to perform\nsegmentation of unseen objects and enabling cross-domain segmentation.",
        "translated": ""
    },
    {
        "title": "MedContext: Learning Contextual Cues for Efficient Volumetric Medical\n  Segmentation",
        "url": "http://arxiv.org/abs/2402.17725v1",
        "pub_date": "2024-02-27",
        "summary": "Volumetric medical segmentation is a critical component of 3D medical image\nanalysis that delineates different semantic regions. Deep neural networks have\nsignificantly improved volumetric medical segmentation, but they generally\nrequire large-scale annotated data to achieve better performance, which can be\nexpensive and prohibitive to obtain. To address this limitation, existing works\ntypically perform transfer learning or design dedicated pretraining-finetuning\nstages to learn representative features. However, the mismatch between the\nsource and target domain can make it challenging to learn optimal\nrepresentation for volumetric data, while the multi-stage training demands\nhigher compute as well as careful selection of stage-specific design choices.\nIn contrast, we propose a universal training framework called MedContext that\nis architecture-agnostic and can be incorporated into any existing training\nframework for 3D medical segmentation. Our approach effectively learns self\nsupervised contextual cues jointly with the supervised voxel segmentation task\nwithout requiring large-scale annotated volumetric medical data or dedicated\npretraining-finetuning stages. The proposed approach induces contextual\nknowledge in the network by learning to reconstruct the missing organ or parts\nof an organ in the output segmentation space. The effectiveness of MedContext\nis validated across multiple 3D medical datasets and four state-of-the-art\nmodel architectures. Our approach demonstrates consistent gains in segmentation\nperformance across datasets and different architectures even in few-shot data\nscenarios. Our code and pretrained models are available at\nhttps://github.com/hananshafi/MedContext",
        "translated": ""
    },
    {
        "title": "Seeing and Hearing: Open-domain Visual-Audio Generation with Diffusion\n  Latent Aligners",
        "url": "http://arxiv.org/abs/2402.17723v1",
        "pub_date": "2024-02-27",
        "summary": "Video and audio content creation serves as the core technique for the movie\nindustry and professional users. Recently, existing diffusion-based methods\ntackle video and audio generation separately, which hinders the technique\ntransfer from academia to industry. In this work, we aim at filling the gap,\nwith a carefully designed optimization-based framework for cross-visual-audio\nand joint-visual-audio generation. We observe the powerful generation ability\nof off-the-shelf video or audio generation models. Thus, instead of training\nthe giant models from scratch, we propose to bridge the existing strong models\nwith a shared latent representation space. Specifically, we propose a\nmultimodality latent aligner with the pre-trained ImageBind model. Our latent\naligner shares a similar core as the classifier guidance that guides the\ndiffusion denoising process during inference time. Through carefully designed\noptimization strategy and loss functions, we show the superior performance of\nour method on joint video-audio generation, visual-steered audio generation,\nand audio-steered visual generation tasks. The project website can be found at\nhttps://yzxing87.github.io/Seeing-and-Hearing/",
        "translated": ""
    },
    {
        "title": "UniMODE: Unified Monocular 3D Object Detection",
        "url": "http://arxiv.org/abs/2402.18573v1",
        "pub_date": "2024-02-28",
        "summary": "Realizing unified monocular 3D object detection, including both indoor and\noutdoor scenes, holds great importance in applications like robot navigation.\nHowever, involving various scenarios of data to train models poses challenges\ndue to their significantly different characteristics, e.g., diverse geometry\nproperties and heterogeneous domain distributions. To address these challenges,\nwe build a detector based on the bird's-eye-view (BEV) detection paradigm,\nwhere the explicit feature projection is beneficial to addressing the geometry\nlearning ambiguity when employing multiple scenarios of data to train\ndetectors. Then, we split the classical BEV detection architecture into two\nstages and propose an uneven BEV grid design to handle the convergence\ninstability caused by the aforementioned challenges. Moreover, we develop a\nsparse BEV feature projection strategy to reduce computational cost and a\nunified domain alignment method to handle heterogeneous domains. Combining\nthese techniques, a unified detector UniMODE is derived, which surpasses the\nprevious state-of-the-art on the challenging Omni3D dataset (a large-scale\ndataset including both indoor and outdoor scenes) by 4.9% AP_3D, revealing the\nfirst successful generalization of a BEV detector to unified 3D object\ndetection.",
        "translated": ""
    },
    {
        "title": "Selection of appropriate multispectral camera exposure settings and\n  radiometric calibration methods for applications in phenotyping and precision\n  agriculture",
        "url": "http://arxiv.org/abs/2402.18553v1",
        "pub_date": "2024-02-28",
        "summary": "Radiometric accuracy of data is crucial in quantitative precision\nagriculture, to produce reliable and repeatable data for modeling and decision\nmaking. The effect of exposure time and gain settings on the radiometric\naccuracy of multispectral images was not explored enough. The goal of this\nstudy was to determine if having a fixed exposure (FE) time during image\nacquisition improved radiometric accuracy of images, compared to the default\nauto-exposure (AE) settings. This involved quantifying the errors from\nauto-exposure and determining ideal exposure values within which radiometric\nmean absolute percentage error (MAPE) were minimal (&lt; 5%). The results showed\nthat FE orthomosaic was closer to ground-truth (higher R2 and lower MAPE) than\nAE orthomosaic. An ideal exposure range was determined for capturing canopy and\nsoil objects, without loss of information from under-exposure or saturation\nfrom over-exposure. A simulation of errors from AE showed that MAPE &lt; 5% for\nthe blue, green, red, and NIR bands and &lt; 7% for the red edge band for exposure\nsettings within the determined ideal ranges and increased exponentially beyond\nthe ideal exposure upper limit. Further, prediction of total plant nitrogen\nuptake (g/plant) using vegetation indices (VIs) from two different growing\nseasons were closer to the ground truth (mostly, R2 &gt; 0.40, and MAPE = 12 to\n14%, p &lt; 0.05) when FE was used, compared to the prediction from AE images\n(mostly, R2 &lt; 0.13, MAPE = 15 to 18%, p &gt;= 0.05).",
        "translated": ""
    },
    {
        "title": "Gradient Reweighting: Towards Imbalanced Class-Incremental Learning",
        "url": "http://arxiv.org/abs/2402.18528v1",
        "pub_date": "2024-02-28",
        "summary": "Class-Incremental Learning (CIL) trains a model to continually recognize new\nclasses from non-stationary data while retaining learned knowledge. A major\nchallenge of CIL arises when applying to real-world data characterized by\nnon-uniform distribution, which introduces a dual imbalance problem involving\n(i) disparities between stored exemplars of old tasks and new class data\n(inter-phase imbalance), and (ii) severe class imbalances within each\nindividual task (intra-phase imbalance). We show that this dual imbalance issue\ncauses skewed gradient updates with biased weights in FC layers, thus inducing\nover/under-fitting and catastrophic forgetting in CIL. Our method addresses it\nby reweighting the gradients towards balanced optimization and unbiased\nclassifier learning. Additionally, we observe imbalanced forgetting where\nparadoxically the instance-rich classes suffer higher performance degradation\nduring CIL due to a larger amount of training data becoming unavailable in\nsubsequent learning phases. To tackle this, we further introduce a\ndistribution-aware knowledge distillation loss to mitigate forgetting by\naligning output logits proportionally with the distribution of lost training\ndata. We validate our method on CIFAR-100, ImageNetSubset, and Food101 across\nvarious evaluation protocols and demonstrate consistent improvements compared\nto existing works, showing great potential to apply CIL in real-world scenarios\nwith enhanced robustness and effectiveness.",
        "translated": ""
    },
    {
        "title": "Defect Detection in Tire X-Ray Images: Conventional Methods Meet Deep\n  Structures",
        "url": "http://arxiv.org/abs/2402.18527v1",
        "pub_date": "2024-02-28",
        "summary": "This paper introduces a robust approach for automated defect detection in\ntire X-ray images by harnessing traditional feature extraction methods such as\nLocal Binary Pattern (LBP) and Gray Level Co-Occurrence Matrix (GLCM) features,\nas well as Fourier and Wavelet-based features, complemented by advanced machine\nlearning techniques. Recognizing the challenges inherent in the complex\npatterns and textures of tire X-ray images, the study emphasizes the\nsignificance of feature engineering to enhance the performance of defect\ndetection systems. By meticulously integrating combinations of these features\nwith a Random Forest (RF) classifier and comparing them against advanced models\nlike YOLOv8, the research not only benchmarks the performance of traditional\nfeatures in defect detection but also explores the synergy between classical\nand modern approaches. The experimental results demonstrate that these\ntraditional features, when fine-tuned and combined with machine learning\nmodels, can significantly improve the accuracy and reliability of tire defect\ndetection, aiming to set a new standard in automated quality assurance in tire\nmanufacturing.",
        "translated": ""
    },
    {
        "title": "Multimodal Learning To Improve Cardiac Late Mechanical Activation\n  Detection From Cine MR Images",
        "url": "http://arxiv.org/abs/2402.18507v1",
        "pub_date": "2024-02-28",
        "summary": "This paper presents a multimodal deep learning framework that utilizes\nadvanced image techniques to improve the performance of clinical analysis\nheavily dependent on routinely acquired standard images. More specifically, we\ndevelop a joint learning network that for the first time leverages the accuracy\nand reproducibility of myocardial strains obtained from Displacement Encoding\nwith Stimulated Echo (DENSE) to guide the analysis of cine cardiac magnetic\nresonance (CMR) imaging in late mechanical activation (LMA) detection. An image\nregistration network is utilized to acquire the knowledge of cardiac motions,\nan important feature estimator of strain values, from standard cine CMRs. Our\nframework consists of two major components: (i) a DENSE-supervised strain\nnetwork leveraging latent motion features learned from a registration network\nto predict myocardial strains; and (ii) a LMA network taking advantage of the\npredicted strain for effective LMA detection. Experimental results show that\nour proposed work substantially improves the performance of strain analysis and\nLMA detection from cine CMR images, aligning more closely with the achievements\nof DENSE.",
        "translated": ""
    },
    {
        "title": "Detection of Micromobility Vehicles in Urban Traffic Videos",
        "url": "http://arxiv.org/abs/2402.18503v1",
        "pub_date": "2024-02-28",
        "summary": "Urban traffic environments present unique challenges for object detection,\nparticularly with the increasing presence of micromobility vehicles like\ne-scooters and bikes. To address this object detection problem, this work\nintroduces an adapted detection model that combines the accuracy and speed of\nsingle-frame object detection with the richer features offered by video object\ndetection frameworks. This is done by applying aggregated feature maps from\nconsecutive frames processed through motion flow to the YOLOX architecture.\nThis fusion brings a temporal perspective to YOLOX detection abilities,\nallowing for a better understanding of urban mobility patterns and\nsubstantially improving detection reliability. Tested on a custom dataset\ncurated for urban micromobility scenarios, our model showcases substantial\nimprovement over existing state-of-the-art methods, demonstrating the need to\nconsider spatio-temporal information for detecting such small and thin objects.\nOur approach enhances detection in challenging conditions, including\nocclusions, ensuring temporal consistency, and effectively mitigating motion\nblur.",
        "translated": ""
    },
    {
        "title": "Sunshine to Rainstorm: Cross-Weather Knowledge Distillation for Robust\n  3D Object Detection",
        "url": "http://arxiv.org/abs/2402.18493v1",
        "pub_date": "2024-02-28",
        "summary": "LiDAR-based 3D object detection models have traditionally struggled under\nrainy conditions due to the degraded and noisy scanning signals. Previous\nresearch has attempted to address this by simulating the noise from rain to\nimprove the robustness of detection models. However, significant disparities\nexist between simulated and actual rain-impacted data points. In this work, we\npropose a novel rain simulation method, termed DRET, that unifies Dynamics and\nRainy Environment Theory to provide a cost-effective means of expanding the\navailable realistic rain data for 3D detection training. Furthermore, we\npresent a Sunny-to-Rainy Knowledge Distillation (SRKD) approach to enhance 3D\ndetection under rainy conditions. Extensive experiments on the WaymoOpenDataset\nlarge-scale dataset show that, when combined with the state-of-the-art DSVT\nmodel and other classical 3D detectors, our proposed framework demonstrates\nsignificant detection accuracy improvements, without losing efficiency.\nRemarkably, our framework also improves detection capabilities under sunny\nconditions, therefore offering a robust solution for 3D detection regardless of\nwhether the weather is rainy or sunny",
        "translated": ""
    },
    {
        "title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding",
        "url": "http://arxiv.org/abs/2402.18490v1",
        "pub_date": "2024-02-28",
        "summary": "The limited scale of current 3D shape datasets hinders the advancements in 3D\nshape understanding, and motivates multi-modal learning approaches which\ntransfer learned knowledge from data-abundant 2D image and language modalities\nto 3D shapes. However, even though the image and language representations have\nbeen aligned by cross-modal models like CLIP, we find that the image modality\nfails to contribute as much as the language in existing multi-modal 3D\nrepresentation learning methods. This is attributed to the domain shift in the\n2D images and the distinct focus of each modality. To more effectively leverage\nboth modalities in the pre-training, we introduce TriAdapter Multi-Modal\nLearning (TAMM) -- a novel two-stage learning approach based on three\nsynergetic adapters. First, our CLIP Image Adapter mitigates the domain gap\nbetween 3D-rendered images and natural images, by adapting the visual\nrepresentations of CLIP for synthetic image-text pairs. Subsequently, our Dual\nAdapters decouple the 3D shape representation space into two complementary\nsub-spaces: one focusing on visual attributes and the other for semantic\nunderstanding, which ensure a more comprehensive and effective multi-modal\npre-training. Extensive experiments demonstrate that TAMM consistently enhances\n3D representations for a wide range of 3D encoder architectures, pre-training\ndatasets, and downstream tasks. Notably, we boost the zero-shot classification\naccuracy on Objaverse-LVIS from 46.8 to 50.7, and improve the 5-way 10-shot\nlinear probing classification accuracy on ModelNet40 from 96.1 to 99.0. Project\npage: \\url{https://alanzhangcs.github.io/tamm-page}.",
        "translated": ""
    },
    {
        "title": "IBD: Alleviating Hallucinations in Large Vision-Language Models via\n  Image-Biased Decoding",
        "url": "http://arxiv.org/abs/2402.18476v1",
        "pub_date": "2024-02-28",
        "summary": "Despite achieving rapid developments and with widespread applications, Large\nVision-Language Models (LVLMs) confront a serious challenge of being prone to\ngenerating hallucinations. An over-reliance on linguistic priors has been\nidentified as a key factor leading to these hallucinations. In this paper, we\npropose to alleviate this problem by introducing a novel image-biased decoding\n(IBD) technique. Our method derives the next-token probability distribution by\ncontrasting predictions from a conventional LVLM with those of an image-biased\nLVLM, thereby amplifying the correct information highly correlated with image\ncontent while mitigating the hallucinatory errors caused by excessive\ndependence on text. We further conduct a comprehensive statistical analysis to\nvalidate the reliability of our method, and design an adaptive adjustment\nstrategy to achieve robust and flexible handling under varying conditions.\nExperimental results across multiple evaluation metrics verify that our method,\ndespite not requiring additional training data and only with a minimal increase\nin model parameters, can significantly reduce hallucinations in LVLMs and\nenhance the truthfulness of the generated response.",
        "translated": ""
    },
    {
        "title": "Separate and Conquer: Decoupling Co-occurrence via Decomposition and\n  Representation for Weakly Supervised Semantic Segmentation",
        "url": "http://arxiv.org/abs/2402.18467v1",
        "pub_date": "2024-02-28",
        "summary": "Attributed to the frequent coupling of co-occurring objects and the limited\nsupervision from image-level labels, the challenging co-occurrence problem is\nwidely present and leads to false activation of objects in weakly supervised\nsemantic segmentation (WSSS). In this work, we devise a 'Separate and Conquer'\nscheme SeCo to tackle this issue from dimensions of image space and feature\nspace. In the image space, we propose to 'separate' the co-occurring objects\nwith image decomposition by subdividing images into patches. Importantly, we\nassign each patch a category tag from Class Activation Maps (CAMs), which\nspatially helps remove the co-context bias and guide the subsequent\nrepresentation. In the feature space, we propose to 'conquer' the false\nactivation by enhancing semantic representation with multi-granularity\nknowledge contrast. To this end, a dual-teacher-single-student architecture is\ndesigned and tag-guided contrast is conducted to guarantee the correctness of\nknowledge and further facilitate the discrepancy among co-occurring objects. We\nstreamline the multi-staged WSSS pipeline end-to-end and tackle co-occurrence\nwithout external supervision. Extensive experiments are conducted, validating\nthe efficiency of our method tackling co-occurrence and the superiority over\nprevious single-staged and even multi-staged competitors on PASCAL VOC and MS\nCOCO. Code will be available.",
        "translated": ""
    },
    {
        "title": "DistriFusion: Distributed Parallel Inference for High-Resolution\n  Diffusion Models",
        "url": "http://arxiv.org/abs/2402.19481v1",
        "pub_date": "2024-02-29",
        "summary": "Diffusion models have achieved great success in synthesizing high-quality\nimages. However, generating high-resolution images with diffusion models is\nstill challenging due to the enormous computational costs, resulting in a\nprohibitive latency for interactive applications. In this paper, we propose\nDistriFusion to tackle this problem by leveraging parallelism across multiple\nGPUs. Our method splits the model input into multiple patches and assigns each\npatch to a GPU. However, na\\\"{\\i}vely implementing such an algorithm breaks the\ninteraction between patches and loses fidelity, while incorporating such an\ninteraction will incur tremendous communication overhead. To overcome this\ndilemma, we observe the high similarity between the input from adjacent\ndiffusion steps and propose displaced patch parallelism, which takes advantage\nof the sequential nature of the diffusion process by reusing the pre-computed\nfeature maps from the previous timestep to provide context for the current\nstep. Therefore, our method supports asynchronous communication, which can be\npipelined by computation. Extensive experiments show that our method can be\napplied to recent Stable Diffusion XL with no quality degradation and achieve\nup to a 6.1$\\times$ speedup on eight NVIDIA A100s compared to one. Our code is\npublicly available at https://github.com/mit-han-lab/distrifuser.",
        "translated": ""
    },
    {
        "title": "Panda-70M: Captioning 70M Videos with Multiple Cross-Modality Teachers",
        "url": "http://arxiv.org/abs/2402.19479v1",
        "pub_date": "2024-02-29",
        "summary": "The quality of the data and annotation upper-bounds the quality of a\ndownstream model. While there exist large text corpora and image-text pairs,\nhigh-quality video-text data is much harder to collect. First of all, manual\nlabeling is more time-consuming, as it requires an annotator to watch an entire\nvideo. Second, videos have a temporal dimension, consisting of several scenes\nstacked together, and showing multiple actions. Accordingly, to establish a\nvideo dataset with high-quality captions, we propose an automatic approach\nleveraging multimodal inputs, such as textual video description, subtitles, and\nindividual video frames. Specifically, we curate 3.8M high-resolution videos\nfrom the publicly available HD-VILA-100M dataset. We then split them into\nsemantically consistent video clips, and apply multiple cross-modality teacher\nmodels to obtain captions for each video. Next, we finetune a retrieval model\non a small subset where the best caption of each video is manually selected and\nthen employ the model in the whole dataset to select the best caption as the\nannotation. In this way, we get 70M videos paired with high-quality text\ncaptions. We dub the dataset as Panda-70M. We show the value of the proposed\ndataset on three downstream tasks: video captioning, video and text retrieval,\nand text-driven video generation. The models trained on the proposed data score\nsubstantially better on the majority of metrics across all the tasks.",
        "translated": ""
    },
    {
        "title": "Learning a Generalized Physical Face Model From Data",
        "url": "http://arxiv.org/abs/2402.19477v1",
        "pub_date": "2024-02-29",
        "summary": "Physically-based simulation is a powerful approach for 3D facial animation as\nthe resulting deformations are governed by physical constraints, allowing to\neasily resolve self-collisions, respond to external forces and perform\nrealistic anatomy edits. Today's methods are data-driven, where the actuations\nfor finite elements are inferred from captured skin geometry. Unfortunately,\nthese approaches have not been widely adopted due to the complexity of\ninitializing the material space and learning the deformation model for each\ncharacter separately, which often requires a skilled artist followed by lengthy\nnetwork training. In this work, we aim to make physics-based facial animation\nmore accessible by proposing a generalized physical face model that we learn\nfrom a large 3D face dataset in a simulation-free manner. Once trained, our\nmodel can be quickly fit to any unseen identity and produce a ready-to-animate\nphysical face model automatically. Fitting is as easy as providing a single 3D\nface scan, or even a single face image. After fitting, we offer intuitive\nanimation controls, as well as the ability to retarget animations across\ncharacters. All the while, the resulting animations allow for physical effects\nlike collision avoidance, gravity, paralysis, bone reshaping and more.",
        "translated": ""
    },
    {
        "title": "The All-Seeing Project V2: Towards General Relation Comprehension of the\n  Open World",
        "url": "http://arxiv.org/abs/2402.19474v1",
        "pub_date": "2024-02-29",
        "summary": "We present the All-Seeing Project V2: a new model and dataset designed for\nunderstanding object relations in images. Specifically, we propose the\nAll-Seeing Model V2 (ASMv2) that integrates the formulation of text generation,\nobject localization, and relation comprehension into a relation conversation\n(ReC) task. Leveraging this unified task, our model excels not only in\nperceiving and recognizing all objects within the image but also in grasping\nthe intricate relation graph between them, diminishing the relation\nhallucination often encountered by Multi-modal Large Language Models (MLLMs).\nTo facilitate training and evaluation of MLLMs in relation understanding, we\ncreated the first high-quality ReC dataset ({AS-V2) which is aligned with the\nformat of standard instruction tuning data. In addition, we design a new\nbenchmark, termed Circular-based Relation Probing Evaluation (CRPE) for\ncomprehensively evaluating the relation comprehension capabilities of MLLMs.\nNotably, our ASMv2 achieves an overall accuracy of 52.04 on this relation-aware\nbenchmark, surpassing the 43.14 of LLaVA-1.5 by a large margin. We hope that\nour work can inspire more future research and contribute to the evolution\ntowards artificial general intelligence. Our project is released at\nhttps://github.com/OpenGVLab/all-seeing.",
        "translated": ""
    },
    {
        "title": "Retrieval-Augmented Generation for AI-Generated Content: A Survey",
        "url": "http://arxiv.org/abs/2402.19473v1",
        "pub_date": "2024-02-29",
        "summary": "The development of Artificial Intelligence Generated Content (AIGC) has been\nfacilitated by advancements in model algorithms, scalable foundation model\narchitectures, and the availability of ample high-quality datasets. While AIGC\nhas achieved remarkable performance, it still faces challenges, such as the\ndifficulty of maintaining up-to-date and long-tail knowledge, the risk of data\nleakage, and the high costs associated with training and inference.\nRetrieval-Augmented Generation (RAG) has recently emerged as a paradigm to\naddress such challenges. In particular, RAG introduces the information\nretrieval process, which enhances AIGC results by retrieving relevant objects\nfrom available data stores, leading to greater accuracy and robustness. In this\npaper, we comprehensively review existing efforts that integrate RAG technique\ninto AIGC scenarios. We first classify RAG foundations according to how the\nretriever augments the generator. We distill the fundamental abstractions of\nthe augmentation methodologies for various retrievers and generators. This\nunified perspective encompasses all RAG scenarios, illuminating advancements\nand pivotal technologies that help with potential future progress. We also\nsummarize additional enhancements methods for RAG, facilitating effective\nengineering and implementation of RAG systems. Then from another view, we\nsurvey on practical applications of RAG across different modalities and tasks,\noffering valuable references for researchers and practitioners. Furthermore, we\nintroduce the benchmarks for RAG, discuss the limitations of current RAG\nsystems, and suggest potential directions for future research. Project:\nhttps://github.com/hymie122/RAG-Survey",
        "translated": ""
    },
    {
        "title": "Lifelong Benchmarks: Efficient Model Evaluation in an Era of Rapid\n  Progress",
        "url": "http://arxiv.org/abs/2402.19472v1",
        "pub_date": "2024-02-29",
        "summary": "Standardized benchmarks drive progress in machine learning. However, with\nrepeated testing, the risk of overfitting grows as algorithms over-exploit\nbenchmark idiosyncrasies. In our work, we seek to mitigate this challenge by\ncompiling ever-expanding large-scale benchmarks called Lifelong Benchmarks. As\nexemplars of our approach, we create Lifelong-CIFAR10 and Lifelong-ImageNet,\ncontaining (for now) 1.69M and 1.98M test samples, respectively. While reducing\noverfitting, lifelong benchmarks introduce a key challenge: the high cost of\nevaluating a growing number of models across an ever-expanding sample set. To\naddress this challenge, we also introduce an efficient evaluation framework:\nSort \\&amp; Search (S&amp;S), which reuses previously evaluated models by leveraging\ndynamic programming algorithms to selectively rank and sub-select test samples,\nenabling cost-effective lifelong benchmarking. Extensive empirical evaluations\nacross 31,000 models demonstrate that S&amp;S achieves highly-efficient approximate\naccuracy measurement, reducing compute cost from 180 GPU days to 5 GPU hours\n(1000x reduction) on a single A100 GPU, with low approximation error. As such,\nlifelong benchmarks offer a robust, practical solution to the \"benchmark\nexhaustion\" problem.",
        "translated": ""
    },
    {
        "title": "Towards Generalizable Tumor Synthesis",
        "url": "http://arxiv.org/abs/2402.19470v1",
        "pub_date": "2024-02-29",
        "summary": "Tumor synthesis enables the creation of artificial tumors in medical images,\nfacilitating the training of AI models for tumor detection and segmentation.\nHowever, success in tumor synthesis hinges on creating visually realistic\ntumors that are generalizable across multiple organs and, furthermore, the\nresulting AI models being capable of detecting real tumors in images sourced\nfrom different domains (e.g., hospitals). This paper made a progressive stride\ntoward generalizable tumor synthesis by leveraging a critical observation:\nearly-stage tumors (&lt; 2cm) tend to have similar imaging characteristics in\ncomputed tomography (CT), whether they originate in the liver, pancreas, or\nkidneys. We have ascertained that generative AI models, e.g., Diffusion Models,\ncan create realistic tumors generalized to a range of organs even when trained\non a limited number of tumor examples from only one organ. Moreover, we have\nshown that AI models trained on these synthetic tumors can be generalized to\ndetect and segment real tumors from CT volumes, encompassing a broad spectrum\nof patient demographics, imaging protocols, and healthcare facilities.",
        "translated": ""
    },
    {
        "title": "Humanoid Locomotion as Next Token Prediction",
        "url": "http://arxiv.org/abs/2402.19469v1",
        "pub_date": "2024-02-29",
        "summary": "We cast real-world humanoid control as a next token prediction problem, akin\nto predicting the next word in language. Our model is a causal transformer\ntrained via autoregressive prediction of sensorimotor trajectories. To account\nfor the multi-modal nature of the data, we perform prediction in a\nmodality-aligned way, and for each input token predict the next token from the\nsame modality. This general formulation enables us to leverage data with\nmissing modalities, like video trajectories without actions. We train our model\non a collection of simulated trajectories coming from prior neural network\npolicies, model-based controllers, motion capture data, and YouTube videos of\nhumans. We show that our model enables a full-sized humanoid to walk in San\nFrancisco zero-shot. Our model can transfer to the real world even when trained\non only 27 hours of walking data, and can generalize to commands not seen\nduring training like walking backward. These findings suggest a promising path\ntoward learning challenging real-world control tasks by generative modeling of\nsensorimotor trajectories.",
        "translated": ""
    },
    {
        "title": "TV-TREES: Multimodal Entailment Trees for Neuro-Symbolic Video Reasoning",
        "url": "http://arxiv.org/abs/2402.19467v1",
        "pub_date": "2024-02-29",
        "summary": "It is challenging to perform question-answering over complex, multimodal\ncontent such as television clips. This is in part because current\nvideo-language models rely on single-modality reasoning, have lowered\nperformance on long inputs, and lack interpetability. We propose TV-TREES, the\nfirst multimodal entailment tree generator. TV-TREES serves as an approach to\nvideo understanding that promotes interpretable joint-modality reasoning by\nproducing trees of entailment relationships between simple premises directly\nentailed by the videos and higher-level conclusions. We then introduce the task\nof multimodal entailment tree generation to evaluate the reasoning quality of\nsuch methods. Our method's experimental results on the challenging TVQA dataset\ndemonstrate intepretable, state-of-the-art zero-shot performance on full video\nclips, illustrating a best of both worlds contrast to black-box methods.",
        "translated": ""
    },
    {
        "title": "SeMoLi: What Moves Together Belongs Together",
        "url": "http://arxiv.org/abs/2402.19463v1",
        "pub_date": "2024-02-29",
        "summary": "We tackle semi-supervised object detection based on motion cues. Recent\nresults suggest that heuristic-based clustering methods in conjunction with\nobject trackers can be used to pseudo-label instances of moving objects and use\nthese as supervisory signals to train 3D object detectors in Lidar data without\nmanual supervision. We re-think this approach and suggest that both, object\ndetection, as well as motion-inspired pseudo-labeling, can be tackled in a\ndata-driven manner. We leverage recent advances in scene flow estimation to\nobtain point trajectories from which we extract long-term, class-agnostic\nmotion patterns. Revisiting correlation clustering in the context of message\npassing networks, we learn to group those motion patterns to cluster points to\nobject instances. By estimating the full extent of the objects, we obtain\nper-scan 3D bounding boxes that we use to supervise a Lidar object detection\nnetwork. Our method not only outperforms prior heuristic-based approaches (57.5\nAP, +14 improvement over prior work), more importantly, we show we can\npseudo-label and train object detectors across datasets.",
        "translated": ""
    },
    {
        "title": "Enhancing Retinal Vascular Structure Segmentation in Images With a Novel\n  Design Two-Path Interactive Fusion Module Model",
        "url": "http://arxiv.org/abs/2403.01362v1",
        "pub_date": "2024-03-03",
        "summary": "Precision in identifying and differentiating micro and macro blood vessels in\nthe retina is crucial for the diagnosis of retinal diseases, although it poses\na significant challenge. Current autoencoding-based segmentation approaches\nencounter limitations as they are constrained by the encoder and undergo a\nreduction in resolution during the encoding stage. The inability to recover\nlost information in the decoding phase further impedes these approaches.\nConsequently, their capacity to extract the retinal microvascular structure is\nrestricted. To address this issue, we introduce Swin-Res-Net, a specialized\nmodule designed to enhance the precision of retinal vessel segmentation.\nSwin-Res-Net utilizes the Swin transformer which uses shifted windows with\ndisplacement for partitioning, to reduce network complexity and accelerate\nmodel convergence. Additionally, the model incorporates interactive fusion with\na functional module in the Res2Net architecture. The Res2Net leverages\nmulti-scale techniques to enlarge the receptive field of the convolutional\nkernel, enabling the extraction of additional semantic information from the\nimage. This combination creates a new module that enhances the localization and\nseparation of micro vessels in the retina. To improve the efficiency of\nprocessing vascular information, we've added a module to eliminate redundant\ninformation between the encoding and decoding steps.\n  Our proposed architecture produces outstanding results, either meeting or\nsurpassing those of other published models. The AUC reflects significant\nenhancements, achieving values of 0.9956, 0.9931, and 0.9946 in pixel-wise\nsegmentation of retinal vessels across three widely utilized datasets:\nCHASE-DB1, DRIVE, and STARE, respectively. Moreover, Swin-Res-Net outperforms\nalternative architectures, demonstrating superior performance in both IOU and\nF1 measure metrics.",
        "translated": ""
    },
    {
        "title": "ShapeBoost: Boosting Human Shape Estimation with Part-Based\n  Parameterization and Clothing-Preserving Augmentation",
        "url": "http://arxiv.org/abs/2403.01345v1",
        "pub_date": "2024-03-02",
        "summary": "Accurate human shape recovery from a monocular RGB image is a challenging\ntask because humans come in different shapes and sizes and wear different\nclothes. In this paper, we propose ShapeBoost, a new human shape recovery\nframework that achieves pixel-level alignment even for rare body shapes and\nhigh accuracy for people wearing different types of clothes. Unlike previous\napproaches that rely on the use of PCA-based shape coefficients, we adopt a new\nhuman shape parameterization that decomposes the human shape into bone lengths\nand the mean width of each part slice. This part-based parameterization\ntechnique achieves a balance between flexibility and validity using a\nsemi-analytical shape reconstruction algorithm. Based on this new\nparameterization, a clothing-preserving data augmentation module is proposed to\ngenerate realistic images with diverse body shapes and accurate annotations.\nExperimental results show that our method outperforms other state-of-the-art\nmethods in diverse body shape situations as well as in varied clothing\nsituations.",
        "translated": ""
    },
    {
        "title": "Mitigating the Bias in the Model for Continual Test-Time Adaptation",
        "url": "http://arxiv.org/abs/2403.01344v1",
        "pub_date": "2024-03-02",
        "summary": "Continual Test-Time Adaptation (CTA) is a challenging task that aims to adapt\na source pre-trained model to continually changing target domains. In the CTA\nsetting, a model does not know when the target domain changes, thus facing a\ndrastic change in the distribution of streaming inputs during the test-time.\nThe key challenge is to keep adapting the model to the continually changing\ntarget domains in an online manner. We find that a model shows highly biased\npredictions as it constantly adapts to the chaining distribution of the target\ndata. It predicts certain classes more often than other classes, making\ninaccurate over-confident predictions. This paper mitigates this issue to\nimprove performance in the CTA scenario. To alleviate the bias issue, we make\nclass-wise exponential moving average target prototypes with reliable target\nsamples and exploit them to cluster the target features class-wisely. Moreover,\nwe aim to align the target distributions to the source distribution by\nanchoring the target feature to its corresponding source prototype. With\nextensive experiments, our proposed method achieves noteworthy performance gain\nwhen applied on top of existing CTA methods without substantial adaptation time\noverhead.",
        "translated": ""
    },
    {
        "title": "Bespoke Non-Stationary Solvers for Fast Sampling of Diffusion and Flow\n  Models",
        "url": "http://arxiv.org/abs/2403.01329v1",
        "pub_date": "2024-03-02",
        "summary": "This paper introduces Bespoke Non-Stationary (BNS) Solvers, a solver\ndistillation approach to improve sample efficiency of Diffusion and Flow\nmodels. BNS solvers are based on a family of non-stationary solvers that\nprovably subsumes existing numerical ODE solvers and consequently demonstrate\nconsiderable improvement in sample approximation (PSNR) over these baselines.\nCompared to model distillation, BNS solvers benefit from a tiny parameter space\n($&lt;$200 parameters), fast optimization (two orders of magnitude faster),\nmaintain diversity of samples, and in contrast to previous solver distillation\napproaches nearly close the gap from standard distillation methods such as\nProgressive Distillation in the low-medium NFE regime. For example, BNS solver\nachieves 45 PSNR / 1.76 FID using 16 NFE in class-conditional ImageNet-64. We\nexperimented with BNS solvers for conditional image generation, text-to-image\ngeneration, and text-2-audio generation showing significant improvement in\nsample approximation (PSNR) in all.",
        "translated": ""
    },
    {
        "title": "DNA Family: Boosting Weight-Sharing NAS with Block-Wise Supervisions",
        "url": "http://arxiv.org/abs/2403.01326v1",
        "pub_date": "2024-03-02",
        "summary": "Neural Architecture Search (NAS), aiming at automatically designing neural\narchitectures by machines, has been considered a key step toward automatic\nmachine learning. One notable NAS branch is the weight-sharing NAS, which\nsignificantly improves search efficiency and allows NAS algorithms to run on\nordinary computers. Despite receiving high expectations, this category of\nmethods suffers from low search effectiveness. By employing a generalization\nboundedness tool, we demonstrate that the devil behind this drawback is the\nuntrustworthy architecture rating with the oversized search space of the\npossible architectures. Addressing this problem, we modularize a large search\nspace into blocks with small search spaces and develop a family of models with\nthe distilling neural architecture (DNA) techniques. These proposed models,\nnamely a DNA family, are capable of resolving multiple dilemmas of the\nweight-sharing NAS, such as scalability, efficiency, and multi-modal\ncompatibility. Our proposed DNA models can rate all architecture candidates, as\nopposed to previous works that can only access a subsearch space using\nheuristic algorithms. Moreover, under a certain computational complexity\nconstraint, our method can seek architectures with different depths and widths.\nExtensive experimental evaluations show that our models achieve\nstate-of-the-art top-1 accuracy of 78.9% and 83.6% on ImageNet for a mobile\nconvolutional network and a small vision transformer, respectively.\nAdditionally, we provide in-depth empirical analysis and insights into neural\narchitecture ratings. Codes available: \\url{https://github.com/changlin31/DNA}.",
        "translated": ""
    },
    {
        "title": "NeRF-VPT: Learning Novel View Representations with Neural Radiance\n  Fields via View Prompt Tuning",
        "url": "http://arxiv.org/abs/2403.01325v1",
        "pub_date": "2024-03-02",
        "summary": "Neural Radiance Fields (NeRF) have garnered remarkable success in novel view\nsynthesis. Nonetheless, the task of generating high-quality images for novel\nviews persists as a critical challenge. While the existing efforts have\nexhibited commendable progress, capturing intricate details, enhancing\ntextures, and achieving superior Peak Signal-to-Noise Ratio (PSNR) metrics\nwarrant further focused attention and advancement. In this work, we propose\nNeRF-VPT, an innovative method for novel view synthesis to address these\nchallenges. Our proposed NeRF-VPT employs a cascading view prompt tuning\nparadigm, wherein RGB information gained from preceding rendering outcomes\nserves as instructive visual prompts for subsequent rendering stages, with the\naspiration that the prior knowledge embedded in the prompts can facilitate the\ngradual enhancement of rendered image quality. NeRF-VPT only requires sampling\nRGB data from previous stage renderings as priors at each training stage,\nwithout relying on extra guidance or complex techniques. Thus, our NeRF-VPT is\nplug-and-play and can be readily integrated into existing methods. By\nconducting comparative analyses of our NeRF-VPT against several NeRF-based\napproaches on demanding real-scene benchmarks, such as Realistic Synthetic 360,\nReal Forward-Facing, Replica dataset, and a user-captured dataset, we\nsubstantiate that our NeRF-VPT significantly elevates baseline performance and\nproficiently generates more high-quality novel view images than all the\ncompared state-of-the-art methods. Furthermore, the cascading learning of\nNeRF-VPT introduces adaptability to scenarios with sparse inputs, resulting in\na significant enhancement of accuracy for sparse-view novel view synthesis. The\nsource code and dataset are available at\n\\url{https://github.com/Freedomcls/NeRF-VPT}.",
        "translated": ""
    },
    {
        "title": "TUMTraf V2X Cooperative Perception Dataset",
        "url": "http://arxiv.org/abs/2403.01316v1",
        "pub_date": "2024-03-02",
        "summary": "Cooperative perception offers several benefits for enhancing the capabilities\nof autonomous vehicles and improving road safety. Using roadside sensors in\naddition to onboard sensors increases reliability and extends the sensor range.\nExternal sensors offer higher situational awareness for automated vehicles and\nprevent occlusions. We propose CoopDet3D, a cooperative multi-modal fusion\nmodel, and TUMTraf-V2X, a perception dataset, for the cooperative 3D object\ndetection and tracking task. Our dataset contains 2,000 labeled point clouds\nand 5,000 labeled images from five roadside and four onboard sensors. It\nincludes 30k 3D boxes with track IDs and precise GPS and IMU data. We labeled\neight categories and covered occlusion scenarios with challenging driving\nmaneuvers, like traffic violations, near-miss events, overtaking, and U-turns.\nThrough multiple experiments, we show that our CoopDet3D camera-LiDAR fusion\nmodel achieves an increase of +14.36 3D mAP compared to a vehicle camera-LiDAR\nfusion model. Finally, we make our dataset, model, labeling tool, and dev-kit\npublicly available on our website:\nhttps://tum-traffic-dataset.github.io/tumtraf-v2x.",
        "translated": ""
    },
    {
        "title": "Image-Based Dietary Assessment: A Healthy Eating Plate Estimation System",
        "url": "http://arxiv.org/abs/2403.01310v1",
        "pub_date": "2024-03-02",
        "summary": "The nutritional quality of diets has significantly deteriorated over the past\ntwo to three decades, a decline often underestimated by the people. This\ndeterioration, coupled with a hectic lifestyle, has contributed to escalating\nhealth concerns. Recognizing this issue, researchers at Harvard have advocated\nfor a balanced nutritional plate model to promote health. Inspired by this\nresearch, our paper introduces an innovative Image-Based Dietary Assessment\nsystem aimed at evaluating the healthiness of meals through image analysis. Our\nsystem employs advanced image segmentation and classification techniques to\nanalyze food items on a plate, assess their proportions, and calculate meal\nadherence to Harvard's healthy eating recommendations. This approach leverages\nmachine learning and nutritional science to empower individuals with actionable\ninsights for healthier eating choices. Our four-step framework involves\nsegmenting the image, classifying the items, conducting a nutritional\nassessment based on the Harvard Healthy Eating Plate research, and offering\ntailored recommendations. The prototype system has shown promising results in\npromoting healthier eating habits by providing an accessible, evidence-based\ntool for dietary assessment.",
        "translated": ""
    },
    {
        "title": "ICC: Quantifying Image Caption Concreteness for Multimodal Dataset\n  Curation",
        "url": "http://arxiv.org/abs/2403.01306v1",
        "pub_date": "2024-03-02",
        "summary": "Web-scale training on paired text-image data is becoming increasingly central\nto multimodal learning, but is challenged by the highly noisy nature of\ndatasets in the wild. Standard data filtering approaches succeed in removing\nmismatched text-image pairs, but permit semantically related but highly\nabstract or subjective text. These approaches lack the fine-grained ability to\nisolate the most concrete samples that provide the strongest signal for\nlearning in a noisy dataset. In this work, we propose a new metric, image\ncaption concreteness, that evaluates caption text without an image reference to\nmeasure its concreteness and relevancy for use in multimodal learning. Our\napproach leverages strong foundation models for measuring visual-semantic\ninformation loss in multimodal representations. We demonstrate that this\nstrongly correlates with human evaluation of concreteness in both single-word\nand sentence-level texts. Moreover, we show that curation using ICC complements\nexisting approaches: It succeeds in selecting the highest quality samples from\nmultimodal web-scale datasets to allow for efficient training in\nresource-constrained settings.",
        "translated": ""
    },
    {
        "title": "Causal Mode Multiplexer: A Novel Framework for Unbiased Multispectral\n  Pedestrian Detection",
        "url": "http://arxiv.org/abs/2403.01300v1",
        "pub_date": "2024-03-02",
        "summary": "RGBT multispectral pedestrian detection has emerged as a promising solution\nfor safety-critical applications that require day/night operations. However,\nthe modality bias problem remains unsolved as multispectral pedestrian\ndetectors learn the statistical bias in datasets. Specifically, datasets in\nmultispectral pedestrian detection mainly distribute between ROTO (day) and\nRXTO (night) data; the majority of the pedestrian labels statistically co-occur\nwith their thermal features. As a result, multispectral pedestrian detectors\nshow poor generalization ability on examples beyond this statistical\ncorrelation, such as ROTX data. To address this problem, we propose a novel\nCausal Mode Multiplexer (CMM) framework that effectively learns the causalities\nbetween multispectral inputs and predictions. Moreover, we construct a new\ndataset (ROTX-MP) to evaluate modality bias in multispectral pedestrian\ndetection. ROTX-MP mainly includes ROTX examples not presented in previous\ndatasets. Extensive experiments demonstrate that our proposed CMM framework\ngeneralizes well on existing datasets (KAIST, CVC-14, FLIR) and the new\nROTX-MP. We will release our new dataset to the public for future research.",
        "translated": ""
    },
    {
        "title": "FAR: Flexible, Accurate and Robust 6DoF Relative Camera Pose Estimation",
        "url": "http://arxiv.org/abs/2403.03221v1",
        "pub_date": "2024-03-05",
        "summary": "Estimating relative camera poses between images has been a central problem in\ncomputer vision. Methods that find correspondences and solve for the\nfundamental matrix offer high precision in most cases. Conversely, methods\npredicting pose directly using neural networks are more robust to limited\noverlap and can infer absolute translation scale, but at the expense of reduced\nprecision. We show how to combine the best of both methods; our approach yields\nresults that are both precise and robust, while also accurately inferring\ntranslation scales. At the heart of our model lies a Transformer that (1)\nlearns to balance between solved and learned pose estimations, and (2) provides\na prior to guide a solver. A comprehensive analysis supports our design choices\nand demonstrates that our method adapts flexibly to various feature extractors\nand correspondence estimators, showing state-of-the-art performance in 6DoF\npose estimation on Matterport3D, InteriorNet, StreetLearn, and Map-free\nRelocalization.",
        "translated": ""
    },
    {
        "title": "Self-supervised 3D Patient Modeling with Multi-modal Attentive Fusion",
        "url": "http://arxiv.org/abs/2403.03217v1",
        "pub_date": "2024-03-05",
        "summary": "3D patient body modeling is critical to the success of automated patient\npositioning for smart medical scanning and operating rooms. Existing CNN-based\nend-to-end patient modeling solutions typically require a) customized network\ndesigns demanding large amount of relevant training data, covering extensive\nrealistic clinical scenarios (e.g., patient covered by sheets), which leads to\nsuboptimal generalizability in practical deployment, b) expensive 3D human\nmodel annotations, i.e., requiring huge amount of manual effort, resulting in\nsystems that scale poorly. To address these issues, we propose a generic\nmodularized 3D patient modeling method consists of (a) a multi-modal keypoint\ndetection module with attentive fusion for 2D patient joint localization, to\nlearn complementary cross-modality patient body information, leading to\nimproved keypoint localization robustness and generalizability in a wide\nvariety of imaging (e.g., CT, MRI etc.) and clinical scenarios (e.g., heavy\nocclusions); and (b) a self-supervised 3D mesh regression module which does not\nrequire expensive 3D mesh parameter annotations to train, bringing immediate\ncost benefits for clinical deployment. We demonstrate the efficacy of the\nproposed method by extensive patient positioning experiments on both public and\nclinical data. Our evaluation results achieve superior patient positioning\nperformance across various imaging modalities in real clinical scenarios.",
        "translated": ""
    },
    {
        "title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis",
        "url": "http://arxiv.org/abs/2403.03206v1",
        "pub_date": "2024-03-05",
        "summary": "Diffusion models create data from noise by inverting the forward paths of\ndata towards noise and have emerged as a powerful generative modeling technique\nfor high-dimensional, perceptual data such as images and videos. Rectified flow\nis a recent generative model formulation that connects data and noise in a\nstraight line. Despite its better theoretical properties and conceptual\nsimplicity, it is not yet decisively established as standard practice. In this\nwork, we improve existing noise sampling techniques for training rectified flow\nmodels by biasing them towards perceptually relevant scales. Through a\nlarge-scale study, we demonstrate the superior performance of this approach\ncompared to established diffusion formulations for high-resolution\ntext-to-image synthesis. Additionally, we present a novel transformer-based\narchitecture for text-to-image generation that uses separate weights for the\ntwo modalities and enables a bidirectional flow of information between image\nand text tokens, improving text comprehension, typography, and human preference\nratings. We demonstrate that this architecture follows predictable scaling\ntrends and correlates lower validation loss to improved text-to-image synthesis\nas measured by various metrics and human evaluations. Our largest models\noutperform state-of-the-art models, and we will make our experimental data,\ncode, and model weights publicly available.",
        "translated": ""
    },
    {
        "title": "Triple-CFN: Restructuring Conceptual Spaces for Enhancing Abstract\n  Reasoning process",
        "url": "http://arxiv.org/abs/2403.03190v1",
        "pub_date": "2024-03-05",
        "summary": "Abstract reasoning problems pose significant challenges to artificial\nintelligence algorithms, demanding cognitive capabilities beyond those required\nfor perception tasks. This study introduces the Triple-CFN approach to tackle\nthe Bongard-Logo problem, achieving notable reasoning accuracy by implicitly\nreorganizing the concept space of conflicting instances. Additionally, the\nTriple-CFN paradigm proves effective for the RPM problem with necessary\nmodifications, yielding competitive results. To further enhance performance on\nthe RPM issue, we develop the Meta Triple-CFN network, which explicitly\nstructures the problem space while maintaining interpretability on progressive\npatterns. The success of Meta Triple-CFN is attributed to its paradigm of\nmodeling the conceptual space, equivalent to normalizing reasoning information.\nBased on this ideology, we introduce the Re-space layer, enhancing the\nperformance of both Meta Triple-CFN and Triple-CFN. This paper aims to\ncontribute to advancements in machine intelligence by exploring innovative\nnetwork designs for addressing abstract reasoning problems, paving the way for\nfurther breakthroughs in this domain.",
        "translated": ""
    },
    {
        "title": "Solving the bongard-logo problem by modeling a probabilistic model",
        "url": "http://arxiv.org/abs/2403.03173v1",
        "pub_date": "2024-03-05",
        "summary": "Abstract reasoning problems challenge the perceptual and cognitive abilities\nof AI algorithms, demanding deeper pattern discernment and inductive reasoning\nbeyond explicit image features. This study introduces PMoC, a tailored\nprobability model for the Bongard-Logo problem, achieving high reasoning\naccuracy by constructing independent probability models. Additionally, we\npresent Pose-Transformer, an enhanced Transformer-Encoder designed for complex\nabstract reasoning tasks, including Bongard-Logo, RAVEN, I-RAVEN, and PGM.\nPose-Transformer incorporates positional information learning, inspired by\ncapsule networks' pose matrices, enhancing its focus on local positional\nrelationships in image data processing. When integrated with PMoC, it further\nimproves reasoning accuracy. Our approach effectively addresses reasoning\ndifficulties associated with abstract entities' positional changes,\noutperforming previous models on the OIG, D3$\\times$3 subsets of RAVEN, and PGM\ndatabases. This research contributes to advancing AI's capabilities in abstract\nreasoning and cognitive pattern recognition.",
        "translated": ""
    },
    {
        "title": "Design2Code: How Far Are We From Automating Front-End Engineering?",
        "url": "http://arxiv.org/abs/2403.03163v1",
        "pub_date": "2024-03-05",
        "summary": "Generative AI has made rapid advancements in recent years, achieving\nunprecedented capabilities in multimodal understanding and code generation.\nThis can enable a new paradigm of front-end development, in which multimodal\nLLMs might directly convert visual designs into code implementations. In this\nwork, we formalize this as a Design2Code task and conduct comprehensive\nbenchmarking. Specifically, we manually curate a benchmark of 484 diverse\nreal-world webpages as test cases and develop a set of automatic evaluation\nmetrics to assess how well current multimodal LLMs can generate the code\nimplementations that directly render into the given reference webpages, given\nthe screenshots as input. We also complement automatic metrics with\ncomprehensive human evaluations. We develop a suite of multimodal prompting\nmethods and show their effectiveness on GPT-4V and Gemini Pro Vision. We\nfurther finetune an open-source Design2Code-18B model that successfully matches\nthe performance of Gemini Pro Vision. Both human evaluation and automatic\nmetrics show that GPT-4V performs the best on this task compared to other\nmodels. Moreover, annotators think GPT-4V generated webpages can replace the\noriginal reference webpages in 49% of cases in terms of visual appearance and\ncontent; and perhaps surprisingly, in 64% of cases GPT-4V generated webpages\nare considered better than the original reference webpages. Our fine-grained\nbreak-down metrics indicate that open-source models mostly lag in recalling\nvisual elements from the input webpages and in generating correct layout\ndesigns, while aspects like text content and coloring can be drastically\nimproved with proper finetuning.",
        "translated": ""
    },
    {
        "title": "PalmProbNet: A Probabilistic Approach to Understanding Palm\n  Distributions in Ecuadorian Tropical Forest via Transfer Learning",
        "url": "http://arxiv.org/abs/2403.03161v1",
        "pub_date": "2024-03-05",
        "summary": "Palms play an outsized role in tropical forests and are important resources\nfor humans and wildlife. A central question in tropical ecosystems is\nunderstanding palm distribution and abundance. However, accurately identifying\nand localizing palms in geospatial imagery presents significant challenges due\nto dense vegetation, overlapping canopies, and variable lighting conditions in\nmixed-forest landscapes. Addressing this, we introduce PalmProbNet, a\nprobabilistic approach utilizing transfer learning to analyze high-resolution\nUAV-derived orthomosaic imagery, enabling the detection of palm trees within\nthe dense canopy of the Ecuadorian Rainforest. This approach represents a\nsubstantial advancement in automated palm detection, effectively pinpointing\npalm presence and locality in mixed tropical rainforests. Our process begins by\ngenerating an orthomosaic image from UAV images, from which we extract and\nlabel palm and non-palm image patches in two distinct sizes. These patches are\nthen used to train models with an identical architecture, consisting of an\nunaltered pre-trained ResNet-18 and a Multilayer Perceptron (MLP) with\nspecifically trained parameters. Subsequently, PalmProbNet employs a sliding\nwindow technique on the landscape orthomosaic, using both small and large\nwindow sizes to generate a probability heatmap. This heatmap effectively\nvisualizes the distribution of palms, showcasing the scalability and\nadaptability of our approach in various forest densities. Despite the\nchallenging terrain, our method demonstrated remarkable performance, achieving\nan accuracy of 97.32% and a Cohen's kappa of 94.59% in testing.",
        "translated": ""
    },
    {
        "title": "Dual Mean-Teacher: An Unbiased Semi-Supervised Framework for\n  Audio-Visual Source Localization",
        "url": "http://arxiv.org/abs/2403.03145v1",
        "pub_date": "2024-03-05",
        "summary": "Audio-Visual Source Localization (AVSL) aims to locate sounding objects\nwithin video frames given the paired audio clips. Existing methods\npredominantly rely on self-supervised contrastive learning of audio-visual\ncorrespondence. Without any bounding-box annotations, they struggle to achieve\nprecise localization, especially for small objects, and suffer from blurry\nboundaries and false positives. Moreover, the naive semi-supervised method is\npoor in fully leveraging the information of abundant unlabeled data. In this\npaper, we propose a novel semi-supervised learning framework for AVSL, namely\nDual Mean-Teacher (DMT), comprising two teacher-student structures to\ncircumvent the confirmation bias issue. Specifically, two teachers, pre-trained\non limited labeled data, are employed to filter out noisy samples via the\nconsensus between their predictions, and then generate high-quality\npseudo-labels by intersecting their confidence maps. The sufficient utilization\nof both labeled and unlabeled data and the proposed unbiased framework enable\nDMT to outperform current state-of-the-art methods by a large margin, with CIoU\nof 90.4% and 48.8% on Flickr-SoundNet and VGG-Sound Source, obtaining 8.9%,\n9.6% and 4.6%, 6.4% improvements over self- and semi-supervised methods\nrespectively, given only 3% positional-annotations. We also extend our\nframework to some existing AVSL methods and consistently boost their\nperformance.",
        "translated": ""
    },
    {
        "title": "Simplicity in Complexity",
        "url": "http://arxiv.org/abs/2403.03134v1",
        "pub_date": "2024-03-05",
        "summary": "The complexity of visual stimuli plays an important role in many cognitive\nphenomena, including attention, engagement, memorability, time perception and\naesthetic evaluation. Despite its importance, complexity is poorly understood\nand ironically, previous models of image complexity have been quite\n\\textit{complex}. There have been many attempts to find handcrafted features\nthat explain complexity, but these features are usually dataset specific, and\nhence fail to generalise. On the other hand, more recent work has employed deep\nneural networks to predict complexity, but these models remain difficult to\ninterpret, and do not guide a theoretical understanding of the problem. Here we\npropose to model complexity using segment-based representations of images. We\nuse state-of-the-art segmentation models, SAM and FC-CLIP, to quantify the\nnumber of segments at multiple granularities, and the number of classes in an\nimage respectively. We find that complexity is well-explained by a simple\nlinear model with these two features across six diverse image-sets of\nnaturalistic scene and art images. This suggests that the complexity of images\ncan be surprisingly simple.",
        "translated": ""
    },
    {
        "title": "NRDF: Neural Riemannian Distance Fields for Learning Articulated Pose\n  Priors",
        "url": "http://arxiv.org/abs/2403.03122v1",
        "pub_date": "2024-03-05",
        "summary": "Faithfully modeling the space of articulations is a crucial task that allows\nrecovery and generation of realistic poses, and remains a notorious challenge.\nTo this end, we introduce Neural Riemannian Distance Fields (NRDFs),\ndata-driven priors modeling the space of plausible articulations, represented\nas the zero-level-set of a neural field in a high-dimensional\nproduct-quaternion space. To train NRDFs only on positive examples, we\nintroduce a new sampling algorithm, ensuring that the geodesic distances follow\na desired distribution, yielding a principled distance field learning paradigm.\nWe then devise a projection algorithm to map any random pose onto the level-set\nby an adaptive-step Riemannian optimizer, adhering to the product manifold of\njoint rotations at all times. NRDFs can compute the Riemannian gradient via\nbackpropagation and by mathematical analogy, are related to Riemannian flow\nmatching, a recent generative model. We conduct a comprehensive evaluation of\nNRDF against other pose priors in various downstream tasks, i.e., pose\ngeneration, image-based pose estimation, and solving inverse kinematics,\nhighlighting NRDF's superior performance. Besides humans, NRDF's versatility\nextends to hand and animal poses, as it can effectively represent any\narticulation.",
        "translated": ""
    },
    {
        "title": "3D Diffusion Policy",
        "url": "http://arxiv.org/abs/2403.03954v1",
        "pub_date": "2024-03-06",
        "summary": "Imitation learning provides an efficient way to teach robots dexterous\nskills; however, learning complex skills robustly and generalizablely usually\nconsumes large amounts of human demonstrations. To tackle this challenging\nproblem, we present 3D Diffusion Policy (DP3), a novel visual imitation\nlearning approach that incorporates the power of 3D visual representations into\ndiffusion policies, a class of conditional action generative models. The core\ndesign of DP3 is the utilization of a compact 3D visual representation,\nextracted from sparse point clouds with an efficient point encoder. In our\nexperiments involving 72 simulation tasks, DP3 successfully handles most tasks\nwith just 10 demonstrations and surpasses baselines with a 55.3% relative\nimprovement. In 4 real robot tasks, DP3 demonstrates precise control with a\nhigh success rate of 85%, given only 40 demonstrations of each task, and shows\nexcellent generalization abilities in diverse aspects, including space,\nviewpoint, appearance, and instance. Interestingly, in real robot experiments,\nDP3 rarely violates safety requirements, in contrast to baseline methods which\nfrequently do, necessitating human intervention. Our extensive evaluation\nhighlights the critical importance of 3D representations in real-world robot\nlearning. Videos, code, and data are available on\nhttps://3d-diffusion-policy.github.io .",
        "translated": ""
    },
    {
        "title": "DART: Implicit Doppler Tomography for Radar Novel View Synthesis",
        "url": "http://arxiv.org/abs/2403.03896v1",
        "pub_date": "2024-03-06",
        "summary": "Simulation is an invaluable tool for radio-frequency system designers that\nenables rapid prototyping of various algorithms for imaging, target detection,\nclassification, and tracking. However, simulating realistic radar scans is a\nchallenging task that requires an accurate model of the scene, radio frequency\nmaterial properties, and a corresponding radar synthesis function. Rather than\nspecifying these models explicitly, we propose DART - Doppler Aided Radar\nTomography, a Neural Radiance Field-inspired method which uses radar-specific\nphysics to create a reflectance and transmittance-based rendering pipeline for\nrange-Doppler images. We then evaluate DART by constructing a custom data\ncollection platform and collecting a novel radar dataset together with accurate\nposition and instantaneous velocity measurements from lidar-based localization.\nIn comparison to state-of-the-art baselines, DART synthesizes superior radar\nrange-Doppler images from novel views across all datasets and additionally can\nbe used to generate high quality tomographic images.",
        "translated": ""
    },
    {
        "title": "Joint multi-task learning improves weakly-supervised biomarker\n  prediction in computational pathology",
        "url": "http://arxiv.org/abs/2403.03891v1",
        "pub_date": "2024-03-06",
        "summary": "Deep Learning (DL) can predict biomarkers directly from digitized cancer\nhistology in a weakly-supervised setting. Recently, the prediction of\ncontinuous biomarkers through regression-based DL has seen an increasing\ninterest. Nonetheless, clinical decision making often requires a categorical\noutcome. Consequently, we developed a weakly-supervised joint multi-task\nTransformer architecture which has been trained and evaluated on four public\npatient cohorts for the prediction of two key predictive biomarkers,\nmicrosatellite instability (MSI) and homologous recombination deficiency (HRD),\ntrained with auxiliary regression tasks related to the tumor microenvironment.\nMoreover, we perform a comprehensive benchmark of 16 approaches of task\nbalancing for weakly-supervised joint multi-task learning in computational\npathology. Using our novel approach, we improve over the state-of-the-art area\nunder the receiver operating characteristic by +7.7% and +4.1%, as well as\nyielding better clustering of latent embeddings by +8% and +5% for the\nprediction of MSI and HRD in external cohorts, respectively.",
        "translated": ""
    },
    {
        "title": "Hierarchical Diffusion Policy for Kinematics-Aware Multi-Task Robotic\n  Manipulation",
        "url": "http://arxiv.org/abs/2403.03890v1",
        "pub_date": "2024-03-06",
        "summary": "This paper introduces Hierarchical Diffusion Policy (HDP), a hierarchical\nagent for multi-task robotic manipulation. HDP factorises a manipulation policy\ninto a hierarchical structure: a high-level task-planning agent which predicts\na distant next-best end-effector pose (NBP), and a low-level goal-conditioned\ndiffusion policy which generates optimal motion trajectories. The factorised\npolicy representation allows HDP to tackle both long-horizon task planning\nwhile generating fine-grained low-level actions. To generate context-aware\nmotion trajectories while satisfying robot kinematics constraints, we present a\nnovel kinematics-aware goal-conditioned control agent, Robot Kinematics\nDiffuser (RK-Diffuser). Specifically, RK-Diffuser learns to generate both the\nend-effector pose and joint position trajectories, and distill the accurate but\nkinematics-unaware end-effector pose diffuser to the kinematics-aware but less\naccurate joint position diffuser via differentiable kinematics. Empirically, we\nshow that HDP achieves a significantly higher success rate than the\nstate-of-the-art methods in both simulation and real-world.",
        "translated": ""
    },
    {
        "title": "Self and Mixed Supervision to Improve Training Labels for Multi-Class\n  Medical Image Segmentation",
        "url": "http://arxiv.org/abs/2403.03882v1",
        "pub_date": "2024-03-06",
        "summary": "Accurate training labels are a key component for multi-class medical image\nsegmentation. Their annotation is costly and time-consuming because it requires\ndomain expertise. This work aims to develop a dual-branch network and\nautomatically improve training labels for multi-class image segmentation.\nTransfer learning is used to train the network and improve inaccurate weak\nlabels sequentially. The dual-branch network is first trained by weak labels\nalone to initialize model parameters. After the network is stabilized, the\nshared encoder is frozen, and strong and weak decoders are fine-tuned by strong\nand weak labels together. The accuracy of weak labels is iteratively improved\nin the fine-tuning process. The proposed method was applied to a three-class\nsegmentation of muscle, subcutaneous and visceral adipose tissue on abdominal\nCT scans. Validation results on 11 patients showed that the accuracy of\ntraining labels was statistically significantly improved, with the Dice\nsimilarity coefficient of muscle, subcutaneous and visceral adipose tissue\nincreased from 74.2% to 91.5%, 91.2% to 95.6%, and 77.6% to 88.5%, respectively\n(p&lt;0.05). In comparison with our earlier method, the label accuracy was also\nsignificantly improved (p&lt;0.05). These experimental results suggested that the\ncombination of the dual-branch network and transfer learning is an efficient\nmeans to improve training labels for multi-class segmentation.",
        "translated": ""
    },
    {
        "title": "Latent Dataset Distillation with Diffusion Models",
        "url": "http://arxiv.org/abs/2403.03881v1",
        "pub_date": "2024-03-06",
        "summary": "The efficacy of machine learning has traditionally relied on the availability\nof increasingly larger datasets. However, large datasets pose storage\nchallenges and contain non-influential samples, which could be ignored during\ntraining without impacting the final accuracy of the model. In response to\nthese limitations, the concept of distilling the information on a dataset into\na condensed set of (synthetic) samples, namely a distilled dataset, emerged.\nOne crucial aspect is the selected architecture (usually ConvNet) for linking\nthe original and synthetic datasets. However, the final accuracy is lower if\nthe employed model architecture differs from the model used during\ndistillation. Another challenge is the generation of high-resolution images,\ne.g., 128x128 and higher. In this paper, we propose Latent Dataset Distillation\nwith Diffusion Models (LD3M) that combine diffusion in latent space with\ndataset distillation to tackle both challenges. LD3M incorporates a novel\ndiffusion process tailored for dataset distillation, which improves the\ngradient norms for learning synthetic images. By adjusting the number of\ndiffusion steps, LD3M also offers a straightforward way of controlling the\ntrade-off between speed and accuracy. We evaluate our approach in several\nImageNet subsets and for high-resolution images (128x128 and 256x256). As a\nresult, LD3M consistently outperforms state-of-the-art distillation techniques\nby up to 4.8 p.p. and 4.2 p.p. for 1 and 10 images per class, respectively.",
        "translated": ""
    },
    {
        "title": "Redefining cystoscopy with ai: bladder cancer diagnosis using an\n  efficient hybrid cnn-transformer model",
        "url": "http://arxiv.org/abs/2403.03879v1",
        "pub_date": "2024-03-06",
        "summary": "Bladder cancer ranks within the top 10 most diagnosed cancers worldwide and\nis among the most expensive cancers to treat due to the high recurrence rates\nwhich require lifetime follow-ups. The primary tool for diagnosis is\ncystoscopy, which heavily relies on doctors' expertise and interpretation.\nTherefore, annually, numerous cases are either undiagnosed or misdiagnosed and\ntreated as urinary infections. To address this, we suggest a deep learning\napproach for bladder cancer detection and segmentation which combines CNNs with\na lightweight positional-encoding-free transformer and dual attention gates\nthat fuse self and spatial attention for feature enhancement. The architecture\nsuggested in this paper is efficient making it suitable for medical scenarios\nthat require real time inference. Experiments have proven that this model\naddresses the critical need for a balance between computational efficiency and\ndiagnostic accuracy in cystoscopic imaging as despite its small size it rivals\nlarge models in performance.",
        "translated": ""
    },
    {
        "title": "Are Language Models Puzzle Prodigies? Algorithmic Puzzles Unveil Serious\n  Challenges in Multimodal Reasoning",
        "url": "http://arxiv.org/abs/2403.03864v1",
        "pub_date": "2024-03-06",
        "summary": "This paper introduces the novel task of multimodal puzzle solving, framed\nwithin the context of visual question-answering. We present a new dataset,\nAlgoPuzzleVQA designed to challenge and evaluate the capabilities of multimodal\nlanguage models in solving algorithmic puzzles that necessitate both visual\nunderstanding, language understanding, and complex algorithmic reasoning. We\ncreate the puzzles to encompass a diverse array of mathematical and algorithmic\ntopics such as boolean logic, combinatorics, graph theory, optimization,\nsearch, etc., aiming to evaluate the gap between visual data interpretation and\nalgorithmic problem-solving skills. The dataset is generated automatically from\ncode authored by humans. All our puzzles have exact solutions that can be found\nfrom the algorithm without tedious human calculations. It ensures that our\ndataset can be scaled up arbitrarily in terms of reasoning complexity and\ndataset size. Our investigation reveals that large language models (LLMs) such\nas GPT4V and Gemini exhibit limited performance in puzzle-solving tasks. We\nfind that their performance is near random in a multi-choice question-answering\nsetup for a significant number of puzzles. The findings emphasize the\nchallenges of integrating visual, language, and algorithmic knowledge for\nsolving complex reasoning problems.",
        "translated": ""
    },
    {
        "title": "ECAP: Extensive Cut-and-Paste Augmentation for Unsupervised Domain\n  Adaptive Semantic Segmentation",
        "url": "http://arxiv.org/abs/2403.03854v1",
        "pub_date": "2024-03-06",
        "summary": "We consider unsupervised domain adaptation (UDA) for semantic segmentation in\nwhich the model is trained on a labeled source dataset and adapted to an\nunlabeled target dataset. Unfortunately, current self-training methods are\nsusceptible to misclassified pseudo-labels resulting from erroneous\npredictions. Since certain classes are typically associated with less reliable\npredictions in UDA, reducing the impact of such pseudo-labels without skewing\nthe training towards some classes is notoriously difficult. To this end, we\npropose an extensive cut-and-paste strategy (ECAP) to leverage reliable\npseudo-labels through data augmentation. Specifically, ECAP maintains a memory\nbank of pseudo-labeled target samples throughout training and cut-and-pastes\nthe most confident ones onto the current training batch. We implement ECAP on\ntop of the recent method MIC and boost its performance on two synthetic-to-real\ndomain adaptation benchmarks. Notably, MIC+ECAP reaches an unprecedented\nperformance of 69.1 mIoU on the Synthia-&gt;Cityscapes benchmark. Our code is\navailable at https://github.com/ErikBrorsson/ECAP.",
        "translated": ""
    },
    {
        "title": "MedMamba: Vision Mamba for Medical Image Classification",
        "url": "http://arxiv.org/abs/2403.03849v1",
        "pub_date": "2024-03-06",
        "summary": "Medical image classification is a very fundamental and crucial task in the\nfield of computer vision. These years, CNN-based and Transformer-based models\nare widely used in classifying various medical images. Unfortunately, The\nlimitation of CNNs in long-range modeling capabilities prevent them from\neffectively extracting fine-grained features in medical images , while\nTransformers are hampered by their quadratic computational complexity. Recent\nresearch has shown that the state space model (SSM) represented by Mamba can\nefficiently model long-range interactions while maintaining linear\ncomputational complexity. Inspired by this, we propose Vision Mamba for medical\nimage classification (MedMamba). More specifically, we introduce a novel\nConv-SSM module, which combines the local feature extraction ability of\nconvolutional layers with the ability of SSM to capture long-range dependency.\nTo demonstrate the potential of MedMamba, we conduct extensive experiments\nusing three publicly available medical datasets with different imaging\ntechniques (i.e., Kvasir (endoscopic images), FETAL_PLANES_DB (ultrasound\nimages) and Covid19-Pneumonia-Normal Chest X-Ray (X-ray images)) and two\nprivate datasets built by ourselves. Experimental results show that the\nproposed MedMamba performs well in detecting lesions in various medical images.\nTo the best of our knowledge, this is the first Vision Mamba tailored for\nmedical image classification. The purpose of this work is to establish a new\nbaseline for medical image classification tasks and provide valuable insights\nfor the future development of more efficient and effective SSM-based artificial\nintelligence algorithms and application systems in the medical. Source code has\nbeen available at https://github.com/YubiaoYue/MedMamba.",
        "translated": ""
    },
    {
        "title": "Efficient LoFTR: Semi-Dense Local Feature Matching with Sparse-Like\n  Speed",
        "url": "http://arxiv.org/abs/2403.04765v1",
        "pub_date": "2024-03-07",
        "summary": "We present a novel method for efficiently producing semi-dense matches across\nimages. Previous detector-free matcher LoFTR has shown remarkable matching\ncapability in handling large-viewpoint change and texture-poor scenarios but\nsuffers from low efficiency. We revisit its design choices and derive multiple\nimprovements for both efficiency and accuracy. One key observation is that\nperforming the transformer over the entire feature map is redundant due to\nshared local information, therefore we propose an aggregated attention\nmechanism with adaptive token selection for efficiency. Furthermore, we find\nspatial variance exists in LoFTR's fine correlation module, which is adverse to\nmatching accuracy. A novel two-stage correlation layer is proposed to achieve\naccurate subpixel correspondences for accuracy improvement. Our efficiency\noptimized model is $\\sim 2.5\\times$ faster than LoFTR which can even surpass\nstate-of-the-art efficient sparse matching pipeline SuperPoint + LightGlue.\nMoreover, extensive experiments show that our method can achieve higher\naccuracy compared with competitive semi-dense matchers, with considerable\nefficiency benefits. This opens up exciting prospects for large-scale or\nlatency-sensitive applications such as image retrieval and 3D reconstruction.\nProject page: https://zju3dv.github.io/efficientloftr.",
        "translated": ""
    },
    {
        "title": "That's My Point: Compact Object-centric LiDAR Pose Estimation for\n  Large-scale Outdoor Localisation",
        "url": "http://arxiv.org/abs/2403.04755v1",
        "pub_date": "2024-03-07",
        "summary": "This paper is about 3D pose estimation on LiDAR scans with extremely minimal\nstorage requirements to enable scalable mapping and localisation. We achieve\nthis by clustering all points of segmented scans into semantic objects and\nrepresenting them only with their respective centroid and semantic class. In\nthis way, each LiDAR scan is reduced to a compact collection of four-number\nvectors. This abstracts away important structural information from the scenes,\nwhich is crucial for traditional registration approaches. To mitigate this, we\nintroduce an object-matching network based on self- and cross-correlation that\ncaptures geometric and semantic relationships between entities. The respective\nmatches allow us to recover the relative transformation between scans through\nweighted Singular Value Decomposition (SVD) and RANdom SAmple Consensus\n(RANSAC). We demonstrate that such representation is sufficient for metric\nlocalisation by registering point clouds taken under different viewpoints on\nthe KITTI dataset, and at different periods of time localising between KITTI\nand KITTI-360. We achieve accurate metric estimates comparable with\nstate-of-the-art methods with almost half the representation size, specifically\n1.33 kB on average.",
        "translated": ""
    },
    {
        "title": "I Can't Believe It's Not Scene Flow!",
        "url": "http://arxiv.org/abs/2403.04739v1",
        "pub_date": "2024-03-07",
        "summary": "Current scene flow methods broadly fail to describe motion on small objects,\nand current scene flow evaluation protocols hide this failure by averaging over\nmany points, with most drawn larger objects. To fix this evaluation failure, we\npropose a new evaluation protocol, Bucket Normalized EPE, which is class-aware\nand speed-normalized, enabling contextualized error comparisons between object\ntypes that move at vastly different speeds. To highlight current method\nfailures, we propose a frustratingly simple supervised scene flow baseline,\nTrackFlow, built by bolting a high-quality pretrained detector (trained using\nmany class rebalancing techniques) onto a simple tracker, that produces\nstate-of-the-art performance on current standard evaluations and large\nimprovements over prior art on our new evaluation. Our results make it clear\nthat all scene flow evaluations must be class and speed aware, and supervised\nscene flow methods must address point class imbalances. We release the\nevaluation code publicly at\nhttps://github.com/kylevedder/BucketedSceneFlowEval.",
        "translated": ""
    },
    {
        "title": "SnapNTell: Enhancing Entity-Centric Visual Question Answering with\n  Retrieval Augmented Multimodal LLM",
        "url": "http://arxiv.org/abs/2403.04735v1",
        "pub_date": "2024-03-07",
        "summary": "Vision-extended LLMs have made significant strides in Visual Question\nAnswering (VQA). Despite these advancements, VLLMs still encounter substantial\ndifficulties in handling queries involving long-tail entities, with a tendency\nto produce erroneous or hallucinated responses. In this work, we introduce a\nnovel evaluative benchmark named \\textbf{SnapNTell}, specifically tailored for\nentity-centric VQA. This task aims to test the models' capabilities in\nidentifying entities and providing detailed, entity-specific knowledge. We have\ndeveloped the \\textbf{SnapNTell Dataset}, distinct from traditional VQA\ndatasets: (1) It encompasses a wide range of categorized entities, each\nrepresented by images and explicitly named in the answers; (2) It features QA\npairs that require extensive knowledge for accurate responses. The dataset is\norganized into 22 major categories, containing 7,568 unique entities in total.\nFor each entity, we curated 10 illustrative images and crafted 10\nknowledge-intensive QA pairs. To address this novel task, we devised a\nscalable, efficient, and transparent retrieval-augmented multimodal LLM. Our\napproach markedly outperforms existing methods on the SnapNTell dataset,\nachieving a 66.5\\% improvement in the BELURT score. We will soon make the\ndataset and the source code publicly accessible.",
        "translated": ""
    },
    {
        "title": "How Far Are We from Intelligent Visual Deductive Reasoning?",
        "url": "http://arxiv.org/abs/2403.04732v1",
        "pub_date": "2024-03-07",
        "summary": "Vision-Language Models (VLMs) such as GPT-4V have recently demonstrated\nincredible strides on diverse vision language tasks. We dig into vision-based\ndeductive reasoning, a more sophisticated but less explored realm, and find\npreviously unexposed blindspots in the current SOTA VLMs. Specifically, we\nleverage Raven's Progressive Matrices (RPMs), to assess VLMs' abilities to\nperform multi-hop relational and deductive reasoning relying solely on visual\nclues. We perform comprehensive evaluations of several popular VLMs employing\nstandard strategies such as in-context learning, self-consistency, and\nChain-of-thoughts (CoT) on three diverse datasets, including the Mensa IQ test,\nIntelligenceTest, and RAVEN. The results reveal that despite the impressive\ncapabilities of LLMs in text-based reasoning, we are still far from achieving\ncomparable proficiency in visual deductive reasoning. We found that certain\nstandard strategies that are effective when applied to LLMs do not seamlessly\ntranslate to the challenges presented by visual reasoning tasks. Moreover, a\ndetailed analysis reveals that VLMs struggle to solve these tasks mainly\nbecause they are unable to perceive and comprehend multiple, confounding\nabstract patterns in RPM examples.",
        "translated": ""
    },
    {
        "title": "Masked Capsule Autoencoders",
        "url": "http://arxiv.org/abs/2403.04724v1",
        "pub_date": "2024-03-07",
        "summary": "We propose Masked Capsule Autoencoders (MCAE), the first Capsule Network that\nutilises pretraining in a self-supervised manner. Capsule Networks have emerged\nas a powerful alternative to Convolutional Neural Networks (CNNs), and have\nshown favourable properties when compared to Vision Transformers (ViT), but\nhave struggled to effectively learn when presented with more complex data,\nleading to Capsule Network models that do not scale to modern tasks. Our\nproposed MCAE model alleviates this issue by reformulating the Capsule Network\nto use masked image modelling as a pretraining stage before finetuning in a\nsupervised manner. Across several experiments and ablations studies we\ndemonstrate that similarly to CNNs and ViTs, Capsule Networks can also benefit\nfrom self-supervised pretraining, paving the way for further advancements in\nthis neural network domain. For instance, pretraining on the Imagenette\ndataset, a dataset of 10 classes of Imagenet-sized images, we achieve not only\nstate-of-the-art results for Capsule Networks but also a 9% improvement\ncompared to purely supervised training. Thus we propose that Capsule Networks\nbenefit from and should be trained within a masked image modelling framework,\nwith a novel capsule decoder, to improve a Capsule Network's performance on\nrealistic-sized images.",
        "translated": ""
    },
    {
        "title": "ObjectCompose: Evaluating Resilience of Vision-Based Models on\n  Object-to-Background Compositional Changes",
        "url": "http://arxiv.org/abs/2403.04701v1",
        "pub_date": "2024-03-07",
        "summary": "Given the large-scale multi-modal training of recent vision-based models and\ntheir generalization capabilities, understanding the extent of their robustness\nis critical for their real-world deployment. In this work, we evaluate the\nresilience of current vision-based models against diverse object-to-background\ncontext variations. The majority of robustness evaluation methods have\nintroduced synthetic datasets to induce changes to object characteristics\n(viewpoints, scale, color) or utilized image transformation techniques\n(adversarial changes, common corruptions) on real images to simulate shifts in\ndistributions. Recent works have explored leveraging large language models and\ndiffusion models to generate changes in the background. However, these methods\neither lack in offering control over the changes to be made or distort the\nobject semantics, making them unsuitable for the task. Our method, on the other\nhand, can induce diverse object-to-background changes while preserving the\noriginal semantics and appearance of the object. To achieve this goal, we\nharness the generative capabilities of text-to-image, image-to-text, and\nimage-to-segment models to automatically generate a broad spectrum of\nobject-to-background changes. We induce both natural and adversarial background\nchanges by either modifying the textual prompts or optimizing the latents and\ntextual embedding of text-to-image models. This allows us to quantify the role\nof background context in understanding the robustness and generalization of\ndeep neural networks. We produce various versions of standard vision datasets\n(ImageNet, COCO), incorporating either diverse and realistic backgrounds into\nthe images or introducing color, texture, and adversarial changes in the\nbackground. We conduct extensive experiment to analyze the robustness of\nvision-based models against object-to-background context variations across\ndiverse tasks.",
        "translated": ""
    },
    {
        "title": "Delving into the Trajectory Long-tail Distribution for Muti-object\n  Tracking",
        "url": "http://arxiv.org/abs/2403.04700v1",
        "pub_date": "2024-03-07",
        "summary": "Multiple Object Tracking (MOT) is a critical area within computer vision,\nwith a broad spectrum of practical implementations. Current research has\nprimarily focused on the development of tracking algorithms and enhancement of\npost-processing techniques. Yet, there has been a lack of thorough examination\nconcerning the nature of tracking data it self. In this study, we pioneer an\nexploration into the distribution patterns of tracking data and identify a\npronounced long-tail distribution issue within existing MOT datasets. We note a\nsignificant imbalance in the distribution of trajectory lengths across\ndifferent pedestrians, a phenomenon we refer to as \"pedestrians trajectory\nlong-tail distribution\". Addressing this challenge, we introduce a bespoke\nstrategy designed to mitigate the effects of this skewed distribution.\nSpecifically, we propose two data augmentation strategies, including Stationary\nCamera View Data Augmentation (SVA) and Dynamic Camera View Data Augmentation\n(DVA) , designed for viewpoint states and the Group Softmax (GS) module for\nRe-ID. SVA is to backtrack and predict the pedestrian trajectory of tail\nclasses, and DVA is to use diffusion model to change the background of the\nscene. GS divides the pedestrians into unrelated groups and performs softmax\noperation on each group individually. Our proposed strategies can be integrated\ninto numerous existing tracking systems, and extensive experimentation\nvalidates the efficacy of our method in reducing the influence of long-tail\ndistribution on multi-object tracking performance. The code is available at\nhttps://github.com/chen-si-jia/Trajectory-Long-tail-Distribution-for-MOT.",
        "translated": ""
    },
    {
        "title": "AUFormer: Vision Transformers are Parameter-Efficient Facial Action Unit\n  Detectors",
        "url": "http://arxiv.org/abs/2403.04697v1",
        "pub_date": "2024-03-07",
        "summary": "Facial Action Units (AU) is a vital concept in the realm of affective\ncomputing, and AU detection has always been a hot research topic. Existing\nmethods suffer from overfitting issues due to the utilization of a large number\nof learnable parameters on scarce AU-annotated datasets or heavy reliance on\nsubstantial additional relevant data. Parameter-Efficient Transfer Learning\n(PETL) provides a promising paradigm to address these challenges, whereas its\nexisting methods lack design for AU characteristics. Therefore, we innovatively\ninvestigate PETL paradigm to AU detection, introducing AUFormer and proposing a\nnovel Mixture-of-Knowledge Expert (MoKE) collaboration mechanism. An individual\nMoKE specific to a certain AU with minimal learnable parameters first\nintegrates personalized multi-scale and correlation knowledge. Then the MoKE\ncollaborates with other MoKEs in the expert group to obtain aggregated\ninformation and inject it into the frozen Vision Transformer (ViT) to achieve\nparameter-efficient AU detection. Additionally, we design a Margin-truncated\nDifficulty-aware Weighted Asymmetric Loss (MDWA-Loss), which can encourage the\nmodel to focus more on activated AUs, differentiate the difficulty of\nunactivated AUs, and discard potential mislabeled samples. Extensive\nexperiments from various perspectives, including within-domain, cross-domain,\ndata efficiency, and micro-expression domain, demonstrate AUFormer's\nstate-of-the-art performance and robust generalization abilities without\nrelying on additional relevant data. The code for AUFormer is available at\nhttps://github.com/yuankaishen2001/AUFormer.",
        "translated": ""
    },
    {
        "title": "PixArt-Σ: Weak-to-Strong Training of Diffusion Transformer for 4K\n  Text-to-Image Generation",
        "url": "http://arxiv.org/abs/2403.04692v1",
        "pub_date": "2024-03-07",
        "summary": "In this paper, we introduce PixArt-\\Sigma, a Diffusion Transformer\nmodel~(DiT) capable of directly generating images at 4K resolution.\nPixArt-\\Sigma represents a significant advancement over its predecessor,\nPixArt-\\alpha, offering images of markedly higher fidelity and improved\nalignment with text prompts. A key feature of PixArt-\\Sigma is its training\nefficiency. Leveraging the foundational pre-training of PixArt-\\alpha, it\nevolves from the `weaker' baseline to a `stronger' model via incorporating\nhigher quality data, a process we term \"weak-to-strong training\". The\nadvancements in PixArt-\\Sigma are twofold: (1) High-Quality Training Data:\nPixArt-\\Sigma incorporates superior-quality image data, paired with more\nprecise and detailed image captions. (2) Efficient Token Compression: we\npropose a novel attention module within the DiT framework that compresses both\nkeys and values, significantly improving efficiency and facilitating\nultra-high-resolution image generation. Thanks to these improvements,\nPixArt-\\Sigma achieves superior image quality and user prompt adherence\ncapabilities with significantly smaller model size (0.6B parameters) than\nexisting text-to-image diffusion models, such as SDXL (2.6B parameters) and SD\nCascade (5.1B parameters). Moreover, PixArt-\\Sigma's capability to generate 4K\nimages supports the creation of high-resolution posters and wallpapers,\nefficiently bolstering the production of high-quality visual content in\nindustries such as film and gaming.",
        "translated": ""
    },
    {
        "title": "Tell, Don't Show!: Language Guidance Eases Transfer Across Domains in\n  Images and Videos",
        "url": "http://arxiv.org/abs/2403.05535v1",
        "pub_date": "2024-03-08",
        "summary": "We introduce LaGTran, a novel framework that utilizes readily available or\neasily acquired text descriptions to guide robust transfer of discriminative\nknowledge from labeled source to unlabeled target data with domain shifts.\nWhile unsupervised adaptation methods have been established to address this\nproblem, they show limitations in handling challenging domain shifts due to\ntheir exclusive operation within the pixel-space. Motivated by our observation\nthat semantically richer text modality has more favorable transfer properties,\nwe devise a transfer mechanism to use a source-trained text-classifier to\ngenerate predictions on the target text descriptions, and utilize these\npredictions as supervision for the corresponding images. Our approach driven by\nlanguage guidance is surprisingly easy and simple, yet significantly\noutperforms all prior approaches on challenging datasets like GeoNet and\nDomainNet, validating its extreme effectiveness. To further extend the scope of\nour study beyond images, we introduce a new benchmark to study ego-exo transfer\nin videos and find that our language-aided LaGTran yields significant gains in\nthis highly challenging and non-trivial transfer setting. Code, models, and\nproposed datasets are publicly available at\nhttps://tarun005.github.io/lagtran/.",
        "translated": ""
    },
    {
        "title": "Tune without Validation: Searching for Learning Rate and Weight Decay on\n  Training Sets",
        "url": "http://arxiv.org/abs/2403.05532v1",
        "pub_date": "2024-03-08",
        "summary": "We introduce Tune without Validation (Twin), a pipeline for tuning learning\nrate and weight decay without validation sets. We leverage a recent theoretical\nframework concerning learning phases in hypothesis space to devise a heuristic\nthat predicts what hyper-parameter (HP) combinations yield better\ngeneralization. Twin performs a grid search of trials according to an\nearly-/non-early-stopping scheduler and then segments the region that provides\nthe best results in terms of training loss. Among these trials, the weight norm\nstrongly correlates with predicting generalization. To assess the effectiveness\nof Twin, we run extensive experiments on 20 image classification datasets and\ntrain several families of deep networks, including convolutional, transformer,\nand feed-forward models. We demonstrate proper HP selection when training from\nscratch and fine-tuning, emphasizing small-sample scenarios.",
        "translated": ""
    },
    {
        "title": "Beyond Finite Data: Towards Data-free Out-of-distribution Generalization\n  via Extrapola",
        "url": "http://arxiv.org/abs/2403.05523v1",
        "pub_date": "2024-03-08",
        "summary": "Out-of-distribution (OOD) generalization is a favorable yet challenging\nproperty for deep neural networks. The core challenges lie in the limited\navailability of source domains that help models learn an invariant\nrepresentation from the spurious features. Various domain augmentation have\nbeen proposed but largely rely on interpolating existing domains and frequently\nface difficulties in creating truly \"novel\" domains. Humans, on the other hand,\ncan easily extrapolate novel domains, thus, an intriguing question arises: How\ncan neural networks extrapolate like humans and achieve OOD generalization?\n  We introduce a novel approach to domain extrapolation that leverages\nreasoning ability and the extensive knowledge encapsulated within large\nlanguage models (LLMs) to synthesize entirely new domains. Starting with the\nclass of interest, we query the LLMs to extract relevant knowledge for these\nnovel domains. We then bridge the gap between the text-centric knowledge\nderived from LLMs and the pixel input space of the model using text-to-image\ngeneration techniques. By augmenting the training set of domain generalization\ndatasets with high-fidelity, photo-realistic images of these new domains, we\nachieve significant improvements over all existing methods, as demonstrated in\nboth single and multi-domain generalization across various benchmarks.\n  With the ability to extrapolate any domains for any class, our method has the\npotential to learn a generalized model for any task without any data. To\nillustrate, we put forth a much more difficult setting termed, data-free domain\ngeneralization, that aims to learn a generalized model in the absence of any\ncollected data. Our empirical findings support the above argument and our\nmethods exhibit commendable performance in this setting, even surpassing the\nsupervised setting by approximately 1-2\\% on datasets such as VLCS.",
        "translated": ""
    },
    {
        "title": "Probabilistic Image-Driven Traffic Modeling via Remote Sensing",
        "url": "http://arxiv.org/abs/2403.05521v1",
        "pub_date": "2024-03-08",
        "summary": "This work addresses the task of modeling spatiotemporal traffic patterns\ndirectly from overhead imagery, which we refer to as image-driven traffic\nmodeling. We extend this line of work and introduce a multi-modal, multi-task\ntransformer-based segmentation architecture that can be used to create dense\ncity-scale traffic models. Our approach includes a geo-temporal positional\nencoding module for integrating geo-temporal context and a probabilistic\nobjective function for estimating traffic speeds that naturally models temporal\nvariations. We evaluate our method extensively using the Dynamic Traffic Speeds\n(DTS) benchmark dataset and significantly improve the state-of-the-art.\nFinally, we introduce the DTS++ dataset to support mobility-related location\nadaptation experiments.",
        "translated": ""
    },
    {
        "title": "Poly-View Contrastive Learning",
        "url": "http://arxiv.org/abs/2403.05490v1",
        "pub_date": "2024-03-08",
        "summary": "Contrastive learning typically matches pairs of related views among a number\nof unrelated negative views. Views can be generated (e.g. by augmentations) or\nbe observed. We investigate matching when there are more than two related views\nwhich we call poly-view tasks, and derive new representation learning\nobjectives using information maximization and sufficient statistics. We show\nthat with unlimited computation, one should maximize the number of related\nviews, and with a fixed compute budget, it is beneficial to decrease the number\nof unique samples whilst increasing the number of views of those samples. In\nparticular, poly-view contrastive models trained for 128 epochs with batch size\n256 outperform SimCLR trained for 1024 epochs at batch size 4096 on ImageNet1k,\nchallenging the belief that contrastive models require large batch sizes and\nmany training epochs.",
        "translated": ""
    },
    {
        "title": "JointMotion: Joint Self-supervision for Joint Motion Prediction",
        "url": "http://arxiv.org/abs/2403.05489v1",
        "pub_date": "2024-03-08",
        "summary": "We present JointMotion, a self-supervised learning method for joint motion\nprediction in autonomous driving. Our method includes a scene-level objective\nconnecting motion and environments, and an instance-level objective to refine\nlearned representations. Our evaluations show that these objectives are\ncomplementary and outperform recent contrastive and autoencoding methods as\npre-training for joint motion prediction. Furthermore, JointMotion adapts to\nall common types of environment representations used for motion prediction\n(i.e., agent-centric, scene-centric, and pairwise relative), and enables\neffective transfer learning between the Waymo Open Motion and the Argoverse 2\nForecasting datasets. Notably, our method improves the joint final displacement\nerror of Wayformer, Scene Transformer, and HPTR by 3%, 7%, and 11%,\nrespectively.",
        "translated": ""
    },
    {
        "title": "Will GPT-4 Run DOOM?",
        "url": "http://arxiv.org/abs/2403.05468v1",
        "pub_date": "2024-03-08",
        "summary": "We show that GPT-4's reasoning and planning capabilities extend to the 1993\nfirst-person shooter Doom. This large language model (LLM) is able to run and\nplay the game with only a few instructions, plus a textual\ndescription--generated by the model itself from screenshots--about the state of\nthe game being observed. We find that GPT-4 can play the game to a passable\ndegree: it is able to manipulate doors, combat enemies, and perform pathing.\nMore complex prompting strategies involving multiple model calls provide better\nresults. While further work is required to enable the LLM to play the game as\nwell as its classical, reinforcement learning-based counterparts, we note that\nGPT-4 required no training, leaning instead on its own reasoning and\nobservational capabilities. We hope our work pushes the boundaries on\nintelligent, LLM-based agents in video games. We conclude by discussing the\nethical implications of our work.",
        "translated": ""
    },
    {
        "title": "Grasping Trajectory Optimization with Point Clouds",
        "url": "http://arxiv.org/abs/2403.05466v1",
        "pub_date": "2024-03-08",
        "summary": "We introduce a new trajectory optimization method for robotic grasping based\non a point-cloud representation of robots and task spaces. In our method,\nrobots are represented by 3D points on their link surfaces. The task space of a\nrobot is represented by a point cloud that can be obtained from depth sensors.\nUsing the point-cloud representation, goal reaching in grasping can be\nformulated as point matching, while collision avoidance can be efficiently\nachieved by querying the signed distance values of the robot points in the\nsigned distance field of the scene points. Consequently, a constrained\nnon-linear optimization problem is formulated to solve the joint motion and\ngrasp planning problem. The advantage of our method is that the point-cloud\nrepresentation is general to be used with any robot in any environment. We\ndemonstrate the effectiveness of our method by conducting experiments on a\ntabletop scene and a shelf scene for grasping with a Fetch mobile manipulator\nand a Franka Panda arm.",
        "translated": ""
    },
    {
        "title": "The R2D2 deep neural network series paradigm for fast precision imaging\n  in radio astronomy",
        "url": "http://arxiv.org/abs/2403.05452v1",
        "pub_date": "2024-03-08",
        "summary": "Radio-interferometric (RI) imaging entails solving high-resolution\nhigh-dynamic range inverse problems from large data volumes. Recent image\nreconstruction techniques grounded in optimization theory have demonstrated\nremarkable capability for imaging precision, well beyond CLEAN's capability.\nThese range from advanced proximal algorithms propelled by handcrafted\nregularization operators, such as the SARA family, to hybrid plug-and-play\n(PnP) algorithms propelled by learned regularization denoisers, such as AIRI.\nOptimization and PnP structures are however highly iterative, which hinders\ntheir ability to handle the extreme data sizes expected from future\ninstruments. To address this scalability challenge, we introduce a novel deep\nlearning approach, dubbed ``Residual-to-Residual DNN series for high-Dynamic\nrange imaging'. R2D2's reconstruction is formed as a series of residual images,\niteratively estimated as outputs of Deep Neural Networks (DNNs) taking the\nprevious iteration's image estimate and associated data residual as inputs. It\nthus takes a hybrid structure between a PnP algorithm and a learned version of\nthe matching pursuit algorithm that underpins CLEAN. We present a comprehensive\nstudy of our approach, featuring its multiple incarnations distinguished by\ntheir DNN architectures. We provide a detailed description of its training\nprocess, targeting a telescope-specific approach. R2D2's capability to deliver\nhigh precision is demonstrated in simulation, across a variety of image and\nobservation settings using the Very Large Array (VLA). Its reconstruction speed\nis also demonstrated: with only few iterations required to clean data residuals\nat dynamic ranges up to 105, R2D2 opens the door to fast precision imaging.\nR2D2 codes are available in the BASPLib library on GitHub.",
        "translated": ""
    },
    {
        "title": "Attention-guided Feature Distillation for Semantic Segmentation",
        "url": "http://arxiv.org/abs/2403.05451v1",
        "pub_date": "2024-03-08",
        "summary": "In contrast to existing complex methodologies commonly employed for\ndistilling knowledge from a teacher to a student, the pro-posed method\nshowcases the efficacy of a simple yet powerful method for utilizing refined\nfeature maps to transfer attention. The proposed method has proven to be\neffective in distilling rich information, outperforming existing methods in\nsemantic segmentation as a dense prediction task. The proposed Attention-guided\nFeature Distillation (AttnFD) method, em-ploys the Convolutional Block\nAttention Module (CBAM), which refines feature maps by taking into account both\nchannel-specific and spatial information content. By only using the Mean\nSquared Error (MSE) loss function between the refined feature maps of the\nteacher and the student,AttnFD demonstrates outstanding performance in semantic\nsegmentation, achieving state-of-the-art results in terms of mean Intersection\nover Union (mIoU) on the PascalVoc 2012 and Cityscapes datasets. The Code is\navailable at https://github.com/AmirMansurian/AttnFD.",
        "translated": ""
    },
    {
        "title": "Attention Prompt Tuning: Parameter-efficient Adaptation of Pre-trained\n  Models for Spatiotemporal Modeling",
        "url": "http://arxiv.org/abs/2403.06978v1",
        "pub_date": "2024-03-11",
        "summary": "In this paper, we introduce Attention Prompt Tuning (APT) - a computationally\nefficient variant of prompt tuning for video-based applications such as action\nrecognition. Prompt tuning approaches involve injecting a set of learnable\nprompts along with data tokens during fine-tuning while keeping the backbone\nfrozen. This approach greatly reduces the number of learnable parameters\ncompared to full tuning. For image-based downstream tasks, normally a couple of\nlearnable prompts achieve results close to those of full tuning. However,\nvideos, which contain more complex spatiotemporal information, require hundreds\nof tunable prompts to achieve reasonably good results. This reduces the\nparameter efficiency observed in images and significantly increases latency and\nthe number of floating-point operations (FLOPs) during inference. To tackle\nthese issues, we directly inject the prompts into the keys and values of the\nnon-local attention mechanism within the transformer block. Additionally, we\nintroduce a novel prompt reparameterization technique to make APT more robust\nagainst hyperparameter selection. The proposed APT approach greatly reduces the\nnumber of FLOPs and latency while achieving a significant performance boost\nover the existing parameter-efficient tuning methods on UCF101, HMDB51, and\nSSv2 datasets for action recognition. The code and pre-trained models are\navailable at https://github.com/wgcban/apt",
        "translated": ""
    },
    {
        "title": "VideoMamba: State Space Model for Efficient Video Understanding",
        "url": "http://arxiv.org/abs/2403.06977v1",
        "pub_date": "2024-03-11",
        "summary": "Addressing the dual challenges of local redundancy and global dependencies in\nvideo understanding, this work innovatively adapts the Mamba to the video\ndomain. The proposed VideoMamba overcomes the limitations of existing 3D\nconvolution neural networks and video transformers. Its linear-complexity\noperator enables efficient long-term modeling, which is crucial for\nhigh-resolution long video understanding. Extensive evaluations reveal\nVideoMamba's four core abilities: (1) Scalability in the visual domain without\nextensive dataset pretraining, thanks to a novel self-distillation technique;\n(2) Sensitivity for recognizing short-term actions even with fine-grained\nmotion differences; (3) Superiority in long-term video understanding,\nshowcasing significant advancements over traditional feature-based models; and\n(4) Compatibility with other modalities, demonstrating robustness in\nmulti-modal contexts. Through these distinct advantages, VideoMamba sets a new\nbenchmark for video understanding, offering a scalable and efficient solution\nfor comprehensive video understanding. All the code and models are available at\nhttps://github.com/OpenGVLab/VideoMamba.",
        "translated": ""
    },
    {
        "title": "BrushNet: A Plug-and-Play Image Inpainting Model with Decomposed\n  Dual-Branch Diffusion",
        "url": "http://arxiv.org/abs/2403.06976v1",
        "pub_date": "2024-03-11",
        "summary": "Image inpainting, the process of restoring corrupted images, has seen\nsignificant advancements with the advent of diffusion models (DMs). Despite\nthese advancements, current DM adaptations for inpainting, which involve\nmodifications to the sampling strategy or the development of\ninpainting-specific DMs, frequently suffer from semantic inconsistencies and\nreduced image quality. Addressing these challenges, our work introduces a novel\nparadigm: the division of masked image features and noisy latent into separate\nbranches. This division dramatically diminishes the model's learning load,\nfacilitating a nuanced incorporation of essential masked image information in a\nhierarchical fashion. Herein, we present BrushNet, a novel plug-and-play\ndual-branch model engineered to embed pixel-level masked image features into\nany pre-trained DM, guaranteeing coherent and enhanced image inpainting\noutcomes. Additionally, we introduce BrushData and BrushBench to facilitate\nsegmentation-based inpainting training and performance assessment. Our\nextensive experimental analysis demonstrates BrushNet's superior performance\nover existing models across seven key metrics, including image quality, mask\nregion preservation, and textual coherence.",
        "translated": ""
    },
    {
        "title": "Memory-based Adapters for Online 3D Scene Perception",
        "url": "http://arxiv.org/abs/2403.06974v1",
        "pub_date": "2024-03-11",
        "summary": "In this paper, we propose a new framework for online 3D scene perception.\nConventional 3D scene perception methods are offline, i.e., take an already\nreconstructed 3D scene geometry as input, which is not applicable in robotic\napplications where the input data is streaming RGB-D videos rather than a\ncomplete 3D scene reconstructed from pre-collected RGB-D videos. To deal with\nonline 3D scene perception tasks where data collection and perception should be\nperformed simultaneously, the model should be able to process 3D scenes frame\nby frame and make use of the temporal information. To this end, we propose an\nadapter-based plug-and-play module for the backbone of 3D scene perception\nmodel, which constructs memory to cache and aggregate the extracted RGB-D\nfeatures to empower offline models with temporal learning ability.\nSpecifically, we propose a queued memory mechanism to cache the supporting\npoint cloud and image features. Then we devise aggregation modules which\ndirectly perform on the memory and pass temporal information to current frame.\nWe further propose 3D-to-2D adapter to enhance image features with strong\nglobal context. Our adapters can be easily inserted into mainstream offline\narchitectures of different tasks and significantly boost their performance on\nonline tasks. Extensive experiments on ScanNet and SceneNN datasets demonstrate\nour approach achieves leading performance on three 3D scene perception tasks\ncompared with state-of-the-art online methods by simply finetuning existing\noffline models, without any model and task-specific designs.\n\\href{https://xuxw98.github.io/Online3D/}{Project page}.",
        "translated": ""
    },
    {
        "title": "Bayesian Diffusion Models for 3D Shape Reconstruction",
        "url": "http://arxiv.org/abs/2403.06973v1",
        "pub_date": "2024-03-11",
        "summary": "We present Bayesian Diffusion Models (BDM), a prediction algorithm that\nperforms effective Bayesian inference by tightly coupling the top-down (prior)\ninformation with the bottom-up (data-driven) procedure via joint diffusion\nprocesses. We show the effectiveness of BDM on the 3D shape reconstruction\ntask. Compared to prototypical deep learning data-driven approaches trained on\npaired (supervised) data-labels (e.g. image-point clouds) datasets, our BDM\nbrings in rich prior information from standalone labels (e.g. point clouds) to\nimprove the bottom-up 3D reconstruction. As opposed to the standard Bayesian\nframeworks where explicit prior and likelihood are required for the inference,\nBDM performs seamless information fusion via coupled diffusion processes with\nlearned gradient computation networks. The specialty of our BDM lies in its\ncapability to engage the active and effective information exchange and fusion\nof the top-down and bottom-up processes where each itself is a diffusion\nprocess. We demonstrate state-of-the-art results on both synthetic and\nreal-world benchmarks for 3D shape reconstruction.",
        "translated": ""
    },
    {
        "title": "Explainable Transformer Prototypes for Medical Diagnoses",
        "url": "http://arxiv.org/abs/2403.06961v1",
        "pub_date": "2024-03-11",
        "summary": "Deployments of artificial intelligence in medical diagnostics mandate not\njust accuracy and efficacy but also trust, emphasizing the need for\nexplainability in machine decisions. The recent trend in automated medical\nimage diagnostics leans towards the deployment of Transformer-based\narchitectures, credited to their impressive capabilities. Since the\nself-attention feature of transformers contributes towards identifying crucial\nregions during the classification process, they enhance the trustability of the\nmethods. However, the complex intricacies of these attention mechanisms may\nfall short of effectively pinpointing the regions of interest directly\ninfluencing AI decisions. Our research endeavors to innovate a unique attention\nblock that underscores the correlation between 'regions' rather than 'pixels'.\nTo address this challenge, we introduce an innovative system grounded in\nprototype learning, featuring an advanced self-attention mechanism that goes\nbeyond conventional ad-hoc visual explanation techniques by offering\ncomprehensible visual insights. A combined quantitative and qualitative\nmethodological approach was used to demonstrate the effectiveness of the\nproposed method on the large-scale NIH chest X-ray dataset. Experimental\nresults showed that our proposed method offers a promising direction for\nexplainability, which can lead to the development of more trustable systems,\nwhich can facilitate easier and rapid adoption of such technology into routine\nclinics. The code is available at www.github.com/NUBagcilab/r2r_proto.",
        "translated": ""
    },
    {
        "title": "Optimizing Latent Graph Representations of Surgical Scenes for Zero-Shot\n  Domain Transfer",
        "url": "http://arxiv.org/abs/2403.06953v1",
        "pub_date": "2024-03-11",
        "summary": "Purpose: Advances in deep learning have resulted in effective models for\nsurgical video analysis; however, these models often fail to generalize across\nmedical centers due to domain shift caused by variations in surgical workflow,\ncamera setups, and patient demographics. Recently, object-centric learning has\nemerged as a promising approach for improved surgical scene understanding,\ncapturing and disentangling visual and semantic properties of surgical tools\nand anatomy to improve downstream task performance. In this work, we conduct a\nmulti-centric performance benchmark of object-centric approaches, focusing on\nCritical View of Safety assessment in laparoscopic cholecystectomy, then\npropose an improved approach for unseen domain generalization.\n  Methods: We evaluate four object-centric approaches for domain\ngeneralization, establishing baseline performance. Next, leveraging the\ndisentangled nature of object-centric representations, we dissect one of these\nmethods through a series of ablations (e.g. ignoring either visual or semantic\nfeatures for downstream classification). Finally, based on the results of these\nablations, we develop an optimized method specifically tailored for domain\ngeneralization, LG-DG, that includes a novel disentanglement loss function.\n  Results: Our optimized approach, LG-DG, achieves an improvement of 9.28% over\nthe best baseline approach. More broadly, we show that object-centric\napproaches are highly effective for domain generalization thanks to their\nmodular approach to representation learning.\n  Conclusion: We investigate the use of object-centric methods for unseen\ndomain generalization, identify method-agnostic factors critical for\nperformance, and present an optimized approach that substantially outperforms\nexisting methods.",
        "translated": ""
    },
    {
        "title": "SELMA: Learning and Merging Skill-Specific Text-to-Image Experts with\n  Auto-Generated Data",
        "url": "http://arxiv.org/abs/2403.06952v1",
        "pub_date": "2024-03-11",
        "summary": "Recent text-to-image (T2I) generation models have demonstrated impressive\ncapabilities in creating images from text descriptions. However, these T2I\ngeneration models often fall short of generating images that precisely match\nthe details of the text inputs, such as incorrect spatial relationship or\nmissing objects. In this paper, we introduce SELMA: Skill-Specific Expert\nLearning and Merging with Auto-Generated Data, a novel paradigm to improve the\nfaithfulness of T2I models by fine-tuning models on automatically generated,\nmulti-skill image-text datasets, with skill-specific expert learning and\nmerging. First, SELMA leverages an LLM's in-context learning capability to\ngenerate multiple datasets of text prompts that can teach different skills, and\nthen generates the images with a T2I model based on the prompts. Next, SELMA\nadapts the T2I model to the new skills by learning multiple single-skill LoRA\n(low-rank adaptation) experts followed by expert merging. Our independent\nexpert fine-tuning specializes multiple models for different skills, and expert\nmerging helps build a joint multi-skill T2I model that can generate faithful\nimages given diverse text prompts, while mitigating the knowledge conflict from\ndifferent datasets. We empirically demonstrate that SELMA significantly\nimproves the semantic alignment and text faithfulness of state-of-the-art T2I\ndiffusion models on multiple benchmarks (+2.1% on TIFA and +6.9% on DSG), human\npreference metrics (PickScore, ImageReward, and HPS), as well as human\nevaluation. Moreover, fine-tuning with image-text pairs auto-collected via\nSELMA shows comparable performance to fine-tuning with ground truth data.\nLastly, we show that fine-tuning with images from a weaker T2I model can help\nimprove the generation quality of a stronger T2I model, suggesting promising\nweak-to-strong generalization in T2I models.",
        "translated": ""
    },
    {
        "title": "DEADiff: An Efficient Stylization Diffusion Model with Disentangled\n  Representations",
        "url": "http://arxiv.org/abs/2403.06951v2",
        "pub_date": "2024-03-11",
        "summary": "The diffusion-based text-to-image model harbors immense potential in\ntransferring reference style. However, current encoder-based approaches\nsignificantly impair the text controllability of text-to-image models while\ntransferring styles. In this paper, we introduce DEADiff to address this issue\nusing the following two strategies: 1) a mechanism to decouple the style and\nsemantics of reference images. The decoupled feature representations are first\nextracted by Q-Formers which are instructed by different text descriptions.\nThen they are injected into mutually exclusive subsets of cross-attention\nlayers for better disentanglement. 2) A non-reconstructive learning method. The\nQ-Formers are trained using paired images rather than the identical target, in\nwhich the reference image and the ground-truth image are with the same style or\nsemantics. We show that DEADiff attains the best visual stylization results and\noptimal balance between the text controllability inherent in the text-to-image\nmodel and style similarity to the reference image, as demonstrated both\nquantitatively and qualitatively. Our project page is\nhttps://tianhao-qi.github.io/DEADiff/.",
        "translated": ""
    },
    {
        "title": "Applicability of oculomics for individual risk prediction: Repeatability\n  and robustness of retinal Fractal Dimension using DART and AutoMorph",
        "url": "http://arxiv.org/abs/2403.06950v1",
        "pub_date": "2024-03-11",
        "summary": "Purpose: To investigate whether Fractal Dimension (FD)-based oculomics could\nbe used for individual risk prediction by evaluating repeatability and\nrobustness. Methods: We used two datasets: Caledonia, healthy adults imaged\nmultiple times in quick succession for research (26 subjects, 39 eyes, 377\ncolour fundus images), and GRAPE, glaucoma patients with baseline and follow-up\nvisits (106 subjects, 196 eyes, 392 images). Mean follow-up time was 18.3\nmonths in GRAPE, thus it provides a pessimistic lower-bound as vasculature\ncould change. FD was computed with DART and AutoMorph. Image quality was\nassessed with QuickQual, but no images were initially excluded. Pearson,\nSpearman, and Intraclass Correlation (ICC) were used for population-level\nrepeatability. For individual-level repeatability, we introduce measurement\nnoise parameter {\\lambda} which is within-eye Standard Deviation (SD) of FD\nmeasurements in units of between-eyes SD. Results: In Caledonia, ICC was 0.8153\nfor DART and 0.5779 for AutoMorph, Pearson/Spearman correlation (first and last\nimage) 0.7857/0.7824 for DART, and 0.3933/0.6253 for AutoMorph. In GRAPE,\nPearson/Spearman correlation (first and next visit) was 0.7479/0.7474 for DART,\nand 0.7109/0.7208 for AutoMorph (all p&lt;0.0001). Median {\\lambda} in Caledonia\nwithout exclusions was 3.55\\% for DART and 12.65\\% for AutoMorph, and improved\nto up to 1.67\\% and 6.64\\% with quality-based exclusions, respectively. Quality\nexclusions primarily mitigated large outliers. Worst quality in an eye\ncorrelated strongly with {\\lambda} (Pearson 0.5350-0.7550, depending on dataset\nand method, all p&lt;0.0001). Conclusions: Repeatability was sufficient for\nindividual-level predictions in heterogeneous populations. DART performed\nbetter on all metrics and might be able to detect small, longitudinal changes,\nhighlighting the potential of robust methods.",
        "translated": ""
    },
    {
        "title": "Beyond Text: Frozen Large Language Models in Visual Signal Comprehension",
        "url": "http://arxiv.org/abs/2403.07874v1",
        "pub_date": "2024-03-12",
        "summary": "In this work, we investigate the potential of a large language model (LLM) to\ndirectly comprehend visual signals without the necessity of fine-tuning on\nmulti-modal datasets. The foundational concept of our method views an image as\na linguistic entity, and translates it to a set of discrete words derived from\nthe LLM's vocabulary. To achieve this, we present the Vision-to-Language\nTokenizer, abbreviated as V2T Tokenizer, which transforms an image into a\n``foreign language'' with the combined aid of an encoder-decoder, the LLM\nvocabulary, and a CLIP model. With this innovative image encoding, the LLM\ngains the ability not only for visual comprehension but also for image\ndenoising and restoration in an auto-regressive fashion-crucially, without any\nfine-tuning. We undertake rigorous experiments to validate our method,\nencompassing understanding tasks like image recognition, image captioning, and\nvisual question answering, as well as image denoising tasks like inpainting,\noutpainting, deblurring, and shift restoration. Code and models are available\nat https://github.com/zh460045050/V2L-Tokenizer.",
        "translated": ""
    },
    {
        "title": "Bridging Different Language Models and Generative Vision Models for\n  Text-to-Image Generation",
        "url": "http://arxiv.org/abs/2403.07860v1",
        "pub_date": "2024-03-12",
        "summary": "Text-to-image generation has made significant advancements with the\nintroduction of text-to-image diffusion models. These models typically consist\nof a language model that interprets user prompts and a vision model that\ngenerates corresponding images. As language and vision models continue to\nprogress in their respective domains, there is a great potential in exploring\nthe replacement of components in text-to-image diffusion models with more\nadvanced counterparts. A broader research objective would therefore be to\ninvestigate the integration of any two unrelated language and generative vision\nmodels for text-to-image generation. In this paper, we explore this objective\nand propose LaVi-Bridge, a pipeline that enables the integration of diverse\npre-trained language models and generative vision models for text-to-image\ngeneration. By leveraging LoRA and adapters, LaVi-Bridge offers a flexible and\nplug-and-play approach without requiring modifications to the original weights\nof the language and vision models. Our pipeline is compatible with various\nlanguage models and generative vision models, accommodating different\nstructures. Within this framework, we demonstrate that incorporating superior\nmodules, such as more advanced language models or generative vision models,\nresults in notable improvements in capabilities like text alignment or image\nquality. Extensive evaluations have been conducted to verify the effectiveness\nof LaVi-Bridge. Code is available at\nhttps://github.com/ShihaoZhaoZSH/LaVi-Bridge.",
        "translated": ""
    },
    {
        "title": "Distilling the Knowledge in Data Pruning",
        "url": "http://arxiv.org/abs/2403.07854v1",
        "pub_date": "2024-03-12",
        "summary": "With the increasing size of datasets used for training neural networks, data\npruning becomes an attractive field of research. However, most current data\npruning algorithms are limited in their ability to preserve accuracy compared\nto models trained on the full data, especially in high pruning regimes. In this\npaper we explore the application of data pruning while incorporating knowledge\ndistillation (KD) when training on a pruned subset. That is, rather than\nrelying solely on ground-truth labels, we also use the soft predictions from a\nteacher network pre-trained on the complete data. By integrating KD into\ntraining, we demonstrate significant improvement across datasets, pruning\nmethods, and on all pruning fractions. We first establish a theoretical\nmotivation for employing self-distillation to improve training on pruned data.\nThen, we empirically make a compelling and highly practical observation: using\nKD, simple random pruning is comparable or superior to sophisticated pruning\nmethods across all pruning regimes. On ImageNet for example, we achieve\nsuperior accuracy despite training on a random subset of only 50% of the data.\nAdditionally, we demonstrate a crucial connection between the pruning factor\nand the optimal knowledge distillation weight. This helps mitigate the impact\nof samples with noisy labels and low-quality images retained by typical pruning\nalgorithms. Finally, we make an intriguing observation: when using lower\npruning fractions, larger teachers lead to accuracy degradation, while\nsurprisingly, employing teachers with a smaller capacity than the student's may\nimprove results. Our code will be made available.",
        "translated": ""
    },
    {
        "title": "12 mJ per Class On-Device Online Few-Shot Class-Incremental Learning",
        "url": "http://arxiv.org/abs/2403.07851v1",
        "pub_date": "2024-03-12",
        "summary": "Few-Shot Class-Incremental Learning (FSCIL) enables machine learning systems\nto expand their inference capabilities to new classes using only a few labeled\nexamples, without forgetting the previously learned classes. Classical\nbackpropagation-based learning and its variants are often unsuitable for\nbattery-powered, memory-constrained systems at the extreme edge. In this work,\nwe introduce Online Few-Shot Class-Incremental Learning (O-FSCIL), based on a\nlightweight model consisting of a pretrained and metalearned feature extractor\nand an expandable explicit memory storing the class prototypes. The\narchitecture is pretrained with a novel feature orthogonality regularization\nand metalearned with a multi-margin loss. For learning a new class, our\napproach extends the explicit memory with novel class prototypes, while the\nremaining architecture is kept frozen. This allows learning previously unseen\nclasses based on only a few examples with one single pass (hence online).\nO-FSCIL obtains an average accuracy of 68.62% on the FSCIL CIFAR100 benchmark,\nachieving state-of-the-art results. Tailored for ultra-low-power platforms, we\nimplement O-FSCIL on the 60 mW GAP9 microcontroller, demonstrating online\nlearning capabilities within just 12 mJ per new class.",
        "translated": ""
    },
    {
        "title": "MoPE-CLIP: Structured Pruning for Efficient Vision-Language Models with\n  Module-wise Pruning Error Metric",
        "url": "http://arxiv.org/abs/2403.07839v1",
        "pub_date": "2024-03-12",
        "summary": "Vision-language pre-trained models have achieved impressive performance on\nvarious downstream tasks. However, their large model sizes hinder their\nutilization on platforms with limited computational resources. We find that\ndirectly using smaller pre-trained models and applying magnitude-based pruning\non CLIP models leads to inflexibility and inferior performance. Recent efforts\nfor VLP compression either adopt uni-modal compression metrics resulting in\nlimited performance or involve costly mask-search processes with learnable\nmasks. In this paper, we first propose the Module-wise Pruning Error (MoPE)\nmetric, accurately assessing CLIP module importance by performance decline on\ncross-modal tasks. Using the MoPE metric, we introduce a unified pruning\nframework applicable to both pre-training and task-specific fine-tuning\ncompression stages. For pre-training, MoPE-CLIP effectively leverages knowledge\nfrom the teacher model, significantly reducing pre-training costs while\nmaintaining strong zero-shot capabilities. For fine-tuning, consecutive pruning\nfrom width to depth yields highly competitive task-specific models. Extensive\nexperiments in two stages demonstrate the effectiveness of the MoPE metric, and\nMoPE-CLIP outperforms previous state-of-the-art VLP compression methods.",
        "translated": ""
    },
    {
        "title": "When Eye-Tracking Meets Machine Learning: A Systematic Review on\n  Applications in Medical Image Analysis",
        "url": "http://arxiv.org/abs/2403.07834v1",
        "pub_date": "2024-03-12",
        "summary": "Eye-gaze tracking research offers significant promise in enhancing various\nhealthcare-related tasks, above all in medical image analysis and\ninterpretation. Eye tracking, a technology that monitors and records the\nmovement of the eyes, provides valuable insights into human visual attention\npatterns. This technology can transform how healthcare professionals and\nmedical specialists engage with and analyze diagnostic images, offering a more\ninsightful and efficient approach to medical diagnostics. Hence, extracting\nmeaningful features and insights from medical images by leveraging eye-gaze\ndata improves our understanding of how radiologists and other medical experts\nmonitor, interpret, and understand images for diagnostic purposes. Eye-tracking\ndata, with intricate human visual attention patterns embedded, provides a\nbridge to integrating artificial intelligence (AI) development and human\ncognition. This integration allows novel methods to incorporate domain\nknowledge into machine learning (ML) and deep learning (DL) approaches to\nenhance their alignment with human-like perception and decision-making.\nMoreover, extensive collections of eye-tracking data have also enabled novel\nML/DL methods to analyze human visual patterns, paving the way to a better\nunderstanding of human vision, attention, and cognition. This systematic review\ninvestigates eye-gaze tracking applications and methodologies for enhancing\nML/DL algorithms for medical image analysis in depth.",
        "translated": ""
    },
    {
        "title": "Label Dropout: Improved Deep Learning Echocardiography Segmentation\n  Using Multiple Datasets With Domain Shift and Partial Labelling",
        "url": "http://arxiv.org/abs/2403.07818v1",
        "pub_date": "2024-03-12",
        "summary": "Echocardiography (echo) is the first imaging modality used when assessing\ncardiac function. The measurement of functional biomarkers from echo relies\nupon the segmentation of cardiac structures and deep learning models have been\nproposed to automate the segmentation process. However, in order to translate\nthese tools to widespread clinical use it is important that the segmentation\nmodels are robust to a wide variety of images (e.g. acquired from different\nscanners, by operators with different levels of expertise etc.). To achieve\nthis level of robustness it is necessary that the models are trained with\nmultiple diverse datasets. A significant challenge faced when training with\nmultiple diverse datasets is the variation in label presence, i.e. the combined\ndata are often partially-labelled. Adaptations of the cross entropy loss\nfunction have been proposed to deal with partially labelled data. In this paper\nwe show that training naively with such a loss function and multiple diverse\ndatasets can lead to a form of shortcut learning, where the model associates\nlabel presence with domain characteristics, leading to a drop in performance.\nTo address this problem, we propose a novel label dropout scheme to break the\nlink between domain characteristics and the presence or absence of labels. We\ndemonstrate that label dropout improves echo segmentation Dice score by 62% and\n25% on two cardiac structures when training using multiple diverse partially\nlabelled datasets.",
        "translated": ""
    },
    {
        "title": "StyleGaussian: Instant 3D Style Transfer with Gaussian Splatting",
        "url": "http://arxiv.org/abs/2403.07807v1",
        "pub_date": "2024-03-12",
        "summary": "We introduce StyleGaussian, a novel 3D style transfer technique that allows\ninstant transfer of any image's style to a 3D scene at 10 frames per second\n(fps). Leveraging 3D Gaussian Splatting (3DGS), StyleGaussian achieves style\ntransfer without compromising its real-time rendering ability and multi-view\nconsistency. It achieves instant style transfer with three steps: embedding,\ntransfer, and decoding. Initially, 2D VGG scene features are embedded into\nreconstructed 3D Gaussians. Next, the embedded features are transformed\naccording to a reference style image. Finally, the transformed features are\ndecoded into the stylized RGB. StyleGaussian has two novel designs. The first\nis an efficient feature rendering strategy that first renders low-dimensional\nfeatures and then maps them into high-dimensional features while embedding VGG\nfeatures. It cuts the memory consumption significantly and enables 3DGS to\nrender the high-dimensional memory-intensive features. The second is a\nK-nearest-neighbor-based 3D CNN. Working as the decoder for the stylized\nfeatures, it eliminates the 2D CNN operations that compromise strict multi-view\nconsistency. Extensive experiments show that StyleGaussian achieves instant 3D\nstylization with superior stylization quality while preserving real-time\nrendering and strict multi-view consistency. Project page:\nhttps://kunhao-liu.github.io/StyleGaussian/",
        "translated": ""
    },
    {
        "title": "BraSyn 2023 challenge: Missing MRI synthesis and the effect of different\n  learning objectives",
        "url": "http://arxiv.org/abs/2403.07800v1",
        "pub_date": "2024-03-12",
        "summary": "This work is addressing the Brain Magnetic Resonance Image Synthesis for\nTumor Segmentation (BraSyn) challenge which was hosted as part of the Brain\nTumor Segmentation challenge (BraTS) 2023. In this challenge researchers are\ninvited to work on synthesizing a missing magnetic resonance image sequence\ngiven other available sequences to facilitate tumor segmentation pipelines\ntrained on complete sets of image sequences. This problem can be addressed\nusing deep learning in the framework of paired images-to-image translation. In\nthis work, we proposed to investigate the effectiveness of a commonly-used deep\nlearning framework such as Pix2Pix trained under supervision of different\nimage-quality loss functions. Our results indicate that using different loss\nfunctions significantly affects the synthesis quality. We systematically study\nthe impact of different loss functions in the multi-sequence MR image synthesis\nsetting of the BraSyn challenge. Furthermore, we show how image synthesis\nperformance can be optimized by beneficially combining different learning\nobjectives.",
        "translated": ""
    },
    {
        "title": "A Fourier Transform Framework for Domain Adaptation",
        "url": "http://arxiv.org/abs/2403.07798v1",
        "pub_date": "2024-03-12",
        "summary": "By using unsupervised domain adaptation (UDA), knowledge can be transferred\nfrom a label-rich source domain to a target domain that contains relevant\ninformation but lacks labels. Many existing UDA algorithms suffer from directly\nusing raw images as input, resulting in models that overly focus on redundant\ninformation and exhibit poor generalization capability. To address this issue,\nwe attempt to improve the performance of unsupervised domain adaptation by\nemploying the Fourier method (FTF).Specifically, FTF is inspired by the\namplitude of Fourier spectra, which primarily preserves low-level statistical\ninformation. In FTF, we effectively incorporate low-level information from the\ntarget domain into the source domain by fusing the amplitudes of both domains\nin the Fourier domain. Additionally, we observe that extracting features from\nbatches of images can eliminate redundant information while retaining\nclass-specific features relevant to the task. Building upon this observation,\nwe apply the Fourier Transform at the data stream level for the first time. To\nfurther align multiple sources of data, we introduce the concept of correlation\nalignment. To evaluate the effectiveness of our FTF method, we conducted\nevaluations on four benchmark datasets for domain adaptation, including\nOffice-31, Office-Home, ImageCLEF-DA, and Office-Caltech. Our results\ndemonstrate superior performance.",
        "translated": ""
    },
    {
        "title": "FastMAC: Stochastic Spectral Sampling of Correspondence Graph",
        "url": "http://arxiv.org/abs/2403.08770v1",
        "pub_date": "2024-03-13",
        "summary": "3D correspondence, i.e., a pair of 3D points, is a fundamental concept in\ncomputer vision. A set of 3D correspondences, when equipped with compatibility\nedges, forms a correspondence graph. This graph is a critical component in\nseveral state-of-the-art 3D point cloud registration approaches, e.g., the one\nbased on maximal cliques (MAC). However, its properties have not been well\nunderstood. So we present the first study that introduces graph signal\nprocessing into the domain of correspondence graph. We exploit the generalized\ndegree signal on correspondence graph and pursue sampling strategies that\npreserve high-frequency components of this signal. To address time-consuming\nsingular value decomposition in deterministic sampling, we resort to a\nstochastic approximate sampling strategy. As such, the core of our method is\nthe stochastic spectral sampling of correspondence graph. As an application, we\nbuild a complete 3D registration algorithm termed as FastMAC, that reaches\nreal-time speed while leading to little to none performance drop. Through\nextensive experiments, we validate that FastMAC works for both indoor and\noutdoor benchmarks. For example, FastMAC can accelerate MAC by 80 times while\nmaintaining high registration success rate on KITTI. Codes are publicly\navailable at https://github.com/Forrest-110/FastMAC.",
        "translated": ""
    },
    {
        "title": "3DFIRES: Few Image 3D REconstruction for Scenes with Hidden Surface",
        "url": "http://arxiv.org/abs/2403.08768v1",
        "pub_date": "2024-03-13",
        "summary": "This paper introduces 3DFIRES, a novel system for scene-level 3D\nreconstruction from posed images. Designed to work with as few as one view,\n3DFIRES reconstructs the complete geometry of unseen scenes, including hidden\nsurfaces. With multiple view inputs, our method produces full reconstruction\nwithin all camera frustums. A key feature of our approach is the fusion of\nmulti-view information at the feature level, enabling the production of\ncoherent and comprehensive 3D reconstruction. We train our system on\nnon-watertight scans from large-scale real scene dataset. We show it matches\nthe efficacy of single-view reconstruction methods with only one input and\nsurpasses existing techniques in both quantitative and qualitative measures for\nsparse-view 3D reconstruction.",
        "translated": ""
    },
    {
        "title": "MonoOcc: Digging into Monocular Semantic Occupancy Prediction",
        "url": "http://arxiv.org/abs/2403.08766v1",
        "pub_date": "2024-03-13",
        "summary": "Monocular Semantic Occupancy Prediction aims to infer the complete 3D\ngeometry and semantic information of scenes from only 2D images. It has\ngarnered significant attention, particularly due to its potential to enhance\nthe 3D perception of autonomous vehicles. However, existing methods rely on a\ncomplex cascaded framework with relatively limited information to restore 3D\nscenes, including a dependency on supervision solely on the whole network's\noutput, single-frame input, and the utilization of a small backbone. These\nchallenges, in turn, hinder the optimization of the framework and yield\ninferior prediction results, particularly concerning smaller and long-tailed\nobjects. To address these issues, we propose MonoOcc. In particular, we (i)\nimprove the monocular occupancy prediction framework by proposing an auxiliary\nsemantic loss as supervision to the shallow layers of the framework and an\nimage-conditioned cross-attention module to refine voxel features with visual\nclues, and (ii) employ a distillation module that transfers temporal\ninformation and richer knowledge from a larger image backbone to the monocular\nsemantic occupancy prediction framework with low cost of hardware. With these\nadvantages, our method yields state-of-the-art performance on the camera-based\nSemanticKITTI Scene Completion benchmark. Codes and models can be accessed at\nhttps://github.com/ucaszyp/MonoOcc",
        "translated": ""
    },
    {
        "title": "VLOGGER: Multimodal Diffusion for Embodied Avatar Synthesis",
        "url": "http://arxiv.org/abs/2403.08764v1",
        "pub_date": "2024-03-13",
        "summary": "We propose VLOGGER, a method for audio-driven human video generation from a\nsingle input image of a person, which builds on the success of recent\ngenerative diffusion models. Our method consists of 1) a stochastic\nhuman-to-3d-motion diffusion model, and 2) a novel diffusion-based architecture\nthat augments text-to-image models with both spatial and temporal controls.\nThis supports the generation of high quality video of variable length, easily\ncontrollable through high-level representations of human faces and bodies. In\ncontrast to previous work, our method does not require training for each\nperson, does not rely on face detection and cropping, generates the complete\nimage (not just the face or the lips), and considers a broad spectrum of\nscenarios (e.g. visible torso or diverse subject identities) that are critical\nto correctly synthesize humans who communicate. We also curate MENTOR, a new\nand diverse dataset with 3d pose and expression annotations, one order of\nmagnitude larger than previous ones (800,000 identities) and with dynamic\ngestures, on which we train and ablate our main technical contributions.\n  VLOGGER outperforms state-of-the-art methods in three public benchmarks,\nconsidering image quality, identity preservation and temporal consistency while\nalso generating upper-body gestures. We analyze the performance of VLOGGER with\nrespect to multiple diversity metrics, showing that our architectural choices\nand the use of MENTOR benefit training a fair and unbiased model at scale.\nFinally we show applications in video editing and personalization.",
        "translated": ""
    },
    {
        "title": "Segmentation of Knee Bones for Osteoarthritis Assessment: A Comparative\n  Analysis of Supervised, Few-Shot, and Zero-Shot Learning Approaches",
        "url": "http://arxiv.org/abs/2403.08761v1",
        "pub_date": "2024-03-13",
        "summary": "Knee osteoarthritis is a degenerative joint disease that induces chronic pain\nand disability. Bone morphological analysis is a promising tool to understand\nthe mechanical aspect of this disorder. This study proposes a 2D bone\nmorphological analysis using manually segmented bones to explore morphological\nfeatures related to distinct pain conditions. Furthermore, six semantic\nsegmentation algorithms are assessed for extracting femur and tibia bones from\nX-ray images. Our analysis reveals that the morphology of the femur undergoes\nsignificant changes in instances where pain worsens. Conversely, improvements\nin pain may not manifest pronounced alterations in bone shape. The\nfew-shot-learning-based algorithm, UniverSeg, demonstrated superior\nsegmentation results with Dice scores of 99.69% for femur and 99.60% for tibia.\nRegarding pain condition classification, the zero-shot-learning-based\nalgorithm, CP-SAM, achieved the highest accuracy at 66% among all models.\nUniverSeg is recommended for automatic knee bone segmentation, while SAM models\nshow potential with prompt encoder modifications for optimized outcomes. These\nfindings highlight the effectiveness of few-shot learning for semantic\nsegmentation and the potential of zero-shot learning in enhancing\nclassification models for knee osteoarthritis diagnosis.",
        "translated": ""
    },
    {
        "title": "MIM4D: Masked Modeling with Multi-View Video for Autonomous Driving\n  Representation Learning",
        "url": "http://arxiv.org/abs/2403.08760v1",
        "pub_date": "2024-03-13",
        "summary": "Learning robust and scalable visual representations from massive multi-view\nvideo data remains a challenge in computer vision and autonomous driving.\nExisting pre-training methods either rely on expensive supervised learning with\n3D annotations, limiting the scalability, or focus on single-frame or monocular\ninputs, neglecting the temporal information. We propose MIM4D, a novel\npre-training paradigm based on dual masked image modeling (MIM). MIM4D\nleverages both spatial and temporal relations by training on masked multi-view\nvideo inputs. It constructs pseudo-3D features using continuous scene flow and\nprojects them onto 2D plane for supervision. To address the lack of dense 3D\nsupervision, MIM4D reconstruct pixels by employing 3D volumetric differentiable\nrendering to learn geometric representations. We demonstrate that MIM4D\nachieves state-of-the-art performance on the nuScenes dataset for visual\nrepresentation learning in autonomous driving. It significantly improves\nexisting methods on multiple downstream tasks, including BEV segmentation (8.7%\nIoU), 3D object detection (3.5% mAP), and HD map construction (1.4% mAP). Our\nwork offers a new choice for learning representation at scale in autonomous\ndriving. Code and models are released at https://github.com/hustvl/MIM4D",
        "translated": ""
    },
    {
        "title": "Spatiotemporal Diffusion Model with Paired Sampling for Accelerated\n  Cardiac Cine MRI",
        "url": "http://arxiv.org/abs/2403.08758v1",
        "pub_date": "2024-03-13",
        "summary": "Current deep learning reconstruction for accelerated cardiac cine MRI suffers\nfrom spatial and temporal blurring. We aim to improve image sharpness and\nmotion delineation for cine MRI under high undersampling rates. A\nspatiotemporal diffusion enhancement model conditional on an existing deep\nlearning reconstruction along with a novel paired sampling strategy was\ndeveloped. The diffusion model provided sharper tissue boundaries and clearer\nmotion than the original reconstruction in experts evaluation on clinical data.\nThe innovative paired sampling strategy substantially reduced artificial noises\nin the generative results.",
        "translated": ""
    },
    {
        "title": "DAM: Dynamic Adapter Merging for Continual Video QA Learning",
        "url": "http://arxiv.org/abs/2403.08755v1",
        "pub_date": "2024-03-13",
        "summary": "We present a parameter-efficient method for continual video\nquestion-answering (VidQA) learning. Our method, named DAM, uses the proposed\nDynamic Adapter Merging to (i) mitigate catastrophic forgetting, (ii) enable\nefficient adaptation to continually arriving datasets, (iii) handle inputs from\nunknown datasets during inference, and (iv) enable knowledge sharing across\nsimilar dataset domains. Given a set of continually streaming VidQA datasets,\nwe sequentially train dataset-specific adapters for each dataset while freezing\nthe parameters of a large pretrained video-language backbone. During inference,\ngiven a video-question sample from an unknown domain, our method first uses the\nproposed non-parametric router function to compute a probability for each\nadapter, reflecting how relevant that adapter is to the current video-question\ninput instance. Subsequently, the proposed dynamic adapter merging scheme\naggregates all the adapter weights into a new adapter instance tailored for\nthat particular test sample to compute the final VidQA prediction, mitigating\nthe impact of inaccurate router predictions and facilitating knowledge sharing\nacross domains. Our DAM model outperforms prior state-of-the-art continual\nlearning approaches by 9.1% while exhibiting 1.9% less forgetting on 6 VidQA\ndatasets spanning various domains. We further extend DAM to continual image\nclassification and image QA and outperform prior methods by a large margin. The\ncode is publicly available at: https://github.com/klauscc/DAM",
        "translated": ""
    },
    {
        "title": "Clinically Feasible Diffusion Reconstruction for Highly-Accelerated\n  Cardiac Cine MRI",
        "url": "http://arxiv.org/abs/2403.08749v1",
        "pub_date": "2024-03-13",
        "summary": "The currently limited quality of accelerated cardiac cine reconstruction may\npotentially be improved by the emerging diffusion models, but the clinically\nunacceptable long processing time poses a challenge. We aim to develop a\nclinically feasible diffusion-model-based reconstruction pipeline to improve\nthe image quality of cine MRI. A multi-in multi-out diffusion enhancement model\ntogether with fast inference strategies were developed to be used in\nconjunction with a reconstruction model. The diffusion reconstruction reduced\nspatial and temporal blurring in prospectively undersampled clinical data, as\nvalidated by experts inspection. The 1.5s per video processing time enabled the\napproach to be applied in clinical scenarios.",
        "translated": ""
    },
    {
        "title": "Real-time 3D semantic occupancy prediction for autonomous vehicles using\n  memory-efficient sparse convolution",
        "url": "http://arxiv.org/abs/2403.08748v1",
        "pub_date": "2024-03-13",
        "summary": "In autonomous vehicles, understanding the surrounding 3D environment of the\nego vehicle in real-time is essential. A compact way to represent scenes while\nencoding geometric distances and semantic object information is via 3D semantic\noccupancy maps. State of the art 3D mapping methods leverage transformers with\ncross-attention mechanisms to elevate 2D vision-centric camera features into\nthe 3D domain. However, these methods encounter significant challenges in\nreal-time applications due to their high computational demands during\ninference. This limitation is particularly problematic in autonomous vehicles,\nwhere GPU resources must be shared with other tasks such as localization and\nplanning. In this paper, we introduce an approach that extracts features from\nfront-view 2D camera images and LiDAR scans, then employs a sparse convolution\nnetwork (Minkowski Engine), for 3D semantic occupancy prediction. Given that\noutdoor scenes in autonomous driving scenarios are inherently sparse, the\nutilization of sparse convolution is particularly apt. By jointly solving the\nproblems of 3D scene completion of sparse scenes and 3D semantic segmentation,\nwe provide a more efficient learning framework suitable for real-time\napplications in autonomous vehicles. We also demonstrate competitive accuracy\non the nuScenes dataset.",
        "translated": ""
    },
    {
        "title": "GroupContrast: Semantic-aware Self-supervised Representation Learning\n  for 3D Understanding",
        "url": "http://arxiv.org/abs/2403.09639v1",
        "pub_date": "2024-03-14",
        "summary": "Self-supervised 3D representation learning aims to learn effective\nrepresentations from large-scale unlabeled point clouds. Most existing\napproaches adopt point discrimination as the pretext task, which assigns\nmatched points in two distinct views as positive pairs and unmatched points as\nnegative pairs. However, this approach often results in semantically identical\npoints having dissimilar representations, leading to a high number of false\nnegatives and introducing a \"semantic conflict\" problem. To address this issue,\nwe propose GroupContrast, a novel approach that combines segment grouping and\nsemantic-aware contrastive learning. Segment grouping partitions points into\nsemantically meaningful regions, which enhances semantic coherence and provides\nsemantic guidance for the subsequent contrastive representation learning.\nSemantic-aware contrastive learning augments the semantic information extracted\nfrom segment grouping and helps to alleviate the issue of \"semantic conflict\".\nWe conducted extensive experiments on multiple 3D scene understanding tasks.\nThe results demonstrate that GroupContrast learns semantically meaningful\nrepresentations and achieves promising transfer learning performance.",
        "translated": ""
    },
    {
        "title": "SCP-Diff: Photo-Realistic Semantic Image Synthesis with\n  Spatial-Categorical Joint Prior",
        "url": "http://arxiv.org/abs/2403.09638v1",
        "pub_date": "2024-03-14",
        "summary": "Semantic image synthesis (SIS) shows good promises for sensor simulation.\nHowever, current best practices in this field, based on GANs, have not yet\nreached the desired level of quality. As latent diffusion models make\nsignificant strides in image generation, we are prompted to evaluate\nControlNet, a notable method for its dense control capabilities. Our\ninvestigation uncovered two primary issues with its results: the presence of\nweird sub-structures within large semantic areas and the misalignment of\ncontent with the semantic mask. Through empirical study, we pinpointed the\ncause of these problems as a mismatch between the noised training data\ndistribution and the standard normal prior applied at the inference stage. To\naddress this challenge, we developed specific noise priors for SIS,\nencompassing spatial, categorical, and a novel spatial-categorical joint prior\nfor inference. This approach, which we have named SCP-Diff, has yielded\nexceptional results, achieving an FID of 10.53 on Cityscapes and 12.66 on\nADE20K.The code and models can be accessed via the project page.",
        "translated": ""
    },
    {
        "title": "GaussianGrasper: 3D Language Gaussian Splatting for Open-vocabulary\n  Robotic Grasping",
        "url": "http://arxiv.org/abs/2403.09637v1",
        "pub_date": "2024-03-14",
        "summary": "Constructing a 3D scene capable of accommodating open-ended language queries,\nis a pivotal pursuit, particularly within the domain of robotics. Such\ntechnology facilitates robots in executing object manipulations based on human\nlanguage directives. To tackle this challenge, some research efforts have been\ndedicated to the development of language-embedded implicit fields. However,\nimplicit fields (e.g. NeRF) encounter limitations due to the necessity of\nprocessing a large number of input views for reconstruction, coupled with their\ninherent inefficiencies in inference. Thus, we present the GaussianGrasper,\nwhich utilizes 3D Gaussian Splatting to explicitly represent the scene as a\ncollection of Gaussian primitives. Our approach takes a limited set of RGB-D\nviews and employs a tile-based splatting technique to create a feature field.\nIn particular, we propose an Efficient Feature Distillation (EFD) module that\nemploys contrastive learning to efficiently and accurately distill language\nembeddings derived from foundational models. With the reconstructed geometry of\nthe Gaussian field, our method enables the pre-trained grasping model to\ngenerate collision-free grasp pose candidates. Furthermore, we propose a\nnormal-guided grasp module to select the best grasp pose. Through comprehensive\nreal-world experiments, we demonstrate that GaussianGrasper enables robots to\naccurately query and grasp objects with language instructions, providing a new\nsolution for language-guided manipulation tasks. Data and codes can be\navailable at https://github.com/MrSecant/GaussianGrasper.",
        "translated": ""
    },
    {
        "title": "Transformers Get Stable: An End-to-End Signal Propagation Theory for\n  Language Models",
        "url": "http://arxiv.org/abs/2403.09635v1",
        "pub_date": "2024-03-14",
        "summary": "In spite of their huge success, transformer models remain difficult to scale\nin depth. In this work, we develop a unified signal propagation theory and\nprovide formulae that govern the moments of the forward and backward signal\nthrough the transformer model. Our framework can be used to understand and\nmitigate vanishing/exploding gradients, rank collapse, and instability\nassociated with high attention scores. We also propose DeepScaleLM, an\ninitialization and scaling scheme that conserves unit output/gradient moments\nthroughout the model, enabling the training of very deep models with 100s of\nlayers. We find that transformer models could be much deeper - our deep models\nwith fewer parameters outperform shallow models in Language Modeling, Speech\nTranslation, and Image Classification, across Encoder-only, Decoder-only and\nEncoder-Decoder variants, for both Pre-LN and Post-LN transformers, for\nmultiple datasets and model sizes. These improvements also translate into\nimproved performance on downstream Question Answering tasks and improved\nrobustness for image classification.",
        "translated": ""
    },
    {
        "title": "OneTracker: Unifying Visual Object Tracking with Foundation Models and\n  Efficient Tuning",
        "url": "http://arxiv.org/abs/2403.09634v1",
        "pub_date": "2024-03-14",
        "summary": "Visual object tracking aims to localize the target object of each frame based\non its initial appearance in the first frame. Depending on the input modility,\ntracking tasks can be divided into RGB tracking and RGB+X (e.g. RGB+N, and\nRGB+D) tracking. Despite the different input modalities, the core aspect of\ntracking is the temporal matching. Based on this common ground, we present a\ngeneral framework to unify various tracking tasks, termed as OneTracker.\nOneTracker first performs a large-scale pre-training on a RGB tracker called\nFoundation Tracker. This pretraining phase equips the Foundation Tracker with a\nstable ability to estimate the location of the target object. Then we regard\nother modality information as prompt and build Prompt Tracker upon Foundation\nTracker. Through freezing the Foundation Tracker and only adjusting some\nadditional trainable parameters, Prompt Tracker inhibits the strong\nlocalization ability from Foundation Tracker and achieves parameter-efficient\nfinetuning on downstream RGB+X tracking tasks. To evaluate the effectiveness of\nour general framework OneTracker, which is consisted of Foundation Tracker and\nPrompt Tracker, we conduct extensive experiments on 6 popular tracking tasks\nacross 11 benchmarks and our OneTracker outperforms other models and achieves\nstate-of-the-art performance.",
        "translated": ""
    },
    {
        "title": "Holo-Relighting: Controllable Volumetric Portrait Relighting from a\n  Single Image",
        "url": "http://arxiv.org/abs/2403.09632v1",
        "pub_date": "2024-03-14",
        "summary": "At the core of portrait photography is the search for ideal lighting and\nviewpoint. The process often requires advanced knowledge in photography and an\nelaborate studio setup. In this work, we propose Holo-Relighting, a volumetric\nrelighting method that is capable of synthesizing novel viewpoints, and novel\nlighting from a single image. Holo-Relighting leverages the pretrained 3D GAN\n(EG3D) to reconstruct geometry and appearance from an input portrait as a set\nof 3D-aware features. We design a relighting module conditioned on a given\nlighting to process these features, and predict a relit 3D representation in\nthe form of a tri-plane, which can render to an arbitrary viewpoint through\nvolume rendering. Besides viewpoint and lighting control, Holo-Relighting also\ntakes the head pose as a condition to enable head-pose-dependent lighting\neffects. With these novel designs, Holo-Relighting can generate complex\nnon-Lambertian lighting effects (e.g., specular highlights and cast shadows)\nwithout using any explicit physical lighting priors. We train Holo-Relighting\nwith data captured with a light stage, and propose two data-rendering\ntechniques to improve the data quality for training the volumetric relighting\nsystem. Through quantitative and qualitative experiments, we demonstrate\nHolo-Relighting can achieve state-of-the-arts relighting quality with better\nphotorealism, 3D consistency and controllability.",
        "translated": ""
    },
    {
        "title": "3D-VLA: A 3D Vision-Language-Action Generative World Model",
        "url": "http://arxiv.org/abs/2403.09631v1",
        "pub_date": "2024-03-14",
        "summary": "Recent vision-language-action (VLA) models rely on 2D inputs, lacking\nintegration with the broader realm of the 3D physical world. Furthermore, they\nperform action prediction by learning a direct mapping from perception to\naction, neglecting the vast dynamics of the world and the relations between\nactions and dynamics. In contrast, human beings are endowed with world models\nthat depict imagination about future scenarios to plan actions accordingly. To\nthis end, we propose 3D-VLA by introducing a new family of embodied foundation\nmodels that seamlessly link 3D perception, reasoning, and action through a\ngenerative world model. Specifically, 3D-VLA is built on top of a 3D-based\nlarge language model (LLM), and a set of interaction tokens is introduced to\nengage with the embodied environment. Furthermore, to inject generation\nabilities into the model, we train a series of embodied diffusion models and\nalign them into the LLM for predicting the goal images and point clouds. To\ntrain our 3D-VLA, we curate a large-scale 3D embodied instruction dataset by\nextracting vast 3D-related information from existing robotics datasets. Our\nexperiments on held-in datasets demonstrate that 3D-VLA significantly improves\nthe reasoning, multimodal generation, and planning capabilities in embodied\nenvironments, showcasing its potential in real-world applications.",
        "translated": ""
    },
    {
        "title": "Generalized Predictive Model for Autonomous Driving",
        "url": "http://arxiv.org/abs/2403.09630v1",
        "pub_date": "2024-03-14",
        "summary": "In this paper, we introduce the first large-scale video prediction model in\nthe autonomous driving discipline. To eliminate the restriction of high-cost\ndata collection and empower the generalization ability of our model, we acquire\nmassive data from the web and pair it with diverse and high-quality text\ndescriptions. The resultant dataset accumulates over 2000 hours of driving\nvideos, spanning areas all over the world with diverse weather conditions and\ntraffic scenarios. Inheriting the merits from recent latent diffusion models,\nour model, dubbed GenAD, handles the challenging dynamics in driving scenes\nwith novel temporal reasoning blocks. We showcase that it can generalize to\nvarious unseen driving datasets in a zero-shot manner, surpassing general or\ndriving-specific video prediction counterparts. Furthermore, GenAD can be\nadapted into an action-conditioned prediction model or a motion planner,\nholding great potential for real-world driving applications.",
        "translated": ""
    },
    {
        "title": "Video Mamba Suite: State Space Model as a Versatile Alternative for\n  Video Understanding",
        "url": "http://arxiv.org/abs/2403.09626v1",
        "pub_date": "2024-03-14",
        "summary": "Understanding videos is one of the fundamental directions in computer vision\nresearch, with extensive efforts dedicated to exploring various architectures\nsuch as RNN, 3D CNN, and Transformers. The newly proposed architecture of state\nspace model, e.g., Mamba, shows promising traits to extend its success in long\nsequence modeling to video modeling. To assess whether Mamba can be a viable\nalternative to Transformers in the video understanding domain, in this work, we\nconduct a comprehensive set of studies, probing different roles Mamba can play\nin modeling videos, while investigating diverse tasks where Mamba could exhibit\nsuperiority. We categorize Mamba into four roles for modeling videos, deriving\na Video Mamba Suite composed of 14 models/modules, and evaluating them on 12\nvideo understanding tasks. Our extensive experiments reveal the strong\npotential of Mamba on both video-only and video-language tasks while showing\npromising efficiency-performance trade-offs. We hope this work could provide\nvaluable data points and insights for future research on video understanding.\nCode is public: https://github.com/OpenGVLab/video-mamba-suite.",
        "translated": ""
    },
    {
        "title": "Make-Your-3D: Fast and Consistent Subject-Driven 3D Content Generation",
        "url": "http://arxiv.org/abs/2403.09625v1",
        "pub_date": "2024-03-14",
        "summary": "Recent years have witnessed the strong power of 3D generation models, which\noffer a new level of creative flexibility by allowing users to guide the 3D\ncontent generation process through a single image or natural language. However,\nit remains challenging for existing 3D generation methods to create\nsubject-driven 3D content across diverse prompts. In this paper, we introduce a\nnovel 3D customization method, dubbed Make-Your-3D that can personalize\nhigh-fidelity and consistent 3D content from only a single image of a subject\nwith text description within 5 minutes. Our key insight is to harmonize the\ndistributions of a multi-view diffusion model and an identity-specific 2D\ngenerative model, aligning them with the distribution of the desired 3D\nsubject. Specifically, we design a co-evolution framework to reduce the\nvariance of distributions, where each model undergoes a process of learning\nfrom the other through identity-aware optimization and subject-prior\noptimization, respectively. Extensive experiments demonstrate that our method\ncan produce high-quality, consistent, and subject-specific 3D content with\ntext-driven modifications that are unseen in subject image.",
        "translated": ""
    },
    {
        "title": "Implicit Discriminative Knowledge Learning for Visible-Infrared Person\n  Re-Identification",
        "url": "http://arxiv.org/abs/2403.11708v1",
        "pub_date": "2024-03-18",
        "summary": "Visible-Infrared Person Re-identification (VI-ReID) is a challenging\ncross-modal pedestrian retrieval task, due to significant intra-class\nvariations and cross-modal discrepancies among different cameras. Existing\nworks mainly focus on embedding images of different modalities into a unified\nspace to mine modality-shared features. They only seek distinctive information\nwithin these shared features, while ignoring the identity-aware useful\ninformation that is implicit in the modality-specific features. To address this\nissue, we propose a novel Implicit Discriminative Knowledge Learning (IDKL)\nnetwork to uncover and leverage the implicit discriminative information\ncontained within the modality-specific. First, we extract modality-specific and\nmodality-shared features using a novel dual-stream network. Then, the\nmodality-specific features undergo purification to reduce their modality style\ndiscrepancies while preserving identity-aware discriminative knowledge.\nSubsequently, this kind of implicit knowledge is distilled into the\nmodality-shared feature to enhance its distinctiveness. Finally, an alignment\nloss is proposed to minimize modality discrepancy on enhanced modality-shared\nfeatures. Extensive experiments on multiple public datasets demonstrate the\nsuperiority of IDKL network over the state-of-the-art methods. Code is\navailable at https://github.com/1KK077/IDKL.",
        "translated": ""
    },
    {
        "title": "LLaVA-UHD: an LMM Perceiving Any Aspect Ratio and High-Resolution Images",
        "url": "http://arxiv.org/abs/2403.11703v1",
        "pub_date": "2024-03-18",
        "summary": "Visual encoding constitutes the basis of large multimodal models (LMMs) in\nunderstanding the visual world. Conventional LMMs process images in fixed sizes\nand limited resolutions, while recent explorations in this direction are\nlimited in adaptivity, efficiency, and even correctness. In this work, we first\ntake GPT-4V and LLaVA-1.5 as representative examples and expose systematic\nflaws rooted in their visual encoding strategy. To address the challenges, we\npresent LLaVA-UHD, a large multimodal model that can efficiently perceive\nimages in any aspect ratio and high resolution. LLaVA-UHD includes three key\ncomponents: (1) An image modularization strategy that divides native-resolution\nimages into smaller variable-sized slices for efficient and extensible\nencoding, (2) a compression module that further condenses image tokens from\nvisual encoders, and (3) a spatial schema to organize slice tokens for LLMs.\nComprehensive experiments show that LLaVA-UHD outperforms established LMMs\ntrained with 2-3 orders of magnitude more data on 9 benchmarks. Notably, our\nmodel built on LLaVA-1.5 336x336 supports 6 times larger (i.e., 672x1088)\nresolution images using only 94% inference computation, and achieves 6.4\naccuracy improvement on TextVQA. Moreover, the model can be efficiently trained\nin academic settings, within 23 hours on 8 A100 GPUs (vs. 26 hours of\nLLaVA-1.5). We make the data and code publicly available at\nhttps://github.com/thunlp/LLaVA-UHD.",
        "translated": ""
    },
    {
        "title": "A Spatial-Temporal Progressive Fusion Network for Breast Lesion\n  Segmentation in Ultrasound Videos",
        "url": "http://arxiv.org/abs/2403.11699v1",
        "pub_date": "2024-03-18",
        "summary": "Ultrasound video-based breast lesion segmentation provides a valuable\nassistance in early breast lesion detection and treatment. However, existing\nworks mainly focus on lesion segmentation based on ultrasound breast images\nwhich usually can not be adapted well to obtain desirable results on ultrasound\nvideos. The main challenge for ultrasound video-based breast lesion\nsegmentation is how to exploit the lesion cues of both intra-frame and\ninter-frame simultaneously. To address this problem, we propose a novel\nSpatial-Temporal Progressive Fusion Network (STPFNet) for video based breast\nlesion segmentation problem. The main aspects of the proposed STPFNet are\nthreefold. First, we propose to adopt a unified network architecture to capture\nboth spatial dependences within each ultrasound frame and temporal correlations\nbetween different frames together for ultrasound data representation. Second,\nwe propose a new fusion module, termed Multi-Scale Feature Fusion (MSFF), to\nfuse spatial and temporal cues together for lesion detection. MSFF can help to\ndetermine the boundary contour of lesion region to overcome the issue of lesion\nboundary blurring. Third, we propose to exploit the segmentation result of\nprevious frame as the prior knowledge to suppress the noisy background and\nlearn more robust representation. In particular, we introduce a new publicly\navailable ultrasound video breast lesion segmentation dataset, termed UVBLS200,\nwhich is specifically dedicated to breast lesion segmentation. It contains 200\nvideos, including 80 videos of benign lesions and 120 videos of malignant\nlesions. Experiments on the proposed dataset demonstrate that the proposed\nSTPFNet achieves better breast lesion detection performance than\nstate-of-the-art methods.",
        "translated": ""
    },
    {
        "title": "Urban Scene Diffusion through Semantic Occupancy Map",
        "url": "http://arxiv.org/abs/2403.11697v1",
        "pub_date": "2024-03-18",
        "summary": "Generating unbounded 3D scenes is crucial for large-scale scene understanding\nand simulation. Urban scenes, unlike natural landscapes, consist of various\ncomplex man-made objects and structures such as roads, traffic signs, vehicles,\nand buildings. To create a realistic and detailed urban scene, it is crucial to\naccurately represent the geometry and semantics of the underlying objects,\ngoing beyond their visual appearance. In this work, we propose UrbanDiffusion,\na 3D diffusion model that is conditioned on a Bird's-Eye View (BEV) map and\ngenerates an urban scene with geometry and semantics in the form of semantic\noccupancy map. Our model introduces a novel paradigm that learns the data\ndistribution of scene-level structures within a latent space and further\nenables the expansion of the synthesized scene into an arbitrary scale. After\ntraining on real-world driving datasets, our model can generate a wide range of\ndiverse urban scenes given the BEV maps from the held-out set and also\ngeneralize to the synthesized maps from a driving simulator. We further\ndemonstrate its application to scene image synthesis with a pretrained image\ngenerator as a prior.",
        "translated": ""
    },
    {
        "title": "TrajectoryNAS: A Neural Architecture Search for Trajectory Prediction",
        "url": "http://arxiv.org/abs/2403.11695v1",
        "pub_date": "2024-03-18",
        "summary": "Autonomous driving systems are a rapidly evolving technology that enables\ndriverless car production. Trajectory prediction is a critical component of\nautonomous driving systems, enabling cars to anticipate the movements of\nsurrounding objects for safe navigation. Trajectory prediction using Lidar\npoint-cloud data performs better than 2D images due to providing 3D\ninformation. However, processing point-cloud data is more complicated and\ntime-consuming than 2D images. Hence, state-of-the-art 3D trajectory\npredictions using point-cloud data suffer from slow and erroneous predictions.\nThis paper introduces TrajectoryNAS, a pioneering method that focuses on\nutilizing point cloud data for trajectory prediction. By leveraging Neural\nArchitecture Search (NAS), TrajectoryNAS automates the design of trajectory\nprediction models, encompassing object detection, tracking, and forecasting in\na cohesive manner. This approach not only addresses the complex\ninterdependencies among these tasks but also emphasizes the importance of\naccuracy and efficiency in trajectory modeling. Through empirical studies,\nTrajectoryNAS demonstrates its effectiveness in enhancing the performance of\nautonomous driving systems, marking a significant advancement in the\nfield.Experimental results reveal that TrajcetoryNAS yield a minimum of 4.8\nhigger accuracy and 1.1* lower latency over competing methods on the NuScenes\ndataset.",
        "translated": ""
    },
    {
        "title": "Object Segmentation-Assisted Inter Prediction for Versatile Video Coding",
        "url": "http://arxiv.org/abs/2403.11694v1",
        "pub_date": "2024-03-18",
        "summary": "In modern video coding standards, block-based inter prediction is widely\nadopted, which brings high compression efficiency. However, in natural videos,\nthere are usually multiple moving objects of arbitrary shapes, resulting in\ncomplex motion fields that are difficult to compactly represent. This problem\nhas been tackled by more flexible block partitioning methods in the Versatile\nVideo Coding (VVC) standard, but the more flexible partitions require more\noverhead bits to signal and still cannot be made arbitrary shaped. To address\nthis limitation, we propose an object segmentation-assisted inter prediction\nmethod (SAIP), where objects in the reference frames are segmented by some\nadvanced technologies. With a proper indication, the object segmentation mask\nis translated from the reference frame to the current frame as the\narbitrary-shaped partition of different regions without any extra signal. Using\nthe segmentation mask, motion compensation is separately performed for\ndifferent regions, achieving higher prediction accuracy. The segmentation mask\nis further used to code the motion vectors of different regions more\nefficiently. Moreover, segmentation mask is considered in the joint\nrate-distortion optimization for motion estimation and partition estimation to\nderive the motion vector of different regions and partition more accurately.\nThe proposed method is implemented into the VVC reference software, VTM version\n12.0. Experimental results show that the proposed method achieves up to 1.98%,\n1.14%, 0.79%, and on average 0.82%, 0.49%, 0.37% BD-rate reduction for common\ntest sequences, under the Low-delay P, Low-delay B, and Random Access\nconfigurations, respectively.",
        "translated": ""
    },
    {
        "title": "TTT-KD: Test-Time Training for 3D Semantic Segmentation through\n  Knowledge Distillation from Foundation Models",
        "url": "http://arxiv.org/abs/2403.11691v1",
        "pub_date": "2024-03-18",
        "summary": "Test-Time Training (TTT) proposes to adapt a pre-trained network to changing\ndata distributions on-the-fly. In this work, we propose the first TTT method\nfor 3D semantic segmentation, TTT-KD, which models Knowledge Distillation (KD)\nfrom foundation models (e.g. DINOv2) as a self-supervised objective for\nadaptation to distribution shifts at test-time. Given access to paired\nimage-pointcloud (2D-3D) data, we first optimize a 3D segmentation backbone for\nthe main task of semantic segmentation using the pointclouds and the task of 2D\n$\\to$ 3D KD by using an off-the-shelf 2D pre-trained foundation model. At\ntest-time, our TTT-KD updates the 3D segmentation backbone for each test\nsample, by using the self-supervised task of knowledge distillation, before\nperforming the final prediction. Extensive evaluations on multiple indoor and\noutdoor 3D segmentation benchmarks show the utility of TTT-KD, as it improves\nperformance for both in-distribution (ID) and out-of-distribution (ODO) test\ndatasets. We achieve a gain of up to 13% mIoU (7% on average) when the train\nand test distributions are similar and up to 45% (20% on average) when adapting\nto OOD test samples.",
        "translated": ""
    },
    {
        "title": "MoreStyle: Relax Low-frequency Constraint of Fourier-based Image\n  Reconstruction in Generalizable Medical Image Segmentation",
        "url": "http://arxiv.org/abs/2403.11689v2",
        "pub_date": "2024-03-18",
        "summary": "The task of single-source domain generalization (SDG) in medical image\nsegmentation is crucial due to frequent domain shifts in clinical image\ndatasets. To address the challenge of poor generalization across different\ndomains, we introduce a Plug-and-Play module for data augmentation called\nMoreStyle. MoreStyle diversifies image styles by relaxing low-frequency\nconstraints in Fourier space, guiding the image reconstruction network. With\nthe help of adversarial learning, MoreStyle further expands the style range and\npinpoints the most intricate style combinations within latent features. To\nhandle significant style variations, we introduce an uncertainty-weighted loss.\nThis loss emphasizes hard-to-classify pixels resulting only from style shifts\nwhile mitigating true hard-to-classify pixels in both MoreStyle-generated and\noriginal images. Extensive experiments on two widely used benchmarks\ndemonstrate that the proposed MoreStyle effectively helps to achieve good\ndomain generalization ability, and has the potential to further boost the\nperformance of some state-of-the-art SDG methods.",
        "translated": ""
    },
    {
        "title": "MASSTAR: A Multi-Modal and Large-Scale Scene Dataset with a Versatile\n  Toolchain for Surface Prediction and Completion",
        "url": "http://arxiv.org/abs/2403.11681v1",
        "pub_date": "2024-03-18",
        "summary": "Surface prediction and completion have been widely studied in various\napplications. Recently, research in surface completion has evolved from small\nobjects to complex large-scale scenes. As a result, researchers have begun\nincreasing the volume of data and leveraging a greater variety of data\nmodalities including rendered RGB images, descriptive texts, depth images, etc,\nto enhance algorithm performance. However, existing datasets suffer from a\ndeficiency in the amounts of scene-level models along with the corresponding\nmulti-modal information. Therefore, a method to scale the datasets and generate\nmulti-modal information in them efficiently is essential. To bridge this\nresearch gap, we propose MASSTAR: a Multi-modal lArge-scale Scene dataset with\na verSatile Toolchain for surfAce pRediction and completion. We develop a\nversatile and efficient toolchain for processing the raw 3D data from the\nenvironments. It screens out a set of fine-grained scene models and generates\nthe corresponding multi-modal data. Utilizing the toolchain, we then generate\nan example dataset composed of over a thousand scene-level models with partial\nreal-world data added. We compare MASSTAR with the existing datasets, which\nvalidates its superiority: the ability to efficiently extract high-quality\nmodels from complex scenarios to expand the dataset. Additionally, several\nrepresentative surface completion algorithms are benchmarked on MASSTAR, which\nreveals that existing algorithms can hardly deal with scene-level completion.\nWe will release the source code of our toolchain and the dataset. For more\ndetails, please see our project page at https://sysu-star.github.io/MASSTAR.",
        "translated": ""
    },
    {
        "title": "NEDS-SLAM: A Novel Neural Explicit Dense Semantic SLAM Framework using\n  3D Gaussian Splatting",
        "url": "http://arxiv.org/abs/2403.11679v1",
        "pub_date": "2024-03-18",
        "summary": "We propose NEDS-SLAM, an Explicit Dense semantic SLAM system based on 3D\nGaussian representation, that enables robust 3D semantic mapping, accurate\ncamera tracking, and high-quality rendering in real-time. In the system, we\npropose a Spatially Consistent Feature Fusion model to reduce the effect of\nerroneous estimates from pre-trained segmentation head on semantic\nreconstruction, achieving robust 3D semantic Gaussian mapping. Additionally, we\nemploy a lightweight encoder-decoder to compress the high-dimensional semantic\nfeatures into a compact 3D Gaussian representation, mitigating the burden of\nexcessive memory consumption. Furthermore, we leverage the advantage of 3D\nGaussian splatting, which enables efficient and differentiable novel view\nrendering, and propose a Virtual Camera View Pruning method to eliminate\noutlier GS points, thereby effectively enhancing the quality of scene\nrepresentations. Our NEDS-SLAM method demonstrates competitive performance over\nexisting dense semantic SLAM methods in terms of mapping and tracking accuracy\non Replica and ScanNet datasets, while also showing excellent capabilities in\n3D dense semantic mapping.",
        "translated": ""
    },
    {
        "title": "Wear-Any-Way: Manipulable Virtual Try-on via Sparse Correspondence\n  Alignment",
        "url": "http://arxiv.org/abs/2403.12965v1",
        "pub_date": "2024-03-19",
        "summary": "This paper introduces a novel framework for virtual try-on, termed\nWear-Any-Way. Different from previous methods, Wear-Any-Way is a customizable\nsolution. Besides generating high-fidelity results, our method supports users\nto precisely manipulate the wearing style. To achieve this goal, we first\nconstruct a strong pipeline for standard virtual try-on, supporting\nsingle/multiple garment try-on and model-to-model settings in complicated\nscenarios. To make it manipulable, we propose sparse correspondence alignment\nwhich involves point-based control to guide the generation for specific\nlocations. With this design, Wear-Any-Way gets state-of-the-art performance for\nthe standard setting and provides a novel interaction form for customizing the\nwearing style. For instance, it supports users to drag the sleeve to make it\nrolled up, drag the coat to make it open, and utilize clicks to control the\nstyle of tuck, etc. Wear-Any-Way enables more liberated and flexible\nexpressions of the attires, holding profound implications in the fashion\nindustry.",
        "translated": ""
    },
    {
        "title": "Chain-of-Spot: Interactive Reasoning Improves Large Vision-Language\n  Models",
        "url": "http://arxiv.org/abs/2403.12966v1",
        "pub_date": "2024-03-19",
        "summary": "In the realm of vision-language understanding, the proficiency of models in\ninterpreting and reasoning over visual content has become a cornerstone for\nnumerous applications. However, it is challenging for the visual encoder in\nLarge Vision-Language Models (LVLMs) to extract useful features tailored to\nquestions that aid the language model's response. Furthermore, a common\npractice among existing LVLMs is to utilize lower-resolution images, which\nrestricts the ability for visual recognition. Our work introduces the\nChain-of-Spot (CoS) method, which we describe as Interactive Reasoning, a novel\napproach that enhances feature extraction by focusing on key regions of\ninterest (ROI) within the image, corresponding to the posed questions or\ninstructions. This technique allows LVLMs to access more detailed visual\ninformation without altering the original image resolution, thereby offering\nmulti-granularity image features. By integrating Chain-of-Spot with\ninstruct-following LLaVA-1.5 models, the process of image reasoning\nconsistently improves performance across a wide range of multimodal datasets\nand benchmarks without bells and whistles and achieves new state-of-the-art\nresults. Our empirical findings demonstrate a significant improvement in LVLMs'\nability to understand and reason about visual content, paving the way for more\nsophisticated visual instruction-following applications. Code and models are\navailable at https://github.com/dongyh20/Chain-of-Spot",
        "translated": ""
    },
    {
        "title": "Negative Yields Positive: Unified Dual-Path Adapter for Vision-Language\n  Models",
        "url": "http://arxiv.org/abs/2403.12964v1",
        "pub_date": "2024-03-19",
        "summary": "Recently, large-scale pre-trained Vision-Language Models (VLMs) have\ndemonstrated great potential in learning open-world visual representations, and\nexhibit remarkable performance across a wide range of downstream tasks through\nefficient fine-tuning. In this work, we innovatively introduce the concept of\ndual learning into fine-tuning VLMs, i.e., we not only learn what an image is,\nbut also what an image isn't. Building on this concept, we introduce a novel\nDualAdapter approach to enable dual-path adaptation of VLMs from both positive\nand negative perspectives with only limited annotated samples. In the inference\nstage, our DualAdapter performs unified predictions by simultaneously\nconducting complementary positive selection and negative exclusion across\ntarget classes, thereby enhancing the overall recognition accuracy of VLMs in\ndownstream tasks. Our extensive experimental results across 15 datasets\nvalidate that the proposed DualAdapter outperforms existing state-of-the-art\nmethods on both few-shot learning and domain generalization tasks while\nachieving competitive computational efficiency. Code is available at\nhttps://github.com/zhangce01/DualAdapter.",
        "translated": ""
    },
    {
        "title": "FouriScale: A Frequency Perspective on Training-Free High-Resolution\n  Image Synthesis",
        "url": "http://arxiv.org/abs/2403.12963v1",
        "pub_date": "2024-03-19",
        "summary": "In this study, we delve into the generation of high-resolution images from\npre-trained diffusion models, addressing persistent challenges, such as\nrepetitive patterns and structural distortions, that emerge when models are\napplied beyond their trained resolutions. To address this issue, we introduce\nan innovative, training-free approach FouriScale from the perspective of\nfrequency domain analysis. We replace the original convolutional layers in\npre-trained diffusion models by incorporating a dilation technique along with a\nlow-pass operation, intending to achieve structural consistency and scale\nconsistency across resolutions, respectively. Further enhanced by a\npadding-then-crop strategy, our method can flexibly handle text-to-image\ngeneration of various aspect ratios. By using the FouriScale as guidance, our\nmethod successfully balances the structural integrity and fidelity of generated\nimages, achieving an astonishing capacity of arbitrary-size, high-resolution,\nand high-quality generation. With its simplicity and compatibility, our method\ncan provide valuable insights for future explorations into the synthesis of\nultra-high-resolution images. The code will be released at\nhttps://github.com/LeonHLJ/FouriScale.",
        "translated": ""
    },
    {
        "title": "FRESCO: Spatial-Temporal Correspondence for Zero-Shot Video Translation",
        "url": "http://arxiv.org/abs/2403.12962v1",
        "pub_date": "2024-03-19",
        "summary": "The remarkable efficacy of text-to-image diffusion models has motivated\nextensive exploration of their potential application in video domains.\nZero-shot methods seek to extend image diffusion models to videos without\nnecessitating model training. Recent methods mainly focus on incorporating\ninter-frame correspondence into attention mechanisms. However, the soft\nconstraint imposed on determining where to attend to valid features can\nsometimes be insufficient, resulting in temporal inconsistency. In this paper,\nwe introduce FRESCO, intra-frame correspondence alongside inter-frame\ncorrespondence to establish a more robust spatial-temporal constraint. This\nenhancement ensures a more consistent transformation of semantically similar\ncontent across frames. Beyond mere attention guidance, our approach involves an\nexplicit update of features to achieve high spatial-temporal consistency with\nthe input video, significantly improving the visual coherence of the resulting\ntranslated videos. Extensive experiments demonstrate the effectiveness of our\nproposed framework in producing high-quality, coherent videos, marking a\nnotable improvement over existing zero-shot methods.",
        "translated": ""
    },
    {
        "title": "TexTile: A Differentiable Metric for Texture Tileability",
        "url": "http://arxiv.org/abs/2403.12961v1",
        "pub_date": "2024-03-19",
        "summary": "We introduce TexTile, a novel differentiable metric to quantify the degree\nupon which a texture image can be concatenated with itself without introducing\nrepeating artifacts (i.e., the tileability). Existing methods for tileable\ntexture synthesis focus on general texture quality, but lack explicit analysis\nof the intrinsic repeatability properties of a texture. In contrast, our\nTexTile metric effectively evaluates the tileable properties of a texture,\nopening the door to more informed synthesis and analysis of tileable textures.\nUnder the hood, TexTile is formulated as a binary classifier carefully built\nfrom a large dataset of textures of different styles, semantics, regularities,\nand human annotations.Key to our method is a set of architectural modifications\nto baseline pre-train image classifiers to overcome their shortcomings at\nmeasuring tileability, along with a custom data augmentation and training\nregime aimed at increasing robustness and accuracy. We demonstrate that TexTile\ncan be plugged into different state-of-the-art texture synthesis methods,\nincluding diffusion-based strategies, and generate tileable textures while\nkeeping or even improving the overall texture quality. Furthermore, we show\nthat TexTile can objectively evaluate any tileable texture synthesis method,\nwhereas the current mix of existing metrics produces uncorrelated scores which\nheavily hinders progress in the field.",
        "translated": ""
    },
    {
        "title": "FaceXFormer: A Unified Transformer for Facial Analysis",
        "url": "http://arxiv.org/abs/2403.12960v1",
        "pub_date": "2024-03-19",
        "summary": "In this work, we introduce FaceXformer, an end-to-end unified transformer\nmodel for a comprehensive range of facial analysis tasks such as face parsing,\nlandmark detection, head pose estimation, attributes recognition, and\nestimation of age, gender, race, and landmarks visibility. Conventional methods\nin face analysis have often relied on task-specific designs and preprocessing\ntechniques, which limit their approach to a unified architecture. Unlike these\nconventional methods, our FaceXformer leverages a transformer-based\nencoder-decoder architecture where each task is treated as a learnable token,\nenabling the integration of multiple tasks within a single framework. Moreover,\nwe propose a parameter-efficient decoder, FaceX, which jointly processes face\nand task tokens, thereby learning generalized and robust face representations\nacross different tasks. To the best of our knowledge, this is the first work to\npropose a single model capable of handling all these facial analysis tasks\nusing transformers. We conducted a comprehensive analysis of effective\nbackbones for unified face task processing and evaluated different task queries\nand the synergy between them. We conduct experiments against state-of-the-art\nspecialized models and previous multi-task models in both intra-dataset and\ncross-dataset evaluations across multiple benchmarks. Additionally, our model\neffectively handles images \"in-the-wild,\" demonstrating its robustness and\ngeneralizability across eight different tasks, all while maintaining the\nreal-time performance of 37 FPS.",
        "translated": ""
    },
    {
        "title": "WHAC: World-grounded Humans and Cameras",
        "url": "http://arxiv.org/abs/2403.12959v1",
        "pub_date": "2024-03-19",
        "summary": "Estimating human and camera trajectories with accurate scale in the world\ncoordinate system from a monocular video is a highly desirable yet challenging\nand ill-posed problem. In this study, we aim to recover expressive parametric\nhuman models (i.e., SMPL-X) and corresponding camera poses jointly, by\nleveraging the synergy between three critical players: the world, the human,\nand the camera. Our approach is founded on two key observations. Firstly,\ncamera-frame SMPL-X estimation methods readily recover absolute human depth.\nSecondly, human motions inherently provide absolute spatial cues. By\nintegrating these insights, we introduce a novel framework, referred to as\nWHAC, to facilitate world-grounded expressive human pose and shape estimation\n(EHPS) alongside camera pose estimation, without relying on traditional\noptimization techniques. Additionally, we present a new synthetic dataset,\nWHAC-A-Mole, which includes accurately annotated humans and cameras, and\nfeatures diverse interactive human motions as well as realistic camera\ntrajectories. Extensive experiments on both standard and newly established\nbenchmarks highlight the superiority and efficacy of our framework. We will\nmake the code and dataset publicly available.",
        "translated": ""
    },
    {
        "title": "GVGEN: Text-to-3D Generation with Volumetric Representation",
        "url": "http://arxiv.org/abs/2403.12957v1",
        "pub_date": "2024-03-19",
        "summary": "In recent years, 3D Gaussian splatting has emerged as a powerful technique\nfor 3D reconstruction and generation, known for its fast and high-quality\nrendering capabilities. To address these shortcomings, this paper introduces a\nnovel diffusion-based framework, GVGEN, designed to efficiently generate 3D\nGaussian representations from text input. We propose two innovative\ntechniques:(1) Structured Volumetric Representation. We first arrange\ndisorganized 3D Gaussian points as a structured form GaussianVolume. This\ntransformation allows the capture of intricate texture details within a volume\ncomposed of a fixed number of Gaussians. To better optimize the representation\nof these details, we propose a unique pruning and densifying method named the\nCandidate Pool Strategy, enhancing detail fidelity through selective\noptimization. (2) Coarse-to-fine Generation Pipeline. To simplify the\ngeneration of GaussianVolume and empower the model to generate instances with\ndetailed 3D geometry, we propose a coarse-to-fine pipeline. It initially\nconstructs a basic geometric structure, followed by the prediction of complete\nGaussian attributes. Our framework, GVGEN, demonstrates superior performance in\nqualitative and quantitative assessments compared to existing 3D generation\nmethods. Simultaneously, it maintains a fast generation speed ($\\sim$7\nseconds), effectively striking a balance between quality and efficiency.",
        "translated": ""
    },
    {
        "title": "FutureDepth: Learning to Predict the Future Improves Video Depth\n  Estimation",
        "url": "http://arxiv.org/abs/2403.12953v1",
        "pub_date": "2024-03-19",
        "summary": "In this paper, we propose a novel video depth estimation approach,\nFutureDepth, which enables the model to implicitly leverage multi-frame and\nmotion cues to improve depth estimation by making it learn to predict the\nfuture at training. More specifically, we propose a future prediction network,\nF-Net, which takes the features of multiple consecutive frames and is trained\nto predict multi-frame features one time step ahead iteratively. In this way,\nF-Net learns the underlying motion and correspondence information, and we\nincorporate its features into the depth decoding process. Additionally, to\nenrich the learning of multiframe correspondence cues, we further leverage a\nreconstruction network, R-Net, which is trained via adaptively masked\nauto-encoding of multiframe feature volumes. At inference time, both F-Net and\nR-Net are used to produce queries to work with the depth decoder, as well as a\nfinal refinement network. Through extensive experiments on several benchmarks,\ni.e., NYUDv2, KITTI, DDAD, and Sintel, which cover indoor, driving, and\nopen-domain scenarios, we show that FutureDepth significantly improves upon\nbaseline models, outperforms existing video depth estimation methods, and sets\nnew state-of-the-art (SOTA) accuracy. Furthermore, FutureDepth is more\nefficient than existing SOTA video depth estimation models and has similar\nlatencies when comparing to monocular models",
        "translated": ""
    },
    {
        "title": "On Pretraining Data Diversity for Self-Supervised Learning",
        "url": "http://arxiv.org/abs/2403.13808v1",
        "pub_date": "2024-03-20",
        "summary": "We explore the impact of training with more diverse datasets, characterized\nby the number of unique samples, on the performance of self-supervised learning\n(SSL) under a fixed computational budget. Our findings consistently demonstrate\nthat increasing pretraining data diversity enhances SSL performance, albeit\nonly when the distribution distance to the downstream data is minimal. Notably,\neven with an exceptionally large pretraining data diversity achieved through\nmethods like web crawling or diffusion-generated data, among other ways, the\ndistribution shift remains a challenge. Our experiments are comprehensive with\nseven SSL methods using large-scale datasets such as ImageNet and YFCC100M\namounting to over 200 GPU days. Code and trained models will be available at\nhttps://github.com/hammoudhasan/DiversitySSL .",
        "translated": ""
    },
    {
        "title": "Editing Massive Concepts in Text-to-Image Diffusion Models",
        "url": "http://arxiv.org/abs/2403.13807v1",
        "pub_date": "2024-03-20",
        "summary": "Text-to-image diffusion models suffer from the risk of generating outdated,\ncopyrighted, incorrect, and biased content. While previous methods have\nmitigated the issues on a small scale, it is essential to handle them\nsimultaneously in larger-scale real-world scenarios. We propose a two-stage\nmethod, Editing Massive Concepts In Diffusion Models (EMCID). The first stage\nperforms memory optimization for each individual concept with dual\nself-distillation from text alignment loss and diffusion noise prediction loss.\nThe second stage conducts massive concept editing with multi-layer, closed form\nmodel editing. We further propose a comprehensive benchmark, named ImageNet\nConcept Editing Benchmark (ICEB), for evaluating massive concept editing for\nT2I models with two subtasks, free-form prompts, massive concept categories,\nand extensive evaluation metrics. Extensive experiments conducted on our\nproposed benchmark and previous benchmarks demonstrate the superior scalability\nof EMCID for editing up to 1,000 concepts, providing a practical approach for\nfast adjustment and re-deployment of T2I diffusion models in real-world\napplications.",
        "translated": ""
    },
    {
        "title": "RAR: Retrieving And Ranking Augmented MLLMs for Visual Recognition",
        "url": "http://arxiv.org/abs/2403.13805v1",
        "pub_date": "2024-03-20",
        "summary": "CLIP (Contrastive Language-Image Pre-training) uses contrastive learning from\nnoise image-text pairs to excel at recognizing a wide array of candidates, yet\nits focus on broad associations hinders the precision in distinguishing subtle\ndifferences among fine-grained items. Conversely, Multimodal Large Language\nModels (MLLMs) excel at classifying fine-grained categories, thanks to their\nsubstantial knowledge from pre-training on web-level corpora. However, the\nperformance of MLLMs declines with an increase in category numbers, primarily\ndue to growing complexity and constraints of limited context window size. To\nsynergize the strengths of both approaches and enhance the few-shot/zero-shot\nrecognition abilities for datasets characterized by extensive and fine-grained\nvocabularies, this paper introduces RAR, a Retrieving And Ranking augmented\nmethod for MLLMs. We initially establish a multi-modal retriever based on CLIP\nto create and store explicit memory for different categories beyond the\nimmediate context window. During inference, RAR retrieves the top-k similar\nresults from the memory and uses MLLMs to rank and make the final predictions.\nOur proposed approach not only addresses the inherent limitations in\nfine-grained recognition but also preserves the model's comprehensive knowledge\nbase, significantly boosting accuracy across a range of vision-language\nrecognition tasks. Notably, our approach demonstrates a significant improvement\nin performance on 5 fine-grained visual recognition benchmarks, 11 few-shot\nimage recognition datasets, and the 2 object detection datasets under the\nzero-shot recognition setting.",
        "translated": ""
    },
    {
        "title": "RadSplat: Radiance Field-Informed Gaussian Splatting for Robust\n  Real-Time Rendering with 900+ FPS",
        "url": "http://arxiv.org/abs/2403.13806v1",
        "pub_date": "2024-03-20",
        "summary": "Recent advances in view synthesis and real-time rendering have achieved\nphotorealistic quality at impressive rendering speeds. While Radiance\nField-based methods achieve state-of-the-art quality in challenging scenarios\nsuch as in-the-wild captures and large-scale scenes, they often suffer from\nexcessively high compute requirements linked to volumetric rendering. Gaussian\nSplatting-based methods, on the other hand, rely on rasterization and naturally\nachieve real-time rendering but suffer from brittle optimization heuristics\nthat underperform on more challenging scenes. In this work, we present\nRadSplat, a lightweight method for robust real-time rendering of complex\nscenes. Our main contributions are threefold. First, we use radiance fields as\na prior and supervision signal for optimizing point-based scene\nrepresentations, leading to improved quality and more robust optimization.\nNext, we develop a novel pruning technique reducing the overall point count\nwhile maintaining high quality, leading to smaller and more compact scene\nrepresentations with faster inference speeds. Finally, we propose a novel\ntest-time filtering approach that further accelerates rendering and allows to\nscale to larger, house-sized scenes. We find that our method enables\nstate-of-the-art synthesis of complex captures at 900+ FPS.",
        "translated": ""
    },
    {
        "title": "Learning from Models and Data for Visual Grounding",
        "url": "http://arxiv.org/abs/2403.13804v1",
        "pub_date": "2024-03-20",
        "summary": "We introduce SynGround, a novel framework that combines data-driven learning\nand knowledge transfer from various large-scale pretrained models to enhance\nthe visual grounding capabilities of a pretrained vision-and-language model.\nThe knowledge transfer from the models initiates the generation of image\ndescriptions through an image description generator. These descriptions serve\ndual purposes: they act as prompts for synthesizing images through a\ntext-to-image generator, and as queries for synthesizing text, from which\nphrases are extracted using a large language model. Finally, we leverage an\nopen-vocabulary object detector to generate synthetic bounding boxes for the\nsynthetic images and texts. We finetune a pretrained vision-and-language model\non this dataset by optimizing a mask-attention consistency objective that\naligns region annotations with gradient-based model explanations. The resulting\nmodel improves the grounding capabilities of an off-the-shelf\nvision-and-language model. Particularly, SynGround improves the pointing game\naccuracy of ALBEF on the Flickr30k dataset from 79.38% to 87.26%, and on\nRefCOCO+ Test A from 69.35% to 79.06% and on RefCOCO+ Test B from 53.77% to\n63.67%.",
        "translated": ""
    },
    {
        "title": "Bounding Box Stability against Feature Dropout Reflects Detector\n  Generalization across Environments",
        "url": "http://arxiv.org/abs/2403.13803v1",
        "pub_date": "2024-03-20",
        "summary": "Bounding boxes uniquely characterize object detection, where a good detector\ngives accurate bounding boxes of categories of interest. However, in the\nreal-world where test ground truths are not provided, it is non-trivial to find\nout whether bounding boxes are accurate, thus preventing us from assessing the\ndetector generalization ability. In this work, we find under feature map\ndropout, good detectors tend to output bounding boxes whose locations do not\nchange much, while bounding boxes of poor detectors will undergo noticeable\nposition changes. We compute the box stability score (BoS score) to reflect\nthis stability. Specifically, given an image, we compute a normal set of\nbounding boxes and a second set after feature map dropout. To obtain BoS score,\nwe use bipartite matching to find the corresponding boxes between the two sets\nand compute the average Intersection over Union (IoU) across the entire test\nset. We contribute to finding that BoS score has a strong, positive correlation\nwith detection accuracy measured by mean average precision (mAP) under various\ntest environments. This relationship allows us to predict the accuracy of\ndetectors on various real-world test sets without accessing test ground truths,\nverified on canonical detection tasks such as vehicle detection and pedestrian\ndetection. Code and data are available at https://github.com/YangYangGirl/BoS.",
        "translated": ""
    },
    {
        "title": "ZigMa: Zigzag Mamba Diffusion Model",
        "url": "http://arxiv.org/abs/2403.13802v1",
        "pub_date": "2024-03-20",
        "summary": "The diffusion model has long been plagued by scalability and quadratic\ncomplexity issues, especially within transformer-based structures. In this\nstudy, we aim to leverage the long sequence modeling capability of a\nState-Space Model called Mamba to extend its applicability to visual data\ngeneration. Firstly, we identify a critical oversight in most current\nMamba-based vision methods, namely the lack of consideration for spatial\ncontinuity in the scan scheme of Mamba. Secondly, building upon this insight,\nwe introduce a simple, plug-and-play, zero-parameter method named Zigzag Mamba,\nwhich outperforms Mamba-based baselines and demonstrates improved speed and\nmemory utilization compared to transformer-based baselines. Lastly, we\nintegrate Zigzag Mamba with the Stochastic Interpolant framework to investigate\nthe scalability of the model on large-resolution visual datasets, such as\nFacesHQ $1024\\times 1024$ and UCF101, MultiModal-CelebA-HQ, and MS COCO\n$256\\times 256$. Code will be released at https://taohu.me/zigma/",
        "translated": ""
    },
    {
        "title": "TimeRewind: Rewinding Time with Image-and-Events Video Diffusion",
        "url": "http://arxiv.org/abs/2403.13800v1",
        "pub_date": "2024-03-20",
        "summary": "This paper addresses the novel challenge of ``rewinding'' time from a single\ncaptured image to recover the fleeting moments missed just before the shutter\nbutton is pressed. This problem poses a significant challenge in computer\nvision and computational photography, as it requires predicting plausible\npre-capture motion from a single static frame, an inherently ill-posed task due\nto the high degree of freedom in potential pixel movements. We overcome this\nchallenge by leveraging the emerging technology of neuromorphic event cameras,\nwhich capture motion information with high temporal resolution, and integrating\nthis data with advanced image-to-video diffusion models. Our proposed framework\nintroduces an event motion adaptor conditioned on event camera data, guiding\nthe diffusion model to generate videos that are visually coherent and\nphysically grounded in the captured events. Through extensive experimentation,\nwe demonstrate the capability of our approach to synthesize high-quality videos\nthat effectively ``rewind'' time, showcasing the potential of combining event\ncamera technology with generative models. Our work opens new avenues for\nresearch at the intersection of computer vision, computational photography, and\ngenerative modeling, offering a forward-thinking solution to capturing missed\nmoments and enhancing future consumer cameras and smartphones. Please see the\nproject page at https://timerewind.github.io/ for video results and code\nrelease.",
        "translated": ""
    },
    {
        "title": "Hierarchical NeuroSymbolic Approach for Action Quality Assessment",
        "url": "http://arxiv.org/abs/2403.13798v1",
        "pub_date": "2024-03-20",
        "summary": "Action quality assessment (AQA) applies computer vision to quantitatively\nassess the performance or execution of a human action. Current AQA approaches\nare end-to-end neural models, which lack transparency and tend to be biased\nbecause they are trained on subjective human judgements as ground-truth. To\naddress these issues, we introduce a neuro-symbolic paradigm for AQA, which\nuses neural networks to abstract interpretable symbols from video data and\nmakes quality assessments by applying rules to those symbols. We take diving as\nthe case study. We found that domain experts prefer our system and find it more\ninformative than purely neural approaches to AQA in diving. Our system also\nachieves state-of-the-art action recognition and temporal segmentation, and\nautomatically generates a detailed report that breaks the dive down into its\nelements and provides objective scoring with visual evidence. As verified by a\ngroup of domain experts, this report may be used to assist judges in scoring,\nhelp train judges, and provide feedback to divers. We will open-source all of\nour annotated training data and code for ease of reproducibility.",
        "translated": ""
    },
    {
        "title": "Bridge the Modality and Capacity Gaps in Vision-Language Model Selection",
        "url": "http://arxiv.org/abs/2403.13797v1",
        "pub_date": "2024-03-20",
        "summary": "Vision Language Models (VLMs) excel in zero-shot image classification by\npairing images with textual category names. The expanding variety of\nPre-Trained VLMs enhances the likelihood of identifying a suitable VLM for\nspecific tasks. Thus, a promising zero-shot image classification strategy is\nselecting the most appropriate Pre-Trained VLM from the VLM Zoo, relying solely\non the text data of the target dataset without access to the dataset's images.\nIn this paper, we analyze two inherent challenges in assessing the ability of a\nVLM in this Language-Only VLM selection: the \"Modality Gap\" -- the disparity in\nVLM's embeddings across two different modalities, making text a less reliable\nsubstitute for images; and the \"Capability Gap\" -- the discrepancy between the\nVLM's overall ranking and its ranking for target dataset, hindering direct\nprediction of a model's dataset-specific performance from its general\nperformance. We propose VLM Selection With gAp Bridging (SWAB) to mitigate the\nnegative impact of these two gaps. SWAB first adopts optimal transport to\ncapture the relevance between open-source datasets and target dataset with a\ntransportation matrix. It then uses this matrix to transfer useful statistics\nof VLMs from open-source datasets to the target dataset for bridging those two\ngaps and enhancing the VLM's capacity estimation for VLM selection. Experiments\nacross various VLMs and image classification datasets validate SWAB's\neffectiveness.",
        "translated": ""
    },
    {
        "title": "Zero-Shot Multi-Object Shape Completion",
        "url": "http://arxiv.org/abs/2403.14628v1",
        "pub_date": "2024-03-21",
        "summary": "We present a 3D shape completion method that recovers the complete geometry\nof multiple objects in complex scenes from a single RGB-D image. Despite\nnotable advancements in single object 3D shape completion, high-quality\nreconstructions in highly cluttered real-world multi-object scenes remains a\nchallenge. To address this issue, we propose OctMAE, an architecture that\nleverages an Octree U-Net and a latent 3D MAE to achieve high-quality and near\nreal-time multi-object shape completion through both local and global geometric\nreasoning. Because a na\\\"ive 3D MAE can be computationally intractable and\nmemory intensive even in the latent space, we introduce a novel occlusion\nmasking strategy and adopt 3D rotary embeddings, which significantly improves\nthe runtime and shape completion quality. To generalize to a wide range of\nobjects in diverse scenes, we create a large-scale photorealistic dataset,\nfeaturing a diverse set of 12K 3D object models from the Objaverse dataset\nwhich are rendered in multi-object scenes with physics-based positioning. Our\nmethod outperforms the current state-of-the-art on both synthetic and\nreal-world datasets and demonstrates a strong zero-shot capability.",
        "translated": ""
    },
    {
        "title": "MVSplat: Efficient 3D Gaussian Splatting from Sparse Multi-View Images",
        "url": "http://arxiv.org/abs/2403.14627v1",
        "pub_date": "2024-03-21",
        "summary": "We propose MVSplat, an efficient feed-forward 3D Gaussian Splatting model\nlearned from sparse multi-view images. To accurately localize the Gaussian\ncenters, we propose to build a cost volume representation via plane sweeping in\nthe 3D space, where the cross-view feature similarities stored in the cost\nvolume can provide valuable geometry cues to the estimation of depth. We learn\nthe Gaussian primitives' opacities, covariances, and spherical harmonics\ncoefficients jointly with the Gaussian centers while only relying on\nphotometric supervision. We demonstrate the importance of the cost volume\nrepresentation in learning feed-forward Gaussian Splatting models via extensive\nexperimental evaluations. On the large-scale RealEstate10K and ACID benchmarks,\nour model achieves state-of-the-art performance with the fastest feed-forward\ninference speed (22 fps). Compared to the latest state-of-the-art method\npixelSplat, our model uses $10\\times $ fewer parameters and infers more than\n$2\\times$ faster while providing higher appearance and geometry quality as well\nas better cross-dataset generalization.",
        "translated": ""
    },
    {
        "title": "LiFT: A Surprisingly Simple Lightweight Feature Transform for Dense ViT\n  Descriptors",
        "url": "http://arxiv.org/abs/2403.14625v1",
        "pub_date": "2024-03-21",
        "summary": "We present a simple self-supervised method to enhance the performance of ViT\nfeatures for dense downstream tasks. Our Lightweight Feature Transform (LiFT)\nis a straightforward and compact postprocessing network that can be applied to\nenhance the features of any pre-trained ViT backbone. LiFT is fast and easy to\ntrain with a self-supervised objective, and it boosts the density of ViT\nfeatures for minimal extra inference cost. Furthermore, we demonstrate that\nLiFT can be applied with approaches that use additional task-specific\ndownstream modules, as we integrate LiFT with ViTDet for COCO detection and\nsegmentation. Despite the simplicity of LiFT, we find that it is not simply\nlearning a more complex version of bilinear interpolation. Instead, our LiFT\ntraining protocol leads to several desirable emergent properties that benefit\nViT features in dense downstream tasks. This includes greater scale invariance\nfor features, and better object boundary maps. By simply training LiFT for a\nfew epochs, we show improved performance on keypoint correspondence, detection,\nsegmentation, and object discovery tasks. Overall, LiFT provides an easy way to\nunlock the benefits of denser feature arrays for a fraction of the\ncomputational cost. For more details, refer to our project page at\nhttps://www.cs.umd.edu/~sakshams/LiFT/.",
        "translated": ""
    },
    {
        "title": "ODTFormer: Efficient Obstacle Detection and Tracking with Stereo Cameras\n  Based on Transformer",
        "url": "http://arxiv.org/abs/2403.14626v1",
        "pub_date": "2024-03-21",
        "summary": "Obstacle detection and tracking represent a critical component in robot\nautonomous navigation. In this paper, we propose ODTFormer, a Transformer-based\nmodel to address both obstacle detection and tracking problems. For the\ndetection task, our approach leverages deformable attention to construct a 3D\ncost volume, which is decoded progressively in the form of voxel occupancy\ngrids. We further track the obstacles by matching the voxels between\nconsecutive frames. The entire model can be optimized in an end-to-end manner.\nThrough extensive experiments on DrivingStereo and KITTI benchmarks, our model\nachieves state-of-the-art performance in the obstacle detection task. We also\nreport comparable accuracy to state-of-the-art obstacle tracking models while\nrequiring only a fraction of their computation cost, typically ten-fold to\ntwenty-fold less. The code and model weights will be publicly released.",
        "translated": ""
    },
    {
        "title": "MathVerse: Does Your Multi-modal LLM Truly See the Diagrams in Visual\n  Math Problems?",
        "url": "http://arxiv.org/abs/2403.14624v1",
        "pub_date": "2024-03-21",
        "summary": "The remarkable progress of Multi-modal Large Language Models (MLLMs) has\ngarnered unparalleled attention, due to their superior performance in visual\ncontexts. However, their capabilities in visual math problem-solving remain\ninsufficiently evaluated and understood. We investigate current benchmarks to\nincorporate excessive visual content within textual questions, which\npotentially assist MLLMs in deducing answers without truly interpreting the\ninput diagrams. To this end, we introduce MathVerse, an all-around visual math\nbenchmark designed for an equitable and in-depth evaluation of MLLMs. We\nmeticulously collect 2,612 high-quality, multi-subject math problems with\ndiagrams from publicly available sources. Each problem is then transformed by\nhuman annotators into six distinct versions, each offering varying degrees of\ninformation content in multi-modality, contributing to 15K test samples in\ntotal. This approach allows MathVerse to comprehensively assess whether and how\nmuch MLLMs can truly understand the visual diagrams for mathematical reasoning.\nIn addition, we propose a Chain-of-Thought (CoT) evaluation strategy for a\nfine-grained assessment of the output answers. Rather than naively judging True\nor False, we employ GPT-4(V) to adaptively extract crucial reasoning steps, and\nthen score each step with detailed error analysis, which can reveal the\nintermediate CoT reasoning quality by MLLMs. We hope the MathVerse benchmark\nmay provide unique insights to guide the future development of MLLMs. Project\npage: https://mathverse-cuhk.github.io",
        "translated": ""
    },
    {
        "title": "Simplified Diffusion Schrödinger Bridge",
        "url": "http://arxiv.org/abs/2403.14623v1",
        "pub_date": "2024-03-21",
        "summary": "This paper introduces a novel theoretical simplification of the Diffusion\nSchr\\\"odinger Bridge (DSB) that facilitates its unification with Score-based\nGenerative Models (SGMs), addressing the limitations of DSB in complex data\ngeneration and enabling faster convergence and enhanced performance. By\nemploying SGMs as an initial solution for DSB, our approach capitalizes on the\nstrengths of both frameworks, ensuring a more efficient training process and\nimproving the performance of SGM. We also propose a reparameterization\ntechnique that, despite theoretical approximations, practically improves the\nnetwork's fitting capabilities. Our extensive experimental evaluations confirm\nthe effectiveness of the simplified DSB, demonstrating its significant\nimprovements. We believe the contributions of this work pave the way for\nadvanced generative modeling. The code is available at\nhttps://github.com/tzco/Simplified-Diffusion-Schrodinger-Bridge.",
        "translated": ""
    },
    {
        "title": "Language Repository for Long Video Understanding",
        "url": "http://arxiv.org/abs/2403.14622v1",
        "pub_date": "2024-03-21",
        "summary": "Language has become a prominent modality in computer vision with the rise of\nmulti-modal LLMs. Despite supporting long context-lengths, their effectiveness\nin handling long-term information gradually declines with input length. This\nbecomes critical, especially in applications such as long-form video\nunderstanding. In this paper, we introduce a Language Repository (LangRepo) for\nLLMs, that maintains concise and structured information as an interpretable\n(i.e., all-textual) representation. Our repository is updated iteratively based\non multi-scale video chunks. We introduce write and read operations that focus\non pruning redundancies in text, and extracting information at various temporal\nscales. The proposed framework is evaluated on zero-shot visual\nquestion-answering benchmarks including EgoSchema, NExT-QA, IntentQA and\nNExT-GQA, showing state-of-the-art performance at its scale. Our code is\navailable at https://github.com/kkahatapitiya/LangRepo.",
        "translated": ""
    },
    {
        "title": "GRM: Large Gaussian Reconstruction Model for Efficient 3D Reconstruction\n  and Generation",
        "url": "http://arxiv.org/abs/2403.14621v1",
        "pub_date": "2024-03-21",
        "summary": "We introduce GRM, a large-scale reconstructor capable of recovering a 3D\nasset from sparse-view images in around 0.1s. GRM is a feed-forward\ntransformer-based model that efficiently incorporates multi-view information to\ntranslate the input pixels into pixel-aligned Gaussians, which are unprojected\nto create a set of densely distributed 3D Gaussians representing a scene.\nTogether, our transformer architecture and the use of 3D Gaussians unlock a\nscalable and efficient reconstruction framework. Extensive experimental results\ndemonstrate the superiority of our method over alternatives regarding both\nreconstruction quality and efficiency. We also showcase the potential of GRM in\ngenerative tasks, i.e., text-to-3D and image-to-3D, by integrating it with\nexisting multi-view diffusion models. Our project website is at:\nhttps://justimyhxu.github.io/projects/grm/.",
        "translated": ""
    },
    {
        "title": "ClusteringSDF: Self-Organized Neural Implicit Surfaces for 3D\n  Decomposition",
        "url": "http://arxiv.org/abs/2403.14619v1",
        "pub_date": "2024-03-21",
        "summary": "3D decomposition/segmentation still remains a challenge as large-scale 3D\nannotated data is not readily available. Contemporary approaches typically\nleverage 2D machine-generated segments, integrating them for 3D consistency.\nWhile the majority of these methods are based on NeRFs, they face a potential\nweakness that the instance/semantic embedding features derive from independent\nMLPs, thus preventing the segmentation network from learning the geometric\ndetails of the objects directly through radiance and density. In this paper, we\npropose ClusteringSDF, a novel approach to achieve both segmentation and\nreconstruction in 3D via the neural implicit surface representation,\nspecifically Signal Distance Function (SDF), where the segmentation rendering\nis directly integrated with the volume rendering of neural implicit surfaces.\nAlthough based on ObjectSDF++, ClusteringSDF no longer requires the\nground-truth segments for supervision while maintaining the capability of\nreconstructing individual object surfaces, but purely with the noisy and\ninconsistent labels from pre-trained models.As the core of ClusteringSDF, we\nintroduce a high-efficient clustering mechanism for lifting the 2D labels to 3D\nand the experimental results on the challenging scenes from ScanNet and Replica\ndatasets show that ClusteringSDF can achieve competitive performance compared\nagainst the state-of-the-art with significantly reduced training time.",
        "translated": ""
    },
    {
        "title": "Videoshop: Localized Semantic Video Editing with Noise-Extrapolated\n  Diffusion Inversion",
        "url": "http://arxiv.org/abs/2403.14617v1",
        "pub_date": "2024-03-21",
        "summary": "We introduce Videoshop, a training-free video editing algorithm for localized\nsemantic edits. Videoshop allows users to use any editing software, including\nPhotoshop and generative inpainting, to modify the first frame; it\nautomatically propagates those changes, with semantic, spatial, and temporally\nconsistent motion, to the remaining frames. Unlike existing methods that enable\nedits only through imprecise textual instructions, Videoshop allows users to\nadd or remove objects, semantically change objects, insert stock photos into\nvideos, etc. with fine-grained control over locations and appearance. We\nachieve this through image-based video editing by inverting latents with noise\nextrapolation, from which we generate videos conditioned on the edited image.\nVideoshop produces higher quality edits against 6 baselines on 2 editing\nbenchmarks using 10 evaluation metrics.",
        "translated": ""
    },
    {
        "title": "DiffusionMTL: Learning Multi-Task Denoising Diffusion Model from\n  Partially Annotated Data",
        "url": "http://arxiv.org/abs/2403.15389v1",
        "pub_date": "2024-03-22",
        "summary": "Recently, there has been an increased interest in the practical problem of\nlearning multiple dense scene understanding tasks from partially annotated\ndata, where each training sample is only labeled for a subset of the tasks. The\nmissing of task labels in training leads to low-quality and noisy predictions,\nas can be observed from state-of-the-art methods. To tackle this issue, we\nreformulate the partially-labeled multi-task dense prediction as a pixel-level\ndenoising problem, and propose a novel multi-task denoising diffusion framework\ncoined as DiffusionMTL. It designs a joint diffusion and denoising paradigm to\nmodel a potential noisy distribution in the task prediction or feature maps and\ngenerate rectified outputs for different tasks. To exploit multi-task\nconsistency in denoising, we further introduce a Multi-Task Conditioning\nstrategy, which can implicitly utilize the complementary nature of the tasks to\nhelp learn the unlabeled tasks, leading to an improvement in the denoising\nperformance of the different tasks. Extensive quantitative and qualitative\nexperiments demonstrate that the proposed multi-task denoising diffusion model\ncan significantly improve multi-task prediction maps, and outperform the\nstate-of-the-art methods on three challenging multi-task benchmarks, under two\ndifferent partial-labeling evaluation settings. The code is available at\nhttps://prismformore.github.io/diffusionmtl/.",
        "translated": ""
    },
    {
        "title": "LLaVA-PruMerge: Adaptive Token Reduction for Efficient Large Multimodal\n  Models",
        "url": "http://arxiv.org/abs/2403.15388v1",
        "pub_date": "2024-03-22",
        "summary": "Large Multimodal Models (LMMs) have shown significant reasoning capabilities\nby connecting a visual encoder and a large language model. LMMs typically use a\nfixed amount of visual tokens, such as the penultimate layer features in the\nCLIP visual encoder, as the prefix content. Recent LMMs incorporate more\ncomplex visual inputs, such as high-resolution images and videos, which\nincrease the number of visual tokens significantly. However, due to the design\nof the Transformer architecture, computational costs associated with these\nmodels tend to increase quadratically with the number of input tokens. To\ntackle this problem, we explore a token reduction mechanism and find, similar\nto prior work, that many visual tokens are spatially redundant. Based on this,\nwe propose PruMerge, a novel adaptive visual token reduction approach, which\nlargely reduces the number of visual tokens while maintaining comparable model\nperformance. We first select the unpruned visual tokens based on their\nsimilarity to class tokens and spatial tokens. We then cluster the pruned\ntokens based on key similarity and merge the clustered tokens with the unpruned\ntokens to supplement their information. Empirically, when applied to LLaVA-1.5,\nour approach can compress the visual tokens by 14.4 times on average, and\nachieve comparable performance across diverse visual question-answering and\nreasoning tasks. Code and checkpoints are at https://llava-prumerge.github.io/.",
        "translated": ""
    },
    {
        "title": "LATTE3D: Large-scale Amortized Text-To-Enhanced3D Synthesis",
        "url": "http://arxiv.org/abs/2403.15385v1",
        "pub_date": "2024-03-22",
        "summary": "Recent text-to-3D generation approaches produce impressive 3D results but\nrequire time-consuming optimization that can take up to an hour per prompt.\nAmortized methods like ATT3D optimize multiple prompts simultaneously to\nimprove efficiency, enabling fast text-to-3D synthesis. However, they cannot\ncapture high-frequency geometry and texture details and struggle to scale to\nlarge prompt sets, so they generalize poorly. We introduce LATTE3D, addressing\nthese limitations to achieve fast, high-quality generation on a significantly\nlarger prompt set. Key to our method is 1) building a scalable architecture and\n2) leveraging 3D data during optimization through 3D-aware diffusion priors,\nshape regularization, and model initialization to achieve robustness to diverse\nand complex training prompts. LATTE3D amortizes both neural field and textured\nsurface generation to produce highly detailed textured meshes in a single\nforward pass. LATTE3D generates 3D objects in 400ms, and can be further\nenhanced with fast test-time optimization.",
        "translated": ""
    },
    {
        "title": "ThemeStation: Generating Theme-Aware 3D Assets from Few Exemplars",
        "url": "http://arxiv.org/abs/2403.15383v1",
        "pub_date": "2024-03-22",
        "summary": "Real-world applications often require a large gallery of 3D assets that share\na consistent theme. While remarkable advances have been made in general 3D\ncontent creation from text or image, synthesizing customized 3D assets\nfollowing the shared theme of input 3D exemplars remains an open and\nchallenging problem. In this work, we present ThemeStation, a novel approach\nfor theme-aware 3D-to-3D generation. ThemeStation synthesizes customized 3D\nassets based on given few exemplars with two goals: 1) unity for generating 3D\nassets that thematically align with the given exemplars and 2) diversity for\ngenerating 3D assets with a high degree of variations. To this end, we design a\ntwo-stage framework that draws a concept image first, followed by a\nreference-informed 3D modeling stage. We propose a novel dual score\ndistillation (DSD) loss to jointly leverage priors from both the input\nexemplars and the synthesized concept image. Extensive experiments and user\nstudies confirm that ThemeStation surpasses prior works in producing diverse\ntheme-aware 3D models with impressive quality. ThemeStation also enables\nvarious applications such as controllable 3D-to-3D generation.",
        "translated": ""
    },
    {
        "title": "DragAPart: Learning a Part-Level Motion Prior for Articulated Objects",
        "url": "http://arxiv.org/abs/2403.15382v1",
        "pub_date": "2024-03-22",
        "summary": "We introduce DragAPart, a method that, given an image and a set of drags as\ninput, can generate a new image of the same object in a new state, compatible\nwith the action of the drags. Differently from prior works that focused on\nrepositioning objects, DragAPart predicts part-level interactions, such as\nopening and closing a drawer. We study this problem as a proxy for learning a\ngeneralist motion model, not restricted to a specific kinematic structure or\nobject category. To this end, we start from a pre-trained image generator and\nfine-tune it on a new synthetic dataset, Drag-a-Move, which we introduce.\nCombined with a new encoding for the drags and dataset randomization, the new\nmodel generalizes well to real images and different categories. Compared to\nprior motion-controlled generators, we demonstrate much better part-level\nmotion understanding.",
        "translated": ""
    },
    {
        "title": "Long-CLIP: Unlocking the Long-Text Capability of CLIP",
        "url": "http://arxiv.org/abs/2403.15378v1",
        "pub_date": "2024-03-22",
        "summary": "Contrastive Language-Image Pre-training (CLIP) has been the cornerstone for\nzero-shot classification, text-image retrieval, and text-image generation by\naligning image and text modalities. Despite its widespread adoption, a\nsignificant limitation of CLIP lies in the inadequate length of text input. The\nlength of the text token is restricted to 77, and an empirical study shows the\nactual effective length is even less than 20. This prevents CLIP from handling\ndetailed descriptions, limiting its applications for image retrieval and\ntext-to-image generation with extensive prerequisites. To this end, we propose\nLong-CLIP as a plug-and-play alternative to CLIP that supports long-text input,\nretains or even surpasses its zero-shot generalizability, and aligns the CLIP\nlatent space, making it readily replace CLIP without any further adaptation in\ndownstream frameworks. Nevertheless, achieving this goal is far from\nstraightforward, as simplistic fine-tuning can result in a significant\ndegradation of CLIP's performance. Moreover, substituting the text encoder with\na language model supporting longer contexts necessitates pretraining with vast\namounts of data, incurring significant expenses. Accordingly, Long-CLIP\nintroduces an efficient fine-tuning solution on CLIP with two novel strategies\ndesigned to maintain the original capabilities, including (1) a\nknowledge-preserved stretching of positional embedding and (2) a primary\ncomponent matching of CLIP features. With leveraging just one million extra\nlong text-image pairs, Long-CLIP has shown the superiority to CLIP for about\n20% in long caption text-image retrieval and 6% in traditional text-image\nretrieval tasks, e.g., COCO and Flickr30k. Furthermore, Long-CLIP offers\nenhanced capabilities for generating images from detailed text descriptions by\nreplacing CLIP in a plug-and-play manner.",
        "translated": ""
    },
    {
        "title": "InternVideo2: Scaling Video Foundation Models for Multimodal Video\n  Understanding",
        "url": "http://arxiv.org/abs/2403.15377v1",
        "pub_date": "2024-03-22",
        "summary": "We introduce InternVideo2, a new video foundation model (ViFM) that achieves\nthe state-of-the-art performance in action recognition, video-text tasks, and\nvideo-centric dialogue. Our approach employs a progressive training paradigm\nthat unifies the different self- or weakly-supervised learning frameworks of\nmasked video token reconstruction, cross-modal contrastive learning, and next\ntoken prediction. Different training stages would guide our model to capture\ndifferent levels of structure and semantic information through different\npretext tasks. At the data level, we prioritize the spatiotemporal consistency\nby semantically segmenting videos and generating video-audio-speech captions.\nThis improves the alignment between video and text. We scale both data and\nmodel size for our InternVideo2. Through extensive experiments, we validate our\ndesigns and demonstrate the state-of-the-art performance on over 60 video and\naudio tasks. Notably, our model outperforms others on various video-related\ncaptioning, dialogue, and long video understanding benchmarks, highlighting its\nability to reason and comprehend long temporal contexts. Code and models are\navailable at https://github.com/OpenGVLab/InternVideo2/.",
        "translated": ""
    },
    {
        "title": "Augmented Reality based Simulated Data (ARSim) with multi-view\n  consistency for AV perception networks",
        "url": "http://arxiv.org/abs/2403.15370v1",
        "pub_date": "2024-03-22",
        "summary": "Detecting a diverse range of objects under various driving scenarios is\nessential for the effectiveness of autonomous driving systems. However, the\nreal-world data collected often lacks the necessary diversity presenting a\nlong-tail distribution. Although synthetic data has been utilized to overcome\nthis issue by generating virtual scenes, it faces hurdles such as a significant\ndomain gap and the substantial efforts required from 3D artists to create\nrealistic environments. To overcome these challenges, we present ARSim, a fully\nautomated, comprehensive, modular framework designed to enhance real multi-view\nimage data with 3D synthetic objects of interest. The proposed method\nintegrates domain adaptation and randomization strategies to address covariate\nshift between real and simulated data by inferring essential domain attributes\nfrom real data and employing simulation-based randomization for other\nattributes. We construct a simplified virtual scene using real data and\nstrategically place 3D synthetic assets within it. Illumination is achieved by\nestimating light distribution from multiple images capturing the surroundings\nof the vehicle. Camera parameters from real data are employed to render\nsynthetic assets in each frame. The resulting augmented multi-view consistent\ndataset is used to train a multi-camera perception network for autonomous\nvehicles. Experimental results on various AV perception tasks demonstrate the\nsuperior performance of networks trained on the augmented dataset.",
        "translated": ""
    },
    {
        "title": "Learning Topological Representations for Deep Image Understanding",
        "url": "http://arxiv.org/abs/2403.15361v1",
        "pub_date": "2024-03-22",
        "summary": "In many scenarios, especially biomedical applications, the correct\ndelineation of complex fine-scaled structures such as neurons, tissues, and\nvessels is critical for downstream analysis. Despite the strong predictive\npower of deep learning methods, they do not provide a satisfactory\nrepresentation of these structures, thus creating significant barriers in\nscalable annotation and downstream analysis. In this dissertation, we tackle\nsuch challenges by proposing novel representations of these topological\nstructures in a deep learning framework. We leverage the mathematical tools\nfrom topological data analysis, i.e., persistent homology and discrete Morse\ntheory, to develop principled methods for better segmentation and uncertainty\nestimation, which will become powerful tools for scalable annotation.",
        "translated": ""
    },
    {
        "title": "SiMBA: Simplified Mamba-Based Architecture for Vision and Multivariate\n  Time series",
        "url": "http://arxiv.org/abs/2403.15360v1",
        "pub_date": "2024-03-22",
        "summary": "Transformers have widely adopted attention networks for sequence mixing and\nMLPs for channel mixing, playing a pivotal role in achieving breakthroughs\nacross domains. However, recent literature highlights issues with attention\nnetworks, including low inductive bias and quadratic complexity concerning\ninput sequence length. State Space Models (SSMs) like S4 and others (Hippo,\nGlobal Convolutions, liquid S4, LRU, Mega, and Mamba), have emerged to address\nthe above issues to help handle longer sequence lengths. Mamba, while being the\nstate-of-the-art SSM, has a stability issue when scaled to large networks for\ncomputer vision datasets. We propose SiMBA, a new architecture that introduces\nEinstein FFT (EinFFT) for channel modeling by specific eigenvalue computations\nand uses the Mamba block for sequence modeling. Extensive performance studies\nacross image and time-series benchmarks demonstrate that SiMBA outperforms\nexisting SSMs, bridging the performance gap with state-of-the-art transformers.\nNotably, SiMBA establishes itself as the new state-of-the-art SSM on ImageNet\nand transfer learning benchmarks such as Stanford Car and Flower as well as\ntask learning benchmarks as well as seven time series benchmark datasets. The\nproject page is available on this website\n~\\url{https://github.com/badripatro/Simba}.",
        "translated": ""
    },
    {
        "title": "DPStyler: Dynamic PromptStyler for Source-Free Domain Generalization",
        "url": "http://arxiv.org/abs/2403.16697v1",
        "pub_date": "2024-03-25",
        "summary": "Source-Free Domain Generalization (SFDG) aims to develop a model that works\nfor unseen target domains without relying on any source domain. Recent work,\nPromptStyler, employs text prompts to simulate different distribution shifts in\nthe joint vision-language space, allowing the model to generalize effectively\nto unseen domains without using any images. However, 1) PromptStyler's style\ngeneration strategy has limitations, as all style patterns are fixed after the\nfirst training phase. This leads to the training set in the second training\nphase being restricted to a limited set of styles. Additionally, 2) the frozen\ntext encoder in PromptStyler result in the encoder's output varying with the\nstyle of the input text prompts, making it difficult for the model to learn\ndomain-invariant features. In this paper, we introduce Dynamic PromptStyler\n(DPStyler), comprising Style Generation and Style Removal modules to address\nthese issues. The Style Generation module refreshes all styles at every\ntraining epoch, while the Style Removal module eliminates variations in the\nencoder's output features caused by input styles. Moreover, since the Style\nGeneration module, responsible for generating style word vectors using random\nsampling or style mixing, makes the model sensitive to input text prompts, we\nintroduce a model ensemble method to mitigate this sensitivity. Extensive\nexperiments demonstrate that our framework outperforms state-of-the-art methods\non benchmark datasets.",
        "translated": ""
    },
    {
        "title": "Assessing the Performance of Deep Learning for Automated Gleason Grading\n  in Prostate Cancer",
        "url": "http://arxiv.org/abs/2403.16695v1",
        "pub_date": "2024-03-25",
        "summary": "Prostate cancer is a dominant health concern calling for advanced diagnostic\ntools. Utilizing digital pathology and artificial intelligence, this study\nexplores the potential of 11 deep neural network architectures for automated\nGleason grading in prostate carcinoma focusing on comparing traditional and\nrecent architectures. A standardized image classification pipeline, based on\nthe AUCMEDI framework, facilitated robust evaluation using an in-house dataset\nconsisting of 34,264 annotated tissue tiles. The results indicated varying\nsensitivity across architectures, with ConvNeXt demonstrating the strongest\nperformance. Notably, newer architectures achieved superior performance, even\nthough with challenges in differentiating closely related Gleason grades. The\nConvNeXt model was capable of learning a balance between complexity and\ngeneralizability. Overall, this study lays the groundwork for enhanced Gleason\ngrading systems, potentially improving diagnostic efficiency for prostate\ncancer.",
        "translated": ""
    },
    {
        "title": "Synapse: Learning Preferential Concepts from Visual Demonstrations",
        "url": "http://arxiv.org/abs/2403.16689v1",
        "pub_date": "2024-03-25",
        "summary": "This paper addresses the problem of preference learning, which aims to learn\nuser-specific preferences (e.g., \"good parking spot\", \"convenient drop-off\nlocation\") from visual input. Despite its similarity to learning factual\nconcepts (e.g., \"red cube\"), preference learning is a fundamentally harder\nproblem due to its subjective nature and the paucity of person-specific\ntraining data. We address this problem using a new framework called Synapse,\nwhich is a neuro-symbolic approach designed to efficiently learn preferential\nconcepts from limited demonstrations. Synapse represents preferences as\nneuro-symbolic programs in a domain-specific language (DSL) that operates over\nimages, and leverages a novel combination of visual parsing, large language\nmodels, and program synthesis to learn programs representing individual\npreferences. We evaluate Synapse through extensive experimentation including a\nuser case study focusing on mobility-related concepts in mobile robotics and\nautonomous driving. Our evaluation demonstrates that Synapse significantly\noutperforms existing baselines as well as its own ablations. The code and other\ndetails can be found on the project website https://amrl.cs.utexas.edu/synapse .",
        "translated": ""
    },
    {
        "title": "DeepGleason: a System for Automated Gleason Grading of Prostate Cancer\n  using Deep Neural Networks",
        "url": "http://arxiv.org/abs/2403.16678v1",
        "pub_date": "2024-03-25",
        "summary": "Advances in digital pathology and artificial intelligence (AI) offer\npromising opportunities for clinical decision support and enhancing diagnostic\nworkflows. Previous studies already demonstrated AI's potential for automated\nGleason grading, but lack state-of-the-art methodology and model reusability.\nTo address this issue, we propose DeepGleason: an open-source deep neural\nnetwork based image classification system for automated Gleason grading using\nwhole-slide histopathology images from prostate tissue sections. Implemented\nwith the standardized AUCMEDI framework, our tool employs a tile-wise\nclassification approach utilizing fine-tuned image preprocessing techniques in\ncombination with a ConvNeXt architecture which was compared to various\nstate-of-the-art architectures. The neural network model was trained and\nvalidated on an in-house dataset of 34,264 annotated tiles from 369 prostate\ncarcinoma slides. We demonstrated that DeepGleason is capable of highly\naccurate and reliable Gleason grading with a macro-averaged F1-score of 0.806,\nAUC of 0.991, and Accuracy of 0.974. The internal architecture comparison\nrevealed that the ConvNeXt model was superior performance-wise on our dataset\nto established and other modern architectures like transformers. Furthermore,\nwe were able to outperform the current state-of-the-art in tile-wise\nfine-classification with a sensitivity and specificity of 0.94 and 0.98 for\nbenign vs malignant detection as well as of 0.91 and 0.75 for Gleason 3 vs\nGleason 4 &amp; 5 classification, respectively. Our tool contributes to the wider\nadoption of AI-based Gleason grading within the research community and paves\nthe way for broader clinical application of deep learning models in digital\npathology. DeepGleason is open-source and publicly available for research\napplication in the following Git repository:\nhttps://github.com/frankkramer-lab/DeepGleason.",
        "translated": ""
    },
    {
        "title": "FOOL: Addressing the Downlink Bottleneck in Satellite Computing with\n  Neural Feature Compression",
        "url": "http://arxiv.org/abs/2403.16677v1",
        "pub_date": "2024-03-25",
        "summary": "Nanosatellite constellations equipped with sensors capturing large geographic\nregions provide unprecedented opportunities for Earth observation. As\nconstellation sizes increase, network contention poses a downlink bottleneck.\nOrbital Edge Computing (OEC) leverages limited onboard compute resources to\nreduce transfer costs by processing the raw captures at the source. However,\ncurrent solutions have limited practicability due to reliance on crude\nfiltering methods or over-prioritizing particular downstream tasks.\n  This work presents FOOL, an OEC-native and task-agnostic feature compression\nmethod that preserves prediction performance. FOOL partitions high-resolution\nsatellite imagery to maximize throughput. Further, it embeds context and\nleverages inter-tile dependencies to lower transfer costs with negligible\noverhead. While FOOL is a feature compressor, it can recover images with\ncompetitive scores on perceptual quality measures at lower bitrates. We\nextensively evaluate transfer cost reduction by including the peculiarity of\nintermittently available network connections in low earth orbit. Lastly, we\ntest the feasibility of our system for standardized nanosatellite form factors.\nWe demonstrate that FOOL permits downlinking over 100x the data volume without\nrelying on prior information on the downstream tasks.",
        "translated": ""
    },
    {
        "title": "Domain Adaptive Detection of MAVs: A Benchmark and Noise Suppression\n  Network",
        "url": "http://arxiv.org/abs/2403.16669v1",
        "pub_date": "2024-03-25",
        "summary": "Visual detection of Micro Air Vehicles (MAVs) has attracted increasing\nattention in recent years due to its important application in various tasks.\nThe existing methods for MAV detection assume that the training set and testing\nset have the same distribution. As a result, when deployed in new domains, the\ndetectors would have a significant performance degradation due to domain\ndiscrepancy. In this paper, we study the problem of cross-domain MAV detection.\nThe contributions of this paper are threefold. 1) We propose a\nMulti-MAV-Multi-Domain (M3D) dataset consisting of both simulation and\nrealistic images. Compared to other existing datasets, the proposed one is more\ncomprehensive in the sense that it covers rich scenes, diverse MAV types, and\nvarious viewing angles. A new benchmark for cross-domain MAV detection is\nproposed based on the proposed dataset. 2) We propose a Noise Suppression\nNetwork (NSN) based on the framework of pseudo-labeling and a large-to-small\ntraining procedure. To reduce the challenging pseudo-label noises, two novel\nmodules are designed in this network. The first is a prior-based curriculum\nlearning module for allocating adaptive thresholds for pseudo labels with\ndifferent difficulties. The second is a masked copy-paste augmentation module\nfor pasting truly-labeled MAVs on unlabeled target images and thus decreasing\npseudo-label noises. 3) Extensive experimental results verify the superior\nperformance of the proposed method compared to the state-of-the-art ones. In\nparticular, it achieves mAP of 46.9%(+5.8%), 50.5%(+3.7%), and 61.5%(+11.3%) on\nthe tasks of simulation-to-real adaptation, cross-scene adaptation, and\ncross-camera adaptation, respectively.",
        "translated": ""
    },
    {
        "title": "Clustering Propagation for Universal Medical Image Segmentation",
        "url": "http://arxiv.org/abs/2403.16646v1",
        "pub_date": "2024-03-25",
        "summary": "Prominent solutions for medical image segmentation are typically tailored for\nautomatic or interactive setups, posing challenges in facilitating progress\nachieved in one task to another.$_{\\!}$ This$_{\\!}$ also$_{\\!}$\nnecessitates$_{\\!}$ separate$_{\\!}$ models for each task, duplicating both\ntraining time and parameters.$_{\\!}$ To$_{\\!}$ address$_{\\!}$ above$_{\\!}$\nissues,$_{\\!}$ we$_{\\!}$ introduce$_{\\!}$ S2VNet,$_{\\!}$ a$_{\\!}$\nuniversal$_{\\!}$ framework$_{\\!}$ that$_{\\!}$ leverages$_{\\!}$\nSlice-to-Volume$_{\\!}$ propagation$_{\\!}$ to$_{\\!}$ unify automatic/interactive\nsegmentation within a single model and one training session. Inspired by\nclustering-based segmentation techniques, S2VNet makes full use of the\nslice-wise structure of volumetric data by initializing cluster centers from\nthe cluster$_{\\!}$ results$_{\\!}$ of$_{\\!}$ previous$_{\\!}$ slice.$_{\\!}$ This\nenables knowledge acquired from prior slices to assist in the segmentation of\nthe current slice, further efficiently bridging the communication between\nremote slices using mere 2D networks. Moreover, such a framework readily\naccommodates interactive segmentation with no architectural change, simply by\ninitializing centroids from user inputs. S2VNet distinguishes itself by swift\ninference speeds and reduced memory consumption compared to prevailing 3D\nsolutions. It can also handle multi-class interactions with each of them\nserving to initialize different centroids. Experiments on three benchmarks\ndemonstrate S2VNet surpasses task-specified solutions on both\nautomatic/interactive setups.",
        "translated": ""
    },
    {
        "title": "Self-Adaptive Reality-Guided Diffusion for Artifact-Free\n  Super-Resolution",
        "url": "http://arxiv.org/abs/2403.16643v1",
        "pub_date": "2024-03-25",
        "summary": "Artifact-free super-resolution (SR) aims to translate low-resolution images\ninto their high-resolution counterparts with a strict integrity of the original\ncontent, eliminating any distortions or synthetic details. While traditional\ndiffusion-based SR techniques have demonstrated remarkable abilities to enhance\nimage detail, they are prone to artifact introduction during iterative\nprocedures. Such artifacts, ranging from trivial noise to unauthentic textures,\ndeviate from the true structure of the source image, thus challenging the\nintegrity of the super-resolution process. In this work, we propose\nSelf-Adaptive Reality-Guided Diffusion (SARGD), a training-free method that\ndelves into the latent space to effectively identify and mitigate the\npropagation of artifacts. Our SARGD begins by using an artifact detector to\nidentify implausible pixels, creating a binary mask that highlights artifacts.\nFollowing this, the Reality Guidance Refinement (RGR) process refines artifacts\nby integrating this mask with realistic latent representations, improving\nalignment with the original image. Nonetheless, initial realistic-latent\nrepresentations from lower-quality images result in over-smoothing in the final\noutput. To address this, we introduce a Self-Adaptive Guidance (SAG) mechanism.\nIt dynamically computes a reality score, enhancing the sharpness of the\nrealistic latent. These alternating mechanisms collectively achieve\nartifact-free super-resolution. Extensive experiments demonstrate the\nsuperiority of our method, delivering detailed artifact-free high-resolution\nimages while reducing sampling steps by 2X. We release our code at\nhttps://github.com/ProAirVerse/Self-Adaptive-Guidance-Diffusion.git.",
        "translated": ""
    },
    {
        "title": "Multi-Scale Texture Loss for CT denoising with GANs",
        "url": "http://arxiv.org/abs/2403.16640v1",
        "pub_date": "2024-03-25",
        "summary": "Generative Adversarial Networks (GANs) have proved as a powerful framework\nfor denoising applications in medical imaging. However, GAN-based denoising\nalgorithms still suffer from limitations in capturing complex relationships\nwithin the images. In this regard, the loss function plays a crucial role in\nguiding the image generation process, encompassing how much a synthetic image\ndiffers from a real image. To grasp highly complex and non-linear textural\nrelationships in the training process, this work presents a loss function that\nleverages the intrinsic multi-scale nature of the Gray-Level-Co-occurrence\nMatrix (GLCM). Although the recent advances in deep learning have demonstrated\nsuperior performance in classification and detection tasks, we hypothesize that\nits information content can be valuable when integrated into GANs' training. To\nthis end, we propose a differentiable implementation of the GLCM suited for\ngradient-based optimization. Our approach also introduces a self-attention\nlayer that dynamically aggregates the multi-scale texture information extracted\nfrom the images. We validate our approach by carrying out extensive experiments\nin the context of low-dose CT denoising, a challenging application that aims to\nenhance the quality of noisy CT scans. We utilize three publicly available\ndatasets, including one simulated and two real datasets. The results are\npromising as compared to other well-established loss functions, being also\nconsistent across three different GAN architectures. The code is available at:\nhttps://github.com/FrancescoDiFeola/DenoTextureLoss",
        "translated": ""
    },
    {
        "title": "AI-Generated Video Detection via Spatio-Temporal Anomaly Learning",
        "url": "http://arxiv.org/abs/2403.16638v1",
        "pub_date": "2024-03-25",
        "summary": "The advancement of generation models has led to the emergence of highly\nrealistic artificial intelligence (AI)-generated videos. Malicious users can\neasily create non-existent videos to spread false information. This letter\nproposes an effective AI-generated video detection (AIGVDet) scheme by\ncapturing the forensic traces with a two-branch spatio-temporal convolutional\nneural network (CNN). Specifically, two ResNet sub-detectors are learned\nseparately for identifying the anomalies in spatical and optical flow domains,\nrespectively. Results of such sub-detectors are fused to further enhance the\ndiscrimination ability. A large-scale generated video dataset (GVD) is\nconstructed as a benchmark for model training and evaluation. Extensive\nexperimental results verify the high generalization and robustness of our\nAIGVDet scheme. Code and dataset will be available at\nhttps://github.com/multimediaFor/AIGVDet.",
        "translated": ""
    },
    {
        "title": "Efficient Video Object Segmentation via Modulated Cross-Attention Memory",
        "url": "http://arxiv.org/abs/2403.17937v1",
        "pub_date": "2024-03-26",
        "summary": "Recently, transformer-based approaches have shown promising results for\nsemi-supervised video object segmentation. However, these approaches typically\nstruggle on long videos due to increased GPU memory demands, as they frequently\nexpand the memory bank every few frames. We propose a transformer-based\napproach, named MAVOS, that introduces an optimized and dynamic long-term\nmodulated cross-attention (MCA) memory to model temporal smoothness without\nrequiring frequent memory expansion. The proposed MCA effectively encodes both\nlocal and global features at various levels of granularity while efficiently\nmaintaining consistent speed regardless of the video length. Extensive\nexperiments on multiple benchmarks, LVOS, Long-Time Video, and DAVIS 2017,\ndemonstrate the effectiveness of our proposed contributions leading to\nreal-time inference and markedly reduced memory demands without any degradation\nin segmentation accuracy on long videos. Compared to the best existing\ntransformer-based approach, our MAVOS increases the speed by 7.6x, while\nsignificantly reducing the GPU memory by 87% with comparable segmentation\nperformance on short and long video datasets. Notably on the LVOS dataset, our\nMAVOS achieves a J&amp;F score of 63.3% while operating at 37 frames per second\n(FPS) on a single V100 GPU. Our code and models will be publicly available at:\nhttps://github.com/Amshaker/MAVOS.",
        "translated": ""
    },
    {
        "title": "ConvoFusion: Multi-Modal Conversational Diffusion for Co-Speech Gesture\n  Synthesis",
        "url": "http://arxiv.org/abs/2403.17936v1",
        "pub_date": "2024-03-26",
        "summary": "Gestures play a key role in human communication. Recent methods for co-speech\ngesture generation, while managing to generate beat-aligned motions, struggle\ngenerating gestures that are semantically aligned with the utterance. Compared\nto beat gestures that align naturally to the audio signal, semantically\ncoherent gestures require modeling the complex interactions between the\nlanguage and human motion, and can be controlled by focusing on certain words.\nTherefore, we present ConvoFusion, a diffusion-based approach for multi-modal\ngesture synthesis, which can not only generate gestures based on multi-modal\nspeech inputs, but can also facilitate controllability in gesture synthesis.\nOur method proposes two guidance objectives that allow the users to modulate\nthe impact of different conditioning modalities (e.g. audio vs text) as well as\nto choose certain words to be emphasized during gesturing. Our method is\nversatile in that it can be trained either for generating monologue gestures or\neven the conversational gestures. To further advance the research on\nmulti-party interactive gestures, the DnD Group Gesture dataset is released,\nwhich contains 6 hours of gesture data showing 5 people interacting with one\nanother. We compare our method with several recent works and demonstrate\neffectiveness of our method on a variety of tasks. We urge the reader to watch\nour supplementary video at our website.",
        "translated": ""
    },
    {
        "title": "OmniVid: A Generative Framework for Universal Video Understanding",
        "url": "http://arxiv.org/abs/2403.17935v1",
        "pub_date": "2024-03-26",
        "summary": "The core of video understanding tasks, such as recognition, captioning, and\ntracking, is to automatically detect objects or actions in a video and analyze\ntheir temporal evolution. Despite sharing a common goal, different tasks often\nrely on distinct model architectures and annotation formats. In contrast,\nnatural language processing benefits from a unified output space, i.e., text\nsequences, which simplifies the training of powerful foundational language\nmodels, such as GPT-3, with extensive training corpora. Inspired by this, we\nseek to unify the output space of video understanding tasks by using languages\nas labels and additionally introducing time and box tokens. In this way, a\nvariety of video tasks could be formulated as video-grounded token generation.\nThis enables us to address various types of video tasks, including\nclassification (such as action recognition), captioning (covering clip\ncaptioning, video question answering, and dense video captioning), and\nlocalization tasks (such as visual object tracking) within a fully shared\nencoder-decoder architecture, following a generative framework. Through\ncomprehensive experiments, we demonstrate such a simple and straightforward\nidea is quite effective and can achieve state-of-the-art or competitive results\non seven video benchmarks, providing a novel perspective for more universal\nvideo understanding. Code is available at https://github.com/wangjk666/OmniVid.",
        "translated": ""
    },
    {
        "title": "AiOS: All-in-One-Stage Expressive Human Pose and Shape Estimation",
        "url": "http://arxiv.org/abs/2403.17934v1",
        "pub_date": "2024-03-26",
        "summary": "Expressive human pose and shape estimation (a.k.a. 3D whole-body mesh\nrecovery) involves the human body, hand, and expression estimation. Most\nexisting methods have tackled this task in a two-stage manner, first detecting\nthe human body part with an off-the-shelf detection model and inferring the\ndifferent human body parts individually. Despite the impressive results\nachieved, these methods suffer from 1) loss of valuable contextual information\nvia cropping, 2) introducing distractions, and 3) lacking inter-association\namong different persons and body parts, inevitably causing performance\ndegradation, especially for crowded scenes. To address these issues, we\nintroduce a novel all-in-one-stage framework, AiOS, for multiple expressive\nhuman pose and shape recovery without an additional human detection step.\nSpecifically, our method is built upon DETR, which treats multi-person\nwhole-body mesh recovery task as a progressive set prediction problem with\nvarious sequential detection. We devise the decoder tokens and extend them to\nour task. Specifically, we first employ a human token to probe a human location\nin the image and encode global features for each instance, which provides a\ncoarse location for the later transformer block. Then, we introduce a\njoint-related token to probe the human joint in the image and encoder a\nfine-grained local feature, which collaborates with the global feature to\nregress the whole-body mesh. This straightforward but effective model\noutperforms previous state-of-the-art methods by a 9% reduction in NMVE on\nAGORA, a 30% reduction in PVE on EHF, a 10% reduction in PVE on ARCTIC, and a\n3% reduction in PVE on EgoBody.",
        "translated": ""
    },
    {
        "title": "SLEDGE: Synthesizing Simulation Environments for Driving Agents with\n  Generative Models",
        "url": "http://arxiv.org/abs/2403.17933v1",
        "pub_date": "2024-03-26",
        "summary": "SLEDGE is the first generative simulator for vehicle motion planning trained\non real-world driving logs. Its core component is a learned model that is able\nto generate agent bounding boxes and lane graphs. The model's outputs serve as\nan initial state for traffic simulation. The unique properties of the entities\nto be generated for SLEDGE, such as their connectivity and variable count per\nscene, render the naive application of most modern generative models to this\ntask non-trivial. Therefore, together with a systematic study of existing lane\ngraph representations, we introduce a novel raster-to-vector autoencoder\n(RVAE). It encodes agents and the lane graph into distinct channels in a\nrasterized latent map. This facilitates both lane-conditioned agent generation\nand combined generation of lanes and agents with a Diffusion Transformer. Using\ngenerated entities in SLEDGE enables greater control over the simulation, e.g.\nupsampling turns or increasing traffic density. Further, SLEDGE can support\n500m long routes, a capability not found in existing data-driven simulators\nlike nuPlan. It presents new challenges for planning algorithms, evidenced by\nfailure rates of over 40% for PDM, the winner of the 2023 nuPlan challenge,\nwhen tested on hard routes and dense traffic generated by our model. Compared\nto nuPlan, SLEDGE requires 500$\\times$ less storage to set up (&lt;4GB), making it\na more accessible option and helping with democratizing future research in this\nfield.",
        "translated": ""
    },
    {
        "title": "Track Everything Everywhere Fast and Robustly",
        "url": "http://arxiv.org/abs/2403.17931v1",
        "pub_date": "2024-03-26",
        "summary": "We propose a novel test-time optimization approach for efficiently and\nrobustly tracking any pixel at any time in a video. The latest state-of-the-art\noptimization-based tracking technique, OmniMotion, requires a prohibitively\nlong optimization time, rendering it impractical for downstream applications.\nOmniMotion is sensitive to the choice of random seeds, leading to unstable\nconvergence. To improve efficiency and robustness, we introduce a novel\ninvertible deformation network, CaDeX++, which factorizes the function\nrepresentation into a local spatial-temporal feature grid and enhances the\nexpressivity of the coupling blocks with non-linear functions. While CaDeX++\nincorporates a stronger geometric bias within its architectural design, it also\ntakes advantage of the inductive bias provided by the vision foundation models.\nOur system utilizes monocular depth estimation to represent scene geometry and\nenhances the objective by incorporating DINOv2 long-term semantics to regulate\nthe optimization process. Our experiments demonstrate a substantial improvement\nin training speed (more than \\textbf{10 times} faster), robustness, and\naccuracy in tracking over the SoTA optimization-based method OmniMotion.",
        "translated": ""
    },
    {
        "title": "Towards Explaining Hypercomplex Neural Networks",
        "url": "http://arxiv.org/abs/2403.17929v1",
        "pub_date": "2024-03-26",
        "summary": "Hypercomplex neural networks are gaining increasing interest in the deep\nlearning community. The attention directed towards hypercomplex models\noriginates from several aspects, spanning from purely theoretical and\nmathematical characteristics to the practical advantage of lightweight models\nover conventional networks, and their unique properties to capture both global\nand local relations. In particular, a branch of these architectures,\nparameterized hypercomplex neural networks (PHNNs), has also gained popularity\ndue to their versatility across a multitude of application domains.\nNonetheless, only few attempts have been made to explain or interpret their\nintricacies. In this paper, we propose inherently interpretable PHNNs and\nquaternion-like networks, thus without the need for any post-hoc method. To\nachieve this, we define a type of cosine-similarity transform within the\nparameterized hypercomplex domain. This PHB-cos transform induces weight\nalignment with relevant input features and allows to reduce the model into a\nsingle linear transform, rendering it directly interpretable. In this work, we\nstart to draw insights into how this unique branch of neural models operates.\nWe observe that hypercomplex networks exhibit a tendency to concentrate on the\nshape around the main object of interest, in addition to the shape of the\nobject itself. We provide a thorough analysis, studying single neurons of\ndifferent layers and comparing them against how real-valued networks learn. The\ncode of the paper is available at https://github.com/ispamm/HxAI.",
        "translated": ""
    },
    {
        "title": "FastCAR: Fast Classification And Regression Multi-Task Learning via Task\n  Consolidation for Modelling a Continuous Property Variable of Object Classes",
        "url": "http://arxiv.org/abs/2403.17926v1",
        "pub_date": "2024-03-26",
        "summary": "FastCAR is a novel task consolidation approach in Multi-Task Learning (MTL)\nfor a classification and a regression task, despite task heterogeneity with\nonly subtle correlation. It addresses object classification and continuous\nproperty variable regression, a crucial use case in science and engineering.\nFastCAR involves a labeling transformation approach that can be used with a\nsingle-task regression network architecture. FastCAR outperforms traditional\nMTL model families, parametrized in the landscape of architecture and loss\nweighting schemes, when learning of both tasks are collectively considered\n(classification accuracy of 99.54%, regression mean absolute percentage error\nof 2.3%). The experiments performed used an Advanced Steel Property dataset\ncontributed by us. The dataset comprises 4536 images of 224x224 pixels,\nannotated with object classes and hardness properties that take continuous\nvalues. With the labeling transformation and single-task regression network\narchitecture, FastCAR achieves reduced latency and time efficiency.",
        "translated": ""
    },
    {
        "title": "AID: Attention Interpolation of Text-to-Image Diffusion",
        "url": "http://arxiv.org/abs/2403.17924v1",
        "pub_date": "2024-03-26",
        "summary": "Conditional diffusion models can create unseen images in various settings,\naiding image interpolation. Interpolation in latent spaces is well-studied, but\ninterpolation with specific conditions like text or poses is less understood.\nSimple approaches, such as linear interpolation in the space of conditions,\noften result in images that lack consistency, smoothness, and fidelity. To that\nend, we introduce a novel training-free technique named Attention Interpolation\nvia Diffusion (AID). Our key contributions include 1) proposing an inner/outer\ninterpolated attention layer; 2) fusing the interpolated attention with\nself-attention to boost fidelity; and 3) applying beta distribution to\nselection to increase smoothness. We also present a variant, Prompt-guided\nAttention Interpolation via Diffusion (PAID), that considers interpolation as a\ncondition-dependent generative process. This method enables the creation of new\nimages with greater consistency, smoothness, and efficiency, and offers control\nover the exact path of interpolation. Our approach demonstrates effectiveness\nfor conceptual and spatial interpolation. Code and demo are available at\nhttps://github.com/QY-H00/attention-interpolation-diffusion.",
        "translated": ""
    },
    {
        "title": "TC4D: Trajectory-Conditioned Text-to-4D Generation",
        "url": "http://arxiv.org/abs/2403.17920v1",
        "pub_date": "2024-03-26",
        "summary": "Recent techniques for text-to-4D generation synthesize dynamic 3D scenes\nusing supervision from pre-trained text-to-video models. However, existing\nrepresentations for motion, such as deformation models or time-dependent neural\nrepresentations, are limited in the amount of motion they can generate-they\ncannot synthesize motion extending far beyond the bounding box used for volume\nrendering. The lack of a more flexible motion model contributes to the gap in\nrealism between 4D generation methods and recent, near-photorealistic video\ngeneration models. Here, we propose TC4D: trajectory-conditioned text-to-4D\ngeneration, which factors motion into global and local components. We represent\nthe global motion of a scene's bounding box using rigid transformation along a\ntrajectory parameterized by a spline. We learn local deformations that conform\nto the global trajectory using supervision from a text-to-video model. Our\napproach enables the synthesis of scenes animated along arbitrary trajectories,\ncompositional scene generation, and significant improvements to the realism and\namount of generated motion, which we evaluate qualitatively and through a user\nstudy. Video results can be viewed on our website:\nhttps://sherwinbahmani.github.io/tc4d.",
        "translated": ""
    },
    {
        "title": "Real Acoustic Fields: An Audio-Visual Room Acoustics Dataset and\n  Benchmark",
        "url": "http://arxiv.org/abs/2403.18821v1",
        "pub_date": "2024-03-27",
        "summary": "We present a new dataset called Real Acoustic Fields (RAF) that captures real\nacoustic room data from multiple modalities. The dataset includes high-quality\nand densely captured room impulse response data paired with multi-view images,\nand precise 6DoF pose tracking data for sound emitters and listeners in the\nrooms. We used this dataset to evaluate existing methods for novel-view\nacoustic synthesis and impulse response generation which previously relied on\nsynthetic data. In our evaluation, we thoroughly assessed existing audio and\naudio-visual models against multiple criteria and proposed settings to enhance\ntheir performance on real-world data. We also conducted experiments to\ninvestigate the impact of incorporating visual data (i.e., images and depth)\ninto neural acoustic field models. Additionally, we demonstrated the\neffectiveness of a simple sim2real approach, where a model is pre-trained with\nsimulated data and fine-tuned with sparse real-world data, resulting in\nsignificant improvements in the few-shot learning approach. RAF is the first\ndataset to provide densely captured room acoustic data, making it an ideal\nresource for researchers working on audio and audio-visual neural acoustic\nfield modeling techniques. Demos and datasets are available on our project\npage: https://facebookresearch.github.io/real-acoustic-fields/",
        "translated": ""
    },
    {
        "title": "MetaCap: Meta-learning Priors from Multi-View Imagery for Sparse-view\n  Human Performance Capture and Rendering",
        "url": "http://arxiv.org/abs/2403.18820v1",
        "pub_date": "2024-03-27",
        "summary": "Faithful human performance capture and free-view rendering from sparse RGB\nobservations is a long-standing problem in Vision and Graphics. The main\nchallenges are the lack of observations and the inherent ambiguities of the\nsetting, e.g. occlusions and depth ambiguity. As a result, radiance fields,\nwhich have shown great promise in capturing high-frequency appearance and\ngeometry details in dense setups, perform poorly when na\\\"ively supervising\nthem on sparse camera views, as the field simply overfits to the sparse-view\ninputs. To address this, we propose MetaCap, a method for efficient and\nhigh-quality geometry recovery and novel view synthesis given very sparse or\neven a single view of the human. Our key idea is to meta-learn the radiance\nfield weights solely from potentially sparse multi-view videos, which can serve\nas a prior when fine-tuning them on sparse imagery depicting the human. This\nprior provides a good network weight initialization, thereby effectively\naddressing ambiguities in sparse-view capture. Due to the articulated structure\nof the human body and motion-induced surface deformations, learning such a\nprior is non-trivial. Therefore, we propose to meta-learn the field weights in\na pose-canonicalized space, which reduces the spatial feature range and makes\nfeature learning more effective. Consequently, one can fine-tune our field\nparameters to quickly generalize to unseen poses, novel illumination conditions\nas well as novel and sparse (even monocular) camera views. For evaluating our\nmethod under different scenarios, we collect a new dataset, WildDynaCap, which\ncontains subjects captured in, both, a dense camera dome and in-the-wild sparse\ncamera rigs, and demonstrate superior results compared to recent\nstate-of-the-art methods on both public and WildDynaCap dataset.",
        "translated": ""
    },
    {
        "title": "Benchmarking Object Detectors with COCO: A New Path Forward",
        "url": "http://arxiv.org/abs/2403.18819v1",
        "pub_date": "2024-03-27",
        "summary": "The Common Objects in Context (COCO) dataset has been instrumental in\nbenchmarking object detectors over the past decade. Like every dataset, COCO\ncontains subtle errors and imperfections stemming from its annotation\nprocedure. With the advent of high-performing models, we ask whether these\nerrors of COCO are hindering its utility in reliably benchmarking further\nprogress. In search for an answer, we inspect thousands of masks from COCO\n(2017 version) and uncover different types of errors such as imprecise mask\nboundaries, non-exhaustively annotated instances, and mislabeled masks. Due to\nthe prevalence of COCO, we choose to correct these errors to maintain\ncontinuity with prior research. We develop COCO-ReM (Refined Masks), a cleaner\nset of annotations with visibly better mask quality than COCO-2017. We evaluate\nfifty object detectors and find that models that predict visually sharper masks\nscore higher on COCO-ReM, affirming that they were being incorrectly penalized\ndue to errors in COCO-2017. Moreover, our models trained using COCO-ReM\nconverge faster and score higher than their larger variants trained using\nCOCO-2017, highlighting the importance of data quality in improving object\ndetectors. With these findings, we advocate using COCO-ReM for future object\ndetection research. Our dataset is available at https://cocorem.xyz",
        "translated": ""
    },
    {
        "title": "ObjectDrop: Bootstrapping Counterfactuals for Photorealistic Object\n  Removal and Insertion",
        "url": "http://arxiv.org/abs/2403.18818v1",
        "pub_date": "2024-03-27",
        "summary": "Diffusion models have revolutionized image editing but often generate images\nthat violate physical laws, particularly the effects of objects on the scene,\ne.g., occlusions, shadows, and reflections. By analyzing the limitations of\nself-supervised approaches, we propose a practical solution centered on a\n\\q{counterfactual} dataset. Our method involves capturing a scene before and\nafter removing a single object, while minimizing other changes. By fine-tuning\na diffusion model on this dataset, we are able to not only remove objects but\nalso their effects on the scene. However, we find that applying this approach\nfor photorealistic object insertion requires an impractically large dataset. To\ntackle this challenge, we propose bootstrap supervision; leveraging our object\nremoval model trained on a small counterfactual dataset, we synthetically\nexpand this dataset considerably. Our approach significantly outperforms prior\nmethods in photorealistic object removal and insertion, particularly at\nmodeling the effects of objects on the scene.",
        "translated": ""
    },
    {
        "title": "Garment3DGen: 3D Garment Stylization and Texture Generation",
        "url": "http://arxiv.org/abs/2403.18816v1",
        "pub_date": "2024-03-27",
        "summary": "We introduce Garment3DGen a new method to synthesize 3D garment assets from a\nbase mesh given a single input image as guidance. Our proposed approach allows\nusers to generate 3D textured clothes based on both real and synthetic images,\nsuch as those generated by text prompts. The generated assets can be directly\ndraped and simulated on human bodies. First, we leverage the recent progress of\nimage to 3D diffusion methods to generate 3D garment geometries. However, since\nthese geometries cannot be utilized directly for downstream tasks, we propose\nto use them as pseudo ground-truth and set up a mesh deformation optimization\nprocedure that deforms a base template mesh to match the generated 3D target.\nSecond, we introduce carefully designed losses that allow the input base mesh\nto freely deform towards the desired target, yet preserve mesh quality and\ntopology such that they can be simulated. Finally, a texture estimation module\ngenerates high-fidelity texture maps that are globally and locally consistent\nand faithfully capture the input guidance, allowing us to render the generated\n3D assets. With Garment3DGen users can generate the textured 3D garment of\ntheir choice without the need of artist intervention. One can provide a textual\nprompt describing the garment they desire to generate a simulation-ready 3D\nasset. We present a plethora of quantitative and qualitative comparisons on\nvarious assets both real and generated and provide use-cases of how one can\ngenerate simulation-ready 3D garments.",
        "translated": ""
    },
    {
        "title": "Mini-Gemini: Mining the Potential of Multi-modality Vision Language\n  Models",
        "url": "http://arxiv.org/abs/2403.18814v1",
        "pub_date": "2024-03-27",
        "summary": "In this work, we introduce Mini-Gemini, a simple and effective framework\nenhancing multi-modality Vision Language Models (VLMs). Despite the\nadvancements in VLMs facilitating basic visual dialog and reasoning, a\nperformance gap persists compared to advanced models like GPT-4 and Gemini. We\ntry to narrow the gap by mining the potential of VLMs for better performance\nand any-to-any workflow from three aspects, i.e., high-resolution visual\ntokens, high-quality data, and VLM-guided generation. To enhance visual tokens,\nwe propose to utilize an additional visual encoder for high-resolution\nrefinement without increasing the visual token count. We further construct a\nhigh-quality dataset that promotes precise image comprehension and\nreasoning-based generation, expanding the operational scope of current VLMs. In\ngeneral, Mini-Gemini further mines the potential of VLMs and empowers current\nframeworks with image understanding, reasoning, and generation simultaneously.\nMini-Gemini supports a series of dense and MoE Large Language Models (LLMs)\nfrom 2B to 34B. It is demonstrated to achieve leading performance in several\nzero-shot benchmarks and even surpasses the developed private models. Code and\nmodels are available at https://github.com/dvlab-research/MiniGemini.",
        "translated": ""
    },
    {
        "title": "Duolando: Follower GPT with Off-Policy Reinforcement Learning for Dance\n  Accompaniment",
        "url": "http://arxiv.org/abs/2403.18811v1",
        "pub_date": "2024-03-27",
        "summary": "We introduce a novel task within the field of 3D dance generation, termed\ndance accompaniment, which necessitates the generation of responsive movements\nfrom a dance partner, the \"follower\", synchronized with the lead dancer's\nmovements and the underlying musical rhythm. Unlike existing solo or group\ndance generation tasks, a duet dance scenario entails a heightened degree of\ninteraction between the two participants, requiring delicate coordination in\nboth pose and position. To support this task, we first build a large-scale and\ndiverse duet interactive dance dataset, DD100, by recording about 117 minutes\nof professional dancers' performances. To address the challenges inherent in\nthis task, we propose a GPT-based model, Duolando, which autoregressively\npredicts the subsequent tokenized motion conditioned on the coordinated\ninformation of the music, the leader's and the follower's movements. To further\nenhance the GPT's capabilities of generating stable results on unseen\nconditions (music and leader motions), we devise an off-policy reinforcement\nlearning strategy that allows the model to explore viable trajectories from\nout-of-distribution samplings, guided by human-defined rewards. Based on the\ncollected dataset and proposed method, we establish a benchmark with several\ncarefully designed metrics.",
        "translated": ""
    },
    {
        "title": "ECoDepth: Effective Conditioning of Diffusion Models for Monocular Depth\n  Estimation",
        "url": "http://arxiv.org/abs/2403.18807v1",
        "pub_date": "2024-03-27",
        "summary": "In the absence of parallax cues, a learning-based single image depth\nestimation (SIDE) model relies heavily on shading and contextual cues in the\nimage. While this simplicity is attractive, it is necessary to train such\nmodels on large and varied datasets, which are difficult to capture. It has\nbeen shown that using embeddings from pre-trained foundational models, such as\nCLIP, improves zero shot transfer in several applications. Taking inspiration\nfrom this, in our paper we explore the use of global image priors generated\nfrom a pre-trained ViT model to provide more detailed contextual information.\nWe argue that the embedding vector from a ViT model, pre-trained on a large\ndataset, captures greater relevant information for SIDE than the usual route of\ngenerating pseudo image captions, followed by CLIP based text embeddings. Based\non this idea, we propose a new SIDE model using a diffusion backbone which is\nconditioned on ViT embeddings. Our proposed design establishes a new\nstate-of-the-art (SOTA) for SIDE on NYUv2 dataset, achieving Abs Rel error of\n0.059(14% improvement) compared to 0.069 by the current SOTA (VPD). And on\nKITTI dataset, achieving Sq Rel error of 0.139 (2% improvement) compared to\n0.142 by the current SOTA (GEDepth). For zero-shot transfer with a model\ntrained on NYUv2, we report mean relative improvement of (20%, 23%, 81%, 25%)\nover NeWCRFs on (Sun-RGBD, iBims1, DIODE, HyperSim) datasets, compared to (16%,\n18%, 45%, 9%) by ZoeDepth. The code is available at\nhttps://github.com/Aradhye2002/EcoDepth.",
        "translated": ""
    },
    {
        "title": "Gamba: Marry Gaussian Splatting with Mamba for single view 3D\n  reconstruction",
        "url": "http://arxiv.org/abs/2403.18795v1",
        "pub_date": "2024-03-27",
        "summary": "We tackle the challenge of efficiently reconstructing a 3D asset from a\nsingle image with growing demands for automated 3D content creation pipelines.\nPrevious methods primarily rely on Score Distillation Sampling (SDS) and Neural\nRadiance Fields (NeRF). Despite their significant success, these approaches\nencounter practical limitations due to lengthy optimization and considerable\nmemory usage. In this report, we introduce Gamba, an end-to-end amortized 3D\nreconstruction model from single-view images, emphasizing two main insights:\n(1) 3D representation: leveraging a large number of 3D Gaussians for an\nefficient 3D Gaussian splatting process; (2) Backbone design: introducing a\nMamba-based sequential network that facilitates context-dependent reasoning and\nlinear scalability with the sequence (token) length, accommodating a\nsubstantial number of Gaussians. Gamba incorporates significant advancements in\ndata preprocessing, regularization design, and training methodologies. We\nassessed Gamba against existing optimization-based and feed-forward 3D\ngeneration approaches using the real-world scanned OmniObject3D dataset. Here,\nGamba demonstrates competitive generation capabilities, both qualitatively and\nquantitatively, while achieving remarkable speed, approximately 0.6 second on a\nsingle NVIDIA A100 GPU.",
        "translated": ""
    },
    {
        "title": "Object Pose Estimation via the Aggregation of Diffusion Features",
        "url": "http://arxiv.org/abs/2403.18791v1",
        "pub_date": "2024-03-27",
        "summary": "Estimating the pose of objects from images is a crucial task of 3D scene\nunderstanding, and recent approaches have shown promising results on very large\nbenchmarks. However, these methods experience a significant performance drop\nwhen dealing with unseen objects. We believe that it results from the limited\ngeneralizability of image features. To address this problem, we have an\nin-depth analysis on the features of diffusion models, e.g. Stable Diffusion,\nwhich hold substantial potential for modeling unseen objects. Based on this\nanalysis, we then innovatively introduce these diffusion features for object\npose estimation. To achieve this, we propose three distinct architectures that\ncan effectively capture and aggregate diffusion features of different\ngranularity, greatly improving the generalizability of object pose estimation.\nOur approach outperforms the state-of-the-art methods by a considerable margin\non three popular benchmark datasets, LM, O-LM, and T-LESS. In particular, our\nmethod achieves higher accuracy than the previous best arts on unseen objects:\n98.2% vs. 93.5% on Unseen LM, 85.9% vs. 76.3% on Unseen O-LM, showing the\nstrong generalizability of our method. Our code is released at\nhttps://github.com/Tianfu18/diff-feats-pose.",
        "translated": ""
    },
    {
        "title": "GaussianCube: Structuring Gaussian Splatting using Optimal Transport for\n  3D Generative Modeling",
        "url": "http://arxiv.org/abs/2403.19655v1",
        "pub_date": "2024-03-28",
        "summary": "3D Gaussian Splatting (GS) have achieved considerable improvement over Neural\nRadiance Fields in terms of 3D fitting fidelity and rendering speed. However,\nthis unstructured representation with scattered Gaussians poses a significant\nchallenge for generative modeling. To address the problem, we introduce\nGaussianCube, a structured GS representation that is both powerful and\nefficient for generative modeling. We achieve this by first proposing a\nmodified densification-constrained GS fitting algorithm which can yield\nhigh-quality fitting results using a fixed number of free Gaussians, and then\nre-arranging the Gaussians into a predefined voxel grid via Optimal Transport.\nThe structured grid representation allows us to use standard 3D U-Net as our\nbackbone in diffusion generative modeling without elaborate designs. Extensive\nexperiments conducted on ShapeNet and OmniObject3D show that our model achieves\nstate-of-the-art generation results both qualitatively and quantitatively,\nunderscoring the potential of GaussianCube as a powerful and versatile 3D\nrepresentation.",
        "translated": ""
    },
    {
        "title": "RSMamba: Remote Sensing Image Classification with State Space Model",
        "url": "http://arxiv.org/abs/2403.19654v1",
        "pub_date": "2024-03-28",
        "summary": "Remote sensing image classification forms the foundation of various\nunderstanding tasks, serving a crucial function in remote sensing image\ninterpretation. The recent advancements of Convolutional Neural Networks (CNNs)\nand Transformers have markedly enhanced classification accuracy. Nonetheless,\nremote sensing scene classification remains a significant challenge, especially\ngiven the complexity and diversity of remote sensing scenarios and the\nvariability of spatiotemporal resolutions. The capacity for whole-image\nunderstanding can provide more precise semantic cues for scene discrimination.\nIn this paper, we introduce RSMamba, a novel architecture for remote sensing\nimage classification. RSMamba is based on the State Space Model (SSM) and\nincorporates an efficient, hardware-aware design known as the Mamba. It\nintegrates the advantages of both a global receptive field and linear modeling\ncomplexity. To overcome the limitation of the vanilla Mamba, which can only\nmodel causal sequences and is not adaptable to two-dimensional image data, we\npropose a dynamic multi-path activation mechanism to augment Mamba's capacity\nto model non-causal data. Notably, RSMamba maintains the inherent modeling\nmechanism of the vanilla Mamba, yet exhibits superior performance across\nmultiple remote sensing image classification datasets. This indicates that\nRSMamba holds significant potential to function as the backbone of future\nvisual foundation models. The code will be available at\n\\url{https://github.com/KyanChen/RSMamba}.",
        "translated": ""
    },
    {
        "title": "Detecting Image Attribution for Text-to-Image Diffusion Models in RGB\n  and Beyond",
        "url": "http://arxiv.org/abs/2403.19653v1",
        "pub_date": "2024-03-28",
        "summary": "Modern text-to-image (T2I) diffusion models can generate images with\nremarkable realism and creativity. These advancements have sparked research in\nfake image detection and attribution, yet prior studies have not fully explored\nthe practical and scientific dimensions of this task. In addition to\nattributing images to 12 state-of-the-art T2I generators, we provide extensive\nanalyses on what inference stage hyperparameters and image modifications are\ndiscernible. Our experiments reveal that initialization seeds are highly\ndetectable, along with other subtle variations in the image generation process\nto some extent. We further investigate what visual traces are leveraged in\nimage attribution by perturbing high-frequency details and employing mid-level\nrepresentations of image style and structure. Notably, altering high-frequency\ninformation causes only slight reductions in accuracy, and training an\nattributor on style representations outperforms training on RGB images. Our\nanalyses underscore that fake images are detectable and attributable at various\nlevels of visual granularity than previously explored.",
        "translated": ""
    },
    {
        "title": "InterDreamer: Zero-Shot Text to 3D Dynamic Human-Object Interaction",
        "url": "http://arxiv.org/abs/2403.19652v1",
        "pub_date": "2024-03-28",
        "summary": "Text-conditioned human motion generation has experienced significant\nadvancements with diffusion models trained on extensive motion capture data and\ncorresponding textual annotations. However, extending such success to 3D\ndynamic human-object interaction (HOI) generation faces notable challenges,\nprimarily due to the lack of large-scale interaction data and comprehensive\ndescriptions that align with these interactions. This paper takes the\ninitiative and showcases the potential of generating human-object interactions\nwithout direct training on text-interaction pair data. Our key insight in\nachieving this is that interaction semantics and dynamics can be decoupled.\nBeing unable to learn interaction semantics through supervised training, we\ninstead leverage pre-trained large models, synergizing knowledge from a large\nlanguage model and a text-to-motion model. While such knowledge offers\nhigh-level control over interaction semantics, it cannot grasp the intricacies\nof low-level interaction dynamics. To overcome this issue, we further introduce\na world model designed to comprehend simple physics, modeling how human actions\ninfluence object motion. By integrating these components, our novel framework,\nInterDreamer, is able to generate text-aligned 3D HOI sequences in a zero-shot\nmanner. We apply InterDreamer to the BEHAVE and CHAIRS datasets, and our\ncomprehensive experimental analysis demonstrates its capability to generate\nrealistic and coherent interaction sequences that seamlessly align with the\ntext directives.",
        "translated": ""
    },
    {
        "title": "MagicLens: Self-Supervised Image Retrieval with Open-Ended Instructions",
        "url": "http://arxiv.org/abs/2403.19651v1",
        "pub_date": "2024-03-28",
        "summary": "Image retrieval, i.e., finding desired images given a reference image,\ninherently encompasses rich, multi-faceted search intents that are difficult to\ncapture solely using image-based measures. Recent work leverages text\ninstructions to allow users to more freely express their search intents.\nHowever, existing work primarily focuses on image pairs that are visually\nsimilar and/or can be characterized by a small set of pre-defined relations.\nThe core thesis of this paper is that text instructions can enable retrieving\nimages with richer relations beyond visual similarity. To show this, we\nintroduce MagicLens, a series of self-supervised image retrieval models that\nsupport open-ended instructions. MagicLens is built on a key novel insight:\nimage pairs that naturally occur on the same web pages contain a wide range of\nimplicit relations (e.g., inside view of), and we can bring those implicit\nrelations explicit by synthesizing instructions via large multimodal models\n(LMMs) and large language models (LLMs). Trained on 36.7M (query image,\ninstruction, target image) triplets with rich semantic relations mined from the\nweb, MagicLens achieves comparable or better results on eight benchmarks of\nvarious image retrieval tasks than prior state-of-the-art (SOTA) methods.\nRemarkably, it outperforms previous SOTA but with a 50X smaller model size on\nmultiple benchmarks. Additional human analyses on a 1.4M-image unseen corpus\nfurther demonstrate the diversity of search intents supported by MagicLens.",
        "translated": ""
    },
    {
        "title": "GraspXL: Generating Grasping Motions for Diverse Objects at Scale",
        "url": "http://arxiv.org/abs/2403.19649v1",
        "pub_date": "2024-03-28",
        "summary": "Human hands possess the dexterity to interact with diverse objects such as\ngrasping specific parts of the objects and/or approaching them from desired\ndirections. More importantly, humans can grasp objects of any shape without\nobject-specific skills. Recent works synthesize grasping motions following\nsingle objectives such as a desired approach heading direction or a grasping\narea. Moreover, they usually rely on expensive 3D hand-object data during\ntraining and inference, which limits their capability to synthesize grasping\nmotions for unseen objects at scale. In this paper, we unify the generation of\nhand-object grasping motions across multiple motion objectives, diverse object\nshapes and dexterous hand morphologies in a policy learning framework GraspXL.\nThe objectives are composed of the graspable area, heading direction during\napproach, wrist rotation, and hand position. Without requiring any 3D\nhand-object interaction data, our policy trained with 58 objects can robustly\nsynthesize diverse grasping motions for more than 500k unseen objects with a\nsuccess rate of 82.2%. At the same time, the policy adheres to objectives,\nwhich enables the generation of diverse grasps per object. Moreover, we show\nthat our framework can be deployed to different dexterous hands and work with\nreconstructed or generated objects. We quantitatively and qualitatively\nevaluate our method to show the efficacy of our approach. Our model and code\nwill be available.",
        "translated": ""
    },
    {
        "title": "Change-Agent: Towards Interactive Comprehensive Change Interpretation\n  and Analysis from Change Detection and Change Captioning",
        "url": "http://arxiv.org/abs/2403.19646v1",
        "pub_date": "2024-03-28",
        "summary": "Monitoring changes in the Earth's surface is crucial for understanding\nnatural processes and human impacts, necessitating precise and comprehensive\ninterpretation methodologies. Remote sensing satellite imagery offers a unique\nperspective for monitoring these changes, leading to the emergence of remote\nsensing image change interpretation (RSICI) as a significant research focus.\nCurrent RSICI technology encompasses change detection and change captioning,\neach with its limitations in providing comprehensive interpretation. To address\nthis, we propose an interactive Change-Agent which integrates a multi-level\nchange interpretation (MCI) model as eyes and a large language model (LLM) as\nthe brain. Our Change-Agent can follow user instructions to achieve\ncomprehensive change interpretation and insightful analysis according to user\ninstructions, such as change detection and change captioning, change object\ncounting, change cause analysis, etc. Our proposed MCI model contains two\nbranches of pixel-level change detection and semantic-level change captioning,\nin which multiple BI-temporal Iterative Interaction (BI3) layers utilize Local\nPerception Enhancement (LPE) and the Global Difference Fusion Attention (GDFA)\nmodules to enhance the model's discriminative feature representation\ncapabilities. To train the MCI model, we build the LEVIR-MCI dataset with\nchange masks and captions of bi-temporal images. Extensive experiments\ndemonstrate the effectiveness of the proposed change interpretation model and\nhighlight the promising potential of our Change-Agent in facilitating\ncomprehensive and intelligent interpretation of surface changes. We will make\nour dataset and codebase of the change interpretation model and Change-Agent\npublicly available to facilitate future research at\nhttps://github.com/Chen-Yang-Liu/Change-Agent",
        "translated": ""
    },
    {
        "title": "GANTASTIC: GAN-based Transfer of Interpretable Directions for\n  Disentangled Image Editing in Text-to-Image Diffusion Models",
        "url": "http://arxiv.org/abs/2403.19645v1",
        "pub_date": "2024-03-28",
        "summary": "The rapid advancement in image generation models has predominantly been\ndriven by diffusion models, which have demonstrated unparalleled success in\ngenerating high-fidelity, diverse images from textual prompts. Despite their\nsuccess, diffusion models encounter substantial challenges in the domain of\nimage editing, particularly in executing disentangled edits-changes that target\nspecific attributes of an image while leaving irrelevant parts untouched. In\ncontrast, Generative Adversarial Networks (GANs) have been recognized for their\nsuccess in disentangled edits through their interpretable latent spaces. We\nintroduce GANTASTIC, a novel framework that takes existing directions from\npre-trained GAN models-representative of specific, controllable attributes-and\ntransfers these directions into diffusion-based models. This novel approach not\nonly maintains the generative quality and diversity that diffusion models are\nknown for but also significantly enhances their capability to perform precise,\ntargeted image edits, thereby leveraging the best of both worlds.",
        "translated": ""
    },
    {
        "title": "Siamese Vision Transformers are Scalable Audio-visual Learners",
        "url": "http://arxiv.org/abs/2403.19638v1",
        "pub_date": "2024-03-28",
        "summary": "Traditional audio-visual methods rely on independent audio and visual\nbackbones, which is costly and not scalable. In this work, we investigate using\nan audio-visual siamese network (AVSiam) for efficient and scalable\naudio-visual pretraining. Our framework uses a single shared vision transformer\nbackbone to process audio and visual inputs, improving its parameter\nefficiency, reducing the GPU memory footprint, and allowing us to scale our\nmethod to larger datasets and model sizes. We pretrain our model using a\ncontrastive audio-visual matching objective with a multi-ratio random masking\nscheme, which enables our model to process larger audio-visual instance\nbatches, helpful for contrastive learning. Unlike prior audio-visual methods,\nour method can robustly handle audio, visual, and audio-visual inputs with a\nsingle shared ViT backbone. Furthermore, despite using the shared backbone for\nboth modalities, AVSiam achieves competitive or even better results than prior\nmethods on AudioSet and VGGSound for audio-visual classification and retrieval.\nOur code is available at https://github.com/GenjiB/AVSiam",
        "translated": ""
    },
    {
        "title": "GauStudio: A Modular Framework for 3D Gaussian Splatting and Beyond",
        "url": "http://arxiv.org/abs/2403.19632v1",
        "pub_date": "2024-03-28",
        "summary": "We present GauStudio, a novel modular framework for modeling 3D Gaussian\nSplatting (3DGS) to provide standardized, plug-and-play components for users to\neasily customize and implement a 3DGS pipeline. Supported by our framework, we\npropose a hybrid Gaussian representation with foreground and skyball background\nmodels. Experiments demonstrate this representation reduces artifacts in\nunbounded outdoor scenes and improves novel view synthesis. Finally, we propose\nGaussian Splatting Surface Reconstruction (GauS), a novel render-then-fuse\napproach for high-fidelity mesh reconstruction from 3DGS inputs without\nfine-tuning. Overall, our GauStudio framework, hybrid representation, and GauS\napproach enhance 3DGS modeling and rendering capabilities, enabling\nhigher-quality novel view synthesis and surface reconstruction.",
        "translated": ""
    },
    {
        "title": "Unsolvable Problem Detection: Evaluating Trustworthiness of Vision\n  Language Models",
        "url": "http://arxiv.org/abs/2403.20331v1",
        "pub_date": "2024-03-29",
        "summary": "This paper introduces a novel and significant challenge for Vision Language\nModels (VLMs), termed Unsolvable Problem Detection (UPD). UPD examines the\nVLM's ability to withhold answers when faced with unsolvable problems in the\ncontext of Visual Question Answering (VQA) tasks. UPD encompasses three\ndistinct settings: Absent Answer Detection (AAD), Incompatible Answer Set\nDetection (IASD), and Incompatible Visual Question Detection (IVQD). To deeply\ninvestigate the UPD problem, extensive experiments indicate that most VLMs,\nincluding GPT-4V and LLaVA-Next-34B, struggle with our benchmarks to varying\nextents, highlighting significant room for the improvements. To address UPD, we\nexplore both training-free and training-based solutions, offering new insights\ninto their effectiveness and limitations. We hope our insights, together with\nfuture efforts within the proposed UPD settings, will enhance the broader\nunderstanding and development of more practical and reliable VLMs.",
        "translated": ""
    },
    {
        "title": "Are We on the Right Way for Evaluating Large Vision-Language Models?",
        "url": "http://arxiv.org/abs/2403.20330v1",
        "pub_date": "2024-03-29",
        "summary": "Large vision-language models (LVLMs) have recently achieved rapid progress,\nsparking numerous studies to evaluate their multi-modal capabilities. However,\nwe dig into current evaluation works and identify two primary issues: 1) Visual\ncontent is unnecessary for many samples. The answers can be directly inferred\nfrom the questions and options, or the world knowledge embedded in LLMs. This\nphenomenon is prevalent across current benchmarks. For instance, GeminiPro\nachieves 42.9% on the MMMU benchmark without any visual input, and outperforms\nthe random choice baseline across six benchmarks over 20% on average. 2)\nUnintentional data leakage exists in LLM and LVLM training. LLM and LVLM could\nstill answer some visual-necessary questions without visual content, indicating\nthe memorizing of these samples within large-scale training data. For example,\nSphinx-X-MoE gets 43.6% on MMMU without accessing images, surpassing its LLM\nbackbone with 17.9%. Both problems lead to misjudgments of actual multi-modal\ngains and potentially misguide the study of LVLM. To this end, we present\nMMStar, an elite vision-indispensable multi-modal benchmark comprising 1,500\nsamples meticulously selected by humans. MMStar benchmarks 6 core capabilities\nand 18 detailed axes, aiming to evaluate LVLMs' multi-modal capacities with\ncarefully balanced and purified samples. These samples are first roughly\nselected from current benchmarks with an automated pipeline, human review is\nthen involved to ensure each curated sample exhibits visual dependency, minimal\ndata leakage, and requires advanced multi-modal capabilities. Moreover, two\nmetrics are developed to measure data leakage and actual performance gain in\nmulti-modal training. We evaluate 16 leading LVLMs on MMStar to assess their\nmulti-modal capabilities, and on 7 benchmarks with the proposed metrics to\ninvestigate their data leakage and actual multi-modal gain.",
        "translated": ""
    },
    {
        "title": "MTLoRA: A Low-Rank Adaptation Approach for Efficient Multi-Task Learning",
        "url": "http://arxiv.org/abs/2403.20320v1",
        "pub_date": "2024-03-29",
        "summary": "Adapting models pre-trained on large-scale datasets to a variety of\ndownstream tasks is a common strategy in deep learning. Consequently,\nparameter-efficient fine-tuning methods have emerged as a promising way to\nadapt pre-trained models to different tasks while training only a minimal\nnumber of parameters. While most of these methods are designed for single-task\nadaptation, parameter-efficient training in Multi-Task Learning (MTL)\narchitectures is still unexplored. In this paper, we introduce MTLoRA, a novel\nframework for parameter-efficient training of MTL models. MTLoRA employs\nTask-Agnostic and Task-Specific Low-Rank Adaptation modules, which effectively\ndisentangle the parameter space in MTL fine-tuning, thereby enabling the model\nto adeptly handle both task specialization and interaction within MTL contexts.\nWe applied MTLoRA to hierarchical-transformer-based MTL architectures, adapting\nthem to multiple downstream dense prediction tasks. Our extensive experiments\non the PASCAL dataset show that MTLoRA achieves higher accuracy on downstream\ntasks compared to fully fine-tuning the MTL model while reducing the number of\ntrainable parameters by 3.6x. Furthermore, MTLoRA establishes a Pareto-optimal\ntrade-off between the number of trainable parameters and the accuracy of the\ndownstream tasks, outperforming current state-of-the-art parameter-efficient\ntraining methods in both accuracy and efficiency. Our code is publicly\navailable.",
        "translated": ""
    },
    {
        "title": "SeaBird: Segmentation in Bird's View with Dice Loss Improves Monocular\n  3D Detection of Large Objects",
        "url": "http://arxiv.org/abs/2403.20318v1",
        "pub_date": "2024-03-29",
        "summary": "Monocular 3D detectors achieve remarkable performance on cars and smaller\nobjects. However, their performance drops on larger objects, leading to fatal\naccidents. Some attribute the failures to training data scarcity or their\nreceptive field requirements of large objects. In this paper, we highlight this\nunderstudied problem of generalization to large objects. We find that modern\nfrontal detectors struggle to generalize to large objects even on nearly\nbalanced datasets. We argue that the cause of failure is the sensitivity of\ndepth regression losses to noise of larger objects. To bridge this gap, we\ncomprehensively investigate regression and dice losses, examining their\nrobustness under varying error levels and object sizes. We mathematically prove\nthat the dice loss leads to superior noise-robustness and model convergence for\nlarge objects compared to regression losses for a simplified case. Leveraging\nour theoretical insights, we propose SeaBird (Segmentation in Bird's View) as\nthe first step towards generalizing to large objects. SeaBird effectively\nintegrates BEV segmentation on foreground objects for 3D detection, with the\nsegmentation head trained with the dice loss. SeaBird achieves SoTA results on\nthe KITTI-360 leaderboard and improves existing detectors on the nuScenes\nleaderboard, particularly for large objects. Code and models at\nhttps://github.com/abhi1kumar/SeaBird",
        "translated": ""
    },
    {
        "title": "Convolutional Prompting meets Language Models for Continual Learning",
        "url": "http://arxiv.org/abs/2403.20317v1",
        "pub_date": "2024-03-29",
        "summary": "Continual Learning (CL) enables machine learning models to learn from\ncontinuously shifting new training data in absence of data from old tasks.\nRecently, pretrained vision transformers combined with prompt tuning have shown\npromise for overcoming catastrophic forgetting in CL. These approaches rely on\na pool of learnable prompts which can be inefficient in sharing knowledge\nacross tasks leading to inferior performance. In addition, the lack of\nfine-grained layer specific prompts does not allow these to fully express the\nstrength of the prompts for CL. We address these limitations by proposing\nConvPrompt, a novel convolutional prompt creation mechanism that maintains\nlayer-wise shared embeddings, enabling both layer-specific learning and better\nconcept transfer across tasks. The intelligent use of convolution enables us to\nmaintain a low parameter overhead without compromising performance. We further\nleverage Large Language Models to generate fine-grained text descriptions of\neach category which are used to get task similarity and dynamically decide the\nnumber of prompts to be learned. Extensive experiments demonstrate the\nsuperiority of ConvPrompt and improves SOTA by ~3% with significantly less\nparameter overhead. We also perform strong ablation over various modules to\ndisentangle the importance of different components.",
        "translated": ""
    },
    {
        "title": "Learn \"No\" to Say \"Yes\" Better: Improving Vision-Language Models via\n  Negations",
        "url": "http://arxiv.org/abs/2403.20312v1",
        "pub_date": "2024-03-29",
        "summary": "Existing vision-language models (VLMs) treat text descriptions as a unit,\nconfusing individual concepts in a prompt and impairing visual semantic\nmatching and reasoning. An important aspect of reasoning in logic and language\nis negations. This paper highlights the limitations of popular VLMs such as\nCLIP, at understanding the implications of negations, i.e., the effect of the\nword \"not\" in a given prompt. To enable evaluation of VLMs on fluent prompts\nwith negations, we present CC-Neg, a dataset containing 228,246 images, true\ncaptions and their corresponding negated captions. Using CC-Neg along with\nmodifications to the contrastive loss of CLIP, our proposed CoN-CLIP framework,\nhas an improved understanding of negations. This training paradigm improves\nCoN-CLIP's ability to encode semantics reliably, resulting in 3.85% average\ngain in top-1 accuracy for zero-shot image classification across 8 datasets.\nFurther, CoN-CLIP outperforms CLIP on challenging compositionality benchmarks\nsuch as SugarCREPE by 4.4%, showcasing emergent compositional understanding of\nobjects, relations, and attributes in text. Overall, our work addresses a\ncrucial limitation of VLMs by introducing a dataset and framework that\nstrengthens semantic associations between images and text, demonstrating\nimproved large-scale foundation models with significantly reduced computational\ncost, promoting efficiency and accessibility.",
        "translated": ""
    },
    {
        "title": "InstantSplat: Unbounded Sparse-view Pose-free Gaussian Splatting in 40\n  Seconds",
        "url": "http://arxiv.org/abs/2403.20309v1",
        "pub_date": "2024-03-29",
        "summary": "While novel view synthesis (NVS) has made substantial progress in 3D computer\nvision, it typically requires an initial estimation of camera intrinsics and\nextrinsics from dense viewpoints. This pre-processing is usually conducted via\na Structure-from-Motion (SfM) pipeline, a procedure that can be slow and\nunreliable, particularly in sparse-view scenarios with insufficient matched\nfeatures for accurate reconstruction. In this work, we integrate the strengths\nof point-based representations (e.g., 3D Gaussian Splatting, 3D-GS) with\nend-to-end dense stereo models (DUSt3R) to tackle the complex yet unresolved\nissues in NVS under unconstrained settings, which encompasses pose-free and\nsparse view challenges. Our framework, InstantSplat, unifies dense stereo\npriors with 3D-GS to build 3D Gaussians of large-scale scenes from sparseview &amp;\npose-free images in less than 1 minute. Specifically, InstantSplat comprises a\nCoarse Geometric Initialization (CGI) module that swiftly establishes a\npreliminary scene structure and camera parameters across all training views,\nutilizing globally-aligned 3D point maps derived from a pre-trained dense\nstereo pipeline. This is followed by the Fast 3D-Gaussian Optimization (F-3DGO)\nmodule, which jointly optimizes the 3D Gaussian attributes and the initialized\nposes with pose regularization. Experiments conducted on the large-scale\noutdoor Tanks &amp; Temples datasets demonstrate that InstantSplat significantly\nimproves SSIM (by 32%) while concurrently reducing Absolute Trajectory Error\n(ATE) by 80%. These establish InstantSplat as a viable solution for scenarios\ninvolving posefree and sparse-view conditions. Project page:\ninstantsplat.github.io.",
        "translated": ""
    },
    {
        "title": "Benchmarking Counterfactual Image Generation",
        "url": "http://arxiv.org/abs/2403.20287v1",
        "pub_date": "2024-03-29",
        "summary": "Counterfactual image generation is pivotal for understanding the causal\nrelations of variables, with applications in interpretability and generation of\nunbiased synthetic data. However, evaluating image generation is a\nlong-standing challenge in itself. The need to evaluate counterfactual\ngeneration compounds on this challenge, precisely because counterfactuals, by\ndefinition, are hypothetical scenarios without observable ground truths. In\nthis paper, we present a novel comprehensive framework aimed at benchmarking\ncounterfactual image generation methods. We incorporate metrics that focus on\nevaluating diverse aspects of counterfactuals, such as composition,\neffectiveness, minimality of interventions, and image realism. We assess the\nperformance of three distinct conditional image generation model types, based\non the Structural Causal Model paradigm. Our work is accompanied by a\nuser-friendly Python package which allows to further evaluate and benchmark\nexisting and future counterfactual image generation methods. Our framework is\nextendable to additional SCM and other causal methods, generative models, and\ndatasets.",
        "translated": ""
    },
    {
        "title": "Snap-it, Tap-it, Splat-it: Tactile-Informed 3D Gaussian Splatting for\n  Reconstructing Challenging Surfaces",
        "url": "http://arxiv.org/abs/2403.20275v1",
        "pub_date": "2024-03-29",
        "summary": "Touch and vision go hand in hand, mutually enhancing our ability to\nunderstand the world. From a research perspective, the problem of mixing touch\nand vision is underexplored and presents interesting challenges. To this end,\nwe propose Tactile-Informed 3DGS, a novel approach that incorporates touch data\n(local depth maps) with multi-view vision data to achieve surface\nreconstruction and novel view synthesis. Our method optimises 3D Gaussian\nprimitives to accurately model the object's geometry at points of contact. By\ncreating a framework that decreases the transmittance at touch locations, we\nachieve a refined surface reconstruction, ensuring a uniformly smooth depth\nmap. Touch is particularly useful when considering non-Lambertian objects (e.g.\nshiny or reflective surfaces) since contemporary methods tend to fail to\nreconstruct with fidelity specular highlights. By combining vision and tactile\nsensing, we achieve more accurate geometry reconstructions with fewer images\nthan prior methods. We conduct evaluation on objects with glossy and reflective\nsurfaces and demonstrate the effectiveness of our approach, offering\nsignificant improvements in reconstruction quality.",
        "translated": ""
    },
    {
        "title": "CATSNet: a context-aware network for Height Estimation in a Forested\n  Area based on Pol-TomoSAR data",
        "url": "http://arxiv.org/abs/2403.20273v1",
        "pub_date": "2024-03-29",
        "summary": "Tropical forests are a key component of the global carbon cycle. With plans\nfor upcoming space-borne missions like BIOMASS to monitor forestry, several\nairborne missions, including TropiSAR and AfriSAR campaigns, have been\nsuccessfully launched and experimented. Typical Synthetic Aperture Radar\nTomography (TomoSAR) methods involve complex models with low accuracy and high\ncomputation costs. In recent years, deep learning methods have also gained\nattention in the TomoSAR framework, showing interesting performance. Recently,\na solution based on a fully connected Tomographic Neural Network (TSNN) has\ndemonstrated its effectiveness in accurately estimating forest and ground\nheights by exploiting the pixel-wise elements of the covariance matrix derived\nfrom TomoSAR data. This work instead goes beyond the pixel-wise approach to\ndefine a context-aware deep learning-based solution named CATSNet. A\nconvolutional neural network is considered to leverage patch-based information\nand extract features from a neighborhood rather than focus on a single pixel.\nThe training is conducted by considering TomoSAR data as the input and Light\nDetection and Ranging (LiDAR) values as the ground truth. The experimental\nresults show striking advantages in both performance and generalization ability\nby leveraging context information within Multiple Baselines (MB) TomoSAR data\nacross different polarimetric modalities, surpassing existing techniques.",
        "translated": ""
    },
    {
        "title": "Segment Any 3D Object with Language",
        "url": "http://arxiv.org/abs/2404.02157v1",
        "pub_date": "2024-04-02",
        "summary": "In this paper, we investigate Open-Vocabulary 3D Instance Segmentation\n(OV-3DIS) with free-form language instructions. Earlier works that rely on only\nannotated base categories for training suffer from limited generalization to\nunseen novel categories. Recent works mitigate poor generalizability to novel\ncategories by generating class-agnostic masks or projecting generalized masks\nfrom 2D to 3D, but disregard semantic or geometry information, leading to\nsub-optimal performance. Instead, generating generalizable but semantic-related\nmasks directly from 3D point clouds would result in superior outcomes. In this\npaper, we introduce Segment any 3D Object with LanguagE (SOLE), which is a\nsemantic and geometric-aware visual-language learning framework with strong\ngeneralizability by generating semantic-related masks directly from 3D point\nclouds. Specifically, we propose a multimodal fusion network to incorporate\nmultimodal semantics in both backbone and decoder. In addition, to align the 3D\nsegmentation model with various language instructions and enhance the mask\nquality, we introduce three types of multimodal associations as supervision.\nOur SOLE outperforms previous methods by a large margin on ScanNetv2,\nScanNet200, and Replica benchmarks, and the results are even close to the\nfully-supervised counterpart despite the absence of class annotations in the\ntraining. Furthermore, extensive qualitative results demonstrate the\nversatility of our SOLE to language instructions.",
        "translated": ""
    },
    {
        "title": "Alpha Invariance: On Inverse Scaling Between Distance and Volume Density\n  in Neural Radiance Fields",
        "url": "http://arxiv.org/abs/2404.02155v1",
        "pub_date": "2024-04-02",
        "summary": "Scale-ambiguity in 3D scene dimensions leads to magnitude-ambiguity of\nvolumetric densities in neural radiance fields, i.e., the densities double when\nscene size is halved, and vice versa. We call this property alpha invariance.\nFor NeRFs to better maintain alpha invariance, we recommend 1) parameterizing\nboth distance and volume densities in log space, and 2) a\ndiscretization-agnostic initialization strategy to guarantee high ray\ntransmittance. We revisit a few popular radiance field models and find that\nthese systems use various heuristics to deal with issues arising from scene\nscaling. We test their behaviors and show our recipe to be more robust.",
        "translated": ""
    },
    {
        "title": "Dynamic Pre-training: Towards Efficient and Scalable All-in-One Image\n  Restoration",
        "url": "http://arxiv.org/abs/2404.02154v1",
        "pub_date": "2024-04-02",
        "summary": "All-in-one image restoration tackles different types of degradations with a\nunified model instead of having task-specific, non-generic models for each\ndegradation. The requirement to tackle multiple degradations using the same\nmodel can lead to high-complexity designs with fixed configuration that lack\nthe adaptability to more efficient alternatives. We propose DyNet, a dynamic\nfamily of networks designed in an encoder-decoder style for all-in-one image\nrestoration tasks. Our DyNet can seamlessly switch between its bulkier and\nlightweight variants, thereby offering flexibility for efficient model\ndeployment with a single round of training. This seamless switching is enabled\nby our weights-sharing mechanism, forming the core of our architecture and\nfacilitating the reuse of initialized module weights. Further, to establish\nrobust weights initialization, we introduce a dynamic pre-training strategy\nthat trains variants of the proposed DyNet concurrently, thereby achieving a\n50% reduction in GPU hours. To tackle the unavailability of large-scale dataset\nrequired in pre-training, we curate a high-quality, high-resolution image\ndataset named Million-IRD having 2M image samples. We validate our DyNet for\nimage denoising, deraining, and dehazing in all-in-one setting, achieving\nstate-of-the-art results with 31.34% reduction in GFlops and a 56.75% reduction\nin parameters compared to baseline models. The source codes and trained models\nare available at https://github.com/akshaydudhane16/DyNet.",
        "translated": ""
    },
    {
        "title": "GeneAvatar: Generic Expression-Aware Volumetric Head Avatar Editing from\n  a Single Image",
        "url": "http://arxiv.org/abs/2404.02152v1",
        "pub_date": "2024-04-02",
        "summary": "Recently, we have witnessed the explosive growth of various volumetric\nrepresentations in modeling animatable head avatars. However, due to the\ndiversity of frameworks, there is no practical method to support high-level\napplications like 3D head avatar editing across different representations. In\nthis paper, we propose a generic avatar editing approach that can be\nuniversally applied to various 3DMM driving volumetric head avatars. To achieve\nthis goal, we design a novel expression-aware modification generative model,\nwhich enables lift 2D editing from a single image to a consistent 3D\nmodification field. To ensure the effectiveness of the generative modification\nprocess, we develop several techniques, including an expression-dependent\nmodification distillation scheme to draw knowledge from the large-scale head\navatar model and 2D facial texture editing tools, implicit latent space\nguidance to enhance model convergence, and a segmentation-based loss reweight\nstrategy for fine-grained texture inversion. Extensive experiments demonstrate\nthat our method delivers high-quality and consistent results across multiple\nexpression and viewpoints. Project page: https://zju3dv.github.io/geneavatar/",
        "translated": ""
    },
    {
        "title": "Diffusion$^2$: Dynamic 3D Content Generation via Score Composition of\n  Orthogonal Diffusion Models",
        "url": "http://arxiv.org/abs/2404.02148v1",
        "pub_date": "2024-04-02",
        "summary": "Recent advancements in 3D generation are predominantly propelled by\nimprovements in 3D-aware image diffusion models which are pretrained on\nInternet-scale image data and fine-tuned on massive 3D data, offering the\ncapability of producing highly consistent multi-view images. However, due to\nthe scarcity of synchronized multi-view video data, it is impractical to adapt\nthis paradigm to 4D generation directly. Despite that, the available video and\n3D data are adequate for training video and multi-view diffusion models that\ncan provide satisfactory dynamic and geometric priors respectively. In this\npaper, we present Diffusion$^2$, a novel framework for dynamic 3D content\ncreation that leverages the knowledge about geometric consistency and temporal\nsmoothness from these models to directly sample dense multi-view and\nmulti-frame images which can be employed to optimize continuous 4D\nrepresentation. Specifically, we design a simple yet effective denoising\nstrategy via score composition of video and multi-view diffusion models based\non the probability structure of the images to be generated. Owing to the high\nparallelism of the image generation and the efficiency of the modern 4D\nreconstruction pipeline, our framework can generate 4D content within few\nminutes. Furthermore, our method circumvents the reliance on 4D data, thereby\nhaving the potential to benefit from the scalability of the foundation video\nand multi-view diffusion models. Extensive experiments demonstrate the efficacy\nof our proposed framework and its capability to flexibly adapt to various types\nof prompts.",
        "translated": ""
    },
    {
        "title": "Iterated Learning Improves Compositionality in Large Vision-Language\n  Models",
        "url": "http://arxiv.org/abs/2404.02145v1",
        "pub_date": "2024-04-02",
        "summary": "A fundamental characteristic common to both human vision and natural language\nis their compositional nature. Yet, despite the performance gains contributed\nby large vision and language pretraining, recent investigations find that\nmost-if not all-our state-of-the-art vision-language models struggle at\ncompositionality. They are unable to distinguish between images of \" a girl in\nwhite facing a man in black\" and \"a girl in black facing a man in white\".\nMoreover, prior work suggests that compositionality doesn't arise with scale:\nlarger model sizes or training data don't help. This paper develops a new\niterated training algorithm that incentivizes compositionality. We draw on\ndecades of cognitive science research that identifies cultural transmission-the\nneed to teach a new generation-as a necessary inductive prior that incentivizes\nhumans to develop compositional languages. Specifically, we reframe\nvision-language contrastive learning as the Lewis Signaling Game between a\nvision agent and a language agent, and operationalize cultural transmission by\niteratively resetting one of the agent's weights during training. After every\niteration, this training paradigm induces representations that become \"easier\nto learn\", a property of compositional languages: e.g. our model trained on\nCC3M and CC12M improves standard CLIP by 4.7%, 4.0% respectfully in the\nSugarCrepe benchmark.",
        "translated": ""
    },
    {
        "title": "ResNet with Integrated Convolutional Block Attention Module for Ship\n  Classification Using Transfer Learning on Optical Satellite Imagery",
        "url": "http://arxiv.org/abs/2404.02135v1",
        "pub_date": "2024-04-02",
        "summary": "This study proposes a novel transfer learning framework for effective ship\nclassification using high-resolution optical remote sensing satellite imagery.\nThe framework is based on the deep convolutional neural network model ResNet50\nand incorporates the Convolutional Block Attention Module (CBAM) to enhance\nperformance. CBAM enables the model to attend to salient features in the\nimages, allowing it to better discriminate between subtle differences between\nships and backgrounds. Furthermore, this study adopts a transfer learning\napproach tailored for accurately classifying diverse types of ships by\nfine-tuning a pre-trained model for the specific task. Experimental results\ndemonstrate the efficacy of the proposed framework in ship classification using\noptical remote sensing imagery, achieving a high classification accuracy of 94%\nacross 5 classes, outperforming existing methods. This research holds potential\napplications in maritime surveillance and management, illegal fishing\ndetection, and maritime traffic monitoring.",
        "translated": ""
    },
    {
        "title": "ViTamin: Designing Scalable Vision Models in the Vision-Language Era",
        "url": "http://arxiv.org/abs/2404.02132v1",
        "pub_date": "2024-04-02",
        "summary": "Recent breakthroughs in vision-language models (VLMs) start a new page in the\nvision community. The VLMs provide stronger and more generalizable feature\nembeddings compared to those from ImageNet-pretrained models, thanks to the\ntraining on the large-scale Internet image-text pairs. However, despite the\namazing achievement from the VLMs, vanilla Vision Transformers (ViTs) remain\nthe default choice for the image encoder. Although pure transformer proves its\neffectiveness in the text encoding area, it remains questionable whether it is\nalso the case for image encoding, especially considering that various types of\nnetworks are proposed on the ImageNet benchmark, which, unfortunately, are\nrarely studied in VLMs. Due to small data/model scale, the original conclusions\nof model design on ImageNet can be limited and biased. In this paper, we aim at\nbuilding an evaluation protocol of vision models in the vision-language era\nunder the contrastive language-image pretraining (CLIP) framework. We provide a\ncomprehensive way to benchmark different vision models, covering their\nzero-shot performance and scalability in both model and training data sizes. To\nthis end, we introduce ViTamin, a new vision models tailored for VLMs.\nViTamin-L significantly outperforms ViT-L by 2.0% ImageNet zero-shot accuracy,\nwhen using the same publicly available DataComp-1B dataset and the same\nOpenCLIP training scheme. ViTamin-L presents promising results on 60 diverse\nbenchmarks, including classification, retrieval, open-vocabulary detection and\nsegmentation, and large multi-modal models. When further scaling up the model\nsize, our ViTamin-XL with only 436M parameters attains 82.9% ImageNet zero-shot\naccuracy, surpassing 82.0% achieved by EVA-E that has ten times more parameters\n(4.4B).",
        "translated": ""
    },
    {
        "title": "3D Congealing: 3D-Aware Image Alignment in the Wild",
        "url": "http://arxiv.org/abs/2404.02125v1",
        "pub_date": "2024-04-02",
        "summary": "We propose 3D Congealing, a novel problem of 3D-aware alignment for 2D images\ncapturing semantically similar objects. Given a collection of unlabeled\nInternet images, our goal is to associate the shared semantic parts from the\ninputs and aggregate the knowledge from 2D images to a shared 3D canonical\nspace. We introduce a general framework that tackles the task without assuming\nshape templates, poses, or any camera parameters. At its core is a canonical 3D\nrepresentation that encapsulates geometric and semantic information. The\nframework optimizes for the canonical representation together with the pose for\neach input image, and a per-image coordinate map that warps 2D pixel\ncoordinates to the 3D canonical frame to account for the shape matching. The\noptimization procedure fuses prior knowledge from a pre-trained image\ngenerative model and semantic information from input images. The former\nprovides strong knowledge guidance for this under-constraint task, while the\nlatter provides the necessary information to mitigate the training data bias\nfrom the pre-trained model. Our framework can be used for various tasks such as\ncorrespondence matching, pose estimation, and image editing, achieving strong\nresults on real-world image datasets under challenging illumination conditions\nand on in-the-wild online image collections.",
        "translated": ""
    },
    {
        "title": "Pre-trained Vision and Language Transformers Are Few-Shot Incremental\n  Learners",
        "url": "http://arxiv.org/abs/2404.02117v1",
        "pub_date": "2024-04-02",
        "summary": "Few-Shot Class Incremental Learning (FSCIL) is a task that requires a model\nto learn new classes incrementally without forgetting when only a few samples\nfor each class are given. FSCIL encounters two significant challenges:\ncatastrophic forgetting and overfitting, and these challenges have driven prior\nstudies to primarily rely on shallow models, such as ResNet-18. Even though\ntheir limited capacity can mitigate both forgetting and overfitting issues, it\nleads to inadequate knowledge transfer during few-shot incremental sessions. In\nthis paper, we argue that large models such as vision and language transformers\npre-trained on large datasets can be excellent few-shot incremental learners.\nTo this end, we propose a novel FSCIL framework called PriViLege, Pre-trained\nVision and Language transformers with prompting functions and knowledge\ndistillation. Our framework effectively addresses the challenges of\ncatastrophic forgetting and overfitting in large models through new pre-trained\nknowledge tuning (PKT) and two losses: entropy-based divergence loss and\nsemantic knowledge distillation loss. Experimental results show that the\nproposed PriViLege significantly outperforms the existing state-of-the-art\nmethods with a large margin, e.g., +9.38% in CUB200, +20.58% in CIFAR-100, and\n+13.36% in miniImageNet. Our implementation code is available at\nhttps://github.com/KHU-AGI/PriViLege.",
        "translated": ""
    },
    {
        "title": "Visual Autoregressive Modeling: Scalable Image Generation via Next-Scale\n  Prediction",
        "url": "http://arxiv.org/abs/2404.02905v1",
        "pub_date": "2024-04-03",
        "summary": "We present Visual AutoRegressive modeling (VAR), a new generation paradigm\nthat redefines the autoregressive learning on images as coarse-to-fine\n\"next-scale prediction\" or \"next-resolution prediction\", diverging from the\nstandard raster-scan \"next-token prediction\". This simple, intuitive\nmethodology allows autoregressive (AR) transformers to learn visual\ndistributions fast and generalize well: VAR, for the first time, makes AR\nmodels surpass diffusion transformers in image generation. On ImageNet 256x256\nbenchmark, VAR significantly improve AR baseline by improving Frechet inception\ndistance (FID) from 18.65 to 1.80, inception score (IS) from 80.4 to 356.4,\nwith around 20x faster inference speed. It is also empirically verified that\nVAR outperforms the Diffusion Transformer (DiT) in multiple dimensions\nincluding image quality, inference speed, data efficiency, and scalability.\nScaling up VAR models exhibits clear power-law scaling laws similar to those\nobserved in LLMs, with linear correlation coefficients near -0.998 as solid\nevidence. VAR further showcases zero-shot generalization ability in downstream\ntasks including image in-painting, out-painting, and editing. These results\nsuggest VAR has initially emulated the two important properties of LLMs:\nScaling Laws and zero-shot task generalization. We have released all models and\ncodes to promote the exploration of AR/VAR models for visual generation and\nunified learning.",
        "translated": ""
    },
    {
        "title": "ALOHa: A New Measure for Hallucination in Captioning Models",
        "url": "http://arxiv.org/abs/2404.02904v1",
        "pub_date": "2024-04-03",
        "summary": "Despite recent advances in multimodal pre-training for visual description,\nstate-of-the-art models still produce captions containing errors, such as\nhallucinating objects not present in a scene. The existing prominent metric for\nobject hallucination, CHAIR, is limited to a fixed set of MS COCO objects and\nsynonyms. In this work, we propose a modernized open-vocabulary metric, ALOHa,\nwhich leverages large language models (LLMs) to measure object hallucinations.\nSpecifically, we use an LLM to extract groundable objects from a candidate\ncaption, measure their semantic similarity to reference objects from captions\nand object detections, and use Hungarian matching to produce a final\nhallucination score. We show that ALOHa correctly identifies 13.6% more\nhallucinated objects than CHAIR on HAT, a new gold-standard subset of MS COCO\nCaptions annotated for hallucinations, and 30.8% more on nocaps, where objects\nextend beyond MS COCO categories. Our code is available at\nhttps://davidmchan.github.io/aloha/.",
        "translated": ""
    },
    {
        "title": "LidarDM: Generative LiDAR Simulation in a Generated World",
        "url": "http://arxiv.org/abs/2404.02903v1",
        "pub_date": "2024-04-03",
        "summary": "We present LidarDM, a novel LiDAR generative model capable of producing\nrealistic, layout-aware, physically plausible, and temporally coherent LiDAR\nvideos. LidarDM stands out with two unprecedented capabilities in LiDAR\ngenerative modeling: (i) LiDAR generation guided by driving scenarios, offering\nsignificant potential for autonomous driving simulations, and (ii) 4D LiDAR\npoint cloud generation, enabling the creation of realistic and temporally\ncoherent sequences. At the heart of our model is a novel integrated 4D world\ngeneration framework. Specifically, we employ latent diffusion models to\ngenerate the 3D scene, combine it with dynamic actors to form the underlying 4D\nworld, and subsequently produce realistic sensory observations within this\nvirtual environment. Our experiments indicate that our approach outperforms\ncompeting algorithms in realism, temporal coherency, and layout consistency. We\nadditionally show that LidarDM can be used as a generative world model\nsimulator for training and testing perception models.",
        "translated": ""
    },
    {
        "title": "DeiT-LT Distillation Strikes Back for Vision Transformer Training on\n  Long-Tailed Datasets",
        "url": "http://arxiv.org/abs/2404.02900v1",
        "pub_date": "2024-04-03",
        "summary": "Vision Transformer (ViT) has emerged as a prominent architecture for various\ncomputer vision tasks. In ViT, we divide the input image into patch tokens and\nprocess them through a stack of self attention blocks. However, unlike\nConvolutional Neural Networks (CNN), ViTs simple architecture has no\ninformative inductive bias (e.g., locality,etc. ). Due to this, ViT requires a\nlarge amount of data for pre-training. Various data efficient approaches (DeiT)\nhave been proposed to train ViT on balanced datasets effectively. However,\nlimited literature discusses the use of ViT for datasets with long-tailed\nimbalances. In this work, we introduce DeiT-LT to tackle the problem of\ntraining ViTs from scratch on long-tailed datasets. In DeiT-LT, we introduce an\nefficient and effective way of distillation from CNN via distillation DIST\ntoken by using out-of-distribution images and re-weighting the distillation\nloss to enhance focus on tail classes. This leads to the learning of local\nCNN-like features in early ViT blocks, improving generalization for tail\nclasses. Further, to mitigate overfitting, we propose distilling from a flat\nCNN teacher, which leads to learning low-rank generalizable features for DIST\ntokens across all ViT blocks. With the proposed DeiT-LT scheme, the\ndistillation DIST token becomes an expert on the tail classes, and the\nclassifier CLS token becomes an expert on the head classes. The experts help to\neffectively learn features corresponding to both the majority and minority\nclasses using a distinct set of tokens within the same ViT architecture. We\nshow the effectiveness of DeiT-LT for training ViT from scratch on datasets\nranging from small-scale CIFAR-10 LT to large-scale iNaturalist-2018.",
        "translated": ""
    },
    {
        "title": "MatAtlas: Text-driven Consistent Geometry Texturing and Material\n  Assignment",
        "url": "http://arxiv.org/abs/2404.02899v1",
        "pub_date": "2024-04-03",
        "summary": "We present MatAtlas, a method for consistent text-guided 3D model texturing.\nFollowing recent progress we leverage a large scale text-to-image generation\nmodel (e.g., Stable Diffusion) as a prior to texture a 3D model. We carefully\ndesign an RGB texturing pipeline that leverages a grid pattern diffusion,\ndriven by depth and edges. By proposing a multi-step texture refinement\nprocess, we significantly improve the quality and 3D consistency of the\ntexturing output. To further address the problem of baked-in lighting, we move\nbeyond RGB colors and pursue assigning parametric materials to the assets.\nGiven the high-quality initial RGB texture, we propose a novel material\nretrieval method capitalized on Large Language Models (LLM), enabling\neditabiliy and relightability. We evaluate our method on a wide variety of\ngeometries and show that our method significantly outperform prior arts. We\nalso analyze the role of each component through a detailed ablation study.",
        "translated": ""
    },
    {
        "title": "Deep Image Composition Meets Image Forgery",
        "url": "http://arxiv.org/abs/2404.02897v1",
        "pub_date": "2024-04-03",
        "summary": "Image forgery is a topic that has been studied for many years. Before the\nbreakthrough of deep learning, forged images were detected using handcrafted\nfeatures that did not require training. These traditional methods failed to\nperform satisfactorily even on datasets much worse in quality than real-life\nimage manipulations. Advances in deep learning have impacted image forgery\ndetection as much as they have impacted other areas of computer vision and have\nimproved the state of the art. Deep learning models require large amounts of\nlabeled data for training. In the case of image forgery, labeled data at the\npixel level is a very important factor for the models to learn. None of the\nexisting datasets have sufficient size, realism and pixel-level labeling at the\nsame time. This is due to the high cost of producing and labeling quality\nimages. It can take hours for an image editing expert to manipulate just one\nimage. To bridge this gap, we automate data generation using image composition\ntechniques that are very related to image forgery. Unlike other automated data\ngeneration frameworks, we use state of the art image composition deep learning\nmodels to generate spliced images close to the quality of real-life\nmanipulations. Finally, we test the generated dataset on the SOTA image\nmanipulation detection model and show that its prediction performance is lower\ncompared to existing datasets, i.e. we produce realistic images that are more\ndifficult to detect. Dataset will be available at\nhttps://github.com/99eren99/DIS25k .",
        "translated": ""
    },
    {
        "title": "Steganographic Passport: An Owner and User Verifiable Credential for\n  Deep Model IP Protection Without Retraining",
        "url": "http://arxiv.org/abs/2404.02889v1",
        "pub_date": "2024-04-03",
        "summary": "Ensuring the legal usage of deep models is crucial to promoting trustable,\naccountable, and responsible artificial intelligence innovation. Current\npassport-based methods that obfuscate model functionality for license-to-use\nand ownership verifications suffer from capacity and quality constraints, as\nthey require retraining the owner model for new users. They are also vulnerable\nto advanced Expanded Residual Block ambiguity attacks. We propose\nSteganographic Passport, which uses an invertible steganographic network to\ndecouple license-to-use from ownership verification by hiding the user's\nidentity images into the owner-side passport and recovering them from their\nrespective user-side passports. An irreversible and collision-resistant hash\nfunction is used to avoid exposing the owner-side passport from the derived\nuser-side passports and increase the uniqueness of the model signature. To\nsafeguard both the passport and model's weights against advanced ambiguity\nattacks, an activation-level obfuscation is proposed for the verification\nbranch of the owner's model. By jointly training the verification and\ndeployment branches, their weights become tightly coupled. The proposed method\nsupports agile licensing of deep models by providing a strong ownership proof\nand license accountability without requiring a separate model retraining for\nthe admission of every new user. Experiment results show that our\nSteganographic Passport outperforms other passport-based deep model protection\nmethods in robustness against various known attacks.",
        "translated": ""
    },
    {
        "title": "PoCo: Point Context Cluster for RGBD Indoor Place Recognition",
        "url": "http://arxiv.org/abs/2404.02885v1",
        "pub_date": "2024-04-03",
        "summary": "We present a novel end-to-end algorithm (PoCo) for the indoor RGB-D place\nrecognition task, aimed at identifying the most likely match for a given query\nframe within a reference database. The task presents inherent challenges\nattributed to the constrained field of view and limited range of perception\nsensors. We propose a new network architecture, which generalizes the recent\nContext of Clusters (CoCs) to extract global descriptors directly from the\nnoisy point clouds through end-to-end learning. Moreover, we develop the\narchitecture by integrating both color and geometric modalities into the point\nfeatures to enhance the global descriptor representation. We conducted\nevaluations on public datasets ScanNet-PR and ARKit with 807 and 5047\nscenarios, respectively. PoCo achieves SOTA performance: on ScanNet-PR, we\nachieve R@1 of 64.63%, a 5.7% improvement from the best-published result CGis\n(61.12%); on Arkit, we achieve R@1 of 45.12%, a 13.3% improvement from the\nbest-published result CGis (39.82%). In addition, PoCo shows higher efficiency\nthan CGis in inference time (1.75X-faster), and we demonstrate the\neffectiveness of PoCo in recognizing places within a real-world laboratory\nenvironment.",
        "translated": ""
    },
    {
        "title": "On the Scalability of Diffusion-based Text-to-Image Generation",
        "url": "http://arxiv.org/abs/2404.02883v1",
        "pub_date": "2024-04-03",
        "summary": "Scaling up model and data size has been quite successful for the evolution of\nLLMs. However, the scaling law for the diffusion based text-to-image (T2I)\nmodels is not fully explored. It is also unclear how to efficiently scale the\nmodel for better performance at reduced cost. The different training settings\nand expensive training cost make a fair model comparison extremely difficult.\nIn this work, we empirically study the scaling properties of diffusion based\nT2I models by performing extensive and rigours ablations on scaling both\ndenoising backbones and training set, including training scaled UNet and\nTransformer variants ranging from 0.4B to 4B parameters on datasets upto 600M\nimages. For model scaling, we find the location and amount of cross attention\ndistinguishes the performance of existing UNet designs. And increasing the\ntransformer blocks is more parameter-efficient for improving text-image\nalignment than increasing channel numbers. We then identify an efficient UNet\nvariant, which is 45% smaller and 28% faster than SDXL's UNet. On the data\nscaling side, we show the quality and diversity of the training set matters\nmore than simply dataset size. Increasing caption density and diversity\nimproves text-image alignment performance and the learning efficiency. Finally,\nwe provide scaling functions to predict the text-image alignment performance as\nfunctions of the scale of model size, compute and dataset size.",
        "translated": ""
    },
    {
        "title": "FlightScope: A Deep Comprehensive Assessment of Aircraft Detection\n  Algorithms in Satellite Imagery",
        "url": "http://arxiv.org/abs/2404.02877v1",
        "pub_date": "2024-04-03",
        "summary": "Object detection in remotely sensed satellite pictures is fundamental in many\nfields such as biophysical, and environmental monitoring. While deep learning\nalgorithms are constantly evolving, they have been mostly implemented and\ntested on popular ground-based taken photos. This paper critically evaluates\nand compares a suite of advanced object detection algorithms customized for the\ntask of identifying aircraft within satellite imagery. Using the large\nHRPlanesV2 dataset, together with a rigorous validation with the GDIT dataset,\nthis research encompasses an array of methodologies including YOLO versions 5\nand 8, Faster RCNN, CenterNet, RetinaNet, RTMDet, and DETR, all trained from\nscratch. This exhaustive training and validation study reveal YOLOv5 as the\npreeminent model for the specific case of identifying airplanes from remote\nsensing data, showcasing high precision and adaptability across diverse imaging\nconditions. This research highlight the nuanced performance landscapes of these\nalgorithms, with YOLOv5 emerging as a robust solution for aerial object\ndetection, underlining its importance through superior mean average precision,\nRecall, and Intersection over Union scores. The findings described here\nunderscore the fundamental role of algorithm selection aligned with the\nspecific demands of satellite imagery analysis and extend a comprehensive\nframework to evaluate model efficacy. The benchmark toolkit and codes,\navailable via https://github.com/toelt-llc/FlightScope_Bench, aims to further\nexploration and innovation in the realm of remote sensing object detection,\npaving the way for improved analytical methodologies in satellite imagery\napplications.",
        "translated": ""
    },
    {
        "title": "Know Your Neighbors: Improving Single-View Reconstruction via Spatial\n  Vision-Language Reasoning",
        "url": "http://arxiv.org/abs/2404.03658v1",
        "pub_date": "2024-04-04",
        "summary": "Recovering the 3D scene geometry from a single view is a fundamental yet\nill-posed problem in computer vision. While classical depth estimation methods\ninfer only a 2.5D scene representation limited to the image plane, recent\napproaches based on radiance fields reconstruct a full 3D representation.\nHowever, these methods still struggle with occluded regions since inferring\ngeometry without visual observation requires (i) semantic knowledge of the\nsurroundings, and (ii) reasoning about spatial context. We propose KYN, a novel\nmethod for single-view scene reconstruction that reasons about semantic and\nspatial context to predict each point's density. We introduce a vision-language\nmodulation module to enrich point features with fine-grained semantic\ninformation. We aggregate point representations across the scene through a\nlanguage-guided spatial attention mechanism to yield per-point density\npredictions aware of the 3D semantic context. We show that KYN improves 3D\nshape recovery compared to predicting density for each 3D point in isolation.\nWe achieve state-of-the-art results in scene and object reconstruction on\nKITTI-360, and show improved zero-shot generalization compared to prior work.\nProject page: https://ruili3.github.io/kyn.",
        "translated": ""
    },
    {
        "title": "OW-VISCap: Open-World Video Instance Segmentation and Captioning",
        "url": "http://arxiv.org/abs/2404.03657v1",
        "pub_date": "2024-04-04",
        "summary": "Open-world video instance segmentation is an important video understanding\ntask. Yet most methods either operate in a closed-world setting, require an\nadditional user-input, or use classic region-based proposals to identify never\nbefore seen objects. Further, these methods only assign a one-word label to\ndetected objects, and don't generate rich object-centric descriptions. They\nalso often suffer from highly overlapping predictions. To address these issues,\nwe propose Open-World Video Instance Segmentation and Captioning (OW-VISCap),\nan approach to jointly segment, track, and caption previously seen or unseen\nobjects in a video. For this, we introduce open-world object queries to\ndiscover never before seen objects without additional user-input. We generate\nrich and descriptive object-centric captions for each detected object via a\nmasked attention augmented LLM input. We introduce an inter-query contrastive\nloss to ensure that the object queries differ from one another. Our generalized\napproach matches or surpasses state-of-the-art on three tasks: open-world video\ninstance segmentation on the BURST dataset, dense video object captioning on\nthe VidSTG dataset, and closed-world video instance segmentation on the OVIS\ndataset.",
        "translated": ""
    },
    {
        "title": "MVD-Fusion: Single-view 3D via Depth-consistent Multi-view Generation",
        "url": "http://arxiv.org/abs/2404.03656v1",
        "pub_date": "2024-04-04",
        "summary": "We present MVD-Fusion: a method for single-view 3D inference via generative\nmodeling of multi-view-consistent RGB-D images. While recent methods pursuing\n3D inference advocate learning novel-view generative models, these generations\nare not 3D-consistent and require a distillation process to generate a 3D\noutput. We instead cast the task of 3D inference as directly generating\nmutually-consistent multiple views and build on the insight that additionally\ninferring depth can provide a mechanism for enforcing this consistency.\nSpecifically, we train a denoising diffusion model to generate multi-view RGB-D\nimages given a single RGB input image and leverage the (intermediate noisy)\ndepth estimates to obtain reprojection-based conditioning to maintain\nmulti-view consistency. We train our model using large-scale synthetic dataset\nObajverse as well as the real-world CO3D dataset comprising of generic camera\nviewpoints. We demonstrate that our approach can yield more accurate synthesis\ncompared to recent state-of-the-art, including distillation-based 3D inference\nand prior multi-view generation methods. We also evaluate the geometry induced\nby our multi-view depth prediction and find that it yields a more accurate\nrepresentation than other direct 3D inference approaches.",
        "translated": ""
    },
    {
        "title": "RaFE: Generative Radiance Fields Restoration",
        "url": "http://arxiv.org/abs/2404.03654v1",
        "pub_date": "2024-04-04",
        "summary": "NeRF (Neural Radiance Fields) has demonstrated tremendous potential in novel\nview synthesis and 3D reconstruction, but its performance is sensitive to input\nimage quality, which struggles to achieve high-fidelity rendering when provided\nwith low-quality sparse input viewpoints. Previous methods for NeRF restoration\nare tailored for specific degradation type, ignoring the generality of\nrestoration. To overcome this limitation, we propose a generic radiance fields\nrestoration pipeline, named RaFE, which applies to various types of\ndegradations, such as low resolution, blurriness, noise, compression artifacts,\nor their combinations. Our approach leverages the success of off-the-shelf 2D\nrestoration methods to recover the multi-view images individually. Instead of\nreconstructing a blurred NeRF by averaging inconsistencies, we introduce a\nnovel approach using Generative Adversarial Networks (GANs) for NeRF generation\nto better accommodate the geometric and appearance inconsistencies present in\nthe multi-view images. Specifically, we adopt a two-level tri-plane\narchitecture, where the coarse level remains fixed to represent the low-quality\nNeRF, and a fine-level residual tri-plane to be added to the coarse level is\nmodeled as a distribution with GAN to capture potential variations in\nrestoration. We validate RaFE on both synthetic and real cases for various\nrestoration tasks, demonstrating superior performance in both quantitative and\nqualitative evaluations, surpassing other 3D restoration methods specific to\nsingle task. Please see our project website\nhttps://zkaiwu.github.io/RaFE-Project/.",
        "translated": ""
    },
    {
        "title": "CoMat: Aligning Text-to-Image Diffusion Model with Image-to-Text Concept\n  Matching",
        "url": "http://arxiv.org/abs/2404.03653v1",
        "pub_date": "2024-04-04",
        "summary": "Diffusion models have demonstrated great success in the field of\ntext-to-image generation. However, alleviating the misalignment between the\ntext prompts and images is still challenging. The root reason behind the\nmisalignment has not been extensively investigated. We observe that the\nmisalignment is caused by inadequate token attention activation. We further\nattribute this phenomenon to the diffusion model's insufficient condition\nutilization, which is caused by its training paradigm. To address the issue, we\npropose CoMat, an end-to-end diffusion model fine-tuning strategy with an\nimage-to-text concept matching mechanism. We leverage an image captioning model\nto measure image-to-text alignment and guide the diffusion model to revisit\nignored tokens. A novel attribute concentration module is also proposed to\naddress the attribute binding problem. Without any image or human preference\ndata, we use only 20K text prompts to fine-tune SDXL to obtain CoMat-SDXL.\nExtensive experiments show that CoMat-SDXL significantly outperforms the\nbaseline model SDXL in two text-to-image alignment benchmarks and achieves\nstart-of-the-art performance.",
        "translated": ""
    },
    {
        "title": "The More You See in 2D, the More You Perceive in 3D",
        "url": "http://arxiv.org/abs/2404.03652v1",
        "pub_date": "2024-04-04",
        "summary": "Humans can infer 3D structure from 2D images of an object based on past\nexperience and improve their 3D understanding as they see more images. Inspired\nby this behavior, we introduce SAP3D, a system for 3D reconstruction and novel\nview synthesis from an arbitrary number of unposed images. Given a few unposed\nimages of an object, we adapt a pre-trained view-conditioned diffusion model\ntogether with the camera poses of the images via test-time fine-tuning. The\nadapted diffusion model and the obtained camera poses are then utilized as\ninstance-specific priors for 3D reconstruction and novel view synthesis. We\nshow that as the number of input images increases, the performance of our\napproach improves, bridging the gap between optimization-based prior-less 3D\nreconstruction methods and single-image-to-3D diffusion-based methods. We\ndemonstrate our system on real images as well as standard synthetic benchmarks.\nOur ablation studies confirm that this adaption behavior is key for more\naccurate 3D understanding.",
        "translated": ""
    },
    {
        "title": "OpenNeRF: Open Set 3D Neural Scene Segmentation with Pixel-Wise Features\n  and Rendered Novel Views",
        "url": "http://arxiv.org/abs/2404.03650v1",
        "pub_date": "2024-04-04",
        "summary": "Large visual-language models (VLMs), like CLIP, enable open-set image\nsegmentation to segment arbitrary concepts from an image in a zero-shot manner.\nThis goes beyond the traditional closed-set assumption, i.e., where models can\nonly segment classes from a pre-defined training set. More recently, first\nworks on open-set segmentation in 3D scenes have appeared in the literature.\nThese methods are heavily influenced by closed-set 3D convolutional approaches\nthat process point clouds or polygon meshes. However, these 3D scene\nrepresentations do not align well with the image-based nature of the\nvisual-language models. Indeed, point cloud and 3D meshes typically have a\nlower resolution than images and the reconstructed 3D scene geometry might not\nproject well to the underlying 2D image sequences used to compute pixel-aligned\nCLIP features. To address these challenges, we propose OpenNeRF which naturally\noperates on posed images and directly encodes the VLM features within the NeRF.\nThis is similar in spirit to LERF, however our work shows that using pixel-wise\nVLM features (instead of global CLIP features) results in an overall less\ncomplex architecture without the need for additional DINO regularization. Our\nOpenNeRF further leverages NeRF's ability to render novel views and extract\nopen-set VLM features from areas that are not well observed in the initial\nposed images. For 3D point cloud segmentation on the Replica dataset, OpenNeRF\noutperforms recent open-vocabulary methods such as LERF and OpenScene by at\nleast +4.9 mIoU.",
        "translated": ""
    },
    {
        "title": "Decoupling Static and Hierarchical Motion Perception for Referring Video\n  Segmentation",
        "url": "http://arxiv.org/abs/2404.03645v1",
        "pub_date": "2024-04-04",
        "summary": "Referring video segmentation relies on natural language expressions to\nidentify and segment objects, often emphasizing motion clues. Previous works\ntreat a sentence as a whole and directly perform identification at the\nvideo-level, mixing up static image-level cues with temporal motion cues.\nHowever, image-level features cannot well comprehend motion cues in sentences,\nand static cues are not crucial for temporal perception. In fact, static cues\ncan sometimes interfere with temporal perception by overshadowing motion cues.\nIn this work, we propose to decouple video-level referring expression\nunderstanding into static and motion perception, with a specific emphasis on\nenhancing temporal comprehension. Firstly, we introduce an\nexpression-decoupling module to make static cues and motion cues perform their\ndistinct role, alleviating the issue of sentence embeddings overlooking motion\ncues. Secondly, we propose a hierarchical motion perception module to capture\ntemporal information effectively across varying timescales. Furthermore, we\nemploy contrastive learning to distinguish the motions of visually similar\nobjects. These contributions yield state-of-the-art performance across five\ndatasets, including a remarkable $\\textbf{9.2%}$ $\\mathcal{J\\&amp;F}$ improvement\non the challenging $\\textbf{MeViS}$ dataset. Code is available at\nhttps://github.com/heshuting555/DsHmp.",
        "translated": ""
    },
    {
        "title": "DiffBody: Human Body Restoration by Imagining with Generative Diffusion\n  Prior",
        "url": "http://arxiv.org/abs/2404.03642v1",
        "pub_date": "2024-04-04",
        "summary": "Human body restoration plays a vital role in various applications related to\nthe human body. Despite recent advances in general image restoration using\ngenerative models, their performance in human body restoration remains\nmediocre, often resulting in foreground and background blending, over-smoothing\nsurface textures, missing accessories, and distorted limbs. Addressing these\nchallenges, we propose a novel approach by constructing a human body-aware\ndiffusion model that leverages domain-specific knowledge to enhance\nperformance. Specifically, we employ a pretrained body attention module to\nguide the diffusion model's focus on the foreground, addressing issues caused\nby blending between the subject and background. We also demonstrate the value\nof revisiting the language modality of the diffusion model in restoration tasks\nby seamlessly incorporating text prompt to improve the quality of surface\ntexture and additional clothing and accessories details. Additionally, we\nintroduce a diffusion sampler tailored for fine-grained human body parts,\nutilizing local semantic information to rectify limb distortions. Lastly, we\ncollect a comprehensive dataset for benchmarking and advancing the field of\nhuman body restoration. Extensive experimental validation showcases the\nsuperiority of our approach, both quantitatively and qualitatively, over\nexisting methods.",
        "translated": ""
    },
    {
        "title": "WorDepth: Variational Language Prior for Monocular Depth Estimation",
        "url": "http://arxiv.org/abs/2404.03635v1",
        "pub_date": "2024-04-04",
        "summary": "Three-dimensional (3D) reconstruction from a single image is an ill-posed\nproblem with inherent ambiguities, i.e. scale. Predicting a 3D scene from text\ndescription(s) is similarly ill-posed, i.e. spatial arrangements of objects\ndescribed. We investigate the question of whether two inherently ambiguous\nmodalities can be used in conjunction to produce metric-scaled reconstructions.\nTo test this, we focus on monocular depth estimation, the problem of predicting\na dense depth map from a single image, but with an additional text caption\ndescribing the scene. To this end, we begin by encoding the text caption as a\nmean and standard deviation; using a variational framework, we learn the\ndistribution of the plausible metric reconstructions of 3D scenes corresponding\nto the text captions as a prior. To \"select\" a specific reconstruction or depth\nmap, we encode the given image through a conditional sampler that samples from\nthe latent space of the variational text encoder, which is then decoded to the\noutput depth map. Our approach is trained alternatingly between the text and\nimage branches: in one optimization step, we predict the mean and standard\ndeviation from the text description and sample from a standard Gaussian, and in\nthe other, we sample using a (image) conditional sampler. Once trained, we\ndirectly predict depth from the encoded text using the conditional sampler. We\ndemonstrate our approach on indoor (NYUv2) and outdoor (KITTI) scenarios, where\nwe show that language can consistently improve performance in both.",
        "translated": ""
    },
    {
        "title": "Sigma: Siamese Mamba Network for Multi-Modal Semantic Segmentation",
        "url": "http://arxiv.org/abs/2404.04256v1",
        "pub_date": "2024-04-05",
        "summary": "Multi-modal semantic segmentation significantly enhances AI agents'\nperception and scene understanding, especially under adverse conditions like\nlow-light or overexposed environments. Leveraging additional modalities\n(X-modality) like thermal and depth alongside traditional RGB provides\ncomplementary information, enabling more robust and reliable segmentation. In\nthis work, we introduce Sigma, a Siamese Mamba network for multi-modal semantic\nsegmentation, utilizing the Selective Structured State Space Model, Mamba.\nUnlike conventional methods that rely on CNNs, with their limited local\nreceptive fields, or Vision Transformers (ViTs), which offer global receptive\nfields at the cost of quadratic complexity, our model achieves global receptive\nfields coverage with linear complexity. By employing a Siamese encoder and\ninnovating a Mamba fusion mechanism, we effectively select essential\ninformation from different modalities. A decoder is then developed to enhance\nthe channel-wise modeling ability of the model. Our method, Sigma, is\nrigorously evaluated on both RGB-Thermal and RGB-Depth segmentation tasks,\ndemonstrating its superiority and marking the first successful application of\nState Space Models (SSMs) in multi-modal perception tasks. Code is available at\nhttps://github.com/zifuwan/Sigma.",
        "translated": ""
    },
    {
        "title": "Watermark-based Detection and Attribution of AI-Generated Content",
        "url": "http://arxiv.org/abs/2404.04254v1",
        "pub_date": "2024-04-05",
        "summary": "Several companies--such as Google, Microsoft, and OpenAI--have deployed\ntechniques to watermark AI-generated content to enable proactive detection.\nHowever, existing literature mainly focuses on user-agnostic detection.\nAttribution aims to further trace back the user of a generative-AI service who\ngenerated a given content detected as AI-generated. Despite its growing\nimportance, attribution is largely unexplored. In this work, we aim to bridge\nthis gap by providing the first systematic study on watermark-based, user-aware\ndetection and attribution of AI-generated content. Specifically, we\ntheoretically study the detection and attribution performance via rigorous\nprobabilistic analysis. Moreover, we develop an efficient algorithm to select\nwatermarks for the users to enhance attribution performance. Both our\ntheoretical and empirical results show that watermark-based detection and\nattribution inherit the accuracy and (non-)robustness properties of the\nwatermarking method.",
        "translated": ""
    },
    {
        "title": "Who Evaluates the Evaluations? Objectively Scoring Text-to-Image Prompt\n  Coherence Metrics with T2IScoreScore (TS2)",
        "url": "http://arxiv.org/abs/2404.04251v1",
        "pub_date": "2024-04-05",
        "summary": "With advances in the quality of text-to-image (T2I) models has come interest\nin benchmarking their prompt faithfulness-the semantic coherence of generated\nimages to the prompts they were conditioned on. A variety of T2I faithfulness\nmetrics have been proposed, leveraging advances in cross-modal embeddings and\nvision-language models (VLMs). However, these metrics are not rigorously\ncompared and benchmarked, instead presented against few weak baselines by\ncorrelation to human Likert scores over a set of easy-to-discriminate images.\n  We introduce T2IScoreScore (TS2), a curated set of semantic error graphs\ncontaining a prompt and a set increasingly erroneous images. These allow us to\nrigorously judge whether a given prompt faithfulness metric can correctly order\nimages with respect to their objective error count and significantly\ndiscriminate between different error nodes, using meta-metric scores derived\nfrom established statistical tests. Surprisingly, we find that the\nstate-of-the-art VLM-based metrics (e.g., TIFA, DSG, LLMScore, VIEScore) we\ntested fail to significantly outperform simple feature-based metrics like\nCLIPScore, particularly on a hard subset of naturally-occurring T2I model\nerrors. TS2 will enable the development of better T2I prompt faithfulness\nmetrics through more rigorous comparison of their conformity to expected\norderings and separations under objective criteria.",
        "translated": ""
    },
    {
        "title": "Evaluating Adversarial Robustness: A Comparison Of FGSM, Carlini-Wagner\n  Attacks, And The Role of Distillation as Defense Mechanism",
        "url": "http://arxiv.org/abs/2404.04245v1",
        "pub_date": "2024-04-05",
        "summary": "This technical report delves into an in-depth exploration of adversarial\nattacks specifically targeted at Deep Neural Networks (DNNs) utilized for image\nclassification. The study also investigates defense mechanisms aimed at\nbolstering the robustness of machine learning models. The research focuses on\ncomprehending the ramifications of two prominent attack methodologies: the Fast\nGradient Sign Method (FGSM) and the Carlini-Wagner (CW) approach. These attacks\nare examined concerning three pre-trained image classifiers: Resnext50_32x4d,\nDenseNet-201, and VGG-19, utilizing the Tiny-ImageNet dataset. Furthermore, the\nstudy proposes the robustness of defensive distillation as a defense mechanism\nto counter FGSM and CW attacks. This defense mechanism is evaluated using the\nCIFAR-10 dataset, where CNN models, specifically resnet101 and Resnext50_32x4d,\nserve as the teacher and student models, respectively. The proposed defensive\ndistillation model exhibits effectiveness in thwarting attacks such as FGSM.\nHowever, it is noted to remain susceptible to more sophisticated techniques\nlike the CW attack. The document presents a meticulous validation of the\nproposed scheme. It provides detailed and comprehensive results, elucidating\nthe efficacy and limitations of the defense mechanisms employed. Through\nrigorous experimentation and analysis, the study offers insights into the\ndynamics of adversarial attacks on DNNs, as well as the effectiveness of\ndefensive strategies in mitigating their impact.",
        "translated": ""
    },
    {
        "title": "DiffOp-net: A Differential Operator-based Fully Convolutional Network\n  for Unsupervised Deformable Image Registration",
        "url": "http://arxiv.org/abs/2404.04244v1",
        "pub_date": "2024-04-05",
        "summary": "Existing unsupervised deformable image registration methods usually rely on\nmetrics applied to the gradients of predicted displacement or velocity fields\nas a regularization term to ensure transformation smoothness, which potentially\nlimits registration accuracy. In this study, we propose a novel approach to\nenhance unsupervised deformable image registration by introducing a new\ndifferential operator into the registration framework. This operator, acting on\nthe velocity field and mapping it to a dual space, ensures the smoothness of\nthe velocity field during optimization, facilitating accurate deformable\nregistration. In addition, to tackle the challenge of capturing large\ndeformations inside image pairs, we introduce a Cross-Coordinate Attention\nmodule (CCA) and embed it into a proposed Fully Convolutional Networks\n(FCNs)-based multi-resolution registration architecture. Evaluation experiments\nare conducted on two magnetic resonance imaging (MRI) datasets. Compared to\nvarious state-of-the-art registration approaches, including a traditional\nalgorithm and three representative unsupervised learning-based methods, our\nmethod achieves superior accuracies, maintaining desirable diffeomorphic\nproperties, and exhibiting promising registration speed.",
        "translated": ""
    },
    {
        "title": "Identity Decoupling for Multi-Subject Personalization of Text-to-Image\n  Models",
        "url": "http://arxiv.org/abs/2404.04243v1",
        "pub_date": "2024-04-05",
        "summary": "Text-to-image diffusion models have shown remarkable success in generating a\npersonalized subject based on a few reference images. However, current methods\nstruggle with handling multiple subjects simultaneously, often resulting in\nmixed identities with combined attributes from different subjects. In this\nwork, we present MuDI, a novel framework that enables multi-subject\npersonalization by effectively decoupling identities from multiple subjects.\nOur main idea is to utilize segmented subjects generated by the Segment\nAnything Model for both training and inference, as a form of data augmentation\nfor training and initialization for the generation process. Our experiments\ndemonstrate that MuDI can produce high-quality personalized images without\nidentity mixing, even for highly similar subjects as shown in Figure 1. In\nhuman evaluation, MuDI shows twice as many successes for personalizing multiple\nsubjects without identity mixing over existing baselines and is preferred over\n70% compared to the strongest baseline. More results are available at\nhttps://mudi-t2i.github.io/.",
        "translated": ""
    },
    {
        "title": "Physical Property Understanding from Language-Embedded Feature Fields",
        "url": "http://arxiv.org/abs/2404.04242v1",
        "pub_date": "2024-04-05",
        "summary": "Can computers perceive the physical properties of objects solely through\nvision? Research in cognitive science and vision science has shown that humans\nexcel at identifying materials and estimating their physical properties based\npurely on visual appearance. In this paper, we present a novel approach for\ndense prediction of the physical properties of objects using a collection of\nimages. Inspired by how humans reason about physics through vision, we leverage\nlarge language models to propose candidate materials for each object. We then\nconstruct a language-embedded point cloud and estimate the physical properties\nof each 3D point using a zero-shot kernel regression approach. Our method is\naccurate, annotation-free, and applicable to any object in the open world.\nExperiments demonstrate the effectiveness of the proposed approach in various\nphysical property reasoning tasks, such as estimating the mass of common\nobjects, as well as other properties like friction and hardness.",
        "translated": ""
    },
    {
        "title": "Image-Text Co-Decomposition for Text-Supervised Semantic Segmentation",
        "url": "http://arxiv.org/abs/2404.04231v1",
        "pub_date": "2024-04-05",
        "summary": "This paper addresses text-supervised semantic segmentation, aiming to learn a\nmodel capable of segmenting arbitrary visual concepts within images by using\nonly image-text pairs without dense annotations. Existing methods have\ndemonstrated that contrastive learning on image-text pairs effectively aligns\nvisual segments with the meanings of texts. We notice that there is a\ndiscrepancy between text alignment and semantic segmentation: A text often\nconsists of multiple semantic concepts, whereas semantic segmentation strives\nto create semantically homogeneous segments. To address this issue, we propose\na novel framework, Image-Text Co-Decomposition (CoDe), where the paired image\nand text are jointly decomposed into a set of image regions and a set of word\nsegments, respectively, and contrastive learning is developed to enforce\nregion-word alignment. To work with a vision-language model, we present a\nprompt learning mechanism that derives an extra representation to highlight an\nimage segment or a word segment of interest, with which more effective features\ncan be extracted from that segment. Comprehensive experimental results\ndemonstrate that our method performs favorably against existing text-supervised\nsemantic segmentation methods on six benchmark datasets.",
        "translated": ""
    },
    {
        "title": "Robust Gaussian Splatting",
        "url": "http://arxiv.org/abs/2404.04211v1",
        "pub_date": "2024-04-05",
        "summary": "In this paper, we address common error sources for 3D Gaussian Splatting\n(3DGS) including blur, imperfect camera poses, and color inconsistencies, with\nthe goal of improving its robustness for practical applications like\nreconstructions from handheld phone captures. Our main contribution involves\nmodeling motion blur as a Gaussian distribution over camera poses, allowing us\nto address both camera pose refinement and motion blur correction in a unified\nway. Additionally, we propose mechanisms for defocus blur compensation and for\naddressing color in-consistencies caused by ambient light, shadows, or due to\ncamera-related factors like varying white balancing settings. Our proposed\nsolutions integrate in a seamless way with the 3DGS formulation while\nmaintaining its benefits in terms of training efficiency and rendering speed.\nWe experimentally validate our contributions on relevant benchmark datasets\nincluding Scannet++ and Deblur-NeRF, obtaining state-of-the-art results and\nthus consistent improvements over relevant baselines.",
        "translated": ""
    },
    {
        "title": "Deep-learning Segmentation of Small Volumes in CT images for\n  Radiotherapy Treatment Planning",
        "url": "http://arxiv.org/abs/2404.04202v1",
        "pub_date": "2024-04-05",
        "summary": "Our understanding of organs at risk is progressing to include physical small\ntissues such as coronary arteries and the radiosensitivities of many small\norgans and tissues are high. Therefore, the accurate segmentation of small\nvolumes in external radiotherapy is crucial to protect them from\nover-irradiation. Moreover, with the development of the particle therapy and\non-board imaging, the treatment becomes more accurate and precise. The purpose\nof this work is to optimize organ segmentation algorithms for small organs. We\nused 50 three-dimensional (3-D) computed tomography (CT) head and neck images\nfrom StructSeg2019 challenge to develop a general-purpose V-Net model to\nsegment 20 organs in the head and neck region. We applied specific strategies\nto improve the segmentation accuracy of the small volumes in this anatomical\nregion, i.e., the lens of the eye. Then, we used 17 additional head images from\nOSF healthcare to validate the robustness of the V Net model optimized for\nsmall-volume segmentation. With the study of the StructSeg2019 images, we found\nthat the optimization of the image normalization range and classification\nthreshold yielded a segmentation improvement of the lens of the eye of\napproximately 50%, compared to the use of the V-Net not optimized for small\nvolumes. We used the optimized model to segment 17 images acquired using\nheterogeneous protocols. We obtained comparable Dice coefficient values for the\nclinical and StructSeg2019 images (0.61 plus/minus 0.07 and 0.58 plus/minus\n0.10 for the left and right lens of the eye, respectively)",
        "translated": ""
    },
    {
        "title": "Finding Visual Task Vectors",
        "url": "http://arxiv.org/abs/2404.05729v1",
        "pub_date": "2024-04-08",
        "summary": "Visual Prompting is a technique for teaching models to perform a visual task\nvia in-context examples, without any additional training. In this work, we\nanalyze the activations of MAE-VQGAN, a recent Visual Prompting model, and find\ntask vectors, activations that encode task-specific information. Equipped with\nthis insight, we demonstrate that it is possible to identify the task vectors\nand use them to guide the network towards performing different tasks without\nproviding any input-output examples. To find task vectors, we compute the\naverage intermediate activations per task and use the REINFORCE algorithm to\nsearch for the subset of task vectors. The resulting task vectors guide the\nmodel towards performing a task better than the original model without the need\nfor input-output examples.",
        "translated": ""
    },
    {
        "title": "MA-LMM: Memory-Augmented Large Multimodal Model for Long-Term Video\n  Understanding",
        "url": "http://arxiv.org/abs/2404.05726v1",
        "pub_date": "2024-04-08",
        "summary": "With the success of large language models (LLMs), integrating the vision\nmodel into LLMs to build vision-language foundation models has gained much more\ninterest recently. However, existing LLM-based large multimodal models (e.g.,\nVideo-LLaMA, VideoChat) can only take in a limited number of frames for short\nvideo understanding. In this study, we mainly focus on designing an efficient\nand effective model for long-term video understanding. Instead of trying to\nprocess more frames simultaneously like most existing work, we propose to\nprocess videos in an online manner and store past video information in a memory\nbank. This allows our model to reference historical video content for long-term\nanalysis without exceeding LLMs' context length constraints or GPU memory\nlimits. Our memory bank can be seamlessly integrated into current multimodal\nLLMs in an off-the-shelf manner. We conduct extensive experiments on various\nvideo understanding tasks, such as long-video understanding, video question\nanswering, and video captioning, and our model can achieve state-of-the-art\nperformances across multiple datasets. Code available at\nhttps://boheumd.github.io/MA-LMM/.",
        "translated": ""
    },
    {
        "title": "Ferret-UI: Grounded Mobile UI Understanding with Multimodal LLMs",
        "url": "http://arxiv.org/abs/2404.05719v1",
        "pub_date": "2024-04-08",
        "summary": "Recent advancements in multimodal large language models (MLLMs) have been\nnoteworthy, yet, these general-domain MLLMs often fall short in their ability\nto comprehend and interact effectively with user interface (UI) screens. In\nthis paper, we present Ferret-UI, a new MLLM tailored for enhanced\nunderstanding of mobile UI screens, equipped with referring, grounding, and\nreasoning capabilities. Given that UI screens typically exhibit a more\nelongated aspect ratio and contain smaller objects of interest (e.g., icons,\ntexts) than natural images, we incorporate \"any resolution\" on top of Ferret to\nmagnify details and leverage enhanced visual features. Specifically, each\nscreen is divided into 2 sub-images based on the original aspect ratio (i.e.,\nhorizontal division for portrait screens and vertical division for landscape\nscreens). Both sub-images are encoded separately before being sent to LLMs. We\nmeticulously gather training samples from an extensive range of elementary UI\ntasks, such as icon recognition, find text, and widget listing. These samples\nare formatted for instruction-following with region annotations to facilitate\nprecise referring and grounding. To augment the model's reasoning ability, we\nfurther compile a dataset for advanced tasks, including detailed description,\nperception/interaction conversations, and function inference. After training on\nthe curated datasets, Ferret-UI exhibits outstanding comprehension of UI\nscreens and the capability to execute open-ended instructions. For model\nevaluation, we establish a comprehensive benchmark encompassing all the\naforementioned tasks. Ferret-UI excels not only beyond most open-source UI\nMLLMs, but also surpasses GPT-4V on all the elementary UI tasks.",
        "translated": ""
    },
    {
        "title": "SwapAnything: Enabling Arbitrary Object Swapping in Personalized Visual\n  Editing",
        "url": "http://arxiv.org/abs/2404.05717v1",
        "pub_date": "2024-04-08",
        "summary": "Effective editing of personal content holds a pivotal role in enabling\nindividuals to express their creativity, weaving captivating narratives within\ntheir visual stories, and elevate the overall quality and impact of their\nvisual content. Therefore, in this work, we introduce SwapAnything, a novel\nframework that can swap any objects in an image with personalized concepts\ngiven by the reference, while keeping the context unchanged. Compared with\nexisting methods for personalized subject swapping, SwapAnything has three\nunique advantages: (1) precise control of arbitrary objects and parts rather\nthan the main subject, (2) more faithful preservation of context pixels, (3)\nbetter adaptation of the personalized concept to the image. First, we propose\ntargeted variable swapping to apply region control over latent feature maps and\nswap masked variables for faithful context preservation and initial semantic\nconcept swapping. Then, we introduce appearance adaptation, to seamlessly adapt\nthe semantic concept into the original image in terms of target location,\nshape, style, and content during the image generation process. Extensive\nresults on both human and automatic evaluation demonstrate significant\nimprovements of our approach over baseline methods on personalized swapping.\nFurthermore, SwapAnything shows its precise and faithful swapping abilities\nacross single object, multiple objects, partial object, and cross-domain\nswapping tasks. SwapAnything also achieves great performance on text-based\nswapping and tasks beyond swapping such as object insertion.",
        "translated": ""
    },
    {
        "title": "Learning 3D-Aware GANs from Unposed Images with Template Feature Field",
        "url": "http://arxiv.org/abs/2404.05705v1",
        "pub_date": "2024-04-08",
        "summary": "Collecting accurate camera poses of training images has been shown to well\nserve the learning of 3D-aware generative adversarial networks (GANs) yet can\nbe quite expensive in practice. This work targets learning 3D-aware GANs from\nunposed images, for which we propose to perform on-the-fly pose estimation of\ntraining images with a learned template feature field (TeFF). Concretely, in\naddition to a generative radiance field as in previous approaches, we ask the\ngenerator to also learn a field from 2D semantic features while sharing the\ndensity from the radiance field. Such a framework allows us to acquire a\ncanonical 3D feature template leveraging the dataset mean discovered by the\ngenerative model, and further efficiently estimate the pose parameters on real\ndata. Experimental results on various challenging datasets demonstrate the\nsuperiority of our approach over state-of-the-art alternatives from both the\nqualitative and the quantitative perspectives.",
        "translated": ""
    },
    {
        "title": "Evaluating the Efficacy of Cut-and-Paste Data Augmentation in Semantic\n  Segmentation for Satellite Imagery",
        "url": "http://arxiv.org/abs/2404.05693v1",
        "pub_date": "2024-04-08",
        "summary": "Satellite imagery is crucial for tasks like environmental monitoring and\nurban planning. Typically, it relies on semantic segmentation or Land Use Land\nCover (LULC) classification to categorize each pixel. Despite the advancements\nbrought about by Deep Neural Networks (DNNs), their performance in segmentation\ntasks is hindered by challenges such as limited availability of labeled data,\nclass imbalance and the inherent variability and complexity of satellite\nimages. In order to mitigate those issues, our study explores the effectiveness\nof a Cut-and-Paste augmentation technique for semantic segmentation in\nsatellite images. We adapt this augmentation, which usually requires labeled\ninstances, to the case of semantic segmentation. By leveraging the connected\ncomponents in the semantic segmentation labels, we extract instances that are\nthen randomly pasted during training. Using the DynamicEarthNet dataset and a\nU-Net model for evaluation, we found that this augmentation significantly\nenhances the mIoU score on the test set from 37.9 to 44.1. This finding\nhighlights the potential of the Cut-and-Paste augmentation to improve the\ngeneralization capabilities of semantic segmentation models in satellite\nimagery.",
        "translated": ""
    },
    {
        "title": "Retrieval-Augmented Open-Vocabulary Object Detection",
        "url": "http://arxiv.org/abs/2404.05687v1",
        "pub_date": "2024-04-08",
        "summary": "Open-vocabulary object detection (OVD) has been studied with Vision-Language\nModels (VLMs) to detect novel objects beyond the pre-trained categories.\nPrevious approaches improve the generalization ability to expand the knowledge\nof the detector, using 'positive' pseudo-labels with additional 'class' names,\ne.g., sock, iPod, and alligator. To extend the previous methods in two aspects,\nwe propose Retrieval-Augmented Losses and visual Features (RALF). Our method\nretrieves related 'negative' classes and augments loss functions. Also, visual\nfeatures are augmented with 'verbalized concepts' of classes, e.g., worn on the\nfeet, handheld music player, and sharp teeth. Specifically, RALF consists of\ntwo modules: Retrieval Augmented Losses (RAL) and Retrieval-Augmented visual\nFeatures (RAF). RAL constitutes two losses reflecting the semantic similarity\nwith negative vocabularies. In addition, RAF augments visual features with the\nverbalized concepts from a large language model (LLM). Our experiments\ndemonstrate the effectiveness of RALF on COCO and LVIS benchmark datasets. We\nachieve improvement up to 3.4 box AP$_{50}^{\\text{N}}$ on novel categories of\nthe COCO dataset and 3.6 mask AP$_{\\text{r}}$ gains on the LVIS dataset. Code\nis available at https://github.com/mlvlab/RALF .",
        "translated": ""
    },
    {
        "title": "SphereHead: Stable 3D Full-head Synthesis with Spherical Tri-plane\n  Representation",
        "url": "http://arxiv.org/abs/2404.05680v1",
        "pub_date": "2024-04-08",
        "summary": "While recent advances in 3D-aware Generative Adversarial Networks (GANs) have\naided the development of near-frontal view human face synthesis, the challenge\nof comprehensively synthesizing a full 3D head viewable from all angles still\npersists. Although PanoHead proves the possibilities of using a large-scale\ndataset with images of both frontal and back views for full-head synthesis, it\noften causes artifacts for back views. Based on our in-depth analysis, we found\nthe reasons are mainly twofold. First, from network architecture perspective,\nwe found each plane in the utilized tri-plane/tri-grid representation space\ntends to confuse the features from both sides, causing \"mirroring\" artifacts\n(e.g., the glasses appear in the back). Second, from data supervision aspect,\nwe found that existing discriminator training in 3D GANs mainly focuses on the\nquality of the rendered image itself, and does not care much about its\nplausibility with the perspective from which it was rendered. This makes it\npossible to generate \"face\" in non-frontal views, due to its easiness to fool\nthe discriminator. In response, we propose SphereHead, a novel tri-plane\nrepresentation in the spherical coordinate system that fits the human head's\ngeometric characteristics and efficiently mitigates many of the generated\nartifacts. We further introduce a view-image consistency loss for the\ndiscriminator to emphasize the correspondence of the camera parameters and the\nimages. The combination of these efforts results in visually superior outcomes\nwith significantly fewer artifacts. Our code and dataset are publicly available\nat https://lhyfst.github.io/spherehead.",
        "translated": ""
    },
    {
        "title": "Normalizing Flows on the Product Space of SO(3) Manifolds for\n  Probabilistic Human Pose Modeling",
        "url": "http://arxiv.org/abs/2404.05675v1",
        "pub_date": "2024-04-08",
        "summary": "Normalizing flows have proven their efficacy for density estimation in\nEuclidean space, but their application to rotational representations, crucial\nin various domains such as robotics or human pose modeling, remains\nunderexplored. Probabilistic models of the human pose can benefit from\napproaches that rigorously consider the rotational nature of human joints. For\nthis purpose, we introduce HuProSO3, a normalizing flow model that operates on\na high-dimensional product space of SO(3) manifolds, modeling the joint\ndistribution for human joints with three degrees of freedom. HuProSO3's\nadvantage over state-of-the-art approaches is demonstrated through its superior\nmodeling accuracy in three different applications and its capability to\nevaluate the exact likelihood. This work not only addresses the technical\nchallenge of learning densities on SO(3) manifolds, but it also has broader\nimplications for domains where the probabilistic regression of correlated 3D\nrotations is of importance.",
        "translated": ""
    },
    {
        "title": "MoMA: Multimodal LLM Adapter for Fast Personalized Image Generation",
        "url": "http://arxiv.org/abs/2404.05674v1",
        "pub_date": "2024-04-08",
        "summary": "In this paper, we present MoMA: an open-vocabulary, training-free\npersonalized image model that boasts flexible zero-shot capabilities. As\nfoundational text-to-image models rapidly evolve, the demand for robust\nimage-to-image translation grows. Addressing this need, MoMA specializes in\nsubject-driven personalized image generation. Utilizing an open-source,\nMultimodal Large Language Model (MLLM), we train MoMA to serve a dual role as\nboth a feature extractor and a generator. This approach effectively synergizes\nreference image and text prompt information to produce valuable image features,\nfacilitating an image diffusion model. To better leverage the generated\nfeatures, we further introduce a novel self-attention shortcut method that\nefficiently transfers image features to an image diffusion model, improving the\nresemblance of the target object in generated images. Remarkably, as a\ntuning-free plug-and-play module, our model requires only a single reference\nimage and outperforms existing methods in generating images with high detail\nfidelity, enhanced identity-preservation and prompt faithfulness. Our work is\nopen-source, thereby providing universal access to these advancements.",
        "translated": ""
    }
]