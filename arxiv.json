[
    {
        "title": "Exploring the Carbon Footprint of Hugging Face's ML Models: A Repository\n  Mining Study",
        "url": "http://arxiv.org/abs/2305.11164v1",
        "pub_date": "2023-05-18",
        "summary": "The rise of machine learning (ML) systems has exacerbated their carbon\nfootprint due to increased capabilities and model sizes. However, there is\nscarce knowledge on how the carbon footprint of ML models is actually measured,\nreported, and evaluated. In light of this, the paper aims to analyze the\nmeasurement of the carbon footprint of 1,417 ML models and associated datasets\non Hugging Face, which is the most popular repository for pretrained ML models.\nThe goal is to provide insights and recommendations on how to report and\noptimize the carbon efficiency of ML models. The study includes the first\nrepository mining study on the Hugging Face Hub API on carbon emissions. This\nstudy seeks to answer two research questions: (1) how do ML model creators\nmeasure and report carbon emissions on Hugging Face Hub?, and (2) what aspects\nimpact the carbon emissions of training ML models? The study yielded several\nkey findings. These include a decreasing proportion of carbon\nemissions-reporting models, a slight decrease in reported carbon footprint on\nHugging Face over the past 2 years, and a continued dominance of NLP as the\nmain application domain. Furthermore, the study uncovers correlations between\ncarbon emissions and various attributes such as model size, dataset size, and\nML application domains. These results highlight the need for software\nmeasurements to improve energy reporting practices and promote carbon-efficient\nmodel development within the Hugging Face community. In response to this issue,\ntwo classifications are proposed: one for categorizing models based on their\ncarbon emission reporting practices and another for their carbon efficiency.\nThe aim of these classification proposals is to foster transparency and\nsustainable model development within the ML community.",
        "translated": "机器学习(ML)系统的兴起加剧了它们的碳足印，原因是功能和模型尺寸的增加。然而，对于机器学习模型的碳足印实际上是如何测量、报告和评估的，我们知之甚少。有鉴于此，本文旨在分析“拥抱脸”上对1417个机器学习模型及相关数据集的碳足印测量结果。“拥抱脸”是最受欢迎的预训机器学习模型库。目标是就如何报告和优化机器学习模型的碳效率提供见解和建议。这项研究包括第一个关于碳排放的拥抱面中心 API 的知识库挖掘研究。这项研究试图回答两个研究问题: (1)机器学习模型的创建者如何测量和报告拥抱面部中心的碳排放量？以及(2)哪些方面影响训练机器学习模型的碳排放量？这项研究产生了几个关键的发现。其中包括碳排放报告模型的比例下降，过去两年“拥抱脸”上的报告碳足印略有下降，以及自然语言处理作为主要应用领域的持续主导地位。此外，该研究还揭示了碳排放与模型大小、数据集大小和机器学习应用领域等各种属性之间的相关性。这些结果突出了软件测量的必要性，以改善能源报告做法，并促进在拥抱面社区的碳效率模型开发。针对这一问题，提出了两种分类: 一种是根据其碳排放报告做法对模型进行分类，另一种是根据其碳效率进行分类。这些分类建议的目的是在 ML 社区内促进透明度和可持续的模型开发。"
    },
    {
        "title": "TOME: A Two-stage Approach for Model-based Retrieval",
        "url": "http://arxiv.org/abs/2305.11161v1",
        "pub_date": "2023-05-18",
        "summary": "Recently, model-based retrieval has emerged as a new paradigm in text\nretrieval that discards the index in the traditional retrieval model and\ninstead memorizes the candidate corpora using model parameters. This design\nemploys a sequence-to-sequence paradigm to generate document identifiers, which\nenables the complete capture of the relevance between queries and documents and\nsimplifies the classic indexretrieval-rerank pipeline. Despite its attractive\nqualities, there remain several major challenges in model-based retrieval,\nincluding the discrepancy between pre-training and fine-tuning, and the\ndiscrepancy between training and inference. To deal with the above challenges,\nwe propose a novel two-stage model-based retrieval approach called TOME, which\nmakes two major technical contributions, including the utilization of tokenized\nURLs as identifiers and the design of a two-stage generation architecture. We\nalso propose a number of training strategies to deal with the training\ndifficulty as the corpus size increases. Extensive experiments and analysis on\nMS MARCO and Natural Questions demonstrate the effectiveness of our proposed\napproach, and we investigate the scaling laws of TOME by examining various\ninfluencing factors.",
        "translated": "近年来，基于模型的检索已经成为文本检索的一种新范式，它抛弃了传统检索模型中的索引，而是利用模型参数记忆候选语料库。该设计采用序列到序列的方法生成文档标识符，能够完全捕获查询和文档之间的相关性，简化了经典的索引检索-重排序流水线。尽管基于模型的检索具有吸引人的优点，但仍然存在一些主要的挑战，包括预训练和微调之间的差异，以及训练和推理之间的差异。为了应对上述挑战，我们提出了一种新的基于两阶段模型的检索方法，称为 TOME，它做出了两个主要的技术贡献，包括使用标记化 URL 作为标识符和设计一个两阶段生成体系结构。随着语料库规模的增大，我们提出了一些训练策略来解决训练难度。大量的实验和分析 MS MARCO 和自然问题证明了我们提出的方法的有效性，我们研究了 TOME 的缩放规律通过检查各种影响因素。"
    },
    {
        "title": "Preference or Intent? Double Disentangled Collaborative Filtering",
        "url": "http://arxiv.org/abs/2305.11084v1",
        "pub_date": "2023-05-18",
        "summary": "People usually have different intents for choosing items, while their\npreferences under the same intent may also different. In traditional\ncollaborative filtering approaches, both intent and preference factors are\nusually entangled in the modeling process, which significantly limits the\nrobustness and interpretability of recommendation performances. For example,\nthe low-rating items are always treated as negative feedback while they\nactually could provide positive information about user intent. To this end, in\nthis paper, we propose a two-fold representation learning approach, namely\nDouble Disentangled Collaborative Filtering (DDCF), for personalized\nrecommendations. The first-level disentanglement is for separating the\ninfluence factors of intent and preference, while the second-level\ndisentanglement is performed to build independent sparse preference\nrepresentations under individual intent with limited computational complexity.\nSpecifically, we employ two variational autoencoder networks, intent\nrecognition network and preference decomposition network, to learn the intent\nand preference factors, respectively. In this way, the low-rating items will be\ntreated as positive samples for modeling intents while the negative samples for\nmodeling preferences. Finally, extensive experiments on three real-world\ndatasets and four evaluation metrics clearly validate the effectiveness and the\ninterpretability of DDCF.",
        "translated": "人们通常有不同的意图选择项目，而他们的偏好下，相同的意图也可能有所不同。在传统的协同过滤建模方法中，意图和偏好因素通常会在建模过程中纠缠在一起，这极大地限制了推荐性能的稳健性和可解释性。例如，低等级的项目总是被视为负面反馈，而实际上它们可以提供关于用户意图的正面信息。为此，在本文中，我们提出了一种双重表征学习方法，即双重分离协同过滤(DDCF) ，用于个性化推荐。第一级解缠是为了分离意图和偏好的影响因素，而第二级解缠是为了在计算复杂度有限的个体意图下构建独立的稀疏偏好表示。具体来说，我们使用两个变分自动编码器网络，意图识别网络和偏好分解网络，分别学习意图和偏好因素。这样，低等级的项目将被视为建模意图的正面样本，而负面样本将被视为建模偏好。最后，在三个实际数据集和四个评价指标上进行了广泛的实验，验证了 DDCF 的有效性和可解释性。"
    },
    {
        "title": "Contrastive State Augmentations for Reinforcement Learning-Based\n  Recommender Systems",
        "url": "http://arxiv.org/abs/2305.11081v1",
        "pub_date": "2023-05-18",
        "summary": "Learning reinforcement learning (RL)-based recommenders from historical\nuser-item interaction sequences is vital to generate high-reward\nrecommendations and improve long-term cumulative benefits. However, existing RL\nrecommendation methods encounter difficulties (i) to estimate the value\nfunctions for states which are not contained in the offline training data, and\n(ii) to learn effective state representations from user implicit feedback due\nto the lack of contrastive signals. In this work, we propose contrastive state\naugmentations (CSA) for the training of RL-based recommender systems. To tackle\nthe first issue, we propose four state augmentation strategies to enlarge the\nstate space of the offline data. The proposed method improves the\ngeneralization capability of the recommender by making the RL agent visit the\nlocal state regions and ensuring the learned value functions are similar\nbetween the original and augmented states. For the second issue, we propose\nintroducing contrastive signals between augmented states and the state randomly\nsampled from other sessions to improve the state representation learning\nfurther. To verify the effectiveness of the proposed CSA, we conduct extensive\nexperiments on two publicly accessible datasets and one dataset collected from\na real-life e-commerce platform. We also conduct experiments on a simulated\nenvironment as the online evaluation setting. Experimental results demonstrate\nthat CSA can effectively improve recommendation performance.",
        "translated": "从历史用户项目交互序列中学习基于强化学习的推荐对于产生高回报的推荐和提高长期累积效益至关重要。然而，现有的 RL 推荐方法遇到了困难(i)估计不包含在离线训练数据中的状态的值函数，以及(ii)由于缺乏对比信号而从用户隐式反馈中学习有效的状态表示。在这项工作中，我们提出了对比状态增强(CSA)的训练基于 RL 的推荐系统。针对第一个问题，我们提出了四种状态增强策略来扩大离线数据的状态空间。该方法通过使 RL 代理访问局部状态区域，保证学习值函数在原状态和增广状态之间相似，提高了推荐器的泛化能力。对于第二个问题，我们提出在增广状态和从其他会话中随机采样的状态之间引入对比信号，以进一步改善状态表示学习。为了验证所提出的 CSA 的有效性，我们对从现实生活中的电子商务平台收集的两个可公开访问的数据集和一个数据集进行了广泛的实验。我们还进行了模拟环境的实验，作为在线评价设置。实验结果表明，CSA 能有效提高推荐性能。"
    },
    {
        "title": "BERM: Training the Balanced and Extractable Representation for Matching\n  to Improve Generalization Ability of Dense Retrieval",
        "url": "http://arxiv.org/abs/2305.11052v1",
        "pub_date": "2023-05-18",
        "summary": "Dense retrieval has shown promise in the first-stage retrieval process when\ntrained on in-domain labeled datasets. However, previous studies have found\nthat dense retrieval is hard to generalize to unseen domains due to its weak\nmodeling of domain-invariant and interpretable feature (i.e., matching signal\nbetween two texts, which is the essence of information retrieval). In this\npaper, we propose a novel method to improve the generalization of dense\nretrieval via capturing matching signal called BERM. Fully fine-grained\nexpression and query-oriented saliency are two properties of the matching\nsignal. Thus, in BERM, a single passage is segmented into multiple units and\ntwo unit-level requirements are proposed for representation as the constraint\nin training to obtain the effective matching signal. One is semantic unit\nbalance and the other is essential matching unit extractability. Unit-level\nview and balanced semantics make representation express the text in a\nfine-grained manner. Essential matching unit extractability makes passage\nrepresentation sensitive to the given query to extract the pure matching\ninformation from the passage containing complex context. Experiments on BEIR\nshow that our method can be effectively combined with different dense retrieval\ntraining methods (vanilla, hard negatives mining and knowledge distillation) to\nimprove its generalization ability without any additional inference overhead\nand target domain data.",
        "translated": "密集检索在域内标记数据集训练的第一阶段检索过程中显示出希望。然而，先前的研究发现，由于密集检索对领域不变性和可解释特征(即两个文本之间的匹配信号，这是信息检索的本质)的建模较弱，因此很难将其推广到不可见的领域。在本文中，我们提出了一种新的方法来提高通过捕获匹配信号密集检索的泛化称为 BERM。完全细粒度表达式和面向查询的显著性是匹配信号的两个属性。因此，在误码率模型中，将一个通道分割成多个单元，并提出了两个单元级的要求作为训练中获得有效匹配信号的约束条件。一个是语义单元平衡，另一个是必要的匹配单元可提取性。单元级视图和平衡语义使表示以细粒度的方式表示文本。基本匹配单元可提取性使得文本表示对给定的查询敏感，从包含复杂上下文的文本中提取纯匹配信息。在 BEIR 上的实验表明，该方法可以有效地结合不同的密集检索训练方法(普通方法、硬负数挖掘和知识提取) ，在不增加任何推理开销和目标域数据的情况下提高其泛化能力。"
    },
    {
        "title": "Improving Recommendation System Serendipity Through Lexicase Selection",
        "url": "http://arxiv.org/abs/2305.11044v1",
        "pub_date": "2023-05-18",
        "summary": "Recommender systems influence almost every aspect of our digital lives.\nUnfortunately, in striving to give us what we want, they end up restricting our\nopen-mindedness. Current recommender systems promote echo chambers, where\npeople only see the information they want to see, and homophily, where users of\nsimilar background see similar content. We propose a new serendipity metric to\nmeasure the presence of echo chambers and homophily in recommendation systems\nusing cluster analysis. We then attempt to improve the diversity-preservation\nqualities of well known recommendation techniques by adopting a parent\nselection algorithm from the evolutionary computation literature known as\nlexicase selection. Our results show that lexicase selection, or a mixture of\nlexicase selection and ranking, outperforms its purely ranked counterparts in\nterms of personalization, coverage and our specifically designed serendipity\nbenchmark, while only slightly under-performing in terms of accuracy (hit\nrate). We verify these results across a variety of recommendation list sizes.\nIn this work we show that lexicase selection is able to maintain multiple\ndiverse clusters of item recommendations that are each relevant for the\nspecific user, while still maintaining a high hit-rate accuracy, a trade off\nthat is not achieved by other methods.",
        "translated": "推荐系统几乎影响了我们数字生活的方方面面。不幸的是，在努力给予我们想要的东西的过程中，他们最终限制了我们思想的开放性。目前的推荐系统推广回声室，人们只看到他们想看到的信息，同质性，相似背景的用户看到相似的内容。我们提出了一种新的意外发现度量方法，用来衡量使用数据聚类的推荐系统中是否存在回声室和同质性。然后，我们试图通过采用来自进化计算文献的父选择算法(称为 lexicase 选择)来改进众所周知的推荐技术的多样性保持质量。我们的研究结果表明，词汇表选择，或词汇表选择和排名的混合，在个性化，覆盖率和我们专门设计的意外发现基准方面表现优于纯粹的排名对应方，而在准确性(命中率)方面表现稍差。我们通过各种推荐列表大小来验证这些结果。在这项工作中，我们表明，词汇表选择能够维护多个不同的项目推荐集群，每个相关的特定用户，同时仍然保持高命中率的准确性，这是一个权衡，没有实现的其他方法。"
    },
    {
        "title": "Query Performance Prediction: From Ad-hoc to Conversational Search",
        "url": "http://arxiv.org/abs/2305.10923v1",
        "pub_date": "2023-05-18",
        "summary": "Query performance prediction (QPP) is a core task in information retrieval.\nThe QPP task is to predict the retrieval quality of a search system for a query\nwithout relevance judgments. Research has shown the effectiveness and\nusefulness of QPP for ad-hoc search. Recent years have witnessed considerable\nprogress in conversational search (CS). Effective QPP could help a CS system to\ndecide an appropriate action to be taken at the next turn. Despite its\npotential, QPP for CS has been little studied. We address this research gap by\nreproducing and studying the effectiveness of existing QPP methods in the\ncontext of CS. While the task of passage retrieval remains the same in the two\nsettings, a user query in CS depends on the conversational history, introducing\nnovel QPP challenges. In particular, we seek to explore to what extent findings\nfrom QPP methods for ad-hoc search generalize to three CS settings: (i)\nestimating the retrieval quality of different query rewriting-based retrieval\nmethods, (ii) estimating the retrieval quality of a conversational dense\nretrieval method, and (iii) estimating the retrieval quality for top ranks vs.\ndeeper-ranked lists. Our findings can be summarized as follows: (i) supervised\nQPP methods distinctly outperform unsupervised counterparts only when a\nlarge-scale training set is available; (ii) point-wise supervised QPP methods\noutperform their list-wise counterparts in most cases; and (iii) retrieval\nscore-based unsupervised QPP methods show high effectiveness in assessing the\nconversational dense retrieval method, ConvDR.",
        "translated": "查询性能预测是信息检索的核心任务。QPP 任务是在没有相关性判断的情况下预测查询检索系统的检索质量。研究表明 QPP 在自组织搜索中的有效性和实用性。近年来，会话搜索取得了长足的进步。有效的质量保证计划可以帮助 CS 系统决定下一轮要采取的适当行动。尽管 QPP 具有很大的潜力，但是对它的研究还很少。我们通过再现和研究现有的 QPP 方法在 CS 背景下的有效性来弥补这一研究差距。虽然在这两种情况下，文章检索的任务是相同的，但用户在 CS 中的查询依赖于会话历史，引入了新的 QPP 挑战。特别是，我们试图探索用于特别搜索的 QPP 方法的结果在多大程度上概括为三种 CS 设置: (i)估计不同基于查询重写的检索方法的检索质量，(ii)估计会话密集检索方法的检索质量，以及(iii)估计顶级与更深级列表的检索质量。我们的研究结果可以总结如下: (i)监督 QPP 方法只有在大规模训练集可用时才明显优于无监督的对应方法; (ii)点式监督 QPP 方法在大多数情况下优于其列表式对应方法; 和(iii)基于检索评分的无监督 QPP 方法在评估会话密集检索方法，ConvDR 方面显示出高效性。"
    },
    {
        "title": "Adaptive Graph Contrastive Learning for Recommendation",
        "url": "http://arxiv.org/abs/2305.10837v1",
        "pub_date": "2023-05-18",
        "summary": "Recently, graph neural networks (GNNs) have been successfully applied to\nrecommender systems as an effective collaborative filtering (CF) approach. The\nkey idea of GNN-based recommender system is to recursively perform the message\npassing along the user-item interaction edge for refining the encoded\nembeddings, relying on sufficient and high-quality training data. Since user\nbehavior data in practical recommendation scenarios is often noisy and exhibits\nskewed distribution, some recommendation approaches, e.g., SGL and SimGCL,\nleverage self-supervised learning to improve user representations against the\nabove issues. Despite their effectiveness, however, they conduct\nself-supervised learning through creating contrastvie views, depending on the\nexploration of data augmentations with the problem of tedious trial-and-error\nselection of augmentation methods. In this paper, we propose a novel Adaptive\nGraph Contrastive Learning (AdaptiveGCL) framework which conducts graph\ncontrastive learning with two adaptive contrastive view generators to better\nempower CF paradigm. Specifically, we use two trainable view generators, which\nare a graph generative model and a graph denoising model respectively, to\ncreate contrastive views. Two generators are able to create adaptive\ncontrastive views, addressing the problem of model collapse and achieving\nadaptive contrastive learning. With two adaptive contrasive views, more\nadditionally high-quality training signals will be introduced into the CF\nparadigm and help to alleviate the data sparsity and noise issues. Extensive\nexperiments on three benchmark datasets demonstrate the superiority of our\nmodel over various state-of-the-art recommendation methods. Further visual\nanalysis intuitively explains why our AdaptiveGCL outperforms existing\ncontrastive learning approaches based on selected data augmentation methods.",
        "translated": "最近，图形神经网络(GNN)已成功应用于推荐系统，作为一种有效的协同过滤(CF)方法。基于 GNN 的推荐系统的关键思想是依靠充分和高质量的训练数据，递归地执行沿用户项目交互边缘传递的消息，以完善编码的嵌入。由于实际推荐场景中的用户行为数据通常是有噪音的，并且呈现出倾斜的分布，因此一些推荐方法，如 SGL 和 SimGCL，利用自监督学习来改善用户对上述问题的表示。然而，尽管他们的有效性，他们进行自我监督学习通过创建对比观点，依赖于探索数据增强与繁琐的试错选择增强方法的问题。本文提出了一种新的自适应图形对比学习(AdaptiveGCL)框架，该框架使用两个自适应对比视图生成器进行图形对比学习，以更好地支持 CF 范式。具体来说，我们使用两个可训练的视图生成器，分别是一个图形生成模型和一个图形去噪模型，来创建对比视图。两个生成器能够创建自适应对比视图，解决模型崩溃问题，实现自适应对比学习。通过两个自适应对立视图，在 CF 范式中引入更多高质量的训练信号，有助于缓解数据稀疏和噪声问题。在三个基准数据集上的大量实验证明了我们的模型优于各种最先进的推荐方法。进一步的可视化分析直观地解释了为什么我们的 AdaptiveGCL 优于基于所选数据增强方法的现有对比学习方法。"
    },
    {
        "title": "Integrating Item Relevance in Training Loss for Sequential Recommender\n  Systems",
        "url": "http://arxiv.org/abs/2305.10824v1",
        "pub_date": "2023-05-18",
        "summary": "Sequential Recommender Systems (SRSs) are a popular type of recommender\nsystem that learns from a user's history to predict the next item they are\nlikely to interact with. However, user interactions can be affected by noise\nstemming from account sharing, inconsistent preferences, or accidental clicks.\nTo address this issue, we (i) propose a new evaluation protocol that takes\nmultiple future items into account and (ii) introduce a novel relevance-aware\nloss function to train a SRS with multiple future items to make it more robust\nto noise. Our relevance-aware models obtain an improvement of ~1.2% of NDCG@10\nand 0.88% in the traditional evaluation protocol, while in the new evaluation\nprotocol, the improvement is ~1.63% of NDCG@10 and ~1.5% of HR w.r.t the best\nperforming models.",
        "translated": "顺序推荐系统(SRSs)是一种流行的推荐系统，它可以从用户的历史中学习，预测他们可能接触到的下一个项目。然而，用户交互可能受到来自帐户共享、不一致的首选项或偶然点击的噪音的影响。为了解决这个问题，我们(i)提出了一个新的评估协议，考虑到多个未来项目，并且(ii)引入一个新的相关性感知损失函数来训练具有多个未来项目的 SRS，以使其对噪声更加鲁棒。我们的相关意识模型在传统的评估方案中获得了? 1.2% 的 NDCG@10和0.88% 的改善，而在新的评估方案中，改善是? 1.63% 的 NDCG@10和? 1.5% 的最佳表现模型。"
    },
    {
        "title": "When Search Meets Recommendation: Learning Disentangled Search\n  Representation for Recommendation",
        "url": "http://arxiv.org/abs/2305.10822v1",
        "pub_date": "2023-05-18",
        "summary": "Modern online service providers such as online shopping platforms often\nprovide both search and recommendation (S&amp;R) services to meet different user\nneeds. Rarely has there been any effective means of incorporating user behavior\ndata from both S&amp;R services. Most existing approaches either simply treat S&amp;R\nbehaviors separately, or jointly optimize them by aggregating data from both\nservices, ignoring the fact that user intents in S&amp;R can be distinctively\ndifferent. In our paper, we propose a Search-Enhanced framework for the\nSequential Recommendation (SESRec) that leverages users' search interests for\nrecommendation, by disentangling similar and dissimilar representations within\nS&amp;R behaviors. Specifically, SESRec first aligns query and item embeddings\nbased on users' query-item interactions for the computations of their\nsimilarities. Two transformer encoders are used to learn the contextual\nrepresentations of S&amp;R behaviors independently. Then a contrastive learning\ntask is designed to supervise the disentanglement of similar and dissimilar\nrepresentations from behavior sequences of S&amp;R. Finally, we extract user\ninterests by the attention mechanism from three perspectives, i.e., the\ncontextual representations, the two separated behaviors containing similar and\ndissimilar interests. Extensive experiments on both industrial and public\ndatasets demonstrate that SESRec consistently outperforms state-of-the-art\nmodels. Empirical studies further validate that SESRec successfully disentangle\nsimilar and dissimilar user interests from their S&amp;R behaviors.",
        "translated": "现代在线服务提供商，如在线购物平台，经常同时提供搜索和推荐(S & R)服务，以满足不同的用户需求。很少有任何有效的方法来整合来自 S & R 服务的用户行为数据。大多数现有的方法要么单独处理 S & R 行为，要么通过聚合来自两个服务的数据来共同优化它们，忽略了 S & R 中的用户意图可能截然不同的事实。在我们的论文中，我们提出了一个搜索增强的序列推荐框架(SESRec) ，它利用用户的搜索兴趣进行推荐，通过在 S & R 行为中分离相似和不相似的表示。具体来说，SESRec 首先根据用户的查询-项交互对查询和项嵌入进行对齐，以计算它们的相似性。使用两个变压器编码器分别学习 S & R 行为的上下文表示。然后设计了一个对比学习任务来监督 S & R 行为序列中相似和不相似表征的分离。最后，通过注意机制从三个方面提取用户的兴趣，即语境表征，两个分离的具有相似兴趣和不同兴趣的行为。在工业和公共数据集上的大量实验表明，SESRec 始终优于最先进的模型。实证研究进一步验证了 SESRec 成功地将相似和不同的用户兴趣从他们的 S & R 行为中分离出来。"
    },
    {
        "title": "How Does Generative Retrieval Scale to Millions of Passages?",
        "url": "http://arxiv.org/abs/2305.11841v1",
        "pub_date": "2023-05-19",
        "summary": "Popularized by the Differentiable Search Index, the emerging paradigm of\ngenerative retrieval re-frames the classic information retrieval problem into a\nsequence-to-sequence modeling task, forgoing external indices and encoding an\nentire document corpus within a single Transformer. Although many different\napproaches have been proposed to improve the effectiveness of generative\nretrieval, they have only been evaluated on document corpora on the order of\n100k in size. We conduct the first empirical study of generative retrieval\ntechniques across various corpus scales, ultimately scaling up to the entire MS\nMARCO passage ranking task with a corpus of 8.8M passages and evaluating model\nsizes up to 11B parameters. We uncover several findings about scaling\ngenerative retrieval to millions of passages; notably, the central importance\nof using synthetic queries as document representations during indexing, the\nineffectiveness of existing proposed architecture modifications when accounting\nfor compute cost, and the limits of naively scaling model parameters with\nrespect to retrieval performance. While we find that generative retrieval is\ncompetitive with state-of-the-art dual encoders on small corpora, scaling to\nmillions of passages remains an important and unsolved challenge. We believe\nthese findings will be valuable for the community to clarify the current state\nof generative retrieval, highlight the unique challenges, and inspire new\nresearch directions.",
        "translated": "由于可分辨搜索索引的普及，新兴的生成检索范式将经典的信息检索问题重新定义为一个序列到序列的建模任务，放弃外部索引，并在一个 former 中编码整个文档语料库。为了提高生成检索的有效性，人们提出了许多不同的方法，但这些方法在文献语料库中的检索效果只有10万次左右。我们进行了第一次实证研究的生成检索技术在不同的语料库尺度，最终扩大到整个 MS MARCO 段落排序任务与8.8 M 段落的语料库和评估模型大小高达11B 参数。我们发现了关于将生成性检索扩展到数百万段的一些发现; 值得注意的是，在索引过程中使用合成查询作为文档表示的核心重要性，在计算计算成本时现有提议的架构修改的无效性，以及天真地扩展模型参数对检索性能的限制。虽然我们发现生成检索与小型语料库上最先进的双编码器相比具有竞争力，但是扩展到数百万段仍然是一个重要的未解决的挑战。我们相信这些研究结果将有助于社区阐明生成性检索的现状，突出独特的挑战，并启发新的研究方向。"
    },
    {
        "title": "Visualization for Recommendation Explainability: A Survey and New\n  Perspectives",
        "url": "http://arxiv.org/abs/2305.11755v1",
        "pub_date": "2023-05-19",
        "summary": "Providing system-generated explanations for recommendations represents an\nimportant step towards transparent and trustworthy recommender systems.\nExplainable recommender systems provide a human-understandable rationale for\ntheir outputs. Over the last two decades, explainable recommendation has\nattracted much attention in the recommender systems research community. This\npaper aims to provide a comprehensive review of research efforts on visual\nexplanation in recommender systems. More concretely, we systematically review\nthe literature on explanations in recommender systems based on four dimensions,\nnamely explanation goal, explanation scope, explanation style, and explanation\nformat. Recognizing the importance of visualization, we approach the\nrecommender system literature from the angle of explanatory visualizations,\nthat is using visualizations as a display style of explanation. As a result, we\nderive a set of guidelines that might be constructive for designing explanatory\nvisualizations in recommender systems and identify perspectives for future work\nin this field. The aim of this review is to help recommendation researchers and\npractitioners better understand the potential of visually explainable\nrecommendation research and to support them in the systematic design of visual\nexplanations in current and future recommender systems.",
        "translated": "对建议提供系统生成的解释是朝向透明和可靠的推荐系统迈出的重要一步。可解释的推荐系统为其输出提供了一个人类可理解的理由。在过去的二十年中，可解释的推荐引起了推荐系统研究界的广泛关注。本文旨在对推荐系统中视觉解释的研究工作进行综述。更具体地，我们从解释目的、解释范围、解释风格和解释格式四个维度系统地回顾了关于推荐系统中解释的文献。认识到可视化的重要性，我们从解释性可视化的角度来看待推荐系统文献，即使用可视化作为一种解释的显示方式。因此，我们得出了一套指导方针，可能是建设性的设计解释性可视化在推荐系统，并确定未来的工作在这一领域的前景。本综述的目的是帮助推荐研究人员和从业人员更好地理解可视化解释推荐研究的潜力，并支持他们在目前和未来的推荐系统中系统地设计可视化解释。"
    },
    {
        "title": "Inference-time Re-ranker Relevance Feedback for Neural Information\n  Retrieval",
        "url": "http://arxiv.org/abs/2305.11744v1",
        "pub_date": "2023-05-19",
        "summary": "Neural information retrieval often adopts a retrieve-and-rerank framework: a\nbi-encoder network first retrieves K (e.g., 100) candidates that are then\nre-ranked using a more powerful cross-encoder model to rank the better\ncandidates higher. The re-ranker generally produces better candidate scores\nthan the retriever, but is limited to seeing only the top K retrieved\ncandidates, thus providing no improvements in retrieval performance as measured\nby Recall@K. In this work, we leverage the re-ranker to also improve retrieval\nby providing inference-time relevance feedback to the retriever. Concretely, we\nupdate the retriever's query representation for a test instance using a\nlightweight inference-time distillation of the re-ranker's prediction for that\ninstance. The distillation loss is designed to bring the retriever's candidate\nscores closer to those of the re-ranker. A second retrieval step is then\nperformed with the updated query vector. We empirically show that our approach,\nwhich can serve arbitrary retrieve-and-rerank pipelines, significantly improves\nretrieval recall in multiple domains, languages, and modalities.",
        "translated": "神经信息检索通常采用一个检索-重新排序框架: 一个双编码器网络首先检索 K (例如，100)候选人，然后使用一个更强大的交叉编码器模型重新排序，以排序更好的候选人更高。重新排名通常比检索器产生更好的候选人分数，但是仅限于看到被检索的最高 K 的候选人，因此没有提供根据 Recall@K 测量的检索性能的改善。在这项工作中，我们利用重新排名也提高检索提供推理时间关联反馈的检索。具体来说，我们使用重新排序器对测试实例的预测的轻量级推理时间精馏来更新检索器对该实例的查询表示。蒸馏损失的目的是使猎犬的候选分数更接近那些重新排名。然后使用更新的查询向量执行第二个检索步骤。我们的实验表明，我们的方法，可以服务任意的检索和重新排序管道，显着提高检索召回在多个领域，语言和模式。"
    },
    {
        "title": "Exploring the Upper Limits of Text-Based Collaborative Filtering Using\n  Large Language Models: Discoveries and Insights",
        "url": "http://arxiv.org/abs/2305.11700v1",
        "pub_date": "2023-05-19",
        "summary": "Text-based collaborative filtering (TCF) has become the mainstream approach\nfor text and news recommendation, utilizing text encoders, also known as\nlanguage models (LMs), to represent items. However, existing TCF models\nprimarily focus on using small or medium-sized LMs. It remains uncertain what\nimpact replacing the item encoder with one of the largest and most powerful\nLMs, such as the 175-billion parameter GPT-3 model, would have on\nrecommendation performance. Can we expect unprecedented results? To this end,\nwe conduct an extensive series of experiments aimed at exploring the\nperformance limits of the TCF paradigm. Specifically, we increase the size of\nitem encoders from one hundred million to one hundred billion to reveal the\nscaling limits of the TCF paradigm. We then examine whether these extremely\nlarge LMs could enable a universal item representation for the recommendation\ntask. Furthermore, we compare the performance of the TCF paradigm utilizing the\nmost powerful LMs to the currently dominant ID embedding-based paradigm and\ninvestigate the transferability of this TCF paradigm. Finally, we compare TCF\nwith the recently popularized prompt-based recommendation using ChatGPT. Our\nresearch findings have not only yielded positive results but also uncovered\nsome surprising and previously unknown negative outcomes, which can inspire\ndeeper reflection and innovative thinking regarding text-based recommender\nsystems. Codes and datasets will be released for further research.",
        "translated": "基于文本的协同过滤(TCF)已经成为文本和新闻推荐的主流方法，利用文本编码器(也称为语言模型(LMs))来表示项目。然而，现有的 TCF 模型主要侧重于使用中小型 LM。用最大最强的 LM (比如1750亿参数的 GPT-3模型)替换项目编码器会对推荐性能产生什么影响还不确定。我们能期待前所未有的结果吗？为此，我们进行了一系列广泛的实验，旨在探索 TCF 范式的性能极限。具体来说，我们将条目编码器的大小从1亿增加到1亿，以揭示 TCF 范例的伸缩限制。然后，我们检查这些极大的 LM 是否能够为推荐任务启用通用项表示。此外，我们比较了使用最强大的 LM 的 TCF 范式和目前占主导地位的基于 ID 嵌入的 TCF 范式的性能，并研究了这种 TCF 范式的可转移性。最后，我们使用 ChatGPT 比较 TCF 和最近推广的基于提示的推荐。我们的研究结果不仅产生了积极的结果，而且还揭示了一些令人惊讶的和以前未知的负面结果，这可以激发关于基于文本的推荐系统的更深层次的反思和创新思维。代码和数据集将被发布用于进一步的研究。"
    },
    {
        "title": "The Barriers to Online Clothing Websites for Visually Impaired People:\n  An Interview and Observation Approach to Understanding Needs",
        "url": "http://arxiv.org/abs/2305.11559v1",
        "pub_date": "2023-05-19",
        "summary": "Visually impaired (VI) people often face challenges when performing everyday\ntasks and identify shopping for clothes as one of the most challenging. Many\nengage in online shopping, which eliminates some challenges of physical\nshopping. However, clothes shopping online suffers from many other limitations\nand barriers. More research is needed to address these challenges, and extant\nworks often base their findings on interviews alone, providing only subjective,\nrecall-biased information. We conducted two complementary studies using both\nobservational and interview approaches to fill a gap in understanding about VI\npeople's behaviour when selecting and purchasing clothes online. Our findings\nshow that shopping websites suffer from inaccurate, misleading, and\ncontradictory clothing descriptions; that VI people mainly rely on (unreliable)\nsearch tools and check product descriptions by reviewing customer comments. Our\nfindings also indicate that VI people are hesitant to accept assistance from\nautomated, but that trust in such systems could be improved if researchers can\ndevelop systems that better accommodate users' needs and preferences.",
        "translated": "视力受损者在日常工作中经常面临挑战，购买衣服是最具挑战性的工作之一。许多人从事网上购物，这消除了实体购物的一些挑战。然而，网上购物的服装受到许多其他限制和障碍。需要更多的研究来解决这些挑战，现存的工作往往基于他们的调查结果仅仅访谈，只提供主观的，回忆偏见的信息。我们使用观察和访谈的方法进行了两个互补的研究，以填补在线选择和购买服装时对 VI 人群行为的理解差距。我们的研究结果表明，购物网站容易受到不准确、误导和自相矛盾的服装描述的困扰; VI 用户主要依靠(不可靠的)搜索工具，通过查看顾客评论来检查产品描述。我们的研究结果还表明，VI 人士不愿意接受自动化系统的帮助，但如果研究人员能够开发出更好地适应用户需求和偏好的系统，那么对这种系统的信任就可以得到改善。"
    },
    {
        "title": "InstructIE: A Chinese Instruction-based Information Extraction Dataset",
        "url": "http://arxiv.org/abs/2305.11527v1",
        "pub_date": "2023-05-19",
        "summary": "We introduce a new Information Extraction (IE) task dubbed Instruction-based\nIE, which aims to ask the system to follow specific instructions or guidelines\nto extract information. To facilitate research in this area, we construct a\ndataset called InstructIE, consisting of 270,000 weakly supervised data from\nChinese Wikipedia and 1,000 high-quality crowdsourced annotated instances. We\nfurther evaluate the performance of various baseline models on the InstructIE\ndataset. The results reveal that although current models exhibit promising\nperformance, there is still room for improvement. Furthermore, we conduct a\ncomprehensive case study analysis, underlining the challenges inherent in the\nInstruction-based IE task. Code and dataset are available at\nhttps://github.com/zjunlp/DeepKE/tree/main/example/llm.",
        "translated": "我们引入了一个新的基于指令的信息抽取(IE)任务，其目的是要求系统遵循特定的指令或指南来提取信息。为了促进这一领域的研究，我们构建了一个名为 DirectIE 的数据集，由来自中文维基百科的270,000个弱监督数据和1000个高质量的众包注释实例组成。我们进一步评估了各种基线模型在 DirectIE 数据集上的性能。结果表明，虽然目前的模型表现出良好的性能，仍然有改进的空间。此外，我们进行了一个全面的案例研究分析，强调了基于教学的 IE 任务固有的挑战。代码和数据集可在 https://github.com/zjunlp/deepke/tree/main/example/llm 下载。"
    },
    {
        "title": "Recouple Event Field via Probabilistic Bias for Event Extraction",
        "url": "http://arxiv.org/abs/2305.11498v1",
        "pub_date": "2023-05-19",
        "summary": "Event Extraction (EE), aiming to identify and classify event triggers and\narguments from event mentions, has benefited from pre-trained language models\n(PLMs). However, existing PLM-based methods ignore the information of\ntrigger/argument fields, which is crucial for understanding event schemas. To\nthis end, we propose a Probabilistic reCoupling model enhanced Event extraction\nframework (ProCE). Specifically, we first model the syntactic-related event\nfields as probabilistic biases, to clarify the event fields from ambiguous\nentanglement. Furthermore, considering multiple occurrences of the same\ntriggers/arguments in EE, we explore probabilistic interaction strategies among\nmultiple fields of the same triggers/arguments, to recouple the corresponding\nclarified distributions and capture more latent information fields. Experiments\non EE datasets demonstrate the effectiveness and generalization of our proposed\napproach.",
        "translated": "事件提取(EE) ，旨在从事件提及中识别和分类事件触发器和参数，已经受益于预训练语言模型(PLM)。然而，现有的基于 PLM 的方法忽略了触发器/参数字段的信息，这对于理解事件模式是至关重要的。为此，我们提出了一个概率重耦合模型增强的事件抽取框架(ProCE)。具体来说，我们首先将与句法相关的事件字段建模为概率偏差，以澄清来自模糊纠缠的事件字段。此外，考虑到同一触发器/参数在 EE 中的多次出现，我们探索了同一触发器/参数的多个域之间的概率交互策略，以重新耦合相应的澄清分布并捕获更多的潜在信息域。在 EE 数据集上的实验证明了该方法的有效性和推广性。"
    },
    {
        "title": "TELeR: A General Taxonomy of LLM Prompts for Benchmarking Complex Tasks",
        "url": "http://arxiv.org/abs/2305.11430v1",
        "pub_date": "2023-05-19",
        "summary": "While LLMs have shown great success in understanding and generating text in\ntraditional conversational settings, their potential for performing ill-defined\ncomplex tasks is largely under-studied. Indeed, we are yet to conduct\ncomprehensive benchmarking studies with multiple LLMs that are exclusively\nfocused on a complex task. However, conducting such benchmarking studies is\nchallenging because of the large variations in LLMs' performance when different\nprompt types/styles are used and different degrees of detail are provided in\nthe prompts. To address this issue, the paper proposes a general taxonomy that\ncan be used to design prompts with specific properties in order to perform a\nwide range of complex tasks. This taxonomy will allow future benchmarking\nstudies to report the specific categories of prompts used as part of the study,\nenabling meaningful comparisons across different studies. Also, by establishing\na common standard through this taxonomy, researchers will be able to draw more\naccurate conclusions about LLMs' performance on a specific complex task.",
        "translated": "虽然 LLM 在理解和生成传统会话环境中的文本方面取得了巨大的成功，但它们执行定义不清的复杂任务的潜力在很大程度上还没有得到充分的研究。事实上，我们还没有进行全面的基准研究与多个 LLM，专门集中在一个复杂的任务。然而，进行这样的基准测试研究是具有挑战性的，因为当使用不同的提示类型/风格和提示中提供不同程度的细节时，LLM 的性能有很大的差异。为了解决这个问题，本文提出了一个通用分类法，可用于设计具有特定属性的提示符，以便执行范围广泛的复杂任务。这种分类法将使未来的基准研究能够报告作为研究的一部分使用的特定类别的提示，使不同研究之间的有意义的比较成为可能。此外，通过建立一个共同的标准通过这个分类，研究人员将能够得出更准确的结论 LLM 的表现在一个特定的复杂的任务。"
    },
    {
        "title": "Online Learning in a Creator Economy",
        "url": "http://arxiv.org/abs/2305.11381v1",
        "pub_date": "2023-05-19",
        "summary": "The creator economy has revolutionized the way individuals can profit through\nonline platforms. In this paper, we initiate the study of online learning in\nthe creator economy by modeling the creator economy as a three-party game\nbetween the users, platform, and content creators, with the platform\ninteracting with the content creator under a principal-agent model through\ncontracts to encourage better content. Additionally, the platform interacts\nwith the users to recommend new content, receive an evaluation, and ultimately\nprofit from the content, which can be modeled as a recommender system.\n  Our study aims to explore how the platform can jointly optimize the contract\nand recommender system to maximize the utility in an online learning fashion.\nWe primarily analyze and compare two families of contracts: return-based\ncontracts and feature-based contracts. Return-based contracts pay the content\ncreator a fraction of the reward the platform gains. In contrast, feature-based\ncontracts pay the content creator based on the quality or features of the\ncontent, regardless of the reward the platform receives. We show that under\nsmoothness assumptions, the joint optimization of return-based contracts and\nrecommendation policy provides a regret $\\Theta(T^{2/3})$. For the\nfeature-based contract, we introduce a definition of intrinsic dimension $d$ to\ncharacterize the hardness of learning the contract and provide an upper bound\non the regret $\\mathcal{O}(T^{(d+1)/(d+2)})$. The upper bound is tight for the\nlinear family.",
        "translated": "创造者经济彻底改变了个人通过在线平台获利的方式。本文通过将创造者经济建模为用户、平台和内容创造者之间的三方博弈，平台与内容创造者在委托-代理模式下通过契约互动来鼓励更好的内容，开展了创造者经济中在线学习的研究。此外，该平台与用户互动，推荐新内容，接受评估，并最终从内容中获利，这些内容可以被建模为推荐系统。我们的研究旨在探讨这个平台如何能够共同优化合同和推荐系统，以便在网上学习的模式中最大限度地发挥效用。我们主要分析和比较了两类契约: 基于回报的契约和基于特征的契约。基于回报的合同支付给内容创作者的报酬只是平台收益的一小部分。相比之下，基于特性的合同根据内容的质量或特性支付给内容创作者，而不管平台获得什么报酬。结果表明，在平滑假设下，基于回报的合同和推荐策略的联合优化得到了遗憾的 $Θ (T ^ {2/3}) $。对于基于特征的契约，我们引入了本征维度 $d $的定义来描述学习契约的难度，并给出了遗憾 $数学{ O }(T ^ {(d + 1)/(d + 2)}) $的上界。线性族的上界是紧的。"
    },
    {
        "title": "Copy Recurrent Neural Network Structure Network",
        "url": "http://arxiv.org/abs/2305.13250v1",
        "pub_date": "2023-05-22",
        "summary": "Electronic Health Record (EHR) coding involves automatically classifying EHRs\ninto diagnostic codes. While most previous research treats this as a\nmulti-label classification task, generating probabilities for each code and\nselecting those above a certain threshold as labels, these approaches often\noverlook the challenge of identifying complex diseases. In this study, our\nfocus is on detecting complication diseases within EHRs.\n  We propose a novel coarse-to-fine ICD path generation framework called the\nCopy Recurrent Neural Network Structure Network (CRNNet), which employs a Path\nGenerator (PG) and a Path Discriminator (PD) for EHR coding. By using RNNs to\ngenerate sequential outputs and incorporating a copy module, we efficiently\nidentify complication diseases. Our method achieves a 57.30\\% ratio of complex\ndiseases in predictions, outperforming state-of-the-art and previous\napproaches.\n  Additionally, through an ablation study, we demonstrate that the copy\nmechanism plays a crucial role in detecting complex diseases.",
        "translated": "电子健康记录(EHR)编码涉及将 EHR 自动分类为诊断代码。虽然大多数以前的研究将其视为一个多标签分类任务，为每个代码产生概率，并选择那些高于某个阈值的代码作为标签，但这些方法往往忽视了识别复杂疾病的挑战。在这项研究中，我们的重点是检测并发症的 EHR 疾病。我们提出了一个新的由粗到精的 ICD 路径生成框架，称为拷贝递归神经网络结构网络(crnNet) ，它使用路径生成器(PG)和路径鉴别器(PD)进行电子健康记录(eHR)编码。通过使用 RNN 产生序列输出和合并拷贝模块，我们有效地识别并发症疾病。我们的方法在预测复杂疾病方面达到了57.30% 的比例，超过了最先进的方法和以前的方法。此外，通过消融研究，我们证明复制机制在检测复杂疾病中起着至关重要的作用。"
    },
    {
        "title": "Challenging Decoder helps in Masked Auto-Encoder Pre-training for Dense\n  Passage Retrieval",
        "url": "http://arxiv.org/abs/2305.13197v1",
        "pub_date": "2023-05-22",
        "summary": "Recently, various studies have been directed towards exploring dense passage\nretrieval techniques employing pre-trained language models, among which the\nmasked auto-encoder (MAE) pre-training architecture has emerged as the most\npromising. The conventional MAE framework relies on leveraging the passage\nreconstruction of decoder to bolster the text representation ability of\nencoder, thereby enhancing the performance of resulting dense retrieval\nsystems. Within the context of building the representation ability of the\nencoder through passage reconstruction of decoder, it is reasonable to\npostulate that a ``more demanding'' decoder will necessitate a corresponding\nincrease in the encoder's ability. To this end, we propose a novel token\nimportance aware masking strategy based on pointwise mutual information to\nintensify the challenge of the decoder. Importantly, our approach can be\nimplemented in an unsupervised manner, without adding additional expenses to\nthe pre-training phase. Our experiments verify that the proposed method is both\neffective and robust on large-scale supervised passage retrieval datasets and\nout-of-domain zero-shot retrieval benchmarks.",
        "translated": "近年来，各种研究都致力于探索使用预训练语言模型的密集通道检索技术，其中掩蔽自动编码器(MAE)预训练结构是最有前途的一种。传统的 MAE 框架依赖于利用解码器的通道重构来增强编码器的文本表示能力，从而提高密集检索系统的性能。在通过解码器的通道重构来建立编码器的表示能力的背景下，假设“更高要求”的解码器需要相应提高编码器的表示能力是合理的。为此，我们提出了一种新颖的基于点间互信息的标记重要性感知掩蔽策略，以增强解码器的挑战性。重要的是，我们的方法可以在一个无监督的方式实施，而不增加额外的费用，预培训阶段。实验结果表明，该方法对于大规模有监督通道检索数据集和域外零镜头检索基准具有良好的鲁棒性和有效性。"
    },
    {
        "title": "TEIMMA: The First Content Reuse Annotator for Text, Images, and Math",
        "url": "http://arxiv.org/abs/2305.13193v1",
        "pub_date": "2023-05-22",
        "summary": "This demo paper presents the first tool to annotate the reuse of text,\nimages, and mathematical formulae in a document pair -- TEIMMA. Annotating\ncontent reuse is particularly useful to develop plagiarism detection\nalgorithms. Real-world content reuse is often obfuscated, which makes it\nchallenging to identify such cases. TEIMMA allows entering the obfuscation type\nto enable novel classifications for confirmed cases of plagiarism. It enables\nrecording different reuse types for text, images, and mathematical formulae in\nHTML and supports users by visualizing the content reuse in a document pair\nusing similarity detection methods for text and math.",
        "translated": "本演示文件介绍了第一个注释文本、图像和数学公式在文档对中的重用的工具—— TEIMMA。注释内容重用对于开发剽窃检测算法特别有用。真实世界的内容重用通常是模糊的，这使得识别这种情况变得很困难。TEIMMA 允许输入模糊类型，以便对确认的剽窃案件进行新的分类。它支持在 HTML 中记录文本、图像和数学公式的不同重用类型，并通过使用文本和数学的相似性检测方法可视化文档对中的内容重用来支持用户。"
    },
    {
        "title": "Editing Large Language Models: Problems, Methods, and Opportunities",
        "url": "http://arxiv.org/abs/2305.13172v1",
        "pub_date": "2023-05-22",
        "summary": "Recent advancements in deep learning have precipitated the emergence of large\nlanguage models (LLMs) which exhibit an impressive aptitude for understanding\nand producing text akin to human language. Despite the ability to train highly\ncapable LLMs, the methodology for maintaining their relevancy and rectifying\nerrors remains elusive. To that end, the past few years have witnessed a surge\nin techniques for editing LLMs, the objective of which is to alter the behavior\nof LLMs within a specific domain without negatively impacting performance\nacross other inputs. This paper embarks on a deep exploration of the problems,\nmethods, and opportunities relating to model editing for LLMs. In particular,\nwe provide an exhaustive overview of the task definition and challenges\nassociated with model editing, along with an in-depth empirical analysis of the\nmost progressive methods currently at our disposal. We also build a new\nbenchmark dataset to facilitate a more robust evaluation and pinpoint enduring\nissues intrinsic to existing techniques. Our objective is to provide valuable\ninsights into the effectiveness and feasibility of each model editing\ntechnique, thereby assisting the research community in making informed\ndecisions when choosing the most appropriate method for a specific task or\ncontext. Code and datasets will be available at\nhttps://github.com/zjunlp/EasyEdit.",
        "translated": "深度学习的最新进展催生了大型语言模型(LLM)的出现，它们在理解和产生类似于人类语言的文本方面表现出令人印象深刻的天赋。尽管有能力培训高能力的 LLM，但保持其相关性和纠正错误的方法仍然是难以捉摸的。为此，过去几年见证了 LLM 编辑技术的激增，其目标是改变特定领域内 LLM 的行为，而不会对其他输入的性能产生负面影响。本文对 LLM 模型编辑的问题、方法和机会进行了深入的探讨。特别是，我们提供了任务定义和与模型编辑有关的挑战的详尽概述，以及对我们目前掌握的最进步的方法的深入实证分析。我们还建立了一个新的基准数据集，以促进更强大的评估，并确定持久的问题内在的现有技术。我们的目标是提供有价值的见解，每个模型编辑技术的有效性和可行性，从而协助研究界作出知情的决定时，选择最适当的方法，具体的任务或背景。代码和数据集将在 https://github.com/zjunlp/easyedit 提供。"
    },
    {
        "title": "LLMs for Knowledge Graph Construction and Reasoning: Recent Capabilities\n  and Future Opportunities",
        "url": "http://arxiv.org/abs/2305.13168v1",
        "pub_date": "2023-05-22",
        "summary": "This paper presents an exhaustive quantitative and qualitative evaluation of\nLarge Language Models (LLMs) for Knowledge Graph (KG) construction and\nreasoning. We employ eight distinct datasets that encompass aspects including\nentity, relation and event extraction, link prediction, and question answering.\nEmpirically, our findings suggest that GPT-4 outperforms ChatGPT in the\nmajority of tasks and even surpasses fine-tuned models in certain reasoning and\nquestion-answering datasets. Moreover, our investigation extends to the\npotential generalization ability of LLMs for information extraction, which\nculminates in the presentation of the Virtual Knowledge Extraction task and the\ndevelopment of the VINE dataset. Drawing on these empirical findings, we\nfurther propose AutoKG, a multi-agent-based approach employing LLMs for KG\nconstruction and reasoning, which aims to chart the future of this field and\noffer exciting opportunities for advancement. We anticipate that our research\ncan provide invaluable insights for future undertakings of KG\\footnote{Code and\ndatasets will be available in https://github.com/zjunlp/AutoKG.",
        "translated": "本文对知识图(KG)构造和推理的大语言模型(LLM)进行了详尽的定量和定性评价。我们使用八个不同的数据集，包括实体、关系和事件提取、链接预测和问题回答。经验上，我们的研究结果表明，GPT-4在大多数任务中的表现优于 ChatGPT，甚至在某些推理和问答数据集中优于微调模型。此外，我们的研究扩展到 LLM 对信息抽取的潜在推广能力，最终导致虚拟知识提取任务的介绍和 VINE 数据集的开发。基于这些实证研究结果，我们进一步提出了 AutoKG，这是一种基于多主体的方法，利用 LLM 进行 KG 的构建和推理，旨在描绘这一领域的未来，并提供令人兴奋的发展机会。我们期望我们的研究能为幼稚园日后的工作提供宝贵的意见。{ https://github.com/zjunlp/autokg 及数据集将可供参考。"
    },
    {
        "title": "Rethinking the Evaluation for Conversational Recommendation in the Era\n  of Large Language Models",
        "url": "http://arxiv.org/abs/2305.13112v1",
        "pub_date": "2023-05-22",
        "summary": "The recent success of large language models (LLMs) has shown great potential\nto develop more powerful conversational recommender systems (CRSs), which rely\non natural language conversations to satisfy user needs. In this paper, we\nembark on an investigation into the utilization of ChatGPT for conversational\nrecommendation, revealing the inadequacy of the existing evaluation protocol.\nIt might over-emphasize the matching with the ground-truth items or utterances\ngenerated by human annotators, while neglecting the interactive nature of being\na capable CRS. To overcome the limitation, we further propose an interactive\nEvaluation approach based on LLMs named iEvaLM that harnesses LLM-based user\nsimulators. Our evaluation approach can simulate various interaction scenarios\nbetween users and systems. Through the experiments on two publicly available\nCRS datasets, we demonstrate notable improvements compared to the prevailing\nevaluation protocol. Furthermore, we emphasize the evaluation of\nexplainability, and ChatGPT showcases persuasive explanation generation for its\nrecommendations. Our study contributes to a deeper comprehension of the\nuntapped potential of LLMs for CRSs and provides a more flexible and\neasy-to-use evaluation framework for future research endeavors. The codes and\ndata are publicly available at https://github.com/RUCAIBox/iEvaLM-CRS.",
        "translated": "大型语言模型(LLM)最近的成功显示了开发更强大的会话推荐系统(CRS)的巨大潜力，CRS 依赖于自然语言会话来满足用户的需求。本文对 ChatGPT 在会话推荐中的应用进行了研究，揭示了现有评价协议的不足之处。它可能过分强调与地面真相项目或人类注释者产生的话语的匹配，而忽视了作为一个有能力的 CRS 的互动性质。为了克服这一局限性，我们进一步提出了一种基于 LLM 的交互式评估方法 iEvaLM，该方法利用了基于 LLM 的用户模拟器。我们的评估方法可以模拟用户和系统之间的各种交互场景。通过对两个公开可用的 CRS 数据集的实验，我们发现与现行的评估协议相比有显著的改进。此外，我们强调可解释性的评价，ChatGPT 展示了其建议的说服性解释生成。我们的研究有助于更深入地了解 LLM 在 CRS 中尚未开发的潜力，并为未来的研究工作提供了一个更加灵活和易于使用的评估框架。这些代码和数据可以在 https://github.com/rucaibox/ievalm-crs 上公开获得。"
    },
    {
        "title": "Making Language Models Better Tool Learners with Execution Feedback",
        "url": "http://arxiv.org/abs/2305.13068v1",
        "pub_date": "2023-05-22",
        "summary": "Tools serve as pivotal interfaces that enable humans to understand and\nreshape the world. With the advent of foundational models, AI systems can\nutilize tools to expand their capabilities and interact with the world.\nExisting tool learning methodologies, encompassing supervised fine-tuning and\nprompt engineering approaches, often induce language models to utilize tools\nindiscriminately, as complex problems often exceed their own competencies.\nHowever, introducing tools for simple tasks, which the models themselves can\nreadily resolve, can inadvertently propagate errors rather than enhance\nperformance. This leads to the research question: can we teach language models\nwhen and how to use tools? To meet this need, we propose Tool leaRning wIth\nexeCution fEedback (TRICE), a two-stage end-to-end framework that enables the\nmodel to continually learn through feedback derived from tool execution,\nthereby learning when and how to use tools effectively. Experimental results,\nbacked by further analysis, show that TRICE can make the language model to\nselectively use tools by decreasing the model's dependency on tools while\nenhancing the performance. Code and datasets will be available in\nhttps://github.com/zjunlp/trice.",
        "translated": "工具作为关键的接口，使人类能够理解和重塑世界。随着基础模型的出现，人工智能系统可以利用工具来扩展它们的能力，并与世界互动。现有的工具学习方法，包括有监督的微调和及时的工程方法，经常诱导语言模型不加区分地使用工具，因为复杂的问题往往超出了它们自己的能力。然而，引入用于简单任务的工具(模型本身可以很容易地解决这些任务)可能会在无意中传播错误，而不是提高性能。这就引出了一个研究问题: 我们可以教语言模型何时以及如何使用工具吗？为了满足这一需求，我们提出工具学习与执行反馈(TRICE) ，一个两阶段的端到端框架，使模型能够通过反馈不断学习从工具执行，从而学习何时和如何有效地使用工具。实验结果表明，TRICE 能够使语言模型有选择地使用工具，降低模型对工具的依赖性，同时提高语言的性能。代码和数据集将以 https://github.com/zjunlp/trice 形式提供。"
    },
    {
        "title": "Evaluating and Enhancing Structural Understanding Capabilities of Large\n  Language Models on Tables via Input Designs",
        "url": "http://arxiv.org/abs/2305.13062v1",
        "pub_date": "2023-05-22",
        "summary": "Large language models (LLMs) are becoming attractive as few-shot reasoners to\nsolve NL-related tasks. However, there is still much to be learned about how\nwell LLMs understand structured data, such as tables. While it is true that\ntables can be used as inputs to LLMs with serialization, there lack\ncomprehensive studies examining whether LLMs can truly comprehend such data. In\nthis paper we try to understand this by designing a benchmark to evaluate\nstructural understanding capabilities (SUC) of LLMs. The benchmark we create\nincludes seven tasks, each with their own unique challenges, e.g,, cell lookup,\nrow retrieval and size detection. We run a series of evaluations on GPT-3\nfamily models (e.g., text-davinci-003). We discover that the performance varied\ndepending on a number of input choices, including table input format, content\norder, role prompting and partition marks. Drawing from the insights gained\nthrough the benchmark evaluations, we then propose self-augmentation for\neffective structural prompting, e.g., critical value / range identification\nusing LLMs' internal knowledge. When combined with carefully chosen input\nchoices, these structural prompting methods lead to promising improvements in\nLLM performance on a variety of tabular tasks, e.g., TabFact($\\uparrow2.31\\%$),\nHybridQA($\\uparrow2.13\\%$), SQA($\\uparrow2.72\\%$), Feverous($\\uparrow0.84\\%$),\nand ToTTo($\\uparrow5.68\\%$). We believe our benchmark and proposed prompting\nmethods can serve as a simple yet generic selection for future research. The\ncode and data are released in\nhttps://anonymous.4open.science/r/StructuredLLM-76F3.",
        "translated": "大型语言模型(LLM)作为解决大型语言相关任务的少量推理工具正变得越来越有吸引力。然而，关于 LLM 如何很好地理解结构化数据(如表) ，还有很多东西需要学习。虽然表确实可以用作具有序列化的 LLM 的输入，但是缺乏全面的研究来检查 LLM 是否能够真正理解这些数据。在本文中，我们试图通过设计一个基准来评估 LLM 的结构理解能力(SUC)来理解这一点。我们创建的基准测试包括七个任务，每个任务都有其独特的挑战，例如，单元格查找、行检索和大小检测。我们对 GPT-3家族模型(例如 text-davinci-003)进行了一系列的评估。我们发现，性能取决于许多输入选择，包括表输入格式、内容顺序、角色提示和分区标记。根据基准评估所获得的见解，我们提出自我增强的有效结构激励，例如，利用 LLM 的内部知识识识别临界值/范围。当结合精心选择的输入选择时，这些结构化的提示方法导致了各种表格任务的 LLM 性能的有希望的改善，例如 TabFact ($uparrow2.31% $) ，HybridQA ($uparrow2.13% $) ，SQA ($uparrow2.72% $) ，Feverous ($uparrow0.84% $)和 ToTTo ($uparrow5.68% $)。我们相信，我们的基准和提议的激励方法可以作为一个简单而通用的选择，为未来的研究。代码和数据以 https://anonymous.4open.science/r/structuredllm-76f3的形式发布。"
    },
    {
        "title": "Attentive Graph-based Text-aware Preference Modeling for Top-N\n  Recommendation",
        "url": "http://arxiv.org/abs/2305.12976v1",
        "pub_date": "2023-05-22",
        "summary": "Textual data are commonly used as auxiliary information for modeling user\npreference nowadays. While many prior works utilize user reviews for rating\nprediction, few focus on top-N recommendation, and even few try to incorporate\nitem textual contents such as title and description. Though delivering\npromising performance for rating prediction, we empirically find that many\nreview-based models cannot perform comparably well on top-N recommendation.\nAlso, user reviews are not available in some recommendation scenarios, while\nitem textual contents are more prevalent. On the other hand, recent graph\nconvolutional network (GCN) based models demonstrate state-of-the-art\nperformance for top-N recommendation. Thus, in this work, we aim to further\nimprove top-N recommendation by effectively modeling both item textual content\nand high-order connectivity in user-item graph. We propose a new model named\nAttentive Graph-based Text-aware Recommendation Model (AGTM). Extensive\nexperiments are provided to justify the rationality and effectiveness of our\nmodel design.",
        "translated": "文本数据是当今建立用户偏好模型的常用辅助信息。虽然许多以前的作品利用用户评论进行评分预测，但很少关注前 N 名的推荐，甚至很少尝试合并项目文本内容，如标题和描述。通过实证研究，我们发现许多基于评论的模型在排名前 N 的推荐中表现不佳。此外，在某些推荐场景中，用户评论是不可用的，而项目文本内容更为普遍。另一方面，最近基于图卷积网络(GCN)的模型展示了最高 N 推荐的最新性能。因此，在本研究中，我们的目标是通过有效地建立用户项目图中项目文本内容和高阶连通性的模型，进一步改善最高 N 推荐。提出了一种基于注意图的文本感知推荐模型(AGTM)。通过大量实验验证了模型设计的合理性和有效性。"
    },
    {
        "title": "It's Enough: Relaxing Diagonal Constraints in Linear Autoencoders for\n  Recommendation",
        "url": "http://arxiv.org/abs/2305.12922v1",
        "pub_date": "2023-05-22",
        "summary": "Linear autoencoder models learn an item-to-item weight matrix via convex\noptimization with L2 regularization and zero-diagonal constraints. Despite\ntheir simplicity, they have shown remarkable performance compared to\nsophisticated non-linear models. This paper aims to theoretically understand\nthe properties of two terms in linear autoencoders. Through the lens of\nsingular value decomposition (SVD) and principal component analysis (PCA), it\nis revealed that L2 regularization enhances the impact of high-ranked PCs.\nMeanwhile, zero-diagonal constraints reduce the impact of low-ranked PCs,\nleading to performance degradation for unpopular items. Inspired by this\nanalysis, we propose simple-yet-effective linear autoencoder models using\ndiagonal inequality constraints, called Relaxed Linear AutoEncoder (RLAE) and\nRelaxed Denoising Linear AutoEncoder (RDLAE). We prove that they generalize\nlinear autoencoders by adjusting the degree of diagonal constraints.\nExperimental results demonstrate that our models are comparable or superior to\nstate-of-the-art linear and non-linear models on six benchmark datasets; they\nsignificantly improve the accuracy of long-tail items. These results also\nsupport our theoretical insights on regularization and diagonal constraints in\nlinear autoencoders.",
        "translated": "线性自动编码器模型通过 L2正则化和零对角约束的凸优化学习一个项目对项目的权重矩阵。尽管它们很简单，但与复杂的非线性模型相比，它们表现出了显著的性能。本文旨在从理论上理解线性自动编码器中两项的性质。通过奇异值分解(SVD)和主成分分析(PCA)透镜，我们发现二语正规化增强了高等级个人电脑的影响。与此同时，零对角线约束减少了低排名个人电脑的影响，导致不受欢迎项目的性能下降。受此分析的启发，我们提出了使用对角不等式约束的简单而有效的线性自动编码器模型，称为松弛线性自动编码器(RLAE)和松弛去噪线性自动编码器(RDLAE)。我们证明了它们通过调整对角线约束的程度来推广线性自动编码器。实验结果表明，在六个基准数据集上，我们的模型与最先进的线性和非线性模型具有可比性或优越性，它们显著提高了长尾项目的准确性。这些结果也支持我们对线性自动编码器的正则化和对角线约束的理论认识。"
    },
    {
        "title": "Anchor Prediction: Automatic Refinement of Internet Links",
        "url": "http://arxiv.org/abs/2305.14337v1",
        "pub_date": "2023-05-23",
        "summary": "Internet links enable users to deepen their understanding of a topic by\nproviding convenient access to related information. However, the majority of\nlinks are unanchored -- they link to a target webpage as a whole, and readers\nmay expend considerable effort localizing the specific parts of the target\nwebpage that enrich their understanding of the link's source context. To help\nreaders effectively find information in linked webpages, we introduce the task\nof anchor prediction, where the goal is to identify the specific part of the\nlinked target webpage that is most related to the source linking context. We\nrelease the AuthorAnchors dataset, a collection of 34K naturally-occurring\nanchored links, which reflect relevance judgments by the authors of the source\narticle. To model reader relevance judgments, we annotate and release\nReaderAnchors, an evaluation set of anchors that readers find useful. Our\nanalysis shows that effective anchor prediction often requires jointly\nreasoning over lengthy source and target webpages to determine their implicit\nrelations and identify parts of the target webpage that are related but not\nredundant. We benchmark a performant T5-based ranking approach to establish\nbaseline performance on the task, finding ample room for improvement.",
        "translated": ""
    },
    {
        "title": "VIP5: Towards Multimodal Foundation Models for Recommendation",
        "url": "http://arxiv.org/abs/2305.14302v1",
        "pub_date": "2023-05-23",
        "summary": "Computer Vision (CV), Natural Language Processing (NLP), and Recommender\nSystems (RecSys) are three prominent AI applications that have traditionally\ndeveloped independently, resulting in disparate modeling and engineering\nmethodologies. This has impeded the ability for these fields to directly\nbenefit from each other's advancements. With the increasing availability of\nmultimodal data on the web, there is a growing need to consider various\nmodalities when making recommendations for users. With the recent emergence of\nfoundation models, large language models have emerged as a potential\ngeneral-purpose interface for unifying different modalities and problem\nformulations. In light of this, we propose the development of a multimodal\nfoundation model by considering both visual and textual modalities under the P5\nrecommendation paradigm (VIP5) to unify various modalities and recommendation\ntasks. This will enable the processing of vision, language, and personalization\ninformation in a shared architecture for improved recommendations. To achieve\nthis, we introduce multimodal personalized prompts to accommodate multiple\nmodalities under a shared format. Additionally, we propose a\nparameter-efficient training method for foundation models, which involves\nfreezing the backbone and fine-tuning lightweight adapters, resulting in\nimproved recommendation performance and increased efficiency in terms of\ntraining time and memory usage.",
        "translated": ""
    },
    {
        "title": "Simulating News Recommendation Ecosystem for Fun and Profit",
        "url": "http://arxiv.org/abs/2305.14103v1",
        "pub_date": "2023-05-23",
        "summary": "Understanding the evolution of online news communities is essential for\ndesigning more effective news recommender systems. However, due to the lack of\nappropriate datasets and platforms, the existing literature is limited in\nunderstanding the impact of recommender systems on this evolutionary process\nand the underlying mechanisms, resulting in sub-optimal system designs that may\naffect long-term utilities. In this work, we propose SimuLine, a simulation\nplatform to dissect the evolution of news recommendation ecosystems and present\na detailed analysis of the evolutionary process and underlying mechanisms.\nSimuLine first constructs a latent space well reflecting the human behaviors,\nand then simulates the news recommendation ecosystem via agent-based modeling.\nBased on extensive simulation experiments and the comprehensive analysis\nframework consisting of quantitative metrics, visualization, and textual\nexplanations, we analyze the characteristics of each evolutionary phase from\nthe perspective of life-cycle theory, and propose a relationship graph\nillustrating the key factors and affecting mechanisms. Furthermore, we explore\nthe impacts of recommender system designing strategies, including the\nutilization of cold-start news, breaking news, and promotion, on the\nevolutionary process, which shed new light on the design of recommender\nsystems.",
        "translated": ""
    },
    {
        "title": "BM25 Query Augmentation Learned End-to-End",
        "url": "http://arxiv.org/abs/2305.14087v1",
        "pub_date": "2023-05-23",
        "summary": "Given BM25's enduring competitiveness as an information retrieval baseline,\nwe investigate to what extent it can be even further improved by augmenting and\nre-weighting its sparse query-vector representation. We propose an approach to\nlearning an augmentation and a re-weighting end-to-end, and we find that our\napproach improves performance over BM25 while retaining its speed. We\nfurthermore find that the learned augmentations and re-weightings transfer well\nto unseen datasets.",
        "translated": ""
    },
    {
        "title": "Message Intercommunication for Inductive Relation Reasoning",
        "url": "http://arxiv.org/abs/2305.14074v1",
        "pub_date": "2023-05-23",
        "summary": "Inductive relation reasoning for knowledge graphs, aiming to infer missing\nlinks between brand-new entities, has drawn increasing attention. The models\ndeveloped based on Graph Inductive Learning, called GraIL-based models, have\nshown promising potential for this task. However, the uni-directional\nmessage-passing mechanism hinders such models from exploiting hidden mutual\nrelations between entities in directed graphs. Besides, the enclosing subgraph\nextraction in most GraIL-based models restricts the model from extracting\nenough discriminative information for reasoning. Consequently, the expressive\nability of these models is limited. To address the problems, we propose a novel\nGraIL-based inductive relation reasoning model, termed MINES, by introducing a\nMessage Intercommunication mechanism on the Neighbor-Enhanced Subgraph.\nConcretely, the message intercommunication mechanism is designed to capture the\nomitted hidden mutual information. It introduces bi-directed information\ninteractions between connected entities by inserting an undirected/bi-directed\nGCN layer between uni-directed RGCN layers. Moreover, inspired by the success\nof involving more neighbors in other graph-based tasks, we extend the\nneighborhood area beyond the enclosing subgraph to enhance the information\ncollection for inductive relation reasoning. Extensive experiments on twelve\ninductive benchmark datasets demonstrate that our MINES outperforms existing\nstate-of-the-art models, and show the effectiveness of our intercommunication\nmechanism and reasoning on the neighbor-enhanced subgraph.",
        "translated": ""
    },
    {
        "title": "When the Music Stops: Tip-of-the-Tongue Retrieval for Music",
        "url": "http://arxiv.org/abs/2305.14072v1",
        "pub_date": "2023-05-23",
        "summary": "We present a study of Tip-of-the-tongue (ToT) retrieval for music, where a\nsearcher is trying to find an existing music entity, but is unable to succeed\nas they cannot accurately recall important identifying information. ToT\ninformation needs are characterized by complexity, verbosity, uncertainty, and\npossible false memories. We make four contributions. (1) We collect a dataset -\n$ToT_{Music}$ - of 2,278 information needs and ground truth answers. (2) We\nintroduce a schema for these information needs and show that they often involve\nmultiple modalities encompassing several Music IR subtasks such as lyric\nsearch, audio-based search, audio fingerprinting, and text search. (3) We\nunderscore the difficulty of this task by benchmarking a standard text\nretrieval approach on this dataset. (4) We investigate the efficacy of query\nreformulations generated by a large language model (LLM), and show that they\nare not as effective as simply employing the entire information need as a query\n- leaving several open questions for future research.",
        "translated": ""
    },
    {
        "title": "DAPR: A Benchmark on Document-Aware Passage Retrieval",
        "url": "http://arxiv.org/abs/2305.13915v1",
        "pub_date": "2023-05-23",
        "summary": "Recent neural retrieval mainly focuses on ranking short texts and is\nchallenged with long documents. Existing work mainly evaluates either ranking\npassages or whole documents. However, there are many cases where the users want\nto find a relevant passage within a long document from a huge corpus, e.g.\nlegal cases, research papers, etc. In this scenario, the passage often provides\nlittle document context and thus challenges the current approaches to finding\nthe correct document and returning accurate results. To fill this gap, we\npropose and name this task Document-Aware Passage Retrieval (DAPR) and build a\nbenchmark including multiple datasets from various domains, covering both DAPR\nand whole-document retrieval. In experiments, we extend the state-of-the-art\nneural passage retrievers with document-level context via different approaches\nincluding prepending document summary, pooling over passage representations,\nand hybrid retrieval with BM25. The hybrid-retrieval systems, the overall best,\ncan only improve on the DAPR tasks marginally while significantly improving on\nthe document-retrieval tasks. This motivates further research in developing\nbetter retrieval systems for the new task. The code and the data are available\nat https://github.com/kwang2049/dapr",
        "translated": ""
    },
    {
        "title": "Term-Sets Can Be Strong Document Identifiers For Auto-Regressive Search\n  Engines",
        "url": "http://arxiv.org/abs/2305.13859v1",
        "pub_date": "2023-05-23",
        "summary": "Auto-regressive search engines emerge as a promising paradigm for next-gen\ninformation retrieval systems. These methods work with Seq2Seq models, where\neach query can be directly mapped to the identifier of its relevant document.\nAs such, they are praised for merits like being end-to-end differentiable.\nHowever, auto-regressive search engines also confront challenges in retrieval\nquality, given the requirement for the exact generation of the document\nidentifier. That's to say, the targeted document will be missed from the\nretrieval result if a false prediction about its identifier is made in any step\nof the generation process. In this work, we propose a novel framework, namely\nAutoTSG (Auto-regressive Search Engine with Term-Set Generation), which is\nfeatured by 1) the unordered term-based document identifier and 2) the\nset-oriented generation pipeline. With AutoTSG, any permutation of the term-set\nidentifier will lead to the retrieval of the corresponding document, thus\nlargely relaxing the requirement of exact generation. Besides, the Seq2Seq\nmodel is enabled to flexibly explore the optimal permutation of the document\nidentifier for the presented query, which may further contribute to the\nretrieval quality. AutoTSG is empirically evaluated with Natural Questions and\nMS MARCO, where notable improvements can be achieved against the existing\nauto-regressive search engines.",
        "translated": ""
    },
    {
        "title": "Advances and Challenges of Multi-task Learning Method in Recommender\n  System: A Survey",
        "url": "http://arxiv.org/abs/2305.13843v1",
        "pub_date": "2023-05-23",
        "summary": "Multi-task learning has been widely applied in computational vision, natural\nlanguage processing and other fields, which has achieved well performance. In\nrecent years, a lot of work about multi-task learning recommender system has\nbeen yielded, but there is no previous literature to summarize these works. To\nbridge this gap, we provide a systematic literature survey about multi-task\nrecommender systems, aiming to help researchers and practitioners quickly\nunderstand the current progress in this direction. In this survey, we first\nintroduce the background and the motivation of the multi-task learning-based\nrecommender systems. Then we provide a taxonomy of multi-task learning-based\nrecommendation methods according to the different stages of multi-task learning\ntechniques, which including task relationship discovery, model architecture and\noptimization strategy. Finally, we raise discussions on the application and\npromising future directions in this area.",
        "translated": ""
    },
    {
        "title": "Continual Learning on Dynamic Graphs via Parameter Isolation",
        "url": "http://arxiv.org/abs/2305.13825v1",
        "pub_date": "2023-05-23",
        "summary": "Many real-world graph learning tasks require handling dynamic graphs where\nnew nodes and edges emerge. Dynamic graph learning methods commonly suffer from\nthe catastrophic forgetting problem, where knowledge learned for previous\ngraphs is overwritten by updates for new graphs. To alleviate the problem,\ncontinual graph learning methods are proposed. However, existing continual\ngraph learning methods aim to learn new patterns and maintain old ones with the\nsame set of parameters of fixed size, and thus face a fundamental tradeoff\nbetween both goals. In this paper, we propose Parameter Isolation GNN (PI-GNN)\nfor continual learning on dynamic graphs that circumvents the tradeoff via\nparameter isolation and expansion. Our motivation lies in that different\nparameters contribute to learning different graph patterns. Based on the idea,\nwe expand model parameters to continually learn emerging graph patterns.\nMeanwhile, to effectively preserve knowledge for unaffected patterns, we find\nparameters that correspond to them via optimization and freeze them to prevent\nthem from being rewritten. Experiments on eight real-world datasets corroborate\nthe effectiveness of PI-GNN compared to state-of-the-art baselines.",
        "translated": ""
    },
    {
        "title": "NCHO: Unsupervised Learning for Neural 3D Composition of Humans and\n  Objects",
        "url": "http://arxiv.org/abs/2305.14345v1",
        "pub_date": "2023-05-23",
        "summary": "Deep generative models have been recently extended to synthesizing 3D digital\nhumans. However, previous approaches treat clothed humans as a single chunk of\ngeometry without considering the compositionality of clothing and accessories.\nAs a result, individual items cannot be naturally composed into novel\nidentities, leading to limited expressiveness and controllability of generative\n3D avatars. While several methods attempt to address this by leveraging\nsynthetic data, the interaction between humans and objects is not authentic due\nto the domain gap, and manual asset creation is difficult to scale for a wide\nvariety of objects. In this work, we present a novel framework for learning a\ncompositional generative model of humans and objects (backpacks, coats,\nscarves, and more) from real-world 3D scans. Our compositional model is\ninteraction-aware, meaning the spatial relationship between humans and objects,\nand the mutual shape change by physical contact is fully incorporated. The key\nchallenge is that, since humans and objects are in contact, their 3D scans are\nmerged into a single piece. To decompose them without manual annotations, we\npropose to leverage two sets of 3D scans of a single person with and without\nobjects. Our approach learns to decompose objects and naturally compose them\nback into a generative human model in an unsupervised manner. Despite our\nsimple setup requiring only the capture of a single subject with objects, our\nexperiments demonstrate the strong generalization of our model by enabling the\nnatural composition of objects to diverse identities in various poses and the\ncomposition of multiple objects, which is unseen in training data.",
        "translated": "深层生成模型最近已经扩展到合成3D 数字人类。然而，以前的方法没有考虑到衣服和配件的组合性，而是把穿着衣服的人当作一个单一的几何块来对待。因此，个别项目不能自然地组成新的身份，导致有限的表现力和可控性的生成3D 化身。虽然有几种方法试图通过利用合成数据来解决这个问题，但是由于领域间的差距，人与对象之间的交互并不真实，而且手动资产创建难以适用于各种各样的对象。在这项工作中，我们提出了一个新的框架来学习人类和物体(背包，外套，围巾等)的组合生成模型从现实世界的3 d 扫描。我们的构图模型是交互感知的，这意味着人与物体之间的空间关系，以及通过物理接触的相互形状变化被完全纳入。关键的挑战是，因为人类和物体是接触的，他们的3D 扫描合并成一个单一的部分。为了不用手动注释就可以分解它们，我们建议利用一个人的两组3D 扫描，有对象的和没有对象的。我们的方法学会了分解对象，并自然地以无监督的方式将它们重新组合成一个可生成的人类模型。尽管我们的简单设置只需要捕获具有对象的单个主体，但是我们的实验证明了我们的模型的强大泛化，通过使得对象的自然组合以不同姿势的不同身份和多个对象的组合，这在训练数据中是看不到的。"
    },
    {
        "title": "Siamese Masked Autoencoders",
        "url": "http://arxiv.org/abs/2305.14344v1",
        "pub_date": "2023-05-23",
        "summary": "Establishing correspondence between images or scenes is a significant\nchallenge in computer vision, especially given occlusions, viewpoint changes,\nand varying object appearances. In this paper, we present Siamese Masked\nAutoencoders (SiamMAE), a simple extension of Masked Autoencoders (MAE) for\nlearning visual correspondence from videos. SiamMAE operates on pairs of\nrandomly sampled video frames and asymmetrically masks them. These frames are\nprocessed independently by an encoder network, and a decoder composed of a\nsequence of cross-attention layers is tasked with predicting the missing\npatches in the future frame. By masking a large fraction ($95\\%$) of patches in\nthe future frame while leaving the past frame unchanged, SiamMAE encourages the\nnetwork to focus on object motion and learn object-centric representations.\nDespite its conceptual simplicity, features learned via SiamMAE outperform\nstate-of-the-art self-supervised methods on video object segmentation, pose\nkeypoint propagation, and semantic part propagation tasks. SiamMAE achieves\ncompetitive results without relying on data augmentation, handcrafted\ntracking-based pretext tasks, or other techniques to prevent representational\ncollapse.",
        "translated": "建立图像或场景之间的对应关系是计算机视觉中的一个重大挑战，特别是考虑到遮挡、视点变化和不同的物体外观。本文介绍了 Siamese 蒙版自动编码器(SiamMAE) ，它是蒙版自动编码器(MAE)的一个简单扩展，用于从视频中学习视觉对应。SiamMAE 对一对随机采样的视频帧进行操作，并非对称地屏蔽它们。这些帧由编码器网络独立处理，由一系列交叉注意层组成的解码器负责预测未来帧中丢失的补丁。SiamMAE 通过在未来帧中屏蔽大部分补丁(95%) ，同时保持过去帧不变，鼓励网络关注物体运动并学习以物体为中心的表示。尽管概念简单，通过 SiamMAE 学习的特征在视频对象分割、姿态关键点传播和语义部分传播任务方面优于最先进的自监督方法。SiamMAE 无需依赖数据增强、手工制作的基于跟踪的托辞任务或其他技术来防止表象崩溃，就能获得具有竞争力的结果。"
    },
    {
        "title": "Video Prediction Models as Rewards for Reinforcement Learning",
        "url": "http://arxiv.org/abs/2305.14343v1",
        "pub_date": "2023-05-23",
        "summary": "Specifying reward signals that allow agents to learn complex behaviors is a\nlong-standing challenge in reinforcement learning. A promising approach is to\nextract preferences for behaviors from unlabeled videos, which are widely\navailable on the internet. We present Video Prediction Rewards (VIPER), an\nalgorithm that leverages pretrained video prediction models as action-free\nreward signals for reinforcement learning. Specifically, we first train an\nautoregressive transformer on expert videos and then use the video prediction\nlikelihoods as reward signals for a reinforcement learning agent. VIPER enables\nexpert-level control without programmatic task rewards across a wide range of\nDMC, Atari, and RLBench tasks. Moreover, generalization of the video prediction\nmodel allows us to derive rewards for an out-of-distribution environment where\nno expert data is available, enabling cross-embodiment generalization for\ntabletop manipulation. We see our work as starting point for scalable reward\nspecification from unlabeled videos that will benefit from the rapid advances\nin generative modeling. Source code and datasets are available on the project\nwebsite: https://escontrela.me",
        "translated": "指定奖励信号，让代理人学习复杂的行为，是强化学习研究中一个长期存在的挑战。一个有前途的方法是从未标记的视频中提取行为偏好，这些视频在互联网上广泛可用。我们提出了视频预测奖励(VIPER)算法，该算法利用预先训练的视频预测模型作为强化学习的无动作奖励信号。具体来说，我们首先在专家视频上训练一个自回归变换器，然后使用视频预测可能性作为强化学习代理的奖励信号。VIPER 可以实现专家级别的控制，无需程序任务奖励，跨越广泛的 DMC、 Atari 和 RLBench 任务。此外，视频预测模型的推广使我们能够在没有专家数据可用的分布外环境中获得奖励，从而能够对桌面操作进行跨实施例的推广。我们将我们的工作视为从未标记的视频中获得可扩展奖励规范的起点，这些视频将受益于生成建模的快速发展。源代码和数据集可在项目网站下载:  https://escontrela.me"
    },
    {
        "title": "Diffusion Hyperfeatures: Searching Through Time and Space for Semantic\n  Correspondence",
        "url": "http://arxiv.org/abs/2305.14334v1",
        "pub_date": "2023-05-23",
        "summary": "Diffusion models have been shown to be capable of generating high-quality\nimages, suggesting that they could contain meaningful internal representations.\nUnfortunately, the feature maps that encode a diffusion model's internal\ninformation are spread not only over layers of the network, but also over\ndiffusion timesteps, making it challenging to extract useful descriptors. We\npropose Diffusion Hyperfeatures, a framework for consolidating multi-scale and\nmulti-timestep feature maps into per-pixel feature descriptors that can be used\nfor downstream tasks. These descriptors can be extracted for both synthetic and\nreal images using the generation and inversion processes. We evaluate the\nutility of our Diffusion Hyperfeatures on the task of semantic keypoint\ncorrespondence: our method achieves superior performance on the SPair-71k real\nimage benchmark. We also demonstrate that our method is flexible and\ntransferable: our feature aggregation network trained on the inversion features\nof real image pairs can be used on the generation features of synthetic image\npairs with unseen objects and compositions. Our code is available at\n\\url{https://diffusion-hyperfeatures.github.io}.",
        "translated": "扩散模型已被证明能够产生高质量的图像，表明它们可以包含有意义的内部表示。遗憾的是，编码扩散模型内部信息的特征映射不仅分布在网络的各个层上，而且还分布在扩散时间步长上，这使得提取有用的描述符变得非常困难。我们提出了扩散超特征，一个框架，巩固多尺度和多时间步特征映射到每像素特征描述符，可用于下游任务。这些描述符可以提取合成图像和真实图像使用生成和反演过程。我们评估扩散超特征在语义关键点对应任务中的效用: 我们的方法在 SPair-71k 真实图像基准上获得了优越的性能。实验结果表明，该方法具有灵活性和可移植性: 基于实际图像对反演特征的特征聚合网络可以用于具有不可见物体和成分的合成图像对的特征生成。我们的代码可以在 url { https://diffusion-hyperfeatures.github.io }找到。"
    },
    {
        "title": "Prototype Adaption and Projection for Few- and Zero-shot 3D Point Cloud\n  Semantic Segmentation",
        "url": "http://arxiv.org/abs/2305.14335v1",
        "pub_date": "2023-05-23",
        "summary": "In this work, we address the challenging task of few-shot and zero-shot 3D\npoint cloud semantic segmentation. The success of few-shot semantic\nsegmentation in 2D computer vision is mainly driven by the pre-training on\nlarge-scale datasets like imagenet. The feature extractor pre-trained on\nlarge-scale 2D datasets greatly helps the 2D few-shot learning. However, the\ndevelopment of 3D deep learning is hindered by the limited volume and instance\nmodality of datasets due to the significant cost of 3D data collection and\nannotation. This results in less representative features and large intra-class\nfeature variation for few-shot 3D point cloud segmentation. As a consequence,\ndirectly extending existing popular prototypical methods of 2D few-shot\nclassification/segmentation into 3D point cloud segmentation won't work as well\nas in 2D domain. To address this issue, we propose a Query-Guided Prototype\nAdaption (QGPA) module to adapt the prototype from support point clouds feature\nspace to query point clouds feature space. With such prototype adaption, we\ngreatly alleviate the issue of large feature intra-class variation in point\ncloud and significantly improve the performance of few-shot 3D segmentation.\nBesides, to enhance the representation of prototypes, we introduce a\nSelf-Reconstruction (SR) module that enables prototype to reconstruct the\nsupport mask as well as possible. Moreover, we further consider zero-shot 3D\npoint cloud semantic segmentation where there is no support sample. To this\nend, we introduce category words as semantic information and propose a\nsemantic-visual projection model to bridge the semantic and visual spaces. Our\nproposed method surpasses state-of-the-art algorithms by a considerable 7.90%\nand 14.82% under the 2-way 1-shot setting on S3DIS and ScanNet benchmarks,\nrespectively. Code is available at https://github.com/heshuting555/PAP-FZS3D.",
        "translated": "在这项工作中，我们解决了具有挑战性的任务少镜头和零镜头三维点云语义分割。二维计算机视觉中少镜头语义分割的成功主要是由对大规模数据集(如图像网)的预训练所驱动的。在大规模二维数据集上预训练的特征提取器对二维少镜头学习有很大的帮助。然而，由于三维数据的采集和注释成本较高，数据集的体积和实例形式有限，阻碍了三维深度学习的发展。这导致了少镜头三维点云分割的代表性较差的特征和较大的类内特征变化。因此，将现有流行的二维少镜头分类/分割原型方法直接扩展到三维点云分割将不能像在二维领域那样有效。针对这一问题，提出了一种基于查询引导的原型适配(QGPA)模块，将原型从支持点云特征空间调整到查询点云特征空间。采用这种原型自适应方法，大大减轻了点云中特征类内变化大的问题，显著提高了少镜头三维分割的性能。此外，为了提高原型的表示能力，我们引入了自重构(SR)模块，使原型能够尽可能地重构支撑掩模。此外，在没有支持样本的情况下，我们进一步考虑了零拍3D 点云语义分割。为此，我们引入类别词作为语义信息，并提出一个语义-视觉投影模型，以连接语义和视觉空间。在 S3DIS 和 ScanNet 基准的双向单镜头设置下，我们提出的方法分别以7.90% 和14.82% 的优势超越了最先进的算法。密码可于 https://github.com/heshuting555/pap-fzs3d 索取。"
    },
    {
        "title": "Large Language Models are Frame-level Directors for Zero-shot\n  Text-to-Video Generation",
        "url": "http://arxiv.org/abs/2305.14330v1",
        "pub_date": "2023-05-23",
        "summary": "In the paradigm of AI-generated content (AIGC), there has been increasing\nattention in extending pre-trained text-to-image (T2I) models to text-to-video\n(T2V) generation. Despite their effectiveness, these frameworks face challenges\nin maintaining consistent narratives and handling rapid shifts in scene\ncomposition or object placement from a single user prompt. This paper\nintroduces a new framework, dubbed DirecT2V, which leverages instruction-tuned\nlarge language models (LLMs) to generate frame-by-frame descriptions from a\nsingle abstract user prompt. DirecT2V utilizes LLM directors to divide user\ninputs into separate prompts for each frame, enabling the inclusion of\ntime-varying content and facilitating consistent video generation. To maintain\ntemporal consistency and prevent object collapse, we propose a novel value\nmapping method and dual-softmax filtering. Extensive experimental results\nvalidate the effectiveness of the DirecT2V framework in producing visually\ncoherent and consistent videos from abstract user prompts, addressing the\nchallenges of zero-shot video generation.",
        "translated": "在人工智能生成内容(AIGC)的范式中，将预先训练好的文本到图像(T2I)模型扩展到文本到视频(T2V)生成已经受到越来越多的关注。尽管这些框架很有效，但它们在保持一致的叙述和处理来自单一用户提示的场景构成或物体放置的快速变化方面面临挑战。本文介绍了一个名为 DirecT2V 的新框架，它利用指令调优的大型语言模型(LLM)从单个抽象用户提示符生成一帧一帧的描述。DirecT2V 利用 LLM 控制器将用户输入划分为每个帧的单独提示，从而能够包含时变内容并促进一致的视频生成。为了保持时间一致性和防止对象崩溃，提出了一种新的值映射方法和双软极大值滤波。广泛的实验结果验证了 DirecT2V 框架在从抽象用户提示生成视觉一致性和一致性视频方面的有效性，解决了零拍摄视频生成的挑战。"
    },
    {
        "title": "Improving Factuality and Reasoning in Language Models through Multiagent\n  Debate",
        "url": "http://arxiv.org/abs/2305.14325v1",
        "pub_date": "2023-05-23",
        "summary": "Large language models (LLMs) have demonstrated remarkable capabilities in\nlanguage generation, understanding, and few-shot learning in recent years. An\nextensive body of work has explored how their performance may be further\nimproved through the tools of prompting, ranging from verification,\nself-consistency, or intermediate scratchpads. In this paper, we present a\ncomplementary approach to improve language responses where multiple language\nmodel instances propose and debate their individual responses and reasoning\nprocesses over multiple rounds to arrive at a common final answer. Our findings\nindicate that this approach significantly enhances mathematical and strategic\nreasoning across a number of tasks. We also demonstrate that our approach\nimproves the factual validity of generated content, reducing fallacious answers\nand hallucinations that contemporary models are prone to. Our approach may be\ndirectly applied to existing black-box models and uses identical procedure and\nprompts for all tasks we investigate. Overall, our findings suggest that such\n\"society of minds\" approach has the potential to significantly advance the\ncapabilities of LLMs and pave the way for further breakthroughs in language\ngeneration and understanding.",
        "translated": "近年来，大语言模型(LLM)在语言生成、理解和短镜头学习等方面表现出了显著的能力。大量的工作已经探索了如何通过提示工具进一步提高他们的性能，从验证，自我一致性，或中间的暂存器。在本文中，我们提出了一个互补的方法来改善语言反应，其中多个语言模型实例提出和辩论他们的个别反应和推理过程，多轮得出一个共同的最终答案。我们的研究结果表明，这种方法显着提高了数学和战略推理的一些任务。我们还证明，我们的方法提高了生成内容的实际有效性，减少了当代模型容易产生的谬误答案和幻觉。我们的方法可以直接应用于现有的黑盒模型，并对我们调查的所有任务使用相同的过程和提示。总的来说，我们的研究结果表明，这种“心灵社会”的方法有潜力显着提高语言学习者的能力，并为语言生成和理解的进一步突破铺平道路。"
    },
    {
        "title": "Text-guided 3D Human Generation from 2D Collections",
        "url": "http://arxiv.org/abs/2305.14312v1",
        "pub_date": "2023-05-23",
        "summary": "3D human modeling has been widely used for engaging interaction in gaming,\nfilm, and animation. The customization of these characters is crucial for\ncreativity and scalability, which highlights the importance of controllability.\nIn this work, we introduce Text-guided 3D Human Generation (\\texttt{T3H}),\nwhere a model is to generate a 3D human, guided by the fashion description.\nThere are two goals: 1) the 3D human should render articulately, and 2) its\noutfit is controlled by the given text. To address this \\texttt{T3H} task, we\npropose Compositional Cross-modal Human (CCH). CCH adopts cross-modal attention\nto fuse compositional human rendering with the extracted fashion semantics.\nEach human body part perceives relevant textual guidance as its visual\npatterns. We incorporate the human prior and semantic discrimination to enhance\n3D geometry transformation and fine-grained consistency, enabling it to learn\nfrom 2D collections for data efficiency. We conduct evaluations on DeepFashion\nand SHHQ with diverse fashion attributes covering the shape, fabric, and color\nof upper and lower clothing. Extensive experiments demonstrate that CCH\nachieves superior results for \\texttt{T3H} with high efficiency.",
        "translated": "3D 人体建模已经广泛应用于游戏、电影和动画中的交互。这些字符的定制对于创造性和可扩展性至关重要，这突出了可控性的重要性。在这项工作中，我们介绍了文本引导的三维人类生成(texttt { T3H }) ，其中一个模型是生成一个三维人，在时尚描述的指导下。有两个目标: 1)3D 人应该清晰地渲染，2)它的装备是由给定的文本控制。为了解决这个文本{ T3H }任务，我们提出了组合跨模态人(CCH)。CCH 采用交叉模态注意将提取的时尚语义融合到组合人体绘制中。人体的每个部分都将相关的文本指导视为其视觉模式。我们结合了人类先验和语义识别，以增强三维几何变换和细粒度的一致性，使其能够学习从2D 集合的数据效率。我们对 DeepFashion 和 SHHQ 进行评估，它们具有多种时尚属性，包括上衣和下衣的形状、面料和颜色。大量的实验表明，CCH 对 texttt { T3H }具有较高的效率，取得了较好的效果。"
    },
    {
        "title": "Hierarchical Adaptive Voxel-guided Sampling for Real-time Applications\n  in Large-scale Point Clouds",
        "url": "http://arxiv.org/abs/2305.14306v1",
        "pub_date": "2023-05-23",
        "summary": "While point-based neural architectures have demonstrated their efficacy, the\ntime-consuming sampler currently prevents them from performing real-time\nreasoning on scene-level point clouds. Existing methods attempt to overcome\nthis issue by using random sampling strategy instead of the commonly-adopted\nfarthest point sampling~(FPS), but at the expense of lower performance. So the\neffectiveness/efficiency trade-off remains under-explored. In this paper, we\nreveal the key to high-quality sampling is ensuring an even spacing between\npoints in the subset, which can be naturally obtained through a grid. Based on\nthis insight, we propose a hierarchical adaptive voxel-guided point sampler\nwith linear complexity and high parallelization for real-time applications.\nExtensive experiments on large-scale point cloud detection and segmentation\ntasks demonstrate that our method achieves competitive performance with the\nmost powerful FPS, at an amazing speed that is more than 100 times faster. This\nbreakthrough in efficiency addresses the bottleneck of the sampling step when\nhandling scene-level point clouds. Furthermore, our sampler can be easily\nintegrated into existing models and achieves a 20$\\sim$80\\% reduction in\nruntime with minimal effort. The code will be available at\nhttps://github.com/OuyangJunyuan/pointcloud-3d-detector-tensorrt",
        "translated": "虽然基于点的神经结构已经证明了它们的功效，但耗时的采样器目前阻止它们对场景级别的点云进行实时推理。现有的方法试图用随机抽样策略代替常用的最远点抽样，但是以牺牲较低的性能为代价来克服这一问题。因此，有效性/效率之间的权衡仍未得到充分探讨。在本文中，我们揭示了高质量采样的关键是确保子集中点之间的均匀间距，这可以通过网格自然获得。在此基础上，我们提出了一种分层自适应体素引导的点采样器，它具有线性复杂度和高并行性，适用于实时应用。大规模点云检测和分割任务的大量实验表明，我们的方法实现了与最强大的 FPS 具有竞争力的性能，以惊人的速度，超过100倍的速度。这一效率上的突破解决了处理场景级点云时采样步骤的瓶颈。此外，我们的采样器可以很容易地集成到现有的模型，并实现20美元西姆 $80% 的运行时减少最小的努力。代码将在 https://github.com/ouyangjunyuan/pointcloud-3d-detector-tensorrt 公布"
    },
    {
        "title": "A Laplacian Pyramid Based Generative H&amp;E Stain Augmentation Network",
        "url": "http://arxiv.org/abs/2305.14301v1",
        "pub_date": "2023-05-23",
        "summary": "Hematoxylin and Eosin (H&amp;E) staining is a widely used sample preparation\nprocedure for enhancing the saturation of tissue sections and the contrast\nbetween nuclei and cytoplasm in histology images for medical diagnostics.\nHowever, various factors, such as the differences in the reagents used, result\nin high variability in the colors of the stains actually recorded. This\nvariability poses a challenge in achieving generalization for machine-learning\nbased computer-aided diagnostic tools. To desensitize the learned models to\nstain variations, we propose the Generative Stain Augmentation Network (G-SAN)\n-- a GAN-based framework that augments a collection of cell images with\nsimulated yet realistic stain variations. At its core, G-SAN uses a novel and\nhighly computationally efficient Laplacian Pyramid (LP) based generator\narchitecture, that is capable of disentangling stain from cell morphology.\nThrough the task of patch classification and nucleus segmentation, we show that\nusing G-SAN-augmented training data provides on average 15.7% improvement in F1\nscore and 7.3% improvement in panoptic quality, respectively. Our code is\navailable at https://github.com/lifangda01/GSAN-Demo.",
        "translated": "苏木精-伊红(H & E)染色是一种广泛应用的提高组织切片饱和度和增强组织学图像中细胞核和细胞质对比度的样品制备方法。然而，各种因素，例如使用的试剂的差异，导致实际记录的污渍颜色的高度可变性。这种可变性对基于机器学习的计算机辅助诊断工具的普及提出了挑战。为了使学习的模型对染色变化不敏感，我们提出生成染色增强网络(G-SAN)——一种基于 GAN 的框架，用模拟但真实的染色变化增强细胞图像集合。在其核心，G-SAN 使用了一种新的和高度计算效率的拉普拉斯金字塔(LP)为基础的生成器架构，这是能够从细胞形态学分离染色。通过斑块分类和细胞核分割实验，我们发现使用 G-SAN 增强训练数据，F1评分平均提高15.7% ，视觉质量平均提高7.3% 。我们的代码可以在 https://github.com/lifangda01/gsan-demo 找到。"
    },
    {
        "title": "Balancing the Picture: Debiasing Vision-Language Datasets with Synthetic\n  Contrast Sets",
        "url": "http://arxiv.org/abs/2305.15407v1",
        "pub_date": "2023-05-24",
        "summary": "Vision-language models are growing in popularity and public visibility to\ngenerate, edit, and caption images at scale; but their outputs can perpetuate\nand amplify societal biases learned during pre-training on uncurated image-text\npairs from the internet. Although debiasing methods have been proposed, we\nargue that these measurements of model bias lack validity due to dataset bias.\nWe demonstrate there are spurious correlations in COCO Captions, the most\ncommonly used dataset for evaluating bias, between background context and the\ngender of people in-situ. This is problematic because commonly-used bias\nmetrics (such as Bias@K) rely on per-gender base rates. To address this issue,\nwe propose a novel dataset debiasing pipeline to augment the COCO dataset with\nsynthetic, gender-balanced contrast sets, where only the gender of the subject\nis edited and the background is fixed. However, existing image editing methods\nhave limitations and sometimes produce low-quality images; so, we introduce a\nmethod to automatically filter the generated images based on their similarity\nto real images. Using our balanced synthetic contrast sets, we benchmark bias\nin multiple CLIP-based models, demonstrating how metrics are skewed by\nimbalance in the original COCO images. Our results indicate that the proposed\napproach improves the validity of the evaluation, ultimately contributing to\nmore realistic understanding of bias in vision-language models.",
        "translated": "视觉语言模型在大规模生成、编辑和标题图像方面越来越受欢迎和公众可见度越来越高; 但是它们的输出可以延续和放大在互联网上未经策划的图像-文本对的预培训中学到的社会偏见。虽然已经提出了消除偏差的方法，但是我们认为由于数据集的偏差，这些模型偏差的测量缺乏有效性。我们证明在 COCO 标题中存在虚假的相关性，COCO 标题是最常用于评估偏倚的数据集，背景环境和现场人员的性别之间存在虚假的相关性。这是有问题的，因为常用的偏见指标(如偏见@K)依赖于每个性别的基础比率。为了解决这一问题，我们提出了一种新的数据集去偏流水线，以增加合成的，性别平衡的对比集 COCO 数据集，其中只编辑主题的性别和背景是固定的。然而，现有的图像编辑方法存在一定的局限性，有时会产生低质量的图像，因此，我们提出了一种基于图像与真实图像相似度的自动过滤方法。使用我们的平衡合成对比度集，我们在多个基于 CLIP 的模型基准偏差，演示了如何在原始 COCO 图像的不平衡度量偏斜。我们的研究结果表明，提出的方法提高了评价的有效性，最终有助于更现实的理解偏见的视觉语言模型。"
    },
    {
        "title": "RoMa: Revisiting Robust Losses for Dense Feature Matching",
        "url": "http://arxiv.org/abs/2305.15404v1",
        "pub_date": "2023-05-24",
        "summary": "Dense feature matching is an important computer vision task that involves\nestimating all correspondences between two images of a 3D scene. In this paper,\nwe revisit robust losses for matching from a Markov chain perspective, yielding\ntheoretical insights and large gains in performance. We begin by constructing a\nunifying formulation of matching as a Markov chain, based on which we identify\ntwo key stages which we argue should be decoupled for matching. The first is\nthe coarse stage, where the estimated result needs to be globally consistent.\nThe second is the refinement stage, where the model needs precise localization\ncapabilities. Inspired by the insight that these stages concern distinct\nissues, we propose a coarse matcher following the regression-by-classification\nparadigm that provides excellent globally consistent, albeit not exactly\nlocalized, matches. This is followed by a local feature refinement stage using\nwell-motivated robust regression losses, yielding extremely precise matches.\nOur proposed approach, which we call RoMa, achieves significant improvements\ncompared to the state-of-the-art. Code is available at\nhttps://github.com/Parskatt/RoMa",
        "translated": "密集特征匹配是一项重要的计算机视觉任务，它涉及到估计三维场景中两幅图像之间的所有对应关系。在本文中，我们从马尔可夫链的角度重新审视匹配的鲁棒性损失，产生理论见解和性能的大幅提高。我们首先构造一个统一的匹配公式作为一个马尔可夫链，在此基础上，我们确定了两个关键的阶段，我们认为应该解耦匹配。第一个阶段是粗略阶段，其中估计的结果需要具有全局一致性。第二个阶段是精化阶段，在这个阶段模型需要精确的定位能力。受到这些阶段关注不同问题的洞察力的启发，我们提出了遵循分类回归范式的粗匹配器，它提供了优秀的全局一致性匹配，尽管不是完全本地化的匹配。然后是局部特征细化阶段，使用动机良好的鲁棒回归损失，产生非常精确的匹配。我们提出的方法，我们称之为 RoMa，与最先进的技术相比，取得了显著的改进。密码可于 https://github.com/parskatt/roma 索取"
    },
    {
        "title": "Sin3DM: Learning a Diffusion Model from a Single 3D Textured Shape",
        "url": "http://arxiv.org/abs/2305.15399v1",
        "pub_date": "2023-05-24",
        "summary": "Synthesizing novel 3D models that resemble the input example has long been\npursued by researchers and artists in computer graphics. In this paper, we\npresent Sin3DM, a diffusion model that learns the internal patch distribution\nfrom a single 3D textured shape and generates high-quality variations with fine\ngeometry and texture details. Training a diffusion model directly in 3D would\ninduce large memory and computational cost. Therefore, we first compress the\ninput into a lower-dimensional latent space and then train a diffusion model on\nit. Specifically, we encode the input 3D textured shape into triplane feature\nmaps that represent the signed distance and texture fields of the input. The\ndenoising network of our diffusion model has a limited receptive field to avoid\noverfitting, and uses triplane-aware 2D convolution blocks to improve the\nresult quality. Aside from randomly generating new samples, our model also\nfacilitates applications such as retargeting, outpainting and local editing.\nThrough extensive qualitative and quantitative evaluation, we show that our\nmodel can generate 3D shapes of various types with better quality than prior\nmethods.",
        "translated": "长期以来，计算机图形学的研究人员和艺术家一直致力于合成类似于输入样本的新颖3D 模型。本文提出了一种扩散模型 Sin3DM，该模型从单个三维纹理形状中学习内部斑块分布，并生成具有精细几何和纹理细节的高质量变化。直接在三维空间中训练扩散模型会产生大量的内存和计算开销。因此，我们首先将输入压缩到一个低维的潜在空间，然后在其上训练一个扩散模型。具体来说，我们将输入的三维纹理形状编码成三平面特征映射，表示输入的有符号距离和纹理字段。为了避免过拟合，扩散模型的去噪网络具有有限的接收域，并且使用了三平面感知的二维卷积块来提高结果的质量。除了随机产生新的样本，我们的模型还促进应用程序，如重定向，外绘和本地编辑。通过广泛的定性和定量评价，我们表明我们的模型可以生成各种类型的三维形状，比以往的方法更好的质量。"
    },
    {
        "title": "LayoutGPT: Compositional Visual Planning and Generation with Large\n  Language Models",
        "url": "http://arxiv.org/abs/2305.15393v1",
        "pub_date": "2023-05-24",
        "summary": "Attaining a high degree of user controllability in visual generation often\nrequires intricate, fine-grained inputs like layouts. However, such inputs\nimpose a substantial burden on users when compared to simple text inputs. To\naddress the issue, we study how Large Language Models (LLMs) can serve as\nvisual planners by generating layouts from text conditions, and thus\ncollaborate with visual generative models. We propose LayoutGPT, a method to\ncompose in-context visual demonstrations in style sheet language to enhance the\nvisual planning skills of LLMs. LayoutGPT can generate plausible layouts in\nmultiple domains, ranging from 2D images to 3D indoor scenes. LayoutGPT also\nshows superior performance in converting challenging language concepts like\nnumerical and spatial relations to layout arrangements for faithful\ntext-to-image generation. When combined with a downstream image generation\nmodel, LayoutGPT outperforms text-to-image models/systems by 20-40% and\nachieves comparable performance as human users in designing visual layouts for\nnumerical and spatial correctness. Lastly, LayoutGPT achieves comparable\nperformance to supervised methods in 3D indoor scene synthesis, demonstrating\nits effectiveness and potential in multiple visual domains.",
        "translated": "在可视化生成中获得高度的用户可控性通常需要复杂的、细粒度的输入，如布局。然而，与简单的文本输入相比，这种输入对用户造成了相当大的负担。为了解决这个问题，我们研究了大语言模型(LLM)如何通过根据文本条件生成布局来充当可视化规划器，从而与可视化生成模型进行协作。我们提出了 LayoutGPT，一种用样式表语言组合上下文视觉演示的方法，以提高 LLM 的视觉规划技巧。LayoutGPT 可以在多个领域生成合理的布局，从2D 图像到3D 室内场景。LayoutGPT 在将具有挑战性的语言概念(如数值和空间关系)转换为可靠的文本到图像生成的布局安排方面也表现出优越的性能。结合下游图像生成模型，LayoutGPT 的性能比文本到图像的模型/系统高出20-40% ，并且在数值和空间正确性的可视化布局设计方面达到与人类用户相当的性能。最后，LayoutGPT 在三维室内场景合成中实现了与监督方法相当的性能，证明了其在多视觉领域中的有效性和潜力。"
    },
    {
        "title": "A Neural Space-Time Representation for Text-to-Image Personalization",
        "url": "http://arxiv.org/abs/2305.15391v1",
        "pub_date": "2023-05-24",
        "summary": "A key aspect of text-to-image personalization methods is the manner in which\nthe target concept is represented within the generative process. This choice\ngreatly affects the visual fidelity, downstream editability, and disk space\nneeded to store the learned concept. In this paper, we explore a new\ntext-conditioning space that is dependent on both the denoising process\ntimestep (time) and the denoising U-Net layers (space) and showcase its\ncompelling properties. A single concept in the space-time representation is\ncomposed of hundreds of vectors, one for each combination of time and space,\nmaking this space challenging to optimize directly. Instead, we propose to\nimplicitly represent a concept in this space by optimizing a small neural\nmapper that receives the current time and space parameters and outputs the\nmatching token embedding. In doing so, the entire personalized concept is\nrepresented by the parameters of the learned mapper, resulting in a compact,\nyet expressive, representation. Similarly to other personalization methods, the\noutput of our neural mapper resides in the input space of the text encoder. We\nobserve that one can significantly improve the convergence and visual fidelity\nof the concept by introducing a textual bypass, where our neural mapper\nadditionally outputs a residual that is added to the output of the text\nencoder. Finally, we show how one can impose an importance-based ordering over\nour implicit representation, providing users control over the reconstruction\nand editability of the learned concept using a single trained model. We\ndemonstrate the effectiveness of our approach over a range of concepts and\nprompts, showing our method's ability to generate high-quality and controllable\ncompositions without fine-tuning any parameters of the generative model itself.",
        "translated": "文本到图像个性化方法的一个关键方面是目标概念在生成过程中的表示方式。这种选择极大地影响了视觉保真度、下游可编辑性和存储所学概念所需的磁盘空间。在本文中，我们探索了一个新的文本条件空间，它同时依赖于去噪过程的时间步长(时间)和去噪的 U-Net 层(空间) ，并展示了其引人注目的性质。时空表示中的一个概念由数百个向量组成，每个向量对应于时间和空间的组合，这使得直接优化这个空间具有挑战性。相反，我们建议通过优化一个接收当前时间和空间参数并输出匹配令牌嵌入的小型神经映射器来隐式表示这个空间中的一个概念。在这样做时，整个个性化的概念是由学习映射器的参数表示，导致一个紧凑，但表达，表示。与其他个性化方法类似，我们的神经映射器的输出驻留在文本编码器的输入空间中。我们观察到，通过引入文本旁路，可以显著提高概念的收敛性和视觉保真度，其中我们的神经映射器额外输出一个残差，添加到文本编码器的输出。最后，我们展示了如何在我们的隐式表示上强加一个基于重要性的排序，使用一个单一的训练模型为用户提供对所学概念的重构和可编辑性的控制。我们通过一系列的概念和提示展示了我们的方法的有效性，展示了我们的方法能够生成高质量和可控的合成物，而不需要对生成模型本身的任何参数进行微调。"
    },
    {
        "title": "What can generic neural networks learn from a child's visual experience?",
        "url": "http://arxiv.org/abs/2305.15372v1",
        "pub_date": "2023-05-24",
        "summary": "Young children develop sophisticated internal models of the world based on\ntheir egocentric visual experience. How much of this is driven by innate\nconstraints and how much is driven by their experience? To investigate these\nquestions, we train state-of-the-art neural networks on a realistic proxy of a\nchild's visual experience without any explicit supervision or domain-specific\ninductive biases. Specifically, we train both embedding models and generative\nmodels on 200 hours of headcam video from a single child collected over two\nyears. We train a total of 72 different models, exploring a range of model\narchitectures and self-supervised learning algorithms, and comprehensively\nevaluate their performance in downstream tasks. The best embedding models\nperform at 70% of a highly performant ImageNet-trained model on average. They\nalso learn broad semantic categories without any labeled examples and learn to\nlocalize semantic categories in an image without any location supervision.\nHowever, these models are less object-centric and more background-sensitive\nthan comparable ImageNet-trained models. Generative models trained with the\nsame data successfully extrapolate simple properties of partially masked\nobjects, such as their texture, color, orientation, and rough outline, but\nstruggle with finer object details. We replicate our experiments with two other\nchildren and find very similar results. Broadly useful high-level visual\nrepresentations are thus robustly learnable from a representative sample of a\nchild's visual experience without strong inductive biases.",
        "translated": "幼儿根据自我中心的视觉经验，发展出复杂的内在世界模型。这其中有多少是由先天约束驱动的，又有多少是由他们的经验驱动的？为了研究这些问题，我们训练最先进的神经网络，在没有任何明确的监督或领域特定的归纳偏见的情况下，对儿童的视觉经验进行现实的替代。具体来说，我们训练嵌入模型和生成模型的200个小时的头部摄像头视频从一个孩子收集了两年多。我们总共训练了72个不同的模型，探索了一系列模型结构和自我监督学习算法，并全面评估了它们在下游任务中的表现。最好的嵌入模型在高性能 ImageNet 训练模型中的平均执行率为70% 。他们还学习广泛的语义类别没有任何标记的例子和学习本地化的语义类别在一个图像没有任何位置监督。然而，与可比较的 ImageNet 训练模型相比，这些模型更少的以对象为中心，更多的是对背景敏感。使用相同数据训练的生成模型成功地推断出部分遮蔽对象的简单属性，例如它们的纹理、颜色、方向和粗略轮廓，但是难以获得更精细的对象细节。我们在另外两个孩子身上做了同样的实验，得到了非常相似的结果。因此，广泛有用的高层次视觉表征可以从一个具有代表性的儿童视觉经验样本中强有力地学习，而没有强烈的归纳偏见。"
    },
    {
        "title": "SAMScore: A Semantic Structural Similarity Metric for Image Translation\n  Evaluation",
        "url": "http://arxiv.org/abs/2305.15367v1",
        "pub_date": "2023-05-24",
        "summary": "Image translation has wide applications, such as style transfer and modality\nconversion, usually aiming to generate images having both high degrees of\nrealism and faithfulness. These problems remain difficult, especially when it\nis important to preserve semantic structures. Traditional image-level\nsimilarity metrics are of limited use, since the semantics of an image are\nhigh-level, and not strongly governed by pixel-wise faithfulness to an original\nimage. Towards filling this gap, we introduce SAMScore, a generic semantic\nstructural similarity metric for evaluating the faithfulness of image\ntranslation models. SAMScore is based on the recent high-performance Segment\nAnything Model (SAM), which can perform semantic similarity comparisons with\nstandout accuracy. We applied SAMScore on 19 image translation tasks, and found\nthat it is able to outperform all other competitive metrics on all of the\ntasks. We envision that SAMScore will prove to be a valuable tool that will\nhelp to drive the vibrant field of image translation, by allowing for more\nprecise evaluations of new and evolving translation models. The code is\navailable at https://github.com/Kent0n-Li/SAMScore.",
        "translated": "意象翻译在文体转换、情态转换等方面有着广泛的应用，通常意象翻译的目的是生成逼真度高、忠实度高的意象。这些问题仍然很难解决，特别是在保护语义结构很重要的情况下。传统的图像级相似度指标的用途有限，因为图像的语义是高级的，并且不受像素级对原始图像的忠实度的强烈支配。为了填补这个空白，我们引入了 SAMScore，这是一个通用的语义结构相似性指标，用于评估图像翻译模型的可靠性。SAMScore 是基于最新的高性能分段任意模型(Segment AnyModel，SAM) ，它可以执行语义相似度比较，具有突出的准确性。我们将 SAMScore 应用于19个图像翻译任务，发现它在所有任务中的表现都优于其他竞争指标。我们设想 SAMScore 将被证明是一个有价值的工具，通过允许对新的和不断发展的翻译模式进行更精确的评估，将有助于推动图像翻译这一充满活力的领域。密码可在 https://github.com/kent0n-li/samscore 查阅。"
    },
    {
        "title": "Boundary Attention Mapping (BAM): Fine-grained saliency maps for\n  segmentation of Burn Injuries",
        "url": "http://arxiv.org/abs/2305.15365v1",
        "pub_date": "2023-05-24",
        "summary": "Burn injuries can result from mechanisms such as thermal, chemical, and\nelectrical insults. A prompt and accurate assessment of burns is essential for\ndeciding definitive clinical treatments. Currently, the primary approach for\nburn assessments, via visual and tactile observations, is approximately 60%-80%\naccurate. The gold standard is biopsy and a close second would be non-invasive\nmethods like Laser Doppler Imaging (LDI) assessments, which have up to 97%\naccuracy in predicting burn severity and the required healing time. In this\npaper, we introduce a machine learning pipeline for assessing burn severities\nand segmenting the regions of skin that are affected by burn. Segmenting 2D\ncolour images of burns allows for the injured versus non-injured skin to be\ndelineated, clearly marking the extent and boundaries of the localized\nburn/region-of-interest, even during remote monitoring of a burn patient. We\ntrained a convolutional neural network (CNN) to classify four severities of\nburns. We built a saliency mapping method, Boundary Attention Mapping (BAM),\nthat utilises this trained CNN for the purpose of accurately localizing and\nsegmenting the burn regions from skin burn images. We demonstrated the\neffectiveness of our proposed pipeline through extensive experiments and\nevaluations using two datasets; 1) A larger skin burn image dataset consisting\nof 1684 skin burn images of four burn severities, 2) An LDI dataset that\nconsists of a total of 184 skin burn images with their associated LDI scans.\nThe CNN trained using the first dataset achieved an average F1-Score of 78% and\nmicro/macro- average ROC of 85% in classifying the four burn severities.\nMoreover, a comparison between the BAM results and LDI results for measuring\ninjury boundary showed that the segmentations generated by our method achieved\n91.60% accuracy, 78.17% sensitivity, and 93.37% specificity.",
        "translated": "烧伤可能是由热、化学和电气等机制造成的。及时和准确的评估烧伤是决定明确的临床治疗是必不可少的。目前，烧伤评估的主要方法，通过视觉和触觉观察，大约60% -80% 的准确率。黄金标准是活组织检查，紧随其后的是非侵入性方法，如激光多普勒成像(LDI)评估，它在预测烧伤严重程度和所需的愈合时间方面有高达97% 的准确性。在本文中，我们介绍了一个机器学习管道，用于评估烧伤的严重程度和分割皮肤受烧伤影响的区域。分割烧伤的二维彩色图像允许描绘受伤与未受伤的皮肤，即使在远程监测烧伤患者期间，也清楚地标记局部烧伤/感兴趣区域的范围和边界。我们训练了一个卷积神经网络(CNN)来分类烧伤的四种严重程度。我们建立了一个突出映射方法，边界注意映射(BAM) ，利用这个训练的 CNN 的目的是准确定位和分割烧伤区域从皮肤烧伤图像。我们通过使用两个数据集进行广泛的实验和评估，证明了我们提出的管道的有效性; 1)由1684个四种烧伤严重程度的皮肤烧伤图像组成的更大的皮肤烧伤图像数据集，2)总共由184个皮肤烧伤图像及其相关的 LDI 扫描组成的 LDI 数据集。使用第一个数据集训练的美国有线电视新闻网在对四种烧伤严重程度进行分类时，平均达到了78% 的 f1-得分和85% 的微观/宏观平均 ROC。此外，测量损伤边界的 BAM 结果与 LDI 结果之间的比较显示，由我们的方法产生的分割达到91.60% 的准确性，78.17% 的灵敏度和93.37% 的特异性。"
    },
    {
        "title": "Solving Diffusion ODEs with Optimal Boundary Conditions for Better Image\n  Super-Resolution",
        "url": "http://arxiv.org/abs/2305.15357v1",
        "pub_date": "2023-05-24",
        "summary": "Diffusion models, as a kind of powerful generative model, have given\nimpressive results on image super-resolution (SR) tasks. However, due to the\nrandomness introduced in the reverse process of diffusion models, the\nperformances of diffusion-based SR models are fluctuating at every time of\nsampling, especially for samplers with few resampled steps. This inherent\nrandomness of diffusion models results in ineffectiveness and instability,\nmaking it challenging for users to guarantee the quality of SR results.\nHowever, our work takes this randomness as an opportunity: fully analyzing and\nleveraging it leads to the construction of an effective plug-and-play sampling\nmethod that owns the potential to benefit a series of diffusion-based SR\nmethods. More in detail, we propose to steadily sample high-quality SR images\nfrom pretrained diffusion-based SR models by solving diffusion ordinary\ndifferential equations (diffusion ODEs) with optimal boundary conditions (BCs)\nand analyze the characteristics between the choices of BCs and their\ncorresponding SR results. Our analysis shows the route to obtain an\napproximately optimal BC via an efficient exploration in the whole space. The\nquality of SR results sampled by the proposed method with fewer steps\noutperforms the quality of results sampled by current methods with randomness\nfrom the same pretrained diffusion-based SR model, which means that our\nsampling method ``boosts'' current diffusion-based SR models without any\nadditional training.",
        "translated": "扩散模型作为一种强大的生成模型，在图像超分辨率(SR)任务中给出了令人印象深刻的结果。然而，由于扩散模型反向过程的随机性，基于扩散的 SR 模型在每次采样时的性能都会发生波动，特别是对于重采样步数较少的采样者。这种扩散模型固有的随机性导致无效性和不稳定性，使得用户难以保证 SR 结果的质量。然而，我们的工作把这种随机性作为一个机会: 充分分析和利用它导致建立一个有效的即插即用的抽样方法，拥有受益于一系列基于扩散的 SR 方法的潜力。具体来说，我们提出了通过求解具有最优边界条件的扩散常微分方程，从基于扩散的预训练 SR 模型中稳定地采集高质量的 SR 图像，并分析了扩散常微分方程的选择与其相应 SR 结果之间的特点。我们的分析表明，通过在整个空间进行有效的探索，可以获得一个近似最优 BC 的路径。本文提出的方法采样的 SR 结果的质量较少的步骤优于目前的方法采样的结果的质量与随机性从相同的预先训练的扩散为基础的 SR 模型，这意味着我们的采样方法“推动”目前的扩散为基础的 SR 模型没有任何额外的训练。"
    },
    {
        "title": "Mitigating Biased Activation in Weakly-supervised Object Localization\n  via Counterfactual Learning",
        "url": "http://arxiv.org/abs/2305.15354v1",
        "pub_date": "2023-05-24",
        "summary": "In this paper, we focus on an under-explored issue of biased activation in\nprior weakly-supervised object localization methods based on Class Activation\nMapping (CAM). We analyze the cause of this problem from a causal view and\nattribute it to the co-occurring background confounders. Following this\ninsight, we propose a novel Counterfactual Co-occurring Learning (CCL) paradigm\nto synthesize the counterfactual representations via coupling constant\nforeground and unrealized backgrounds in order to cut off their co-occurring\nrelationship. Specifically, we design a new network structure called\nCounterfactual-CAM, which embeds the counterfactual representation perturbation\nmechanism into the vanilla CAM-based model. This mechanism is responsible for\ndecoupling foreground as well as background and synthesizing the counterfactual\nrepresentations. By training the detection model with these synthesized\nrepresentations, we compel the model to focus on the constant foreground\ncontent while minimizing the influence of distracting co-occurring background.\nTo our best knowledge, it is the first attempt in this direction. Extensive\nexperiments on several benchmarks demonstrate that Counterfactual-CAM\nsuccessfully mitigates the biased activation problem, achieving improved object\nlocalization accuracy.",
        "translated": "本文研究了基于类激活映射(CAM)的弱监督目标定位方法中存在的偏向激活问题。我们从因果关系的角度分析了这一问题的原因，并将其归因于共同发生的背景混杂因素。根据这一观点，我们提出了一种新的反事实共现学习范式，通过耦合常数前景和未实现背景来综合反事实表征，以切断它们之间的共现关系。具体来说，我们设计了一种新的网络结构，称为反事实-CAM，它将反事实表示扰动机制嵌入到普通的基于 CAM 的模型中。该机制负责解耦前景和背景，并综合反事实表示。通过训练这些综合表征的检测模型，我们迫使模型集中在恒定的前景内容，同时尽量减少干扰共现背景的影响。据我们所知，这是朝这个方向的第一次尝试。在几个基准上的大量实验表明，反事实 CAM 成功地缓解了有偏激活问题，提高了目标定位精度。"
    },
    {
        "title": "Uni-ControlNet: All-in-One Control to Text-to-Image Diffusion Models",
        "url": "http://arxiv.org/abs/2305.16322v1",
        "pub_date": "2023-05-25",
        "summary": "Text-to-Image diffusion models have made tremendous progress over the past\ntwo years, enabling the generation of highly realistic images based on\nopen-domain text descriptions. However, despite their success, text\ndescriptions often struggle to adequately convey detailed controls, even when\ncomposed of long and complex texts. Moreover, recent studies have also shown\nthat these models face challenges in understanding such complex texts and\ngenerating the corresponding images. Therefore, there is a growing need to\nenable more control modes beyond text description. In this paper, we introduce\nUni-ControlNet, a novel approach that allows for the simultaneous utilization\nof different local controls (e.g., edge maps, depth map, segmentation masks)\nand global controls (e.g., CLIP image embeddings) in a flexible and composable\nmanner within one model. Unlike existing methods, Uni-ControlNet only requires\nthe fine-tuning of two additional adapters upon frozen pre-trained\ntext-to-image diffusion models, eliminating the huge cost of training from\nscratch. Moreover, thanks to some dedicated adapter designs, Uni-ControlNet\nonly necessitates a constant number (i.e., 2) of adapters, regardless of the\nnumber of local or global controls used. This not only reduces the fine-tuning\ncosts and model size, making it more suitable for real-world deployment, but\nalso facilitate composability of different conditions. Through both\nquantitative and qualitative comparisons, Uni-ControlNet demonstrates its\nsuperiority over existing methods in terms of controllability, generation\nquality and composability. Code is available at\n\\url{https://github.com/ShihaoZhaoZSH/Uni-ControlNet}.",
        "translated": "在过去的两年中，文本到图像的扩散模型取得了巨大的进展，使得基于开放域文本描述的高度真实感图像的生成成为可能。然而，尽管成功，文本描述往往难以充分传达详细的控制，即使是组成了长期和复杂的文本。此外，最近的研究也表明，这些模型在理解这些复杂的文本和生成相应的图像面临挑战。因此，越来越需要在文本描述之外启用更多的控制模式。在本文中，我们介绍了 Uni-ControlNet，一种新的方法，允许同时利用不同的局部控件(例如，边缘映射，深度映射，分割掩码)和全局控件(例如，CLIP 图像嵌入)在一个灵活和可组合的方式在一个模型。与现有的方法不同，Uni-ControlNet 只需要在冻结的预先训练的文本到图像扩散模型上对另外两个适配器进行微调，从而消除了从头开始训练的巨大成本。此外，由于一些专用的适配器设计，Uni-ControlNet 只需要一个常数(即2)的适配器，而不管所使用的本地或全局控件的数量。这不仅降低了微调成本和模型大小，使其更适合于实际部署，而且还促进了不同条件的可组合性。通过定量和定性比较，Uni-ControlNet 在可控性、生成质量和可组合性等方面均优于现有方法。代码可在网址{ https://github.com/shihaozhaozsh/uni-controlnet }下载。"
    },
    {
        "title": "Eclipse: Disambiguating Illumination and Materials using Unintended\n  Shadows",
        "url": "http://arxiv.org/abs/2305.16321v1",
        "pub_date": "2023-05-25",
        "summary": "Decomposing an object's appearance into representations of its materials and\nthe surrounding illumination is difficult, even when the object's 3D shape is\nknown beforehand. This problem is ill-conditioned because diffuse materials\nseverely blur incoming light, and is ill-posed because diffuse materials under\nhigh-frequency lighting can be indistinguishable from shiny materials under\nlow-frequency lighting. We show that it is possible to recover precise\nmaterials and illumination -- even from diffuse objects -- by exploiting\nunintended shadows, like the ones cast onto an object by the photographer who\nmoves around it. These shadows are a nuisance in most previous inverse\nrendering pipelines, but here we exploit them as signals that improve\nconditioning and help resolve material-lighting ambiguities. We present a\nmethod based on differentiable Monte Carlo ray tracing that uses images of an\nobject to jointly recover its spatially-varying materials, the surrounding\nillumination environment, and the shapes of the unseen light occluders who\ninadvertently cast shadows upon it.",
        "translated": "分解一个物体的外观到其材料和周围照明的表示是困难的，即使当物体的三维形状是已知的。这个问题是病态的，因为漫反射材料严重模糊入射光，而且是病态的，因为在高频照明下漫反射材料可以与低频照明下发光材料难以区分。我们展示了通过利用意想不到的阴影(如摄影师在物体周围移动时投射到物体上的阴影)来恢复精确的材料和照明——即使是从漫反射的物体上也是可能的。这些阴影在大多数以前的反向渲染管道中是一个麻烦，但在这里我们利用它们作为改善条件反射和帮助解决材质-照明模糊的信号。我们提出了一种基于可微蒙特卡罗射线追踪的方法，该方法利用一个物体的图像来共同恢复其空间变化的材料，周围的照明环境，以及无意中在其上投射阴影的看不见的光遮挡物的形状。"
    },
    {
        "title": "Image is First-order Norm+Linear Autoregressive",
        "url": "http://arxiv.org/abs/2305.16319v1",
        "pub_date": "2023-05-25",
        "summary": "This paper reveals that every image can be understood as a first-order\nnorm+linear autoregressive process, referred to as FINOLA, where norm+linear\ndenotes the use of normalization before the linear model. We demonstrate that\nimages of size 256$\\times$256 can be reconstructed from a compressed vector\nusing autoregression up to a 16$\\times$16 feature map, followed by upsampling\nand convolution. This discovery sheds light on the underlying partial\ndifferential equations (PDEs) governing the latent feature space. Additionally,\nwe investigate the application of FINOLA for self-supervised learning through a\nsimple masked prediction technique. By encoding a single unmasked quadrant\nblock, we can autoregressively predict the surrounding masked region.\nRemarkably, this pre-trained representation proves effective for image\nclassification and object detection tasks, even in lightweight networks,\nwithout requiring fine-tuning. The code will be made publicly available.",
        "translated": "本文揭示了每幅图像都可以理解为一阶范数 + 线性自回归过程，称为 FINOLA，其中范数 + 线性表示在线性模型之前使用归一化。我们证明了大小为256美元乘以256美元的图像可以从一个压缩向量重建使用自回归高达16美元乘以16美元的特征映射，然后上采样和卷积。这一发现揭示了控制潜在特征空间的基本偏微分方程(PDE)。此外，我们还通过一个简单的掩蔽预测技术研究了 FINOLA 在自监督学习中的应用。通过编码一个未遮蔽的象限块，我们可以自回归地预测周围的遮蔽区域。值得注意的是，这种预先训练的表示被证明对图像分类和目标检测任务非常有效，即使在轻量级网络中，也不需要进行微调。代码将公开发布。"
    },
    {
        "title": "Referred by Multi-Modality: A Unified Temporal Transformer for Video\n  Object Segmentation",
        "url": "http://arxiv.org/abs/2305.16318v1",
        "pub_date": "2023-05-25",
        "summary": "Recently, video object segmentation (VOS) referred by multi-modal signals,\ne.g., language and audio, has evoked increasing attention in both industry and\nacademia. It is challenging for exploring the semantic alignment within\nmodalities and the visual correspondence across frames. However, existing\nmethods adopt separate network architectures for different modalities, and\nneglect the inter-frame temporal interaction with references. In this paper, we\npropose MUTR, a Multi-modal Unified Temporal transformer for Referring video\nobject segmentation. With a unified framework for the first time, MUTR adopts a\nDETR-style transformer and is capable of segmenting video objects designated by\neither text or audio reference. Specifically, we introduce two strategies to\nfully explore the temporal relations between videos and multi-modal signals.\nFirstly, for low-level temporal aggregation before the transformer, we enable\nthe multi-modal references to capture multi-scale visual cues from consecutive\nvideo frames. This effectively endows the text or audio signals with temporal\nknowledge and boosts the semantic alignment between modalities. Secondly, for\nhigh-level temporal interaction after the transformer, we conduct inter-frame\nfeature communication for different object embeddings, contributing to better\nobject-wise correspondence for tracking along the video. On Ref-YouTube-VOS and\nAVSBench datasets with respective text and audio references, MUTR achieves\n+4.2% and +4.2% J&amp;F improvements to state-of-the-art methods, demonstrating our\nsignificance for unified multi-modal VOS. Code is released at\nhttps://github.com/OpenGVLab/MUTR.",
        "translated": "近年来，基于语言、音频等多模态信号的视频对象分割技术引起了业界和学术界的广泛关注。这对于探索模式内的语义对齐和跨框架的视觉对应是一个挑战。然而，现有的方法针对不同的模式采用不同的网络结构，而忽略了帧间与参考文献的时间交互。本文提出了一种多模态统一时态转换器 MUTR，用于参考视频对象分割。MUTR 首次采用了统一的框架，采用了 DETR 风格的变换器，能够对文本或音频参考指定的视频对象进行分割。具体来说，我们引入了两种策略来充分探索视频和多模态信号之间的时间关系。首先，对于转换前的低级时间聚合，我们使多模态参考能够从连续的视频帧中捕获多尺度的视觉线索。这有效地赋予了文本或音频信号时间知识，并提高了形态之间的语义对齐。其次，对于变压器后的高层次时间交互，针对不同的目标嵌入进行帧间特征通信，有助于提高视频跟踪的目标对应性。在 Ref-YouTube-VOS 和 AVSBench 数据集上，各自的文本和音频参考，MUTR 对最先进的方法实现了 + 4.2% 和 + 4.2% 的 J & F 改进，表明了我们对统一的多模态 VOS 的重要性。代码在 https://github.com/opengvlab/mutr 发布。"
    },
    {
        "title": "Making Vision Transformers Truly Shift-Equivariant",
        "url": "http://arxiv.org/abs/2305.16316v1",
        "pub_date": "2023-05-25",
        "summary": "For computer vision tasks, Vision Transformers (ViTs) have become one of the\ngo-to deep net architectures. Despite being inspired by Convolutional Neural\nNetworks (CNNs), ViTs remain sensitive to small shifts in the input image. To\naddress this, we introduce novel designs for each of the modules in ViTs, such\nas tokenization, self-attention, patch merging, and positional encoding. With\nour proposed modules, we achieve truly shift-equivariant ViTs on four\nwell-established models, namely, Swin, SwinV2, MViTv2, and CvT, both in theory\nand practice. Empirically, we tested these models on image classification and\nsemantic segmentation, achieving competitive performance across three different\ndatasets while maintaining 100% shift consistency.",
        "translated": "在计算机视觉任务中，视觉变换器(ViTs)已经成为一种常用的深层网络体系结构。尽管受到卷积神经网络(CNN)的启发，ViTs 仍然对输入图像的微小变化敏感。为了解决这个问题，我们为 ViT 中的每个模块引入了新的设计，例如标记化、自注意、补丁合并和位置编码。通过我们提出的模块，我们在理论和实践上实现了四个已经建立的模型，即 Swin，SwinV2，MViTv2和 CvT 上的真正的移位等变 VIT。经验上，我们在图像分类和语义分割上测试了这些模型，在保持100% 移位一致性的同时，在三个不同的数据集上实现了竞争性能。"
    },
    {
        "title": "NAP: Neural 3D Articulation Prior",
        "url": "http://arxiv.org/abs/2305.16315v1",
        "pub_date": "2023-05-25",
        "summary": "We propose Neural 3D Articulation Prior (NAP), the first 3D deep generative\nmodel to synthesize 3D articulated object models. Despite the extensive\nresearch on generating 3D objects, compositions, or scenes, there remains a\nlack of focus on capturing the distribution of articulated objects, a common\nobject category for human and robot interaction. To generate articulated\nobjects, we first design a novel articulation tree/graph parameterization and\nthen apply a diffusion-denoising probabilistic model over this representation\nwhere articulated objects can be generated via denoising from random complete\ngraphs. In order to capture both the geometry and the motion structure whose\ndistribution will affect each other, we design a graph-attention denoising\nnetwork for learning the reverse diffusion process. We propose a novel distance\nthat adapts widely used 3D generation metrics to our novel task to evaluate\ngeneration quality, and experiments demonstrate our high performance in\narticulated object generation. We also demonstrate several conditioned\ngeneration applications, including Part2Motion, PartNet-Imagination,\nMotion2Part, and GAPart2Object.",
        "translated": "我们提出了神经三维关节优先级(nAP) ，这是第一个合成三维关节物体模型的三维深度生成模型。尽管在生成三维物体、构图或场景方面有着广泛的研究，但是对于捕捉关节物体的分布这一人机交互的常见对象类别仍然缺乏关注。为了生成关节对象，我们首先设计了一个新的关节树/图形参量化，然后应用扩散去噪概率模型，通过从随机完整图中去噪来生成关节对象。为了捕捉反向扩散过程中几何和运动结构的相互影响，我们设计了一个图注意去噪网络来学习反向扩散过程。我们提出了一个新的距离，适应广泛使用的三维生成度量的新任务，以评估生成质量，实验表明我们的高性能的铰接对象生成。我们还演示了几个条件生成应用程序，包括 Part2Motion、 PartNet-Imagination、 Motion2Part 和 GAPart2Object。"
    },
    {
        "title": "Banana: Banach Fixed-Point Network for Pointcloud Segmentation with\n  Inter-Part Equivariance",
        "url": "http://arxiv.org/abs/2305.16314v1",
        "pub_date": "2023-05-25",
        "summary": "Equivariance has gained strong interest as a desirable network property that\ninherently ensures robust generalization. However, when dealing with complex\nsystems such as articulated objects or multi-object scenes, effectively\ncapturing inter-part transformations poses a challenge, as it becomes entangled\nwith the overall structure and local transformations. The interdependence of\npart assignment and per-part group action necessitates a novel equivariance\nformulation that allows for their co-evolution. In this paper, we present\nBanana, a Banach fixed-point network for equivariant segmentation with\ninter-part equivariance by construction. Our key insight is to iteratively\nsolve a fixed-point problem, where point-part assignment labels and per-part\nSE(3)-equivariance co-evolve simultaneously. We provide theoretical derivations\nof both per-step equivariance and global convergence, which induces an\nequivariant final convergent state. Our formulation naturally provides a strict\ndefinition of inter-part equivariance that generalizes to unseen inter-part\nconfigurations. Through experiments conducted on both articulated objects and\nmulti-object scans, we demonstrate the efficacy of our approach in achieving\nstrong generalization under inter-part transformations, even when confronted\nwith substantial changes in pointcloud geometry and topology.",
        "translated": "等方差作为一种理想的网络属性，在本质上确保了鲁棒的推广，已经引起了人们的极大兴趣。然而，当处理复杂的系统，如铰接对象或多对象场景，有效地捕获部分间的转换提出了一个挑战，因为它成为纠缠在整体结构和局部转换。部分分配和每部分群体行为的相互依赖性需要一个新的等方差公式，允许它们的协同演化。本文提出了一种基于构造的 Banana 不动点网络，用于部分间等方差的等变分割。我们的主要见解是迭代求解一个不动点问题，其中点部分分配标签和每部分 SE (3)-等方差同时共同演化。我们给出了每步等方差和全局收敛的理论推导，得到了一个等变的最终收敛状态。我们的公式自然提供了一个严格的部分间等方差的定义，推广到看不见的部分间配置。通过对关联对象和多目标扫描的实验，我们证明了该方法在部分间转换下实现强泛化的有效性，即使在面临点云几何和拓扑结构的实质性变化时也是如此。"
    },
    {
        "title": "Break-A-Scene: Extracting Multiple Concepts from a Single Image",
        "url": "http://arxiv.org/abs/2305.16311v1",
        "pub_date": "2023-05-25",
        "summary": "Text-to-image model personalization aims to introduce a user-provided concept\nto the model, allowing its synthesis in diverse contexts. However, current\nmethods primarily focus on the case of learning a single concept from multiple\nimages with variations in backgrounds and poses, and struggle when adapted to a\ndifferent scenario. In this work, we introduce the task of textual scene\ndecomposition: given a single image of a scene that may contain several\nconcepts, we aim to extract a distinct text token for each concept, enabling\nfine-grained control over the generated scenes. To this end, we propose\naugmenting the input image with masks that indicate the presence of target\nconcepts. These masks can be provided by the user or generated automatically by\na pre-trained segmentation model. We then present a novel two-phase\ncustomization process that optimizes a set of dedicated textual embeddings\n(handles), as well as the model weights, striking a delicate balance between\naccurately capturing the concepts and avoiding overfitting. We employ a masked\ndiffusion loss to enable handles to generate their assigned concepts,\ncomplemented by a novel loss on cross-attention maps to prevent entanglement.\nWe also introduce union-sampling, a training strategy aimed to improve the\nability of combining multiple concepts in generated images. We use several\nautomatic metrics to quantitatively compare our method against several\nbaselines, and further affirm the results using a user study. Finally, we\nshowcase several applications of our method. Project page is available at:\nhttps://omriavrahami.com/break-a-scene/",
        "translated": "文本到图像模型个性化旨在向模型引入用户提供的概念，允许在不同的上下文中进行合成。然而，目前的方法主要集中在从背景和姿势不同的多幅图像中学习单一概念的情况下，并在适应不同情景时进行斗争。在这项工作中，我们介绍了文本场景分解的任务: 给定一个场景的单个图像，可能包含几个概念，我们的目标是提取一个不同的文本标记为每个概念，使细粒度控制生成的场景。为此，我们提出用掩码来增强输入图像，以表明目标概念的存在。这些掩码可以由用户提供，也可以由预先训练好的分割模型自动生成。然后，我们提出了一个新的两阶段定制过程，优化了一组专用的文本嵌入(处理) ，以及模型权重，在准确捕捉概念和避免过度拟合之间找到人海万花筒(电影)。我们使用一个掩蔽的扩散损失，使处理能够生成其指定的概念，补充交叉注意地图上的一个新的损失，以防止纠缠。我们还引入了联合采样，这是一种旨在提高生成图像中多个概念组合能力的训练策略。我们使用几个自动指标来定量比较我们的方法与几个基线，并进一步确认结果使用用户研究。最后，我们展示了我们的方法的几个应用。项目网页可于以下 https://omriavrahami.com/break-a-scene/下载:"
    },
    {
        "title": "UMat: Uncertainty-Aware Single Image High Resolution Material Capture",
        "url": "http://arxiv.org/abs/2305.16312v1",
        "pub_date": "2023-05-25",
        "summary": "We propose a learning-based method to recover normals, specularity, and\nroughness from a single diffuse image of a material, using microgeometry\nappearance as our primary cue. Previous methods that work on single images tend\nto produce over-smooth outputs with artifacts, operate at limited resolution,\nor train one model per class with little room for generalization. Previous\nmethods that work on single images tend to produce over-smooth outputs with\nartifacts, operate at limited resolution, or train one model per class with\nlittle room for generalization. In contrast, in this work, we propose a novel\ncapture approach that leverages a generative network with attention and a U-Net\ndiscriminator, which shows outstanding performance integrating global\ninformation at reduced computational complexity. We showcase the performance of\nour method with a real dataset of digitized textile materials and show that a\ncommodity flatbed scanner can produce the type of diffuse illumination required\nas input to our method. Additionally, because the problem might be illposed\n-more than a single diffuse image might be needed to disambiguate the specular\nreflection- or because the training dataset is not representative enough of the\nreal distribution, we propose a novel framework to quantify the model's\nconfidence about its prediction at test time. Our method is the first one to\ndeal with the problem of modeling uncertainty in material digitization,\nincreasing the trustworthiness of the process and enabling more intelligent\nstrategies for dataset creation, as we demonstrate with an active learning\nexperiment.",
        "translated": "我们提出了一个基于学习的方法来恢复法线，反射率和粗糙度从一个单一的漫反射图像的材料，使用显微几何外观作为我们的主要线索。以前用于单幅图像的方法倾向于产生带有工件的过于平滑的输出，在有限的分辨率下操作，或者每类训练一个模型，几乎没有泛化的空间。以前用于单幅图像的方法倾向于产生带有工件的过于平滑的输出，在有限的分辨率下操作，或者每类训练一个模型，几乎没有泛化的空间。相比之下，在这项工作中，我们提出了一种新的捕获方法，利用生成网络的注意力和 U-Net 鉴别器，它显示了在降低计算复杂度的情况下集成全局信息的出色性能。我们展示了我们的方法的性能与数字化纺织材料的真实数据集，并表明，一个商品平板扫描仪可以产生所需的漫反射照明类型作为输入我们的方法。此外，由于问题可能是病态的——消除镜面反射(物理)可能需要不止一张漫反射图像——或者由于训练数据集不足以代表真实分布，我们提出了一个新的框架来量化模型在测试时对其预测的信心。我们的方法是第一个处理材料数字化建模不确定性的问题，提高过程的可信度，使数据集创建更智能的策略，正如我们用一个积极的学习实验所证明的那样。"
    },
    {
        "title": "Securing Deep Generative Models with Universal Adversarial Signature",
        "url": "http://arxiv.org/abs/2305.16310v1",
        "pub_date": "2023-05-25",
        "summary": "Recent advances in deep generative models have led to the development of\nmethods capable of synthesizing high-quality, realistic images. These models\npose threats to society due to their potential misuse. Prior research attempted\nto mitigate these threats by detecting generated images, but the varying traces\nleft by different generative models make it challenging to create a universal\ndetector capable of generalizing to new, unseen generative models. In this\npaper, we propose to inject a universal adversarial signature into an arbitrary\npre-trained generative model, in order to make its generated contents more\ndetectable and traceable. First, the imperceptible optimal signature for each\nimage can be found by a signature injector through adversarial training.\nSubsequently, the signature can be incorporated into an arbitrary generator by\nfine-tuning it with the images processed by the signature injector. In this\nway, the detector corresponding to the signature can be reused for any\nfine-tuned generator for tracking the generator identity. The proposed method\nis validated on the FFHQ and ImageNet datasets with various state-of-the-art\ngenerative models, consistently showing a promising detection rate. Code will\nbe made publicly available at \\url{https://github.com/zengxianyu/genwm}.",
        "translated": "深度生成模型的最新进展导致了能够合成高质量、真实图像的方法的发展。这些模式由于可能被滥用而对社会构成威胁。先前的研究试图通过检测生成的图像来减轻这些威胁，但不同的生成模型留下的不同痕迹使得创建一个能够推广到新的、看不见的生成模型的通用检测器具有挑战性。在这篇文章中，我们建议将一个通用的对抗性签名注入到一个任意的预先训练的生成模型中，以使其生成的内容更加可检测和可追踪。首先，通过对抗训练，利用签名注入器可以找到每幅图像的不可察觉的最优签名。随后，可以通过使用签名注入器处理的图像对签名进行微调，从而将签名合并到任意生成器中。这样，对应于签名的检测器可以重用于任何微调发生器，用于跟踪发生器标识。该方法在 FFHQ 和 ImageNet 数据集上通过各种最先进的生成模型进行了验证，一致地显示出有希望的检测率。代码将在 url { https://github.com/zengxianyu/genwm }公开发布。"
    },
    {
        "title": "NeuManifold: Neural Watertight Manifold Reconstruction with Efficient\n  and High-Quality Rendering Support",
        "url": "http://arxiv.org/abs/2305.17134v1",
        "pub_date": "2023-05-26",
        "summary": "We present a method for generating high-quality watertight manifold meshes\nfrom multi-view input images. Existing volumetric rendering methods are robust\nin optimization but tend to generate noisy meshes with poor topology.\nDifferentiable rasterization-based methods can generate high-quality meshes but\nare sensitive to initialization. Our method combines the benefits of both\nworlds; we take the geometry initialization obtained from neural volumetric\nfields, and further optimize the geometry as well as a compact neural texture\nrepresentation with differentiable rasterizers. Through extensive experiments,\nwe demonstrate that our method can generate accurate mesh reconstructions with\nfaithful appearance that are comparable to previous volume rendering methods\nwhile being an order of magnitude faster in rendering. We also show that our\ngenerated mesh and neural texture reconstruction is compatible with existing\ngraphics pipelines and enables downstream 3D applications such as simulation.\nProject page: https://sarahweiii.github.io/neumanifold/",
        "translated": "提出了一种从多视图输入图像中生成高质量防水流形网格的方法。现有的体绘制方法在优化时具有鲁棒性，但容易产生拓扑结构较差的噪声网格。基于可微栅格化的方法可以生成高质量的网格，但对初始化非常敏感。我们的方法结合了两个世界的好处，我们采取的几何初始化获得的神经体积领域，并进一步优化的几何以及紧凑的神经纹理表示与可微光栅。通过大量的实验，我们证明了我们的方法可以产生准确的网格重建与忠实的外观相比，以前的立体渲染方法，同时是一个数量级更快的渲染。我们还表明，我们生成的网格和神经纹理重建与现有的图形管道兼容，并支持下游3D 应用程序，如模拟。项目主页:  https://sarahweiii.github.io/neumanifold/"
    },
    {
        "title": "Manifold Regularization for Memory-Efficient Training of Deep Neural\n  Networks",
        "url": "http://arxiv.org/abs/2305.17119v1",
        "pub_date": "2023-05-26",
        "summary": "One of the prevailing trends in the machine- and deep-learning community is\nto gravitate towards the use of increasingly larger models in order to keep\npushing the state-of-the-art performance envelope. This tendency makes access\nto the associated technologies more difficult for the average practitioner and\nruns contrary to the desire to democratize knowledge production in the field.\nIn this paper, we propose a framework for achieving improved memory efficiency\nin the process of learning traditional neural networks by leveraging\ninductive-bias-driven network design principles and layer-wise\nmanifold-oriented regularization objectives. Use of the framework results in\nimproved absolute performance and empirical generalization error relative to\ntraditional learning techniques. We provide empirical validation of the\nframework, including qualitative and quantitative evidence of its effectiveness\non two standard image datasets, namely CIFAR-10 and CIFAR-100. The proposed\nframework can be seamlessly combined with existing network compression methods\nfor further memory savings.",
        "translated": "机器和深度学习领域的一个流行趋势是，为了不断推进最先进的性能外壳，越来越多地使用更大的模型。这一趋势使得普通从业者更难获得相关技术，并与实地知识生产民主化的愿望背道而驰。本文提出了一种在学习传统神经网络过程中，利用感应偏置驱动的网络设计原理和面向分层流形的正则化目标来提高存储效率的框架。相对于传统的学习方法，使用这个框架可以提高绝对表现和经验泛化误差。我们提供了框架的经验验证，包括定性和定量的证据，其有效性的两个标准图像数据集，即 CIFAR-10和 CIFAR-100。该框架可以与现有的网络压缩方法无缝结合，以进一步节省内存。"
    },
    {
        "title": "Random-Access Neural Compression of Material Textures",
        "url": "http://arxiv.org/abs/2305.17105v1",
        "pub_date": "2023-05-26",
        "summary": "The continuous advancement of photorealism in rendering is accompanied by a\ngrowth in texture data and, consequently, increasing storage and memory\ndemands. To address this issue, we propose a novel neural compression technique\nspecifically designed for material textures. We unlock two more levels of\ndetail, i.e., 16x more texels, using low bitrate compression, with image\nquality that is better than advanced image compression techniques, such as AVIF\nand JPEG XL. At the same time, our method allows on-demand, real-time\ndecompression with random access similar to block texture compression on GPUs,\nenabling compression on disk and memory. The key idea behind our approach is\ncompressing multiple material textures and their mipmap chains together, and\nusing a small neural network, that is optimized for each material, to\ndecompress them. Finally, we use a custom training implementation to achieve\npractical compression speeds, whose performance surpasses that of general\nframeworks, like PyTorch, by an order of magnitude.",
        "translated": "随着渲染技术的不断进步，纹理数据也不断增加，存储和内存需求也随之增加。为了解决这个问题，我们提出了一种新的神经压缩技术，专门为材质纹理设计。我们使用低比特率压缩技术解锁更多的细节，即16倍以上的文本，其图像质量优于高级图像压缩技术，如 AVIF 和 JPEG XL。与此同时，我们的方法允许按需实时解压缩，随机访问类似于 gpUs 上的块纹理压缩，允许在磁盘和内存上进行压缩。我们方法背后的关键思想是将多种材质的纹理和它们的 mipmap 链压缩在一起，并使用一个针对每种材质优化的小型神经网络来解压缩它们。最后，我们使用一个定制的训练实现来实现实际的压缩速度，其性能超过了一般框架(如 PyTorch)的一个数量级。"
    },
    {
        "title": "GeoVLN: Learning Geometry-Enhanced Visual Representation with Slot\n  Attention for Vision-and-Language Navigation",
        "url": "http://arxiv.org/abs/2305.17102v1",
        "pub_date": "2023-05-26",
        "summary": "Most existing works solving Room-to-Room VLN problem only utilize RGB images\nand do not consider local context around candidate views, which lack sufficient\nvisual cues about surrounding environment. Moreover, natural language contains\ncomplex semantic information thus its correlations with visual inputs are hard\nto model merely with cross attention. In this paper, we propose GeoVLN, which\nlearns Geometry-enhanced visual representation based on slot attention for\nrobust Visual-and-Language Navigation. The RGB images are compensated with the\ncorresponding depth maps and normal maps predicted by Omnidata as visual\ninputs. Technically, we introduce a two-stage module that combine local slot\nattention and CLIP model to produce geometry-enhanced representation from such\ninput. We employ V&amp;L BERT to learn a cross-modal representation that\nincorporate both language and vision informations. Additionally, a novel\nmultiway attention module is designed, encouraging different phrases of input\ninstruction to exploit the most related features from visual input. Extensive\nexperiments demonstrate the effectiveness of our newly designed modules and\nshow the compelling performance of the proposed method.",
        "translated": "现有的解决房间到房间 VLN 问题的作品大多只利用 RGB 图像，没有考虑候选视图周围的局部环境，缺乏对周围环境的足够的视觉线索。此外，自然语言包含复杂的语义信息，因此它与视觉输入的相关性很难仅仅通过交叉注意来建模。本文提出了一种基于时隙注意学习几何增强视觉表示的 GeoVLN，用于鲁棒的视觉语言导航。RGB 图像由相应的深度图和由 Omnidata 预测的法线图作为视觉输入进行补偿。在技术上，我们引入了一个两阶段的模块，结合本地时隙注意和 CLIP 模型，从这样的输入产生几何增强的表示。我们使用 V & L BERT 来学习包含语言和视觉信息的跨模态表示。此外，设计了一个新颖的多路注意模块，鼓励输入指令的不同阶段从视觉输入中发掘最相关的特征。大量的实验证明了我们新设计的模块的有效性，并显示了该方法的引人注目的性能。"
    },
    {
        "title": "ControlVideo: Adding Conditional Control for One Shot Text-to-Video\n  Editing",
        "url": "http://arxiv.org/abs/2305.17098v1",
        "pub_date": "2023-05-26",
        "summary": "In this paper, we present ControlVideo, a novel method for text-driven video\nediting. Leveraging the capabilities of text-to-image diffusion models and\nControlNet, ControlVideo aims to enhance the fidelity and temporal consistency\nof videos that align with a given text while preserving the structure of the\nsource video. This is achieved by incorporating additional conditions such as\nedge maps, fine-tuning the key-frame and temporal attention on the source\nvideo-text pair with carefully designed strategies. An in-depth exploration of\nControlVideo's design is conducted to inform future research on one-shot tuning\nvideo diffusion models. Quantitatively, ControlVideo outperforms a range of\ncompetitive baselines in terms of faithfulness and consistency while still\naligning with the textual prompt. Additionally, it delivers videos with high\nvisual realism and fidelity w.r.t. the source content, demonstrating\nflexibility in utilizing controls containing varying degrees of source video\ninformation, and the potential for multiple control combinations. The project\npage is available at\n\\href{https://ml.cs.tsinghua.edu.cn/controlvideo/}{https://ml.cs.tsinghua.edu.cn/controlvideo/}.",
        "translated": "本文提出了一种新的文本驱动视频编辑方法 ControlVideo。ControlVideo 利用文本到图像扩散模型和 ControlNet 的能力，旨在提高与给定文本对齐的视频的保真度和时间一致性，同时保留源视频的结构。这是通过加入额外的条件，如边缘地图，微调关键帧和时间注意力的源视频文本对与精心设计的策略。本文对 ControlVideo 的设计进行了深入的探索，为今后一次调谐视频扩散模型的研究提供了参考。在数量上，ControlVideo 在忠实性和一致性方面优于一系列竞争性基线，同时仍然与文本提示保持一致。此外，它提供的视频具有高度的视觉真实感和源内容的保真度，展示了在利用包含不同程度的源视频信息的控件方面的灵活性，以及多种控件组合的潜力。项目页面可在 href { https://ml.cs.tsinghua.edu.cn/controlvideo/}{ https://ml.cs.tsinghua.edu.cn/controlvideo/}获得。"
    },
    {
        "title": "GRAtt-VIS: Gated Residual Attention for Auto Rectifying Video Instance\n  Segmentation",
        "url": "http://arxiv.org/abs/2305.17096v1",
        "pub_date": "2023-05-26",
        "summary": "Recent trends in Video Instance Segmentation (VIS) have seen a growing\nreliance on online methods to model complex and lengthy video sequences.\nHowever, the degradation of representation and noise accumulation of the online\nmethods, especially during occlusion and abrupt changes, pose substantial\nchallenges. Transformer-based query propagation provides promising directions\nat the cost of quadratic memory attention. However, they are susceptible to the\ndegradation of instance features due to the above-mentioned challenges and\nsuffer from cascading effects. The detection and rectification of such errors\nremain largely underexplored. To this end, we introduce \\textbf{GRAtt-VIS},\n\\textbf{G}ated \\textbf{R}esidual \\textbf{Att}ention for \\textbf{V}ideo\n\\textbf{I}nstance \\textbf{S}egmentation. Firstly, we leverage a\nGumbel-Softmax-based gate to detect possible errors in the current frame. Next,\nbased on the gate activation, we rectify degraded features from its past\nrepresentation. Such a residual configuration alleviates the need for dedicated\nmemory and provides a continuous stream of relevant instance features.\nSecondly, we propose a novel inter-instance interaction using gate activation\nas a mask for self-attention. This masking strategy dynamically restricts the\nunrepresentative instance queries in the self-attention and preserves vital\ninformation for long-term tracking. We refer to this novel combination of Gated\nResidual Connection and Masked Self-Attention as \\textbf{GRAtt} block, which\ncan easily be integrated into the existing propagation-based framework.\nFurther, GRAtt blocks significantly reduce the attention overhead and simplify\ndynamic temporal modeling. GRAtt-VIS achieves state-of-the-art performance on\nYouTube-VIS and the highly challenging OVIS dataset, significantly improving\nover previous methods. Code is available at\n\\url{https://github.com/Tanveer81/GRAttVIS}.",
        "translated": "视频实例分割(VIS)最近的发展趋势是越来越依赖于在线方法来建模复杂和冗长的视频序列。然而，在线方法的表示退化和噪声积累，特别是在遮挡和突变过程中，带来了实质性的挑战。基于变压器的查询传播以二次内存注意力为代价提供了有希望的方向。然而，由于上述挑战，它们很容易受到实例特性退化的影响，并受到级联效应的影响。这些错误的发现和纠正仍然在很大程度上没有得到充分的探索。为此，我们引入 textbf { GRAtt-VIS } ，textbf { G }化 textbf { R }剩余 textbf { Att }为 textbf { V } ideo textbf { I } nstance textbf { S }分割。首先，我们利用 Gumbel-Softmax 门检测当前帧中可能存在的错误。接下来，基于门激活，我们纠正退化的特征从其过去的表示。这种剩余配置减轻了对专用内存的需求，并提供了相关实例特性的连续流。其次，我们提出了一种新的使用门激活作为自我注意掩蔽的实例间交互。这种掩蔽策略动态地限制了自注意中不具代表性的实例查询，并且为长期跟踪保留了重要信息。我们把这种门控残余连接和掩蔽自我注意的新颖组合称为 textbf { GRAtt }块，它可以很容易地集成到现有的基于传播的框架中。此外，GRAtt 块显著降低了注意开销，简化了动态时间建模。GRATt-VIS 在 YouTube-VIS 和极具挑战性的 OVIS 数据集上实现了最先进的性能，明显改进了以前的方法。代码可在网址{ https://github.com/tanveer81/grattvis }下载。"
    },
    {
        "title": "SSSegmenation: An Open Source Supervised Semantic Segmentation Toolbox\n  Based on PyTorch",
        "url": "http://arxiv.org/abs/2305.17091v1",
        "pub_date": "2023-05-26",
        "summary": "This paper presents SSSegmenation, which is an open source supervised\nsemantic image segmentation toolbox based on PyTorch. The design of this\ntoolbox is motivated by MMSegmentation while it is easier to use because of\nfewer dependencies and achieves superior segmentation performance under a\ncomparable training and testing setup. Moreover, the toolbox also provides\nplenty of trained weights for popular and contemporary semantic segmentation\nmethods, including Deeplab, PSPNet, OCRNet, MaskFormer, \\emph{etc}. We expect\nthat this toolbox can contribute to the future development of semantic\nsegmentation. Codes and model zoos are available at\n\\href{https://github.com/SegmentationBLWX/sssegmentation/}{SSSegmenation}.",
        "translated": "这篇文章介绍了一个基于 PyTorch 的开源监督语义图像分割工具箱。这个工具箱的设计是由 MMS 分割驱动的，它更容易使用，因为较少的依赖性和实现卓越的分割性能在一个可比较的培训和测试设置。此外，该工具箱还为流行的和当代的语义分割方法提供了大量的训练权重，包括 Deeplab、 PSPNet、 OCRNet、 MaskForm、 emph {等}。我们期望这个工具箱能够为语义分割的未来发展做出贡献。代码和模型动物园可在 href { https://github.com/segmentationblwx/sssegmentation/}{ SSSegmenation }获得。"
    },
    {
        "title": "Mindstorms in Natural Language-Based Societies of Mind",
        "url": "http://arxiv.org/abs/2305.17066v1",
        "pub_date": "2023-05-26",
        "summary": "Both Minsky's \"society of mind\" and Schmidhuber's \"learning to think\" inspire\ndiverse societies of large multimodal neural networks (NNs) that solve problems\nby interviewing each other in a \"mindstorm.\" Recent implementations of NN-based\nsocieties of minds consist of large language models (LLMs) and other NN-based\nexperts communicating through a natural language interface. In doing so, they\novercome the limitations of single LLMs, improving multimodal zero-shot\nreasoning. In these natural language-based societies of mind (NLSOMs), new\nagents -- all communicating through the same universal symbolic language -- are\neasily added in a modular fashion. To demonstrate the power of NLSOMs, we\nassemble and experiment with several of them (having up to 129 members),\nleveraging mindstorms in them to solve some practical AI tasks: visual question\nanswering, image captioning, text-to-image synthesis, 3D generation, egocentric\nretrieval, embodied AI, and general language-based task solving. We view this\nas a starting point towards much larger NLSOMs with billions of agents-some of\nwhich may be humans. And with this emergence of great societies of\nheterogeneous minds, many new research questions have suddenly become paramount\nto the future of artificial intelligence. What should be the social structure\nof an NLSOM? What would be the (dis)advantages of having a monarchical rather\nthan a democratic structure? How can principles of NN economies be used to\nmaximize the total reward of a reinforcement learning NLSOM? In this work, we\nidentify, discuss, and try to answer some of these questions.",
        "translated": "明斯基的“心智社会”和施密德胡贝尔的“学会思考”都激发了大型多模式神经网络(NN)的多样化社会，这些神经网络通过在“思维风暴”中相互访谈来解决问题最近基于神经网络的思维社会的实现包括大型语言模型(LLM)和其他基于神经网络的专家通过自然语言接口进行交流。这样，他们克服了单个 LLM 的局限性，改进了多模态零点推理。在这些以自然语言为基础的思维社会(NLSOM)中，新的主体——所有通过相同的通用符号语言进行交流的主体——很容易以模块化的方式添加进来。为了展示 NLSOM 的威力，我们组装并实验了其中的几个(拥有多达129个成员) ，利用其中的思维风暴来解决一些实际的人工智能任务: 视觉问题回答，图像字幕，文本到图像合成，3D 生成，自我中心检索，体现人工智能，以及一般的基于语言的任务解决。我们认为这是通往拥有数十亿代理人(其中一些可能是人类)的更大的非线性有机体的一个起点。随着这个由异质思维组成的伟大社会的出现，许多新的研究问题突然变得对人工智能的未来至关重要。什么是一个 NLSOM 的社会结构？拥有君主制而不是民主制的结构有什么好处？如何运用神经网络经济学的原理来最大限度地提高强化学习非劳动就业市场的总回报？在这项工作中，我们确定，讨论，并试图回答其中的一些问题。"
    },
    {
        "title": "Extremely weakly-supervised blood vessel segmentation with\n  physiologically based synthesis and domain adaptation",
        "url": "http://arxiv.org/abs/2305.17054v1",
        "pub_date": "2023-05-26",
        "summary": "Accurate analysis and modeling of renal functions require a precise\nsegmentation of the renal blood vessels. Micro-CT scans provide image data at\nhigher resolutions, making more small vessels near the renal cortex visible.\nAlthough deep-learning-based methods have shown state-of-the-art performance in\nautomatic blood vessel segmentations, they require a large amount of labeled\ntraining data. However, voxel-wise labeling in micro-CT scans is extremely\ntime-consuming given the huge volume sizes. To mitigate the problem, we\nsimulate synthetic renal vascular trees physiologically while generating\ncorresponding scans of the simulated trees by training a generative model on\nunlabeled scans. This enables the generative model to learn the mapping\nimplicitly without the need for explicit functions to emulate the image\nacquisition process. We further propose an additional segmentation branch over\nthe generative model trained on the generated scans. We demonstrate that the\nmodel can directly segment blood vessels on real scans and validate our method\non both 3D micro-CT scans of rat kidneys and a proof-of-concept experiment on\n2D retinal images. Code and 3D results are available at\nhttps://github.com/miccai2023anony/RenalVesselSeg",
        "translated": "精确的肾功能分析和建模需要精确的肾血管分割。微型 CT 扫描提供更高分辨率的图像数据，使肾皮质附近更多的小血管可见。尽管基于深度学习的方法在自动血管分割中表现出了最先进的性能，但是它们需要大量的标记训练数据。然而，在显微 CT 扫描的体素标记是非常耗时的，因为巨大的体积大小。为了缓解这个问题，我们在生理学上模拟合成肾血管树，同时通过训练一个生成模型对未标记的扫描产生相应的模拟树扫描。这使得生成模型能够隐式地学习映射，而不需要显式的函数来模拟图像采集过程。我们进一步提出了一个额外的分割分支超过生成模型训练生成的扫描。我们证明该模型可以直接分割血管的实际扫描和验证我们的方法都三维显微 CT 扫描大鼠肾脏和二维视网膜图像的概念验证实验。编码及立体效果可于 https://github.com/miccai2023anony/renalvesselseg 下载"
    },
    {
        "title": "SelfClean: A Self-Supervised Data Cleaning Strategy",
        "url": "http://arxiv.org/abs/2305.17048v1",
        "pub_date": "2023-05-26",
        "summary": "Most commonly used benchmark datasets for computer vision contain irrelevant\nimages, near duplicates, and label errors. Consequently, model performance on\nthese benchmarks may not be an accurate estimate of generalization ability.\nThis is a particularly acute concern in computer vision for medicine where\ndatasets are typically small, stakes are high, and annotation processes are\nexpensive and error-prone. In this paper, we propose SelfClean, a general\nprocedure to clean up image datasets exploiting a latent space learned with\nself-supervision. By relying on self-supervised learning, our approach focuses\non intrinsic properties of the data and avoids annotation biases. We formulate\ndataset cleaning as either a set of ranking problems, where human experts can\nmake decisions with significantly reduced effort, or a set of scoring problems,\nwhere decisions can be fully automated based on score distributions. We compare\nSelfClean against other algorithms on common computer vision benchmarks\nenhanced with synthetic noise and demonstrate state-of-the-art performance on\ndetecting irrelevant images, near duplicates, and label errors. In addition, we\napply our method to multiple image datasets and confirm an improvement in\nevaluation reliability.",
        "translated": "最常用的计算机视觉基准数据集包含不相关的图像、接近重复的图像和标签错误。因此，这些基准上的模型性能可能不是泛化能力的准确估计。在医学计算机视觉领域，这是一个特别严重的问题，因为数据集通常很小，风险很高，注释过程昂贵且容易出错。在本文中，我们提出了自我清理，一个通用的程序来清理图像数据集利用潜在的空间学习与自我监督。通过依赖于自监督学习，我们的方法侧重于数据的内在属性，避免了注释偏差。我们将数据集清理描述为一组排序问题，在这些问题中，人类专家可以大大减少工作量来做出决策; 或者将数据集清理描述为一组评分问题，在这些问题中，决策可以根据分数分布完全自动化。我们比较了 SelfClean 和其他基于合成噪声增强的常用计算机视觉基准的算法，并展示了在检测不相关图像、近重复图像和标签错误方面的最新性能。此外，将该方法应用于多个图像数据集，证实了该方法在评价信度方面的改进。"
    },
    {
        "title": "RAPHAEL: Text-to-Image Generation via Large Mixture of Diffusion Paths",
        "url": "http://arxiv.org/abs/2305.18295v1",
        "pub_date": "2023-05-29",
        "summary": "Text-to-image generation has recently witnessed remarkable achievements. We\nintroduce a text-conditional image diffusion model, termed RAPHAEL, to generate\nhighly artistic images, which accurately portray the text prompts, encompassing\nmultiple nouns, adjectives, and verbs. This is achieved by stacking tens of\nmixture-of-experts (MoEs) layers, i.e., space-MoE and time-MoE layers, enabling\nbillions of diffusion paths (routes) from the network input to the output. Each\npath intuitively functions as a \"painter\" for depicting a particular textual\nconcept onto a specified image region at a diffusion timestep. Comprehensive\nexperiments reveal that RAPHAEL outperforms recent cutting-edge models, such as\nStable Diffusion, ERNIE-ViLG 2.0, DeepFloyd, and DALL-E 2, in terms of both\nimage quality and aesthetic appeal. Firstly, RAPHAEL exhibits superior\nperformance in switching images across diverse styles, such as Japanese comics,\nrealism, cyberpunk, and ink illustration. Secondly, a single model with three\nbillion parameters, trained on 1,000 A100 GPUs for two months, achieves a\nstate-of-the-art zero-shot FID score of 6.61 on the COCO dataset. Furthermore,\nRAPHAEL significantly surpasses its counterparts in human evaluation on the\nViLG-300 benchmark. We believe that RAPHAEL holds the potential to propel the\nfrontiers of image generation research in both academia and industry, paving\nthe way for future breakthroughs in this rapidly evolving field. More details\ncan be found on a project webpage: https://raphael-painter.github.io/.",
        "translated": "文本到图像的生成近年来取得了显著的成就。我们引入了一个文本条件的图像扩散模型，称为 RAPHAEL，生成高度艺术化的图像，准确地描述文本提示，包括多个名词，形容词和动词。这是通过叠加数十个混合专家(MoEs)层来实现的，即空间-MoE 层和时间-MoE 层，使得从网络输入到输出的数十亿条扩散路径(路径)成为可能。每条路径直观地起到“画家”的作用，在扩散时间步骤中将特定的文本概念描绘到指定的图像区域。全面的实验表明，RAPHAEL 在图像质量和美学吸引力方面都优于最近的尖端模型，如稳定扩散，ERNIE-ViLG 2.0，DeepFloyd 和 DALL-E2。首先，RAPHAEL 在转换不同风格的图像方面表现出了卓越的表现，比如日本漫画、现实主义、赛博朋克和水墨插图。其次，一个拥有30亿个参数的单一模型，在1000个 A100图形处理器上训练了两个月，在 COCO 数据集上实现了最先进的零拍 FID 得分6.61。此外，RAPHAEL 在 ViLG-300基准的人体评估方面显著超越了同行。我们相信，RAPHAEL 具有推动学术界和工业界图像生成研究前沿的潜力，为这一快速发展领域的未来突破铺平了道路。更多详情可浏览计划网页:  https://raphael-painter.github.io/。"
    },
    {
        "title": "Mix-of-Show: Decentralized Low-Rank Adaptation for Multi-Concept\n  Customization of Diffusion Models",
        "url": "http://arxiv.org/abs/2305.18292v1",
        "pub_date": "2023-05-29",
        "summary": "Public large-scale text-to-image diffusion models, such as Stable Diffusion,\nhave gained significant attention from the community. These models can be\neasily customized for new concepts using low-rank adaptations (LoRAs). However,\nthe utilization of multiple concept LoRAs to jointly support multiple\ncustomized concepts presents a challenge. We refer to this scenario as\ndecentralized multi-concept customization, which involves single-client concept\ntuning and center-node concept fusion. In this paper, we propose a new\nframework called Mix-of-Show that addresses the challenges of decentralized\nmulti-concept customization, including concept conflicts resulting from\nexisting single-client LoRA tuning and identity loss during model fusion.\nMix-of-Show adopts an embedding-decomposed LoRA (ED-LoRA) for single-client\ntuning and gradient fusion for the center node to preserve the in-domain\nessence of single concepts and support theoretically limitless concept fusion.\nAdditionally, we introduce regionally controllable sampling, which extends\nspatially controllable sampling (e.g., ControlNet and T2I-Adaptor) to address\nattribute binding and missing object problems in multi-concept sampling.\nExtensive experiments demonstrate that Mix-of-Show is capable of composing\nmultiple customized concepts with high fidelity, including characters, objects,\nand scenes.",
        "translated": "公共的大规模文本到图像的扩散模型，如稳定扩散，已经引起了社会的重视。这些模型可以很容易地使用低等级适应性(LoRA)为新概念进行定制。然而，利用多个概念 LoRA 共同支持多个定制概念提出了一个挑战。我们将这种情况称为分散式多概念定制，其中包括单客户端概念调优和中心节点概念融合。在本文中，我们提出了一个名为 Mix-of-Show 的新框架，它解决了分散式多概念定制的挑战，包括现有的单客户 LoRA 调优导致的概念冲突和模型融合过程中的身份丢失。Mix-of-Show 采用嵌入分解 LoRA (ED-LoRA)进行单客户端调优和中心节点的梯度融合，以保留单个概念的域内实质，并支持理论上的无限概念融合。此外，本文还引入了区域可控抽样，扩展了空间可控抽样(如 ControlNet 和 T2I-Adaptor) ，解决了多概念抽样中的属性绑定和缺失对象问题。大量的实验表明，Mix-of-Show 能够以高保真度组合多个定制概念，包括人物、对象和场景。"
    },
    {
        "title": "LaFTer: Label-Free Tuning of Zero-shot Classifier using Language and\n  Unlabeled Image Collections",
        "url": "http://arxiv.org/abs/2305.18287v1",
        "pub_date": "2023-05-29",
        "summary": "Recently, large-scale pre-trained Vision and Language (VL) models have set a\nnew state-of-the-art (SOTA) in zero-shot visual classification enabling\nopen-vocabulary recognition of potentially unlimited set of categories defined\nas simple language prompts. However, despite these great advances, the\nperformance of these zeroshot classifiers still falls short of the results of\ndedicated (closed category set) classifiers trained with supervised fine\ntuning. In this paper we show, for the first time, how to reduce this gap\nwithout any labels and without any paired VL data, using an unlabeled image\ncollection and a set of texts auto-generated using a Large Language Model (LLM)\ndescribing the categories of interest and effectively substituting labeled\nvisual instances of those categories. Using our label-free approach, we are\nable to attain significant performance improvements over the zero-shot\nperformance of the base VL model and other contemporary methods and baselines\non a wide variety of datasets, demonstrating absolute improvement of up to\n11.7% (3.8% on average) in the label-free setting. Moreover, despite our\napproach being label-free, we observe 1.3% average gains over leading few-shot\nprompting baselines that do use 5-shot supervision.",
        "translated": "最近，大规模的预先训练的视觉和语言(VL)模型在零拍视觉分类中设置了一个新的最先进的(SOTA) ，使得开放词汇表能够识别定义为简单语言提示的潜在无限的类别集。然而，尽管有这些巨大的进步，这些零拍分类器的性能仍然不能满足专用(封闭类集)分类器的结果与监督微调训练。在本文中，我们首次展示了如何在不使用任何标签和任何配对 VL 数据的情况下，通过使用一个未标签的图像集和一组使用大语言模型(LLM)自动生成的文本来缩小这种差距，并有效地替换这些类别的标签视觉实例。使用我们的无标签方法，我们能够在广泛的数据集上获得比基础 VL 模型和其他当代方法和基线的零射击性能更显著的性能改进，显示在无标签设置下绝对改善高达11.7% (平均3.8%)。此外，尽管我们的方法是没有标签的，我们观察到1.3% 的平均收益超过领先的几杆提示基线，确实使用5杆监督。"
    },
    {
        "title": "Photoswap: Personalized Subject Swapping in Images",
        "url": "http://arxiv.org/abs/2305.18286v1",
        "pub_date": "2023-05-29",
        "summary": "In an era where images and visual content dominate our digital landscape, the\nability to manipulate and personalize these images has become a necessity.\nEnvision seamlessly substituting a tabby cat lounging on a sunlit window sill\nin a photograph with your own playful puppy, all while preserving the original\ncharm and composition of the image. We present Photoswap, a novel approach that\nenables this immersive image editing experience through personalized subject\nswapping in existing images. Photoswap first learns the visual concept of the\nsubject from reference images and then swaps it into the target image using\npre-trained diffusion models in a training-free manner. We establish that a\nwell-conceptualized visual subject can be seamlessly transferred to any image\nwith appropriate self-attention and cross-attention manipulation, maintaining\nthe pose of the swapped subject and the overall coherence of the image.\nComprehensive experiments underscore the efficacy and controllability of\nPhotoswap in personalized subject swapping. Furthermore, Photoswap\nsignificantly outperforms baseline methods in human ratings across subject\nswapping, background preservation, and overall quality, revealing its vast\napplication potential, from entertainment to professional editing.",
        "translated": "在一个图像和视觉内容主导我们的数字景观的时代，操纵和个性化这些图像的能力已成为一种必要。想象一下，在一张照片中，一只虎斑猫懒洋洋地躺在阳光照射下的窗台上，与你自己顽皮的小狗一起，同时保留了照片原有的魅力和构图。我们提出的 Photoswap，一种新颖的方法，使这种沉浸式图像编辑的经验，通过个性化的主题交换在现有的图像。照片交换首先从参考图像中学习目标的视觉概念，然后使用预先训练的扩散模型以一种无需训练的方式将其交换到目标图像中。我们建立了一个概念化的视觉主体可以通过适当的自我注意和交叉注意操作无缝地转移到任何图像上，保持交换主体的姿态和图像的整体一致性。全面的实验强调了 Photoswap 在个性化学科交换方面的有效性和可控性。此外，Photoswap 在主题交换、背景保存和整体质量方面显著优于基线评分方法，显示出其从娱乐到专业编辑的巨大应用潜力。"
    },
    {
        "title": "Contextual Object Detection with Multimodal Large Language Models",
        "url": "http://arxiv.org/abs/2305.18279v1",
        "pub_date": "2023-05-29",
        "summary": "Recent Multimodal Large Language Models (MLLMs) are remarkable in\nvision-language tasks, such as image captioning and question answering, but\nlack the essential perception ability, i.e., object detection. In this work, we\naddress this limitation by introducing a novel research problem of contextual\nobject detection -- understanding visible objects within different human-AI\ninteractive contexts. Three representative scenarios are investigated,\nincluding the language cloze test, visual captioning, and question answering.\nMoreover, we present ContextDET, a unified multimodal model that is capable of\nend-to-end differentiable modeling of visual-language contexts, so as to\nlocate, identify, and associate visual objects with language inputs for\nhuman-AI interaction. Our ContextDET involves three key submodels: (i) a visual\nencoder for extracting visual representations, (ii) a pre-trained LLM for\nmultimodal context decoding, and (iii) a visual decoder for predicting bounding\nboxes given contextual object words. The new generate-then-detect framework\nenables us to detect object words within human vocabulary. Extensive\nexperiments show the advantages of ContextDET on our proposed CODE benchmark,\nopen-vocabulary detection, and referring image segmentation. Github:\nhttps://github.com/yuhangzang/ContextDET.",
        "translated": "最近的多模态大语言模型(mLLMs)在视觉语言任务(如图像字幕和问答)中表现突出，但缺乏基本的感知能力，即目标检测。在这项工作中，我们通过引入一个关于语境目标检测的新的研究问题——在不同的人工智能交互环境中理解可见物体来解决这一局限性。研究了三种典型的情景，包括语言完形填空测试、视觉字幕和问答。此外，本文还提出了一个统一的多模态模型 ContextDET，该模型能够对视觉语言环境进行端到端的可微分建模，从而定位、识别和关联视觉对象和语言输入，实现人机交互。我们的 ContextDET 涉及三个关键子模型: (i)用于提取视觉表示的可视化编码器，(ii)用于多模态上下文解码的预先训练的 LLM，以及(iii)用于预测给定上下文对象词的边界框的可视化解码器。新的生成然后检测框架使我们能够检测人类词汇表中的对象词。大量的实验显示了 ContextDET 在我们提出的 CODE 基准、开放词汇表检测和引用图像分割上的优势。Https://Github.com/yuhangzang/contextdet."
    },
    {
        "title": "3DTeethSeg'22: 3D Teeth Scan Segmentation and Labeling Challenge",
        "url": "http://arxiv.org/abs/2305.18277v1",
        "pub_date": "2023-05-29",
        "summary": "Teeth localization, segmentation, and labeling from intra-oral 3D scans are\nessential tasks in modern dentistry to enhance dental diagnostics, treatment\nplanning, and population-based studies on oral health. However, developing\nautomated algorithms for teeth analysis presents significant challenges due to\nvariations in dental anatomy, imaging protocols, and limited availability of\npublicly accessible data. To address these challenges, the 3DTeethSeg'22\nchallenge was organized in conjunction with the International Conference on\nMedical Image Computing and Computer Assisted Intervention (MICCAI) in 2022,\nwith a call for algorithms tackling teeth localization, segmentation, and\nlabeling from intraoral 3D scans. A dataset comprising a total of 1800 scans\nfrom 900 patients was prepared, and each tooth was individually annotated by a\nhuman-machine hybrid algorithm. A total of 6 algorithms were evaluated on this\ndataset. In this study, we present the evaluation results of the 3DTeethSeg'22\nchallenge. The 3DTeethSeg'22 challenge code can be accessed at:\nhttps://github.com/abenhamadou/3DTeethSeg22_challenge",
        "translated": "口腔内3D 扫描的牙齿定位、分割和标记是现代牙科学加强口腔诊断、治疗计划和基于人群的口腔健康研究的基本任务。然而，开发用于牙齿分析的自动化算法由于牙齿解剖学的变化，成像协议和有限的可公开获取的数据提出了重大挑战。为了应对这些挑战，3DTeethSeg’22挑战与2022年国际医学图像计算和计算机辅助干预会议(MICCAI)一起组织，呼吁从口内3D 扫描中解决牙齿定位，分割和标记的算法。准备了一个包含来自900名患者的总共1800次扫描的数据集，并用人机混合算法对每颗牙齿进行单独注释。在这个数据集上总共评估了6种算法。在这项研究中，我们提出了3DTeethSeg’22挑战的评估结果。3dtethseg’22挑战码可在以下 https://github.com/abenhamadou/3dteethseg22_challenge 查阅:"
    },
    {
        "title": "Reconstructing the Mind's Eye: fMRI-to-Image with Contrastive Learning\n  and Diffusion Priors",
        "url": "http://arxiv.org/abs/2305.18274v1",
        "pub_date": "2023-05-29",
        "summary": "We present MindEye, a novel fMRI-to-image approach to retrieve and\nreconstruct viewed images from brain activity. Our model comprises two parallel\nsubmodules that are specialized for retrieval (using contrastive learning) and\nreconstruction (using a diffusion prior). MindEye can map fMRI brain activity\nto any high dimensional multimodal latent space, like CLIP image space,\nenabling image reconstruction using generative models that accept embeddings\nfrom this latent space. We comprehensively compare our approach with other\nexisting methods, using both qualitative side-by-side comparisons and\nquantitative evaluations, and show that MindEye achieves state-of-the-art\nperformance in both reconstruction and retrieval tasks. In particular, MindEye\ncan retrieve the exact original image even among highly similar candidates\nindicating that its brain embeddings retain fine-grained image-specific\ninformation. This allows us to accurately retrieve images even from large-scale\ndatabases like LAION-5B. We demonstrate through ablations that MindEye's\nperformance improvements over previous methods result from specialized\nsubmodules for retrieval and reconstruction, improved training techniques, and\ntraining models with orders of magnitude more parameters. Furthermore, we show\nthat MindEye can better preserve low-level image features in the\nreconstructions by using img2img, with outputs from a separate autoencoder. All\ncode is available on GitHub.",
        "translated": "我们提出了 MindEye，一种新的功能磁共振成像的方法来检索和重建从大脑活动中看到的图像。我们的模型包括两个并行的子模块，专门用于检索(使用对比学习)和重构(使用扩散先验)。MindEye 可以将 fMRI 大脑活动映射到任何高维多模式潜伏空间，比如 CLIP 图像空间，使得能够使用接受来自这个潜伏空间的嵌入的生成模型进行图像重建。我们全面比较了我们的方法与其他现有的方法，使用定性并列比较和定量评估，并表明 MindEye 实现了最先进的表现，在重建和检索任务。尤其值得一提的是，MindEye 甚至可以在高度相似的候选图像中检索到精确的原始图像，这表明它的大脑嵌入保留了细粒度的图像特定信息。这使我们能够准确地检索图像，甚至从大型数据库，如 LAION-5B。我们通过消融实验证明，MindEye 的性能优于以前的方法是由专门用于检索和重建的子模块、改进的训练技术以及具有更多参数的训练模型数量级的结果。此外，我们表明，MindEye 可以更好地保存低水平的图像特征，在重建使用 img2img，从一个单独的自动编码器的输出。所有代码都可以在 GitHub 上找到。"
    },
    {
        "title": "Pix2Repair: Implicit Shape Restoration from Images",
        "url": "http://arxiv.org/abs/2305.18273v1",
        "pub_date": "2023-05-29",
        "summary": "We present Pix2Repair, an automated shape repair approach that generates\nrestoration shapes from images to repair fractured objects. Prior repair\napproaches require a high-resolution watertight 3D mesh of the fractured object\nas input. Input 3D meshes must be obtained using expensive 3D scanners, and\nscanned meshes require manual cleanup, limiting accessibility and scalability.\nPix2Repair takes an image of the fractured object as input and automatically\ngenerates a 3D printable restoration shape. We contribute a novel shape\nfunction that deconstructs a latent code representing the fractured object into\na complete shape and a break surface. We show restorations for synthetic\nfractures from the Geometric Breaks and Breaking Bad datasets, and cultural\nheritage objects from the QP dataset, and for real fractures from the Fantastic\nBreaks dataset. We overcome challenges in restoring axially symmetric objects\nby predicting view-centered restorations. Our approach outperforms shape\ncompletion approaches adapted for shape repair in terms of chamfer distance,\nearth mover's distance, normal consistency, and percent restorations generated.",
        "translated": "我们介绍了 Pix2修复，一种自动形状修复方法，从图像生成修复形状，以修复断裂的对象。先前的修复方法需要将断裂物体的高分辨率水密3D 网格作为输入。输入3D 网格必须使用昂贵的3D 扫描仪获得，并扫描网格需要手动清理，限制可访问性和可伸缩性。Pix2Amendment 将断裂物体的图像作为输入，并自动生成一个3D 可打印的修复形状。我们贡献了一个新的形状函数，解构一个潜在的代码代表破碎的物体成为一个完整的形状和破裂表面。我们展示了来自几何断裂和绝命毒师数据集的合成骨折的修复，来自 QP 数据集的文化遗产对象，以及来自荒诞断裂数据集的真实骨折的修复。我们通过预测以视点为中心的复原来克服轴对称物体复原的挑战。我们的方法在倒角距离、推土机的距离、法向一致性和恢复百分比方面优于适用于形状修复的形状完成方法。"
    },
    {
        "title": "Gen-L-Video: Multi-Text to Long Video Generation via Temporal\n  Co-Denoising",
        "url": "http://arxiv.org/abs/2305.18264v1",
        "pub_date": "2023-05-29",
        "summary": "Leveraging large-scale image-text datasets and advancements in diffusion\nmodels, text-driven generative models have made remarkable strides in the field\nof image generation and editing. This study explores the potential of extending\nthe text-driven ability to the generation and editing of multi-text conditioned\nlong videos. Current methodologies for video generation and editing, while\ninnovative, are often confined to extremely short videos (typically less than\n24 frames) and are limited to a single text condition. These constraints\nsignificantly limit their applications given that real-world videos usually\nconsist of multiple segments, each bearing different semantic information. To\naddress this challenge, we introduce a novel paradigm dubbed as Gen-L-Video,\ncapable of extending off-the-shelf short video diffusion models for generating\nand editing videos comprising hundreds of frames with diverse semantic segments\nwithout introducing additional training, all while preserving content\nconsistency. We have implemented three mainstream text-driven video generation\nand editing methodologies and extended them to accommodate longer videos imbued\nwith a variety of semantic segments with our proposed paradigm. Our\nexperimental outcomes reveal that our approach significantly broadens the\ngenerative and editing capabilities of video diffusion models, offering new\npossibilities for future research and applications. The code is available at\nhttps://github.com/G-U-N/Gen-L-Video.",
        "translated": "利用大规模图文数据集和扩散模型的进步，文本驱动的生成模型在图像生成和编辑领域取得了显著的进步。本研究探讨文本驱动能力扩展到多文本条件长视频的生成和编辑的潜力。目前的视频生成和编辑方法，虽然具有创新性，但往往局限于极短的视频(通常少于24帧) ，并且仅限于一个文本条件。考虑到现实世界中的视频通常由多个片段组成，每个片段都有不同的语义信息，这些限制极大地限制了它们的应用。为了应对这一挑战，我们引入了一种称为 Gen-L-Video 的新型范例，该范例能够扩展现成的短视频扩散模型，用于生成和编辑包含数百帧具有不同语义片段的视频，而无需引入额外的训练，同时保持内容的一致性。我们已经实现了三种主流的文本驱动的视频生成和编辑方法，并扩展了它们，以适应与我们提出的范例中的各种语义片段灌输的较长的视频。我们的实验结果表明，我们的方法显着扩大了视频扩散模型的生成和编辑能力，为未来的研究和应用提供了新的可能性。密码可在 https://github.com/g-u-n/gen-l-video 查阅。"
    },
    {
        "title": "Synfeal: A Data-Driven Simulator for End-to-End Camera Localization",
        "url": "http://arxiv.org/abs/2305.18260v1",
        "pub_date": "2023-05-29",
        "summary": "Collecting real-world data is often considered the bottleneck of Artificial\nIntelligence, stalling the research progress in several fields, one of which is\ncamera localization. End-to-end camera localization methods are still\noutperformed by traditional methods, and we argue that the inconsistencies\nassociated with the data collection techniques are restraining the potential of\nend-to-end methods. Inspired by the recent data-centric paradigm, we propose a\nframework that synthesizes large localization datasets based on realistic 3D\nreconstructions of the real world. Our framework, termed Synfeal: Synthetic\nfrom Real, is an open-source, data-driven simulator that synthesizes RGB images\nby moving a virtual camera through a realistic 3D textured mesh, while\ncollecting the corresponding ground-truth camera poses. The results validate\nthat the training of camera localization algorithms on datasets generated by\nSynfeal leads to better results when compared to datasets generated by\nstate-of-the-art methods. Using Synfeal, we conducted the first analysis of the\nrelationship between the size of the dataset and the performance of camera\nlocalization algorithms. Results show that the performance significantly\nincreases with the dataset size. Our results also suggest that when a large\nlocalization dataset with high quality is available, training from scratch\nleads to better performances. Synfeal is publicly available at\nhttps://github.com/DanielCoelho112/synfeal.",
        "translated": "真实世界数据的采集往往被认为是人工智能的瓶颈，阻碍了人工智能在多个领域的研究进展，其中摄像机定位就是其中之一。端到端摄像机定位方法仍然比传统的定位方法要好，我们认为与数据采集技术相关的不一致性限制了端到端定位方法的潜力。受最近以数据为中心的范式的启发，我们提出了一个基于真实世界的三维重建的大型定位数据集合成框架。我们的框架，称为 Synfeal: 从真实合成，是一个开源的，数据驱动的模拟器，通过移动虚拟相机通过一个真实的3D 纹理网格合成 RGB 图像，同时收集相应的地面真相相机的姿势。实验结果表明，在 Synfeal 生成的数据集上进行摄像机定位算法的训练，比采用最先进的方法生成的数据集有更好的定位效果。使用 Synfeal，我们首先分析了数据集大小与摄像机定位算法性能之间的关系。结果表明，随着数据集大小的增加，性能显著提高。我们的结果还表明，当一个高质量的大型定位数据集是可用的，从头训练导致更好的性能。Synfeal 可以在 https://github.com/danielcoelho112/Synfeal 上公开使用。"
    },
    {
        "title": "Learning without Forgetting for Vision-Language Models",
        "url": "http://arxiv.org/abs/2305.19270v1",
        "pub_date": "2023-05-30",
        "summary": "Class-Incremental Learning (CIL) or continual learning is a desired\ncapability in the real world, which requires a learning system to adapt to new\ntasks without forgetting former ones. While traditional CIL methods focus on\nvisual information to grasp core features, recent advances in Vision-Language\nModels (VLM) have shown promising capabilities in learning generalizable\nrepresentations with the aid of textual information. However, when continually\ntrained with new classes, VLMs often suffer from catastrophic forgetting of\nformer knowledge. Applying VLMs to CIL poses two major challenges: 1) how to\nadapt the model without forgetting; and 2) how to make full use of the\nmulti-modal information. To this end, we propose PROjectiOn Fusion (PROOF) that\nenables VLMs to learn without forgetting. To handle the first challenge, we\npropose training task-specific projections based on the frozen image/text\nencoders. When facing new tasks, new projections are expanded and former\nprojections are fixed, alleviating the forgetting of old concepts. For the\nsecond challenge, we propose the fusion module to better utilize the\ncross-modality information. By jointly adjusting visual and textual features,\nthe model can capture semantic information with stronger representation\nability. Extensive experiments on nine benchmark datasets validate PROOF\nachieves state-of-the-art performance.",
        "translated": "课堂增量学习(CIL)或连续学习是现实世界需要的一种能力，它需要一个学习系统来适应新的任务而不忘记以前的任务。虽然传统的 CIL 方法侧重于视觉信息来掌握核心特征，但视觉语言模型(VLM)的最新进展已经显示出在借助文本信息学习可推广表示方面的有前途的能力。然而，当不断培训新的类，VLM 往往遭受灾难性的遗忘以前的知识。在 CIL 中应用 VLM 模型提出了两个主要的挑战: 1)如何在不遗忘的情况下调整模型; 2)如何充分利用多模态信息。为此，我们提出了投影融合(PROOF) ，使 VLM 学习而不会忘记。为了应对第一个挑战，我们提出了基于冻结图像/文本编码器的训练任务特定投影。当面对新的任务时，新的预测会被扩展，旧的预测会被固定，从而减轻对旧概念的遗忘。对于第二个挑战，我们提出了融合模块，以更好地利用交叉模态信息。通过对视觉特征和文本特征的联合调整，该模型可以捕捉具有更强表现能力的语义信息。在九个基准数据集上的大量实验验证了 PROOF 实现了最先进的性能。"
    },
    {
        "title": "Ambient Diffusion: Learning Clean Distributions from Corrupted Data",
        "url": "http://arxiv.org/abs/2305.19256v1",
        "pub_date": "2023-05-30",
        "summary": "We present the first diffusion-based framework that can learn an unknown\ndistribution using only highly-corrupted samples. This problem arises in\nscientific applications where access to uncorrupted samples is impossible or\nexpensive to acquire. Another benefit of our approach is the ability to train\ngenerative models that are less likely to memorize individual training samples\nsince they never observe clean training data. Our main idea is to introduce\nadditional measurement distortion during the diffusion process and require the\nmodel to predict the original corrupted image from the further corrupted image.\nWe prove that our method leads to models that learn the conditional expectation\nof the full uncorrupted image given this additional measurement corruption.\nThis holds for any corruption process that satisfies some technical conditions\n(and in particular includes inpainting and compressed sensing). We train models\non standard benchmarks (CelebA, CIFAR-10 and AFHQ) and show that we can learn\nthe distribution even when all the training samples have $90\\%$ of their pixels\nmissing. We also show that we can finetune foundation models on small corrupted\ndatasets (e.g. MRI scans with block corruptions) and learn the clean\ndistribution without memorizing the training set.",
        "translated": "我们提出了第一个扩散为基础的框架，可以学习一个未知的分布，只使用高度腐败的样本。这个问题出现在科学应用领域，因为无法获得未受污染的样品或获得这些样品的费用很高。我们的方法的另一个好处是能够训练生成模型，这些模型不太可能记住单独的训练样本，因为它们从来没有观察到干净的训练数据。我们的主要思想是在扩散过程中引入额外的测量失真，并要求该模型从进一步的损伤图像中预测出原始的损伤图像。我们证明了我们的方法导致模型学习完整未损坏的图像的条件期望，考虑到这种额外的测量损坏。这适用于任何满足某些技术条件的腐败过程(特别是包括油漆和压缩感知)。我们在标准基准(CelebA，CIFAR-10和 AFHQ)上训练模型，并表明即使所有的训练样本都丢失了90% 的像素，我们仍然可以学习分布。我们还展示了我们可以在小的损坏数据集上微调基础模型(例如带有块损坏的 MRI 扫描) ，并且不需要记忆训练集就可以学习干净的分布。"
    },
    {
        "title": "AlteredAvatar: Stylizing Dynamic 3D Avatars with Fast Style Adaptation",
        "url": "http://arxiv.org/abs/2305.19245v1",
        "pub_date": "2023-05-30",
        "summary": "This paper presents a method that can quickly adapt dynamic 3D avatars to\narbitrary text descriptions of novel styles. Among existing approaches for\navatar stylization, direct optimization methods can produce excellent results\nfor arbitrary styles but they are unpleasantly slow. Furthermore, they require\nredoing the optimization process from scratch for every new input. Fast\napproximation methods using feed-forward networks trained on a large dataset of\nstyle images can generate results for new inputs quickly, but tend not to\ngeneralize well to novel styles and fall short in quality. We therefore\ninvestigate a new approach, AlteredAvatar, that combines those two approaches\nusing the meta-learning framework. In the inner loop, the model learns to\noptimize to match a single target style well; while in the outer loop, the\nmodel learns to stylize efficiently across many styles. After training,\nAlteredAvatar learns an initialization that can quickly adapt within a small\nnumber of update steps to a novel style, which can be given using texts, a\nreference image, or a combination of both. We show that AlteredAvatar can\nachieve a good balance between speed, flexibility and quality, while\nmaintaining consistency across a wide range of novel views and facial\nexpressions.",
        "translated": "本文提出了一种快速自适应动态3D 化身的方法，以适应任意文本描述的新风格。在现有的化身风格化方法中，直接优化方法可以对任意风格产生优秀的结果，但是它们的速度慢得令人不快。此外，它们需要从头开始为每个新输入重做优化过程。使用前馈网络训练样式图像的快速逼近方法可以快速生成新输入的结果，但往往不能很好地推广到新的样式和质量不足。因此，我们研究了一种新的方法，AlteredAvatar，它使用元学习框架将这两种方法结合起来。在内部循环中，模型学习如何优化以很好地匹配单个目标样式; 而在外部循环中，模型学习如何有效地跨多种样式进行样式化。经过训练，AlteredAvatar 学会了一种初始化，它可以在少量更新步骤内快速适应一种新的风格，这种风格可以使用文本、参考图像或两者的组合。我们展示了 AlteredAvatar 可以在速度、灵活性和质量之间取得很好的平衡，同时保持广泛的新视图和面部表情的一致性。"
    },
    {
        "title": "Translation-Enhanced Multilingual Text-to-Image Generation",
        "url": "http://arxiv.org/abs/2305.19216v1",
        "pub_date": "2023-05-30",
        "summary": "Research on text-to-image generation (TTI) still predominantly focuses on the\nEnglish language due to the lack of annotated image-caption data in other\nlanguages; in the long run, this might widen inequitable access to TTI\ntechnology. In this work, we thus investigate multilingual TTI (termed mTTI)\nand the current potential of neural machine translation (NMT) to bootstrap mTTI\nsystems. We provide two key contributions. 1) Relying on a multilingual\nmulti-modal encoder, we provide a systematic empirical study of standard\nmethods used in cross-lingual NLP when applied to mTTI: Translate Train,\nTranslate Test, and Zero-Shot Transfer. 2) We propose Ensemble Adapter (EnsAd),\na novel parameter-efficient approach that learns to weigh and consolidate the\nmultilingual text knowledge within the mTTI framework, mitigating the language\ngap and thus improving mTTI performance. Our evaluations on standard mTTI\ndatasets COCO-CN, Multi30K Task2, and LAION-5B demonstrate the potential of\ntranslation-enhanced mTTI systems and also validate the benefits of the\nproposed EnsAd which derives consistent gains across all datasets. Further\ninvestigations on model variants, ablation studies, and qualitative analyses\nprovide additional insights on the inner workings of the proposed mTTI\napproaches.",
        "translated": "由于缺乏其他语言的注释图像标题数据，文本到图像生成(TTI)的研究仍然主要集中在英语上; 从长远来看，这可能会扩大 TTI 技术的不公平使用。在这项工作中，我们因此研究多语言 TTI (称为 mTTI)和神经机器翻译(NMT)目前的潜力引导 mTTI 系统。我们提供了两个关键的贡献。1)以多语言多模态编码器为基础，对跨语言自然语言处理中的标准方法进行了系统的实证研究。2)提出了一种新的参数有效方法 EnsAd，该方法可以在 mTTI 框架内学习权衡和整合多语言文本知识，减小语言差距，从而提高 mTTI 的性能。我们对标准 mTTI 数据集 COCO-CN，Multi30K Task2和 LAION-5B 的评估证明了翻译增强的 mTTI 系统的潜力，并且还验证了所提议的 EnsAd 的益处，其在所有数据集中获得一致的增益。对模型变异、消融研究和定性分析的进一步研究提供了对拟议的 mTTI 方法内部工作的额外见解。"
    },
    {
        "title": "Group Invariant Global Pooling",
        "url": "http://arxiv.org/abs/2305.19207v1",
        "pub_date": "2023-05-30",
        "summary": "Much work has been devoted to devising architectures that build\ngroup-equivariant representations, while invariance is often induced using\nsimple global pooling mechanisms. Little work has been done on creating\nexpressive layers that are invariant to given symmetries, despite the success\nof permutation invariant pooling in various molecular tasks. In this work, we\npresent Group Invariant Global Pooling (GIGP), an invariant pooling layer that\nis provably sufficiently expressive to represent a large class of invariant\nfunctions. We validate GIGP on rotated MNIST and QM9, showing improvements for\nthe latter while attaining identical results for the former. By making the\npooling process group orbit-aware, this invariant aggregation method leads to\nimproved performance, while performing well-principled group aggregation.",
        "translated": "许多工作致力于设计构建群等变表示的体系结构，而不变性通常是使用简单的全局池机制来诱导的。尽管在各种分子任务中排列不变量池的成功，但在创建对给定对称性不变的表达层方面几乎没有做什么工作。在这项工作中，我们提出了群不变全局池(GIGP) ，一个不变的池层，可证明充分表达，以表示一个大类的不变函数。我们在旋转的 MNIST 和 QM9上验证了 GIGP，显示了后者的改进，同时获得了前者相同的结果。这种不变聚集方法通过使池处理过程组轨道感知，提高了性能，同时执行了原则性良好的组聚集。"
    },
    {
        "title": "AMatFormer: Efficient Feature Matching via Anchor Matching Transformer",
        "url": "http://arxiv.org/abs/2305.19205v1",
        "pub_date": "2023-05-30",
        "summary": "Learning based feature matching methods have been commonly studied in recent\nyears. The core issue for learning feature matching is to how to learn (1)\ndiscriminative representations for feature points (or regions) within each\nintra-image and (2) consensus representations for feature points across\ninter-images. Recently, self- and cross-attention models have been exploited to\naddress this issue. However, in many scenes, features are coming with\nlarge-scale, redundant and outliers contaminated. Previous\nself-/cross-attention models generally conduct message passing on all primal\nfeatures which thus lead to redundant learning and high computational cost. To\nmitigate limitations, inspired by recent seed matching methods, in this paper,\nwe propose a novel efficient Anchor Matching Transformer (AMatFormer) for the\nfeature matching problem. AMatFormer has two main aspects: First, it mainly\nconducts self-/cross-attention on some anchor features and leverages these\nanchor features as message bottleneck to learn the representations for all\nprimal features. Thus, it can be implemented efficiently and compactly. Second,\nAMatFormer adopts a shared FFN module to further embed the features of two\nimages into the common domain and thus learn the consensus feature\nrepresentations for the matching problem. Experiments on several benchmarks\ndemonstrate the effectiveness and efficiency of the proposed AMatFormer\nmatching approach.",
        "translated": "基于学习的特征匹配方法是近年来研究的热点。学习特征匹配的核心问题是如何学习(1)图像内特征点(或区域)的区分表示和(2)图像间特征点的一致表示。最近，自我和交叉注意模型已经被用来解决这个问题。然而，在许多场景中，特性都伴随着大规模的、冗余的和受到污染的异常值。以往的自我/交叉注意模型通常将信息传递给所有的原始特征，从而导致冗余学习和高计算成本。为了解决这一问题，本文提出了一种新的基于特征匹配的锚匹配变换器。AMatForm 主要有两个方面: 首先，它主要对一些锚特征进行自我/交叉注意，并利用这些锚特征作为消息瓶颈来学习所有原始特征的表示。因此，它可以有效和紧凑地实现。其次，采用共享 FFN 模块将两幅图像的特征进一步嵌入到公共域中，从而学习匹配问题的一致特征表示。在多个基准上的实验结果表明了该方法的有效性和高效性。"
    },
    {
        "title": "DäRF: Boosting Radiance Fields from Sparse Inputs with Monocular Depth\n  Adaptation",
        "url": "http://arxiv.org/abs/2305.19201v1",
        "pub_date": "2023-05-30",
        "summary": "Neural radiance fields (NeRF) shows powerful performance in novel view\nsynthesis and 3D geometry reconstruction, but it suffers from critical\nperformance degradation when the number of known viewpoints is drastically\nreduced. Existing works attempt to overcome this problem by employing external\npriors, but their success is limited to certain types of scenes or datasets.\nEmploying monocular depth estimation (MDE) networks, pretrained on large-scale\nRGB-D datasets, with powerful generalization capability would be a key to\nsolving this problem: however, using MDE in conjunction with NeRF comes with a\nnew set of challenges due to various ambiguity problems exhibited by monocular\ndepths. In this light, we propose a novel framework, dubbed D\\\"aRF, that\nachieves robust NeRF reconstruction with a handful of real-world images by\ncombining the strengths of NeRF and monocular depth estimation through online\ncomplementary training. Our framework imposes the MDE network's powerful\ngeometry prior to NeRF representation at both seen and unseen viewpoints to\nenhance its robustness and coherence. In addition, we overcome the ambiguity\nproblems of monocular depths through patch-wise scale-shift fitting and\ngeometry distillation, which adapts the MDE network to produce depths aligned\naccurately with NeRF geometry. Experiments show our framework achieves\nstate-of-the-art results both quantitatively and qualitatively, demonstrating\nconsistent and reliable performance in both indoor and outdoor real-world\ndatasets. Project page is available at https://ku-cvlab.github.io/DaRF/.",
        "translated": "神经辐射场(NeRF)在新视点合成和三维几何重建方面表现出强大的性能，但当已知视点数大幅减少时，其性能会出现临界退化。现有的作品试图通过使用外部先验来克服这个问题，但是他们的成功仅限于某些类型的场景或数据集。使用单目深度估计(MDE)网络，在大规模 RGB-D 数据集上预先训练，具有强大的泛化能力将是解决这一问题的关键: 然而，使用 MDE 与 NeRF 结合带来了一系列新的挑战，由于单目深度表现出的各种模糊问题。在此基础上，我们提出了一种新的框架，称为 D“ aRF，通过在线互补训练结合了 NERF 和单目深度估计的优点，实现了对少量真实世界图像的强大的 NERF 重建。我们的框架将 MDE 网络的强大的几何形状强加于 NERF 表示之前，在可见和不可见的观点，以增强其健壮性和一致性。此外，通过分片尺度变换拟合和几何精馏克服了单目深度的模糊性问题，使 MDE 网络能够产生与 NeRF 几何精确对齐的深度。实验表明，我们的框架在定量和定性上都达到了最先进的结果，在室内和室外的真实世界数据集中表现出一致和可靠的性能。项目网页可于 https://ku-cvlab.github.io/darf/下载。"
    },
    {
        "title": "PanoGen: Text-Conditioned Panoramic Environment Generation for\n  Vision-and-Language Navigation",
        "url": "http://arxiv.org/abs/2305.19195v1",
        "pub_date": "2023-05-30",
        "summary": "Vision-and-Language Navigation (VLN) requires the agent to follow language\ninstructions to navigate through 3D environments. One main challenge in VLN is\nthe limited availability of photorealistic training environments, which makes\nit hard to generalize to new and unseen environments. To address this problem,\nwe propose PanoGen, a generation method that can potentially create an infinite\nnumber of diverse panoramic environments conditioned on text. Specifically, we\ncollect room descriptions by captioning the room images in existing\nMatterport3D environments, and leverage a state-of-the-art text-to-image\ndiffusion model to generate the new panoramic environments. We use recursive\noutpainting over the generated images to create consistent 360-degree panorama\nviews. Our new panoramic environments share similar semantic information with\nthe original environments by conditioning on text descriptions, which ensures\nthe co-occurrence of objects in the panorama follows human intuition, and\ncreates enough diversity in room appearance and layout with image outpainting.\nLastly, we explore two ways of utilizing PanoGen in VLN pre-training and\nfine-tuning. We generate instructions for paths in our PanoGen environments\nwith a speaker built on a pre-trained vision-and-language model for VLN\npre-training, and augment the visual observation with our panoramic\nenvironments during agents' fine-tuning to avoid overfitting to seen\nenvironments. Empirically, learning with our PanoGen environments achieves the\nnew state-of-the-art on the Room-to-Room, Room-for-Room, and CVDN datasets.\nPre-training with our PanoGen speaker data is especially effective for CVDN,\nwhich has under-specified instructions and needs commonsense knowledge. Lastly,\nwe show that the agent can benefit from training with more generated panoramic\nenvironments, suggesting promising results for scaling up the PanoGen\nenvironments.",
        "translated": "视觉和语言导航(VLN)要求代理遵循语言指令在3D 环境中导航。VLN 的一个主要挑战是有限的真实感训练环境，这使得它很难推广到新的和看不见的环境。为了解决这个问题，我们提出 PanoGen，一种生成方法，可以潜在地创建无限数量的不同的全景环境的文本条件。具体来说，我们通过在现有 Matterport3D 环境中标注房间图像来收集房间描述，并利用最先进的文本到图像扩散模型来生成新的全景环境。我们使用递归绘制生成的图像来创建一致的360度全景视图。我们新的全景环境与原始环境有着相似的语义信息，通过文字描述来确保全景中物体的同时出现遵循人类的直觉，并创造了足够多样化的房间外观和布局图像。最后，我们探讨了利用 PanoGen 在 VLN 预训练和微调中的两种方法。我们在 PanoGen 环境中使用一个基于预先训练的 VLN 预训视觉和语言模型的扬声器生成路径指令，并在代理的微调期间通过我们的全景环境增强视觉观察，以避免过度适应可见环境。根据经验，使用 PanoGen 环境学习可以在 Room-to-Room、 Room-for-Room 和 CVDN 数据集上实现最新的技术水平。使用 PanoGen 扬声器数据进行预训练对于 CVDN 尤其有效，因为 CVDN 的指令不够详细，而且需要常识性知识。最后，我们表明，该代理可以受益于更多生成的全景环境的培训，提出了扩展 PanoGen 环境的有希望的结果。"
    },
    {
        "title": "Video ControlNet: Towards Temporally Consistent Synthetic-to-Real Video\n  Translation Using Conditional Image Diffusion Models",
        "url": "http://arxiv.org/abs/2305.19193v1",
        "pub_date": "2023-05-30",
        "summary": "In this study, we present an efficient and effective approach for achieving\ntemporally consistent synthetic-to-real video translation in videos of varying\nlengths. Our method leverages off-the-shelf conditional image diffusion models,\nallowing us to perform multiple synthetic-to-real image generations in\nparallel. By utilizing the available optical flow information from the\nsynthetic videos, our approach seamlessly enforces temporal consistency among\ncorresponding pixels across frames. This is achieved through joint noise\noptimization, effectively minimizing spatial and temporal discrepancies. To the\nbest of our knowledge, our proposed method is the first to accomplish diverse\nand temporally consistent synthetic-to-real video translation using conditional\nimage diffusion models. Furthermore, our approach does not require any training\nor fine-tuning of the diffusion models. Extensive experiments conducted on\nvarious benchmarks for synthetic-to-real video translation demonstrate the\neffectiveness of our approach, both quantitatively and qualitatively. Finally,\nwe show that our method outperforms other baseline methods in terms of both\ntemporal consistency and visual quality.",
        "translated": "在这项研究中，我们提出了一个有效的方法来实现时间一致的合成到真实的视频在不同长度的视频翻译。我们的方法利用现成的条件图像扩散模型，允许我们并行执行多个合成到真实的图像生成。该方法利用合成视频中可用的光流信息，实现了帧间对应像素之间的时间一致性。这是通过联合噪声优化，有效地减少空间和时间差异。据我们所知，我们提出的方法是第一个使用条件图像扩散模型来实现多样化和时间一致的合成到真实的视频翻译。此外，我们的方法不需要任何培训或扩散模型的微调。针对合成到真实视频翻译的各种基准进行了大量的实验，从定量和定性两方面证明了该方法的有效性。最后，我们证明了我们的方法在时间一致性和视觉质量方面都优于其他基线方法。"
    },
    {
        "title": "Table Detection for Visually Rich Document Images",
        "url": "http://arxiv.org/abs/2305.19181v1",
        "pub_date": "2023-05-30",
        "summary": "Table Detection (TD) is a fundamental task towards visually rich document\nunderstanding. Current studies usually formulate the TD problem as an object\ndetection problem, then leverage Intersection over Union (IoU) based metrics to\nevaluate the model performance and IoU-based loss functions to optimize the\nmodel. TD applications usually require the prediction results to cover all the\ntable contents and avoid information loss. However, IoU and IoU-based loss\nfunctions cannot directly reflect the degree of information loss for the\nprediction results. Therefore, we propose to decouple IoU into a ground truth\ncoverage term and a prediction coverage term, in which the former can be used\nto measure the information loss of the prediction results.\n  Besides, tables in the documents are usually large, sparsely distributed, and\nhave no overlaps because they are designed to summarize essential information\nto make it easy to read and interpret for human readers. Therefore, in this\nstudy, we use SparseR-CNN as the base model, and further improve the model by\nusing Gaussian Noise Augmented Image Size region proposals and many-to-one\nlabel assignments.\n  To demonstrate the effectiveness of proposed method and compare with\nstate-of-the-art methods fairly, we conduct experiments and use IoU-based\nevaluation metrics to evaluate the model performance. The experimental results\nshow that the proposed method can consistently outperform state-of-the-art\nmethods under different IoU-based metric on a variety of datasets. We conduct\nfurther experiments to show the superiority of the proposed decoupled IoU for\nthe TD applications by replacing the IoU-based loss functions and evaluation\nmetrics with proposed decoupled IoU counterparts. The experimental results show\nthat our proposed decoupled IoU loss can encourage the model to alleviate\ninformation loss.",
        "translated": "表检测(TD)是实现视觉丰富的文档理解的基本任务。目前的研究通常将 TD 问题表述为一个目标检测问题，然后利用基于交叉比联盟(IoU)的度量来评估模型的性能，并利用基于 IoU 的损失函数来优化模型。TD 应用程序通常要求预测结果覆盖表中的所有内容，避免信息丢失。然而，基于 IoU 和 IoU 的损失函数不能直接反映预测结果的信息损失程度。因此，我们提出将 IU 解耦为一个地面真实覆盖项和一个预测覆盖项，其中前者可用来度量预测结果的信息损失。此外，文档中的表格通常很大，分布很稀疏，没有重叠，因为它们旨在总结必要的信息，以便于人类读者阅读和解释。因此，在本研究中，我们以稀疏 R-CNN 为基础模型，并进一步改进模型，使用高斯噪声增强图像大小区域方案和多对一标签分配。为了验证所提方法的有效性，并与现有方法进行比较，我们进行了实验，并使用基于 IoU 的评价指标来评价模型的性能。实验结果表明，在不同的物联网度量下，该方法在各种数据集上的性能均优于目前最先进的方法。通过进一步的实验，我们证明了所提出的解耦 IU 对 TD 应用的优越性，将基于 IU 的损失函数和评估指标替换为所提出的解耦 IU 对应物。实验结果表明，我们提出的解耦的 IU 损失可以鼓励模型，以减轻信息损失。"
    },
    {
        "title": "Humans in 4D: Reconstructing and Tracking Humans with Transformers",
        "url": "http://arxiv.org/abs/2305.20091v1",
        "pub_date": "2023-05-31",
        "summary": "We present an approach to reconstruct humans and track them over time. At the\ncore of our approach, we propose a fully \"transformerized\" version of a network\nfor human mesh recovery. This network, HMR 2.0, advances the state of the art\nand shows the capability to analyze unusual poses that have in the past been\ndifficult to reconstruct from single images. To analyze video, we use 3D\nreconstructions from HMR 2.0 as input to a tracking system that operates in 3D.\nThis enables us to deal with multiple people and maintain identities through\nocclusion events. Our complete approach, 4DHumans, achieves state-of-the-art\nresults for tracking people from monocular video. Furthermore, we demonstrate\nthe effectiveness of HMR 2.0 on the downstream task of action recognition,\nachieving significant improvements over previous pose-based action recognition\napproaches. Our code and models are available on the project website:\nhttps://shubham-goel.github.io/4dhumans/.",
        "translated": "我们提出了一种方法来重建人类，并随着时间的推移跟踪他们。在我们的方法的核心，我们提出了一个完全“转换”的网络版本的人类网格恢复。这个网络，HMR 2.0，提高了最先进的技术水平，并显示了分析不寻常姿势的能力，这些姿势在过去很难从单个图像重建。为了分析视频，我们使用来自 HMR 2.0的3D 重建作为输入到一个3D 跟踪系统中。这使我们能够处理多个人，并通过遮挡事件保持身份。我们的完整方法，4D 人类，实现了最先进的结果跟踪人从单目视频。此外，我们证明了 HMR 2.0在动作识别的下游任务上的有效性，实现了对以前基于姿势的动作识别方法的显著改进。我们的代码和模型可以在项目网站上找到:  https://shubham-goel.github.io/4dhumans/。"
    },
    {
        "title": "Learning Explicit Contact for Implicit Reconstruction of Hand-held\n  Objects from Monocular Images",
        "url": "http://arxiv.org/abs/2305.20089v1",
        "pub_date": "2023-05-31",
        "summary": "Reconstructing hand-held objects from monocular RGB images is an appealing\nyet challenging task. In this task, contacts between hands and objects provide\nimportant cues for recovering the 3D geometry of the hand-held objects. Though\nrecent works have employed implicit functions to achieve impressive progress,\nthey ignore formulating contacts in their frameworks, which results in\nproducing less realistic object meshes. In this work, we explore how to model\ncontacts in an explicit way to benefit the implicit reconstruction of hand-held\nobjects. Our method consists of two components: explicit contact prediction and\nimplicit shape reconstruction. In the first part, we propose a new subtask of\ndirectly estimating 3D hand-object contacts from a single image. The part-level\nand vertex-level graph-based transformers are cascaded and jointly learned in a\ncoarse-to-fine manner for more accurate contact probabilities. In the second\npart, we introduce a novel method to diffuse estimated contact states from the\nhand mesh surface to nearby 3D space and leverage diffused contact\nprobabilities to construct the implicit neural representation for the\nmanipulated object. Benefiting from estimating the interaction patterns between\nthe hand and the object, our method can reconstruct more realistic object\nmeshes, especially for object parts that are in contact with hands. Extensive\nexperiments on challenging benchmarks show that the proposed method outperforms\nthe current state of the arts by a great margin.",
        "translated": "从单目 RGB 图像重建手持物体是一个吸引人的但具有挑战性的任务。在这项任务中，手和物体之间的接触为恢复手持物体的三维几何形状提供了重要的线索。尽管最近的作品使用了隐式函数来取得令人印象深刻的进展，但是他们忽视了在框架中表述联系，这导致了产生不太真实的对象网格。在这项工作中，我们探讨了如何以显性的方式建模接触，以利于手持物体的隐式重建。该方法由显式接触预测和隐式形状重建两部分组成。在第一部分中，我们提出了一个新的子任务直接估计三维手-物体接触从一个单一的图像。基于部件级和顶点级图形的变压器以从粗到精的方式串联和联合学习，以获得更准确的接触概率。在第二部分中，我们提出了一种新的方法来扩散估计接触状态从手网格表面到附近的三维空间，并利用扩散接触概率来构造被操作物体的隐式神经表示。该方法通过估计手与物体之间的交互模式，可以重建出更加逼真的物体网格，特别是对于与手接触的物体部分。对具有挑战性的基准的大量实验表明，该方法的性能远远优于目前的技术水平。"
    },
    {
        "title": "Improving CLIP Training with Language Rewrites",
        "url": "http://arxiv.org/abs/2305.20088v1",
        "pub_date": "2023-05-31",
        "summary": "Contrastive Language-Image Pre-training (CLIP) stands as one of the most\neffective and scalable methods for training transferable vision models using\npaired image and text data. CLIP models are trained using contrastive loss,\nwhich typically relies on data augmentations to prevent overfitting and\nshortcuts. However, in the CLIP training paradigm, data augmentations are\nexclusively applied to image inputs, while language inputs remain unchanged\nthroughout the entire training process, limiting the exposure of diverse texts\nto the same image. In this paper, we introduce Language augmented CLIP\n(LaCLIP), a simple yet highly effective approach to enhance CLIP training\nthrough language rewrites. Leveraging the in-context learning capability of\nlarge language models, we rewrite the text descriptions associated with each\nimage. These rewritten texts exhibit diversity in sentence structure and\nvocabulary while preserving the original key concepts and meanings. During\ntraining, LaCLIP randomly selects either the original texts or the rewritten\nversions as text augmentations for each image. Extensive experiments on CC3M,\nCC12M, RedCaps and LAION-400M datasets show that CLIP pre-training with\nlanguage rewrites significantly improves the transfer performance without\ncomputation or memory overhead during training. Specifically for ImageNet\nzero-shot accuracy, LaCLIP outperforms CLIP by 8.2% on CC12M and 2.4% on\nLAION-400M. Code is available at https://github.com/LijieFan/LaCLIP.",
        "translated": "对比语言-图像预训练(CLIP)是利用成对图像和文本数据训练可转移视觉模型的最有效和可扩展的方法之一。CLIP 模型使用对比损失进行训练，对比损失通常依赖于数据增强以防止过度拟合和快捷方式。然而，在 CLIP 训练范式中，数据增强仅应用于图像输入，而语言输入在整个训练过程中保持不变，限制了不同文本暴露于同一图像。本文介绍了语言增强 CLIP 技术，这是一种通过语言重写来增强 CLIP 训练的简单而高效的方法。利用大型语言模型的上下文学习能力，我们重写与每个图像相关的文本描述。这些改写后的文本在保留原有关键概念和意义的同时，表现出句子结构和词汇的多样性。在训练过程中，LaCLIP 随机选择原始文本或改写版本作为每幅图像的文本增强。在 CC3M、 CC12M、 RedCaps 和 LAION-400M 数据集上进行的大量实验表明，使用语言重写的 CLIP 预训练可以在不增加训练过程中的计算和内存开销的情况下显著提高传输性能。具体到 ImageNet 的零射精度，LaCLIP 在 CC12M 上比 CLIP 高出8.2% ，在 LAION-400M 上高出2.4% 。密码可于 https://github.com/lijiefan/laclip 索取。"
    },
    {
        "title": "Too Large; Data Reduction for Vision-Language Pre-Training",
        "url": "http://arxiv.org/abs/2305.20087v2",
        "pub_date": "2023-05-31",
        "summary": "This paper examines the problems of severe image-text misalignment and high\nredundancy in the widely-used large-scale Vision-Language Pre-Training (VLP)\ndatasets. To address these issues, we propose an efficient and straightforward\nVision-Language learning algorithm called TL;DR, which aims to compress the\nexisting large VLP data into a small, high-quality set. Our approach consists\nof two major steps. First, a codebook-based encoder-decoder captioner is\ndeveloped to select representative samples. Second, a new caption is generated\nto complement the original captions for selected samples, mitigating the\ntext-image misalignment problem while maintaining uniqueness. As the result,\nTL;DR enables us to reduce the large dataset into a small set of high-quality\ndata, which can serve as an alternative pre-training dataset. This algorithm\nsignificantly speeds up the time-consuming pretraining process. Specifically,\nTL;DR can compress the mainstream VLP datasets at a high ratio, e.g., reduce\nwell-cleaned CC3M dataset from 2.82M to 0.67M ($\\sim$24\\%) and noisy YFCC15M\nfrom 15M to 2.5M ($\\sim$16.7\\%). Extensive experiments with three popular VLP\nmodels over seven downstream tasks show that VLP model trained on the\ncompressed dataset provided by TL;DR can perform similar or even better results\ncompared with training on the full-scale dataset. The code will be made\navailable at \\url{https://github.com/showlab/data-centric.vlp}.",
        "translated": "本文研究了目前广泛使用的大规模视觉语言预训练(VLP)数据集中存在的严重图文错位和高冗余问题。为了解决这些问题，我们提出了一种高效、直观的视觉语言学习算法 TL; DR，该算法旨在将现有的大型 VLP 数据压缩成一个小型、高质量的集合。我们的方法包括两个主要步骤。首先，开发了一种基于码本的编解码字幕器来选择有代表性的样本。其次，生成一个新的标题以补充所选样本的原始标题，在保持唯一性的同时缓解文本-图像不对齐问题。结果，TL; DR 使我们能够将大数据集减少为一个小的高质量数据集，这可以作为一个替代的预训练数据集。该算法显著加快了耗时的预训练过程。具体来说，TL; DR 可以高比例地压缩主流 VLP 数据集，例如，将清洁良好的 CC3M 数据集从2.82 M 减少到0.67 M ($sim $24%) ，将噪音较大的 YFCC15M 从15M 减少到250 M ($sim $16.7%)。通过对三种流行的 VLP 模型在7个下游任务上的大量实验表明，VLP 模型在 TL 提供的压缩数据集上进行训练，与在全尺寸数据集上进行训练相比，DR 可以获得相似甚至更好的结果。代码将在 url { https://github.com/showlab/data-centric.vlp }提供。"
    },
    {
        "title": "Understanding and Mitigating Copying in Diffusion Models",
        "url": "http://arxiv.org/abs/2305.20086v1",
        "pub_date": "2023-05-31",
        "summary": "Images generated by diffusion models like Stable Diffusion are increasingly\nwidespread. Recent works and even lawsuits have shown that these models are\nprone to replicating their training data, unbeknownst to the user. In this\npaper, we first analyze this memorization problem in text-to-image diffusion\nmodels. While it is widely believed that duplicated images in the training set\nare responsible for content replication at inference time, we observe that the\ntext conditioning of the model plays a similarly important role. In fact, we\nsee in our experiments that data replication often does not happen for\nunconditional models, while it is common in the text-conditional case.\nMotivated by our findings, we then propose several techniques for reducing data\nreplication at both training and inference time by randomizing and augmenting\nimage captions in the training set.",
        "translated": "由稳定扩散等扩散模型产生的图像越来越广泛。最近的工作，甚至诉讼已经表明，这些模型倾向于复制他们的训练数据，用户不知道。本文首先分析了文本-图像扩散模型中的记忆问题。虽然人们普遍认为训练集中的重复图像负责推理时的内容复制，但是我们观察到模型的文本条件作用也起着类似的重要作用。事实上，在我们的实验中，我们看到数据复制通常不会发生在无条件模型中，而在文本条件的情况下却很常见。在我们的研究结果的激励下，我们提出了几种技术，通过在训练集中随机化和增强图像标题来减少训练和推理时间的数据复制。"
    },
    {
        "title": "Control4D: Dynamic Portrait Editing by Learning 4D GAN from 2D\n  Diffusion-based Editor",
        "url": "http://arxiv.org/abs/2305.20082v1",
        "pub_date": "2023-05-31",
        "summary": "Recent years have witnessed considerable achievements in editing images with\ntext instructions. When applying these editors to dynamic scene editing, the\nnew-style scene tends to be temporally inconsistent due to the frame-by-frame\nnature of these 2D editors. To tackle this issue, we propose Control4D, a novel\napproach for high-fidelity and temporally consistent 4D portrait editing.\nControl4D is built upon an efficient 4D representation with a 2D\ndiffusion-based editor. Instead of using direct supervisions from the editor,\nour method learns a 4D GAN from it and avoids the inconsistent supervision\nsignals. Specifically, we employ a discriminator to learn the generation\ndistribution based on the edited images and then update the generator with the\ndiscrimination signals. For more stable training, multi-level information is\nextracted from the edited images and used to facilitate the learning of the\ngenerator. Experimental results show that Control4D surpasses previous\napproaches and achieves more photo-realistic and consistent 4D editing\nperformances. The link to our project website is\nhttps://control4darxiv.github.io.",
        "translated": "近年来，在利用文本指令编辑图像方面取得了相当大的成就。当这些编辑器应用于动态场景编辑时，由于这些二维编辑器的逐帧性质，新风格的场景往往会出现时间上的不一致。为了解决这个问题，我们提出 Control4D，一种新颖的高保真度和时间一致的4D 肖像编辑方法。Control4D 是建立在一个有效的4D 表示与2D 扩散为基础的编辑器。该方法不需要编辑器的直接监控，而是从编辑器中学习一个4D GAN，避免了监控信号的不一致。具体地说，我们使用一个鉴别器来学习基于编辑后的图像的生成分布，然后用鉴别信号更新生成器。为了获得更稳定的训练，从编辑后的图像中提取多层次信息，以便于生成器的学习。实验结果表明，Control4D 编辑方法优于以往的编辑方法，具有更好的逼真度和一致性。我们项目网站的链接是 https://control4darxiv.github.io 的。"
    },
    {
        "title": "Feature Learning in Image Hierarchies using Functional Maximal\n  Correlation",
        "url": "http://arxiv.org/abs/2305.20074v1",
        "pub_date": "2023-05-31",
        "summary": "This paper proposes the Hierarchical Functional Maximal Correlation Algorithm\n(HFMCA), a hierarchical methodology that characterizes dependencies across two\nhierarchical levels in multiview systems. By framing view similarities as\ndependencies and ensuring contrastivity by imposing orthonormality, HFMCA\nachieves faster convergence and increased stability in self-supervised\nlearning. HFMCA defines and measures dependencies within image hierarchies,\nfrom pixels and patches to full images. We find that the network topology for\napproximating orthonormal basis functions aligns with a vanilla CNN, enabling\nthe decomposition of density ratios between neighboring layers of feature maps.\nThis approach provides powerful interpretability, revealing the resemblance\nbetween supervision and self-supervision through the lens of internal\nrepresentations.",
        "translated": "本文提出了分层函数最大相关算法(HFMCA) ，这是一种描述多视图系统中两个层次之间依赖关系的分层方法。通过将视图相似性框架为依赖关系并通过正交性确保对比度，HFMCA 在自监督学习中实现了更快的收敛和更高的稳定性。HFMCA 定义和度量图像层次结构中的依赖关系，从像素和补丁到完整图像。我们发现，近似网络拓扑标准正交基函数的方法与传统的有线电视新闻网(CNN)方法相一致，能够分解相邻特征映射层之间的密度比。这种方法提供了强大的可解释性，通过内部表征的透镜揭示了监督和自我监督之间的相似性。"
    },
    {
        "title": "Chatting Makes Perfect -- Chat-based Image Retrieval",
        "url": "http://arxiv.org/abs/2305.20062v1",
        "pub_date": "2023-05-31",
        "summary": "Chats emerge as an effective user-friendly approach for information\nretrieval, and are successfully employed in many domains, such as customer\nservice, healthcare, and finance. However, existing image retrieval approaches\ntypically address the case of a single query-to-image round, and the use of\nchats for image retrieval has been mostly overlooked. In this work, we\nintroduce ChatIR: a chat-based image retrieval system that engages in a\nconversation with the user to elicit information, in addition to an initial\nquery, in order to clarify the user's search intent. Motivated by the\ncapabilities of today's foundation models, we leverage Large Language Models to\ngenerate follow-up questions to an initial image description. These questions\nform a dialog with the user in order to retrieve the desired image from a large\ncorpus. In this study, we explore the capabilities of such a system tested on a\nlarge dataset and reveal that engaging in a dialog yields significant gains in\nimage retrieval. We start by building an evaluation pipeline from an existing\nmanually generated dataset and explore different modules and training\nstrategies for ChatIR. Our comparison includes strong baselines derived from\nrelated applications trained with Reinforcement Learning. Our system is capable\nof retrieving the target image from a pool of 50K images with over 78% success\nrate after 5 dialogue rounds, compared to 75% when questions are asked by\nhumans, and 64% for a single shot text-to-image retrieval. Extensive\nevaluations reveal the strong capabilities and examine the limitations of\nCharIR under different settings.",
        "translated": "聊天作为一种有效的用户友好的方式出现在信息检索，并成功地应用于许多领域，如客户服务，医疗保健和金融。然而，现有的图像检索方法通常只处理一轮图像查询，而且大多忽视了聊天对图像检索的作用。在这项工作中，我们介绍了 ChatIR: 一个基于聊天的图像检索系统，除了初始查询之外，它还与用户进行对话以获取信息，从而阐明用户的搜索意图。受到当今基础模型功能的启发，我们利用大型语言模型来生成对初始图像描述的后续问题。这些问题与用户形成一个对话框，以便从大型语料库中检索所需的图像。在这项研究中，我们探讨了这样一个系统的能力，测试了一个大型数据集，并揭示了从事对话产生显着的图像检索收益。我们首先从现有的手动生成的数据集构建评估流水线，并探索 ChatIR 的不同模块和培训策略。我们的比较包括来自受过强化学习培训的相关应用程序的强大基线。我们的系统能够从50K 图像池中检索目标图像，经过5轮对话后，成功率超过78% ，相比之下，人类提问时的成功率为75% ，单镜头文本到图像检索时的成功率为64% 。广泛的评估揭示了强大的能力，并审查了不同设置下的 CharIR 的局限性。"
    },
    {
        "title": "Exploring Regions of Interest: Visualizing Histological Image\n  Classification for Breast Cancer using Deep Learning",
        "url": "http://arxiv.org/abs/2305.20058v1",
        "pub_date": "2023-05-31",
        "summary": "Computer aided detection and diagnosis systems based on deep learning have\nshown promising performance in breast cancer detection. However, there are\ncases where the obtained results lack justification. In this study, our\nobjective is to highlight the regions of interest used by a convolutional\nneural network (CNN) for classifying histological images as benign or\nmalignant. We compare these regions with the regions identified by\npathologists. To achieve this, we employed the VGG19 architecture and tested\nthree visualization methods: Gradient, LRP Z, and LRP Epsilon. Additionally, we\nexperimented with three pixel selection methods: Bins, K-means, and MeanShift.\nBased on the results obtained, the Gradient visualization method and the\nMeanShift selection method yielded satisfactory outcomes for visualizing the\nimages.",
        "translated": "基于深度学习的计算机辅助检测与诊断系统在乳腺癌检测中表现出良好的应用前景。然而，在某些情况下，所得结果缺乏合理性。在这项研究中，我们的目标是突出卷积神经网络(CNN)用于将组织学图像分类为良性或恶性的感兴趣区域。我们将这些区域与病理学家鉴定的区域进行比较。为了实现这一点，我们采用了 VGG19架构，并测试了三种可视化方法: 梯度、 LRP Z 和 LRP Epsilon。此外，我们还试验了三种像素选择方法: Bins、 K- 均值和 mean Shift。在此基础上，采用梯度可视化方法和 MeanShift 选择方法对图像进行可视化处理，取得了满意的效果。"
    },
    {
        "title": "Cross-Domain Car Detection Model with Integrated Convolutional Block\n  Attention Mechanism",
        "url": "http://arxiv.org/abs/2305.20055v1",
        "pub_date": "2023-05-31",
        "summary": "Car detection, particularly through camera vision, has become a major focus\nin the field of computer vision and has gained widespread adoption. While\ncurrent car detection systems are capable of good detection, reliable detection\ncan still be challenging due to factors such as proximity between the car,\nlight intensity, and environmental visibility. To address these issues, we\npropose a cross-domain car detection model that we apply to car recognition for\nautonomous driving and other areas. Our model includes several novelties:\n1)Building a complete cross-domain target detection framework. 2)Developing an\nunpaired target domain picture generation module with an integrated\nconvolutional attention mechanism. 3)Adopting Generalized Intersection over\nUnion (GIOU) as the loss function of the target detection framework.\n4)Designing an object detection model integrated with two-headed Convolutional\nBlock Attention Module(CBAM). 5)Utilizing an effective data enhancement method.\nTo evaluate the model's effectiveness, we performed a reduced will resolution\nprocess on the data in the SSLAD dataset and used it as the benchmark dataset\nfor our task. Experimental results show that the performance of the\ncross-domain car target detection model improves by 40% over the model without\nour framework, and our improvements have a significant impact on cross-domain\ncar recognition.",
        "translated": "汽车检测，特别是通过摄像机视觉，已成为计算机视觉领域的一个主要焦点，并得到了广泛的采用。虽然目前的汽车检测系统能够很好的检测，可靠的检测仍然是具有挑战性的因素，如汽车之间的接近，光强度和环境能见度。为了解决这些问题，我们提出了一个跨领域的汽车检测模型，我们应用于汽车识别的自主驾驶和其他领域。该模型具有以下特点: 1)建立了一个完整的跨域目标检测框架。2)开发具有集成卷积注意机制的非配对目标域图像生成模块。3)采用广义交叉口作为目标检测框架的损失函数。4)设计一个集成了双头卷积块注意模块(CBAM)的目标检测模型。5)采用有效的数据增强方法。为了评估模型的有效性，我们对 SSLAD 数据集中的数据进行了简化的意愿分辨率处理，并将其作为我们任务的基准数据集。实验结果表明，本文提出的跨域汽车目标检测模型的性能比没有本文框架的模型提高了40% ，并且本文的改进对跨域汽车识别具有重要的影响。"
    }
]